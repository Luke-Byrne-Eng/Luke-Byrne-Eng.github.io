---
layout: paper
title:  "Attention Is All You Need"
date:   2 Aug 2023
categories: research
paper_url: https://arxiv.org/pdf/1706.03762
code_url: 
summary: "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
---


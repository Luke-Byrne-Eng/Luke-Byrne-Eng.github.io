---
layout: paper
title:  "Understanding Deep Learning Requires Re- Thinking Generalization"
date:   26 Feb 2017
categories: research
paper_url: https://arxiv.org/pdf/1611.03530
code_url: 
summary: "Extensive experiments reveal that traditional explanations for the strong generalization performance of large neural networks fall short. Despite conventional wisdom attributing success to model properties or regularization techniques, our findings show that state-of-the-art convolutional networks for image classification can easily overfit random labels. This phenomenon persists regardless of explicit regularization or substituting true images with random noise. Theoretical analysis suggests that shallow neural networks achieve perfect finite sample expressivity when parameters outnumber data points, as commonly observed. We interpret these results in contrast to traditional models."
---

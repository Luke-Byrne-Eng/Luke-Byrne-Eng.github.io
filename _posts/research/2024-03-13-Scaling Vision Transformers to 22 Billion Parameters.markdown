---
layout: paper
title:  "Scaling Vision Transformers to 22 Billion Parameters"
date:   10 Feb 2023
categories: research
paper_url: https://arxiv.org/pdf/2302.05442
code_url: 
summary: "We introduce a method for efficiently training a 22B-parameter Vision Transformer (ViT-22B) and conduct various experiments to assess its performance. Compared to existing models, ViT-22B shows improved scalability and benefits such as enhanced fairness-performance tradeoff, alignment with human visual perception, and increased robustness. Our findings suggest promising avenues for achieving large-scale language model-like capabilities in vision tasks."
---


---
layout: paper
title:  "PatchDropout: Economizing Vision Transformers Using Patch Dropout"
date:   4 Oct 2022
categories: research
paper_url: https://arxiv.org/pdf/2208.07220.pdf
code_url: 
summary: "This paper introduces PatchDropout, a simple training technique for Vision Transformers ViT that drops random image patches, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5Ã— reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
---


---
layout: paper
title:  "Training data-efficient image transformers & distillation through attention"
date:   15 Jan 2021
categories: research
paper_url: https://arxiv.org/pdf/2012.12877
code_url: 
summary: "This paper introduces DeiT, an efficient method for training vision transformers. The authors present a unique teacher-student strategy for transformers, leveraging a distillation token for efficient learning from a convolutional network teacher. This method achieves performance on par with convolutional networks, with up to 85.2% accuracy on ImageNet, and demonstrates effective transferability to other tasks."
---


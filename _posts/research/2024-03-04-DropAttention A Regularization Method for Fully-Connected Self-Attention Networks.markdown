---
layout: paper
title:  "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks"
date:   26 Jul 2019
categories: research
paper_url: https://arxiv.org/pdf/1907.11065
code_url: 
summary: "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. DropAttention uses a mask to zero out elements in the attention matrix. Experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
---


---
layout: paper
title:  "Deep Networks Always Grok and Heres Why"
date:   23 Feb 2024
categories: research
paper_url: https://arxiv.org/pdf/2402.15555.pdf
code_url: 
summary: "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
---


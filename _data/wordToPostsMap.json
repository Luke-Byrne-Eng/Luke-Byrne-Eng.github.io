{
  "convolution": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/2024-03-02-Deformable Convolutional Network.markdown",
      "date": "5 Jun 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    },
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/2024-03-03-Robust and Generalizable Visual Representation Learning via Random Convolutions.markdown",
      "date": "3 May 2021",
      "summary": "This study demonstrates that deep neural networks' vulnerability to texture shifts and perturbations can be mitigated by using random convolutions for data augmentation. Random convolutions, which approximately preserve shape while distorting local textures, create an infinite variety of new domains. We tested using these convolutions as new images or in combination with original images during training. Our method significantly enhances performance on unseen domains and domain generalization benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods. Additionally, it provides a more robust pretrained visual representation for downstream tasks."
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/2024-03-04-Progressive Random Convolutions for Single Domain Generalization.markdown",
      "date": "2 Apr 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    }
  ],
  "roi": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/2024-03-02-Deformable Convolutional Network.markdown",
      "date": "5 Jun 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    }
  ],
  "pool": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/2024-03-02-Deformable Convolutional Network.markdown",
      "date": "5 Jun 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    }
  ],
  "module": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/2024-03-02-Deformable Convolutional Network.markdown",
      "date": "5 Jun 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/2024-03-03-VMamba Visual State Space Model.markdown",
      "date": "18 Jan 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    }
  ],
  "transformation": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/2024-03-02-Deformable Convolutional Network.markdown",
      "date": "5 Jun 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    },
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/2024-03-03-A Simple Framework for Contrastive Learning of Visual Representations.markdown",
      "date": "1 Jul 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/2024-03-04-Progressive Random Convolutions for Single Domain Generalization.markdown",
      "date": "2 Apr 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/2024-03-04-Understanding Masked Image Modeling via Learning Occlusion Invariant Feature.markdown",
      "date": "8 Aug 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/2024-03-05-The geometry of hidden representations of large transformer models.markdown",
      "date": "30 Oct 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "cnn": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/2024-03-02-Deformable Convolutional Network.markdown",
      "date": "5 Jun 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    },
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/2024-03-03-Deep Networks Always Grok and Heres Why.markdown",
      "date": "23 Feb 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/2024-03-03-Efficiently Modeling Long Sequences with Structured State Spaces.markdown",
      "date": "5 Aug 2022",
      "summary": "he quest for a universal sequence model capable of handling data across different tasks and modalities, especially long-range dependencies, faces challenges with conventional models like RNNs, CNNs, and Transformers, which struggle with very long sequences. A promising approach using state space models (SSM) showed potential but had high computational demands. We introduce the Structured State Space sequence model (S4), a more efficient parameterization of SSM, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/2024-03-03-PatchDropout Economizing Vision Transformers Using Patch Dropou.markdown",
      "date": "4 Oct 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/2024-03-03-VMamba Visual State Space Model.markdown",
      "date": "18 Jan 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "DropKey",
      "url": "/2024-03-04-DropKey.markdown",
      "date": "11 Apr 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/2024-03-04-Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution.markdown",
      "date": "25 Oct 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/2024-03-05-An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.markdown",
      "date": "3 Jun 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/2024-03-05-Architecture-Agnostic Masked Image Modeling – From ViT back to CNN.markdown",
      "date": "2 Jun 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/2024-03-05-Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions.markdown",
      "date": "20 Apr 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    }
  ],
  "augmentation": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/2024-03-02-Deformable Convolutional Network.markdown",
      "date": "5 Jun 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    },
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/2024-03-03-A Simple Framework for Contrastive Learning of Visual Representations.markdown",
      "date": "1 Jul 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/2024-03-03-Deep neural networks are robust to weight binarization and other non-linear distortions.markdown",
      "date": "7 Jun 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/2024-03-03-Extreme Masking for Learning Instance and Distributed Visual Representations.markdown",
      "date": "8 Mar 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/2024-03-03-I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture.markdown",
      "date": "13 Apr 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/2024-03-03-Robust and Generalizable Visual Representation Learning via Random Convolutions.markdown",
      "date": "3 May 2021",
      "summary": "This study demonstrates that deep neural networks' vulnerability to texture shifts and perturbations can be mitigated by using random convolutions for data augmentation. Random convolutions, which approximately preserve shape while distorting local textures, create an infinite variety of new domains. We tested using these convolutions as new images or in combination with original images during training. Our method significantly enhances performance on unseen domains and domain generalization benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods. Additionally, it provides a more robust pretrained visual representation for downstream tasks."
    },
    {
      "title": "DROPOUT AS DATA AUGMENTATION",
      "url": "/2024-03-04-DROPOUT AS DATA AUGMENTATION.markdown",
      "date": "8 Jan 2016",
      "summary": "Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/2024-03-04-Impact of Noise on Calibration and Generalisation of Neural Networks.markdown",
      "date": "30 Jun 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. We examine different types of noise, such as activation and input augmentation noise, in both in-distribution and out-of-distribution scenarios. Activation noise significantly improves generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/2024-03-04-ON INTERACTION BETWEEN AUGMENTATIONS AND CORRUPTIONS IN NATURAL CORRUPTION ROBUSTNESS.markdown",
      "date": "19 Nov 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/2024-03-04-Progressive Random Convolutions for Single Domain Generalization.markdown",
      "date": "2 Apr 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/2024-03-04-RandAugment Practical automated data augmentation with a reduced search space.markdown",
      "date": "14 Nov 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/2024-03-05-Deeper Insights into the Robustness of ViTs towards Common Corruptions.markdown",
      "date": "19 Aug 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/2024-03-05-Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions.markdown",
      "date": "20 Apr 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/2024-03-05-Pre-training Vision Transformers with Very Limited Synthesized Images.markdown",
      "date": "31 Jul 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    }
  ],
  "sampling": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/2024-03-02-Deformable Convolutional Network.markdown",
      "date": "5 Jun 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    }
  ],
  "offset": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/2024-03-02-Deformable Convolutional Network.markdown",
      "date": "5 Jun 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/2024-03-04-Progressive Random Convolutions for Single Domain Generalization.markdown",
      "date": "2 Apr 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    }
  ],
  "supervision": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/2024-03-02-Deformable Convolutional Network.markdown",
      "date": "5 Jun 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    },
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/2024-03-03-A Simple Framework for Contrastive Learning of Visual Representations.markdown",
      "date": "1 Jul 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/2024-03-03-A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REP- RESENTATIONS.markdown",
      "date": "30 Oct 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    },
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/2024-03-03-Bootstrap Your Own Latent A New Approach to Self-Supervised Learning.markdown",
      "date": "10 Sep 2020",
      "summary": "We introduce BYOL, a novel self-supervised learning method for image representation without using negative pairs. Utilizing two networks, BYOL trains one to predict the representation of an image as seen by the other under a different view, updating the latter with the former's slow-moving average. Surpassing state-of-the-art methods, it achieves up to 79.6% accuracy on ImageNet with ResNet models, performing equally or better on transfer and semi-supervised tasks. Code and models are available on GitHub."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/2024-03-03-DINOv2 Learning Robust Visual Features without Supervision.markdown",
      "date": "2 Feb 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/2024-03-03-I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture.markdown",
      "date": "13 Apr 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/2024-03-04-Delving Deep into Label Smoothing.markdown",
      "date": "22 Jul 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/2024-03-04-On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning.markdown",
      "date": "26 Jun 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/2024-03-04-RandAugment Practical automated data augmentation with a reduced search space.markdown",
      "date": "14 Nov 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/2024-03-04-Understanding Masked Image Modeling via Learning Occlusion Invariant Feature.markdown",
      "date": "8 Aug 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/2024-03-05-A Closer Look at Self-Supervised Lightweight Vision Transformers.markdown",
      "date": "3 May 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/2024-03-05-Adversarially Self-supervised Pre-training Improves Accuracy and Robustness.markdown",
      "date": "10 Mar 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/2024-03-05-An Empirical Study of Training Self-Supervised Vision Transformers.markdown",
      "date": "16 Aug 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/2024-03-05-Architecture-Agnostic Masked Image Modeling – From ViT back to CNN.markdown",
      "date": "2 Jun 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/2024-03-05-Are Labels Necessary for Neural Architecture Search.markdown",
      "date": "3 Aug 2020",
      "summary": "This paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning if effective neural architectures can be discovered using only images without human-annotated labels. Through two experimental setups—sample-based and search-based—we investigate this possibility. In the sample-based approach, we evaluate 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. In the search-based experiments, we employ a recognized NAS algorithm, DARTS, with various unsupervised objectives, finding that architectures identified without labels perform competitively compared to those found with labels. These findings suggest that human-annotated labels may not be necessary for identifying efficient neural architectures, as image statistics alone could be sufficient."
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/2024-03-05-Beyond Masking Demystifying Token-Based Pre-Training for Vision Transformers.markdown",
      "date": "27 Mar 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/2024-03-05-Colorization as a Proxy Task for Visual Understanding.markdown",
      "date": "13 Aug 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learner",
      "url": "/2024-03-05-Masked Autoencoders Are Scalable Vision Learner.markdown",
      "date": "19 Dec 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/2024-03-05-Masked Image Residual Learning for Scaling Deeper Vision Transformers.markdown",
      "date": "15 Nov 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/2024-03-05-Pre-training Vision Transformers with Very Limited Synthesized Images.markdown",
      "date": "31 Jul 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Segment Anything",
      "url": "/2024-03-05-Segment Anything.markdown",
      "date": "5 Apr 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/2024-03-05-The geometry of hidden representations of large transformer models.markdown",
      "date": "30 Oct 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/2024-03-05-Understanding Self-Supervised Pretraining with Part-Aware Representation Learning.markdown",
      "date": "23 Jan 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/2024-03-05-Vision Transformers Need Register.markdown",
      "date": "28 Sep 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "replace": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/2024-03-02-Deformable Convolutional Network.markdown",
      "date": "5 Jun 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/2024-03-05-An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.markdown",
      "date": "3 Jun 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    }
  ],
  "vision": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/2024-03-02-Deformable Convolutional Network.markdown",
      "date": "5 Jun 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    },
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/2024-03-03-Contrastive Masked Autoencoders are Stronger Vision Learners.markdown",
      "date": "29 Jan 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/2024-03-03-DINOv2 Learning Robust Visual Features without Supervision.markdown",
      "date": "2 Feb 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "Fast Feedforward Networks",
      "url": "/2024-03-03-Fast Feedforward Networks.markdown",
      "date": "18 Sep 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/2024-03-03-I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture.markdown",
      "date": "13 Apr 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/2024-03-03-PatchDropout Economizing Vision Transformers Using Patch Dropou.markdown",
      "date": "4 Oct 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/2024-03-03-Vision Mamba Efficient Visual Representation Learning with Bidirectional State Space Model.markdown",
      "date": "10 Feb 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/2024-03-03-VMamba Visual State Space Model.markdown",
      "date": "18 Jan 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "DropKey",
      "url": "/2024-03-04-DropKey.markdown",
      "date": "11 Apr 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/2024-03-04-Dropout A Simple Way to Prevent Neural Networks from Overfitting.markdown",
      "date": "1 Jan 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    },
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/2024-03-04-Dropout Reduces Underfittin.markdown",
      "date": "31 May 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/2024-03-04-ON INTERACTION BETWEEN AUGMENTATIONS AND CORRUPTIONS IN NATURAL CORRUPTION ROBUSTNESS.markdown",
      "date": "19 Nov 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/2024-03-04-Understanding Masked Image Modeling via Learning Occlusion Invariant Feature.markdown",
      "date": "8 Aug 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/2024-03-04-What do Self-supervised vision transformers learn.markdown",
      "date": "1 May 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/2024-03-05-A Closer Look at Self-Supervised Lightweight Vision Transformers.markdown",
      "date": "3 May 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/2024-03-05-An Empirical Study of Training Self-Supervised Vision Transformers.markdown",
      "date": "16 Aug 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/2024-03-05-An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.markdown",
      "date": "3 Jun 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/2024-03-05-Architecture-Agnostic Masked Image Modeling – From ViT back to CNN.markdown",
      "date": "2 Jun 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/2024-03-05-Beyond Masking Demystifying Token-Based Pre-Training for Vision Transformers.markdown",
      "date": "27 Mar 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/2024-03-05-Deeper Insights into the Robustness of ViTs towards Common Corruptions.markdown",
      "date": "19 Aug 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/2024-03-05-Localizing Objects with Self-Supervised Transformers and no Labels.markdown",
      "date": "29 Sept 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learner",
      "url": "/2024-03-05-Masked Autoencoders Are Scalable Vision Learner.markdown",
      "date": "19 Dec 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/2024-03-05-Masked Image Residual Learning for Scaling Deeper Vision Transformers.markdown",
      "date": "15 Nov 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/2024-03-05-Pre-training Vision Transformers with Very Limited Synthesized Images.markdown",
      "date": "31 Jul 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/2024-03-05-Scaling Language-Image Pre-training via Masking.markdown",
      "date": "30 Mar 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    },
    {
      "title": "Segment Anything",
      "url": "/2024-03-05-Segment Anything.markdown",
      "date": "5 Apr 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/2024-03-05-Transformer-Based Visual Segmentation A Surve.markdown",
      "date": "20 Dec 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/2024-03-05-Vision Transformers Need Register.markdown",
      "date": "28 Sep 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "object": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/2024-03-02-Deformable Convolutional Network.markdown",
      "date": "5 Jun 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    },
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/2024-03-03-Contrastive Masked Autoencoders are Stronger Vision Learners.markdown",
      "date": "29 Jan 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "DropKey",
      "url": "/2024-03-04-DropKey.markdown",
      "date": "11 Apr 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Group Normalization",
      "url": "/2024-03-04-Group Normalization.markdown",
      "date": "1 Jun 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/2024-03-04-RandAugment Practical automated data augmentation with a reduced search space.markdown",
      "date": "14 Nov 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/2024-03-05-Localizing Objects with Self-Supervised Transformers and no Labels.markdown",
      "date": "29 Sept 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/2024-03-05-Masked Image Residual Learning for Scaling Deeper Vision Transformers.markdown",
      "date": "15 Nov 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/2024-03-05-Understanding Self-Supervised Pretraining with Part-Aware Representation Learning.markdown",
      "date": "23 Jan 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/2024-03-05-Vision Transformers Need Register.markdown",
      "date": "28 Sep 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    }
  ],
  "detection": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/2024-03-02-Deformable Convolutional Network.markdown",
      "date": "5 Jun 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    },
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/2024-03-03-Contrastive Masked Autoencoders are Stronger Vision Learners.markdown",
      "date": "29 Jan 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "DropKey",
      "url": "/2024-03-04-DropKey.markdown",
      "date": "11 Apr 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Group Normalization",
      "url": "/2024-03-04-Group Normalization.markdown",
      "date": "1 Jun 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/2024-03-04-RandAugment Practical automated data augmentation with a reduced search space.markdown",
      "date": "14 Nov 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/2024-03-04-Reading Digits in Natural Images With Unsupervised Feature Learning.markdown",
      "date": "1 Jan 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/2024-03-05-Masked Image Residual Learning for Scaling Deeper Vision Transformers.markdown",
      "date": "15 Nov 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "segmentation": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/2024-03-02-Deformable Convolutional Network.markdown",
      "date": "5 Jun 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    },
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/2024-03-03-Contrastive Masked Autoencoders are Stronger Vision Learners.markdown",
      "date": "29 Jan 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "Group Normalization",
      "url": "/2024-03-04-Group Normalization.markdown",
      "date": "1 Jun 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/2024-03-05-Colorization as a Proxy Task for Visual Understanding.markdown",
      "date": "13 Aug 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/2024-03-05-Masked Image Residual Learning for Scaling Deeper Vision Transformers.markdown",
      "date": "15 Nov 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Segment Anything",
      "url": "/2024-03-05-Segment Anything.markdown",
      "date": "5 Apr 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/2024-03-05-Transformer-Based Visual Segmentation A Surve.markdown",
      "date": "20 Dec 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "httpsgithubcommsracverdeformableconvnets": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/2024-03-02-Deformable Convolutional Network.markdown",
      "date": "5 Jun 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    }
  ],
  "transformer": [
    {
      "title": "Do Transformer Modifications Transfer Across Implementations and Applications?",
      "url": "/2024-03-02-Do_Transformer_Modifications_Transfer_Across_Implementations_and_Applications.markdown",
      "date": "10 Sep 2021",
      "summary": "This paper evaluates numerous Transformer architecture modifications in a unified experimental framework, focusing on common natural language processing applications. Surprisingly, it finds that most modifications do not significantly enhance performance. The beneficial variants are mostly minor or developed in the same code base used for testing. The study suggests that performance gains may largely depend on implementation details and offers recommendations for improving the generalizability of experimental results."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/2024-03-03-Efficiently Modeling Long Sequences with Structured State Spaces.markdown",
      "date": "5 Aug 2022",
      "summary": "he quest for a universal sequence model capable of handling data across different tasks and modalities, especially long-range dependencies, faces challenges with conventional models like RNNs, CNNs, and Transformers, which struggle with very long sequences. A promising approach using state space models (SSM) showed potential but had high computational demands. We introduce the Structured State Space sequence model (S4), a more efficient parameterization of SSM, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    },
    {
      "title": "Fast Feedforward Networks",
      "url": "/2024-03-03-Fast Feedforward Networks.markdown",
      "date": "18 Sep 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    },
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/2024-03-03-Hungry Hungry Hippos Towards Language Modeling with State Space Models.markdown",
      "date": "29 Apr 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/2024-03-03-I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture.markdown",
      "date": "13 Apr 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/2024-03-03-Mamba Linear-Time Sequence Modeling with Selective State Spaces.markdown",
      "date": "1 Dec 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/2024-03-03-PatchDropout Economizing Vision Transformers Using Patch Dropou.markdown",
      "date": "4 Oct 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/2024-03-03-Vision Mamba Efficient Visual Representation Learning with Bidirectional State Space Model.markdown",
      "date": "10 Feb 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/2024-03-03-VMamba Visual State Space Model.markdown",
      "date": "18 Jan 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/2024-03-04-DropAttention A Regularization Method for Fully-Connected Self-Attention Networks.markdown",
      "date": "26 Jul 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. Unlike existing dropout techniques for other neural network layers, DropAttention specifically targets the unique challenges of self-attention mechanisms. Our experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    },
    {
      "title": "DropKey",
      "url": "/2024-03-04-DropKey.markdown",
      "date": "11 Apr 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/2024-03-04-Scheduled DropHead A Regularization Method for Transformer Models.markdown",
      "date": "1 Nov 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/2024-03-04-Understanding Masked Image Modeling via Learning Occlusion Invariant Feature.markdown",
      "date": "8 Aug 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/2024-03-04-What do Self-supervised vision transformers learn.markdown",
      "date": "1 May 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/2024-03-04-White-Box Transformers via Sparse Rate Reduction.markdown",
      "date": "1 Jun 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/2024-03-05-A Closer Look at Self-Supervised Lightweight Vision Transformers.markdown",
      "date": "3 May 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/2024-03-05-An Empirical Study of Training Self-Supervised Vision Transformers.markdown",
      "date": "16 Aug 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/2024-03-05-An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.markdown",
      "date": "3 Jun 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/2024-03-05-Architecture-Agnostic Masked Image Modeling – From ViT back to CNN.markdown",
      "date": "2 Jun 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/2024-03-05-Beyond Masking Demystifying Token-Based Pre-Training for Vision Transformers.markdown",
      "date": "27 Mar 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/2024-03-05-Deeper Insights into the Robustness of ViTs towards Common Corruptions.markdown",
      "date": "19 Aug 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/2024-03-05-Localizing Objects with Self-Supervised Transformers and no Labels.markdown",
      "date": "29 Sept 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/2024-03-05-Masked Image Residual Learning for Scaling Deeper Vision Transformers.markdown",
      "date": "15 Nov 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/2024-03-05-Pre-training Vision Transformers with Very Limited Synthesized Images.markdown",
      "date": "31 Jul 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/2024-03-05-The geometry of hidden representations of large transformer models.markdown",
      "date": "30 Oct 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/2024-03-05-Transformer-Based Visual Segmentation A Surve.markdown",
      "date": "20 Dec 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    },
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/2024-03-05-Turbo Training with Token Dropout.markdown",
      "date": "10 Oct 2022",
      "summary": "This paper introduces Turbo training, an efficient training method for video-related tasks using Transformers. Turbo training offers three major contributions: Firstly, it presents a simple yet versatile training paradigm applicable to multiple video tasks. Secondly, it demonstrates Turbo training's effectiveness across action classification, video-language representation learning, and long-video activity classification, achieving competitive performance with up to 4× speed-up and reduced memory usage. Thirdly, it enables long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous methods that were resource-intensive."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/2024-03-05-Vision Transformers Need Register.markdown",
      "date": "28 Sep 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "transfer": [
    {
      "title": "Do Transformer Modifications Transfer Across Implementations and Applications?",
      "url": "/2024-03-02-Do_Transformer_Modifications_Transfer_Across_Implementations_and_Applications.markdown",
      "date": "10 Sep 2021",
      "summary": "This paper evaluates numerous Transformer architecture modifications in a unified experimental framework, focusing on common natural language processing applications. Surprisingly, it finds that most modifications do not significantly enhance performance. The beneficial variants are mostly minor or developed in the same code base used for testing. The study suggests that performance gains may largely depend on implementation details and offers recommendations for improving the generalizability of experimental results."
    },
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/2024-03-03-A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REP- RESENTATIONS.markdown",
      "date": "30 Oct 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    },
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/2024-03-03-Bootstrap Your Own Latent A New Approach to Self-Supervised Learning.markdown",
      "date": "10 Sep 2020",
      "summary": "We introduce BYOL, a novel self-supervised learning method for image representation without using negative pairs. Utilizing two networks, BYOL trains one to predict the representation of an image as seen by the other under a different view, updating the latter with the former's slow-moving average. Surpassing state-of-the-art methods, it achieves up to 79.6% accuracy on ImageNet with ResNet models, performing equally or better on transfer and semi-supervised tasks. Code and models are available on GitHub."
    },
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/2024-03-03-Extreme Masking for Learning Instance and Distributed Visual Representations.markdown",
      "date": "8 Mar 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/2024-03-05-Adversarially Self-supervised Pre-training Improves Accuracy and Robustness.markdown",
      "date": "10 Mar 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learner",
      "url": "/2024-03-05-Masked Autoencoders Are Scalable Vision Learner.markdown",
      "date": "19 Dec 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Segment Anything",
      "url": "/2024-03-05-Segment Anything.markdown",
      "date": "5 Apr 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "language": [
    {
      "title": "Do Transformer Modifications Transfer Across Implementations and Applications?",
      "url": "/2024-03-02-Do_Transformer_Modifications_Transfer_Across_Implementations_and_Applications.markdown",
      "date": "10 Sep 2021",
      "summary": "This paper evaluates numerous Transformer architecture modifications in a unified experimental framework, focusing on common natural language processing applications. Surprisingly, it finds that most modifications do not significantly enhance performance. The beneficial variants are mostly minor or developed in the same code base used for testing. The study suggests that performance gains may largely depend on implementation details and offers recommendations for improving the generalizability of experimental results."
    },
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/2024-03-03-DINOv2 Learning Robust Visual Features without Supervision.markdown",
      "date": "2 Feb 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/2024-03-03-Hungry Hungry Hippos Towards Language Modeling with State Space Models.markdown",
      "date": "29 Apr 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/2024-03-03-Mamba Linear-Time Sequence Modeling with Selective State Spaces.markdown",
      "date": "1 Dec 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    },
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/2024-03-03-MambaByte Token-free Selective State Space Model.markdown",
      "date": "24 Jan 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    },
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/2024-03-04-Sigmoid Loss for Language Image Pre-Training.markdown",
      "date": "27 sept 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/2024-03-05-An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.markdown",
      "date": "3 Jun 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/2024-03-05-Scaling Language-Image Pre-training via Masking.markdown",
      "date": "30 Mar 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    },
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/2024-03-05-Turbo Training with Token Dropout.markdown",
      "date": "10 Oct 2022",
      "summary": "This paper introduces Turbo training, an efficient training method for video-related tasks using Transformers. Turbo training offers three major contributions: Firstly, it presents a simple yet versatile training paradigm applicable to multiple video tasks. Secondly, it demonstrates Turbo training's effectiveness across action classification, video-language representation learning, and long-video activity classification, achieving competitive performance with up to 4× speed-up and reduced memory usage. Thirdly, it enables long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous methods that were resource-intensive."
    }
  ],
  "depend": [
    {
      "title": "Do Transformer Modifications Transfer Across Implementations and Applications?",
      "url": "/2024-03-02-Do_Transformer_Modifications_Transfer_Across_Implementations_and_Applications.markdown",
      "date": "10 Sep 2021",
      "summary": "This paper evaluates numerous Transformer architecture modifications in a unified experimental framework, focusing on common natural language processing applications. Surprisingly, it finds that most modifications do not significantly enhance performance. The beneficial variants are mostly minor or developed in the same code base used for testing. The study suggests that performance gains may largely depend on implementation details and offers recommendations for improving the generalizability of experimental results."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/2024-03-06-Hard Patches Mining for Masked Image Modeling.markdown",
      "date": "12 Apr 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "generalizability": [
    {
      "title": "Do Transformer Modifications Transfer Across Implementations and Applications?",
      "url": "/2024-03-02-Do_Transformer_Modifications_Transfer_Across_Implementations_and_Applications.markdown",
      "date": "10 Sep 2021",
      "summary": "This paper evaluates numerous Transformer architecture modifications in a unified experimental framework, focusing on common natural language processing applications. Surprisingly, it finds that most modifications do not significantly enhance performance. The beneficial variants are mostly minor or developed in the same code base used for testing. The study suggests that performance gains may largely depend on implementation details and offers recommendations for improving the generalizability of experimental results."
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/2024-03-05-Designing Network Design Spaces.markdown",
      "date": "30 Mar 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    }
  ],
  "simclr": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/2024-03-03-A Simple Framework for Contrastive Learning of Visual Representations.markdown",
      "date": "1 Jul 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/2024-03-04-On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning.markdown",
      "date": "26 Jun 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    }
  ],
  "specialize": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/2024-03-03-A Simple Framework for Contrastive Learning of Visual Representations.markdown",
      "date": "1 Jul 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    }
  ],
  "memory": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/2024-03-03-A Simple Framework for Contrastive Learning of Visual Representations.markdown",
      "date": "1 Jul 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/2024-03-03-ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.markdown",
      "date": "30 Jan 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/2024-03-03-PatchDropout Economizing Vision Transformers Using Patch Dropou.markdown",
      "date": "4 Oct 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/2024-03-03-Vision Mamba Efficient Visual Representation Learning with Bidirectional State Space Model.markdown",
      "date": "10 Feb 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "Group Normalization",
      "url": "/2024-03-04-Group Normalization.markdown",
      "date": "1 Jun 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/2024-03-05-Scaling Language-Image Pre-training via Masking.markdown",
      "date": "30 Mar 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/2024-03-05-Turbo Training with Token Dropout.markdown",
      "date": "10 Oct 2022",
      "summary": "This paper introduces Turbo training, an efficient training method for video-related tasks using Transformers. Turbo training offers three major contributions: Firstly, it presents a simple yet versatile training paradigm applicable to multiple video tasks. Secondly, it demonstrates Turbo training's effectiveness across action classification, video-language representation learning, and long-video activity classification, achieving competitive performance with up to 4× speed-up and reduced memory usage. Thirdly, it enables long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous methods that were resource-intensive."
    }
  ],
  "bank": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/2024-03-03-A Simple Framework for Contrastive Learning of Visual Representations.markdown",
      "date": "1 Jul 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    }
  ],
  "component": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/2024-03-03-A Simple Framework for Contrastive Learning of Visual Representations.markdown",
      "date": "1 Jul 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/2024-03-05-An Empirical Study of Training Self-Supervised Vision Transformers.markdown",
      "date": "16 Aug 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "composition": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/2024-03-03-A Simple Framework for Contrastive Learning of Visual Representations.markdown",
      "date": "1 Jul 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/2024-03-05-Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions.markdown",
      "date": "20 Apr 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    }
  ],
  "nonlinear": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/2024-03-03-A Simple Framework for Contrastive Learning of Visual Representations.markdown",
      "date": "1 Jul 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    }
  ],
  "loss": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/2024-03-03-A Simple Framework for Contrastive Learning of Visual Representations.markdown",
      "date": "1 Jul 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION",
      "url": "/2024-03-03-AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION.markdown",
      "date": "23 Oct 2020",
      "summary": "Label smoothing is hypothesized to mitigate overfitting and enhance generalization, with supporting empirical evidence. However, its mathematical underpinnings remain unclear. This paper introduces a theoretical framework explaining label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. We identify an optimal label smoothing value that minimizes generalization loss and validate our theory with comprehensive experiments. Our results aim to clarify label smoothing's utility for both theoreticians and practitioners in real-world applications"
    },
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/2024-03-03-DECOUPLED WEIGHT DECAY REGULARIZATION.markdown",
      "date": "4 Jan 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/2024-03-03-Loss of Plasticity in Deep Continual Learning.markdown",
      "date": "18 Aug 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    },
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/2024-03-04-Data is Overrated Perceptual Metrics Can Lead Learning in the Absence of Training Data.markdown",
      "date": "6 Dec 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    },
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/2024-03-04-On Warm-Starting Neural Network Training.markdown",
      "date": "31 Dec 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/2024-03-04-Sigmoid Loss for Language Image Pre-Training.markdown",
      "date": "27 sept 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/2024-03-05-Colorization as a Proxy Task for Visual Understanding.markdown",
      "date": "13 Aug 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/2024-03-06-Hard Patches Mining for Masked Image Modeling.markdown",
      "date": "12 Apr 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "batch": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/2024-03-03-A Simple Framework for Contrastive Learning of Visual Representations.markdown",
      "date": "1 Jul 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/2024-03-03-How Does Batch Normalization Help Optimization.markdown",
      "date": "15 Apr 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/2024-03-03-Vision Mamba Efficient Visual Representation Learning with Bidirectional State Space Model.markdown",
      "date": "10 Feb 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/2024-03-04-Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.markdown",
      "date": "2 Mar 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/2024-03-04-Dropout Reduces Underfittin.markdown",
      "date": "31 May 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "Group Normalization",
      "url": "/2024-03-04-Group Normalization.markdown",
      "date": "1 Jun 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    },
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/2024-03-04-Instance Normalization The Missing Ingredient for Fast Stylization.markdown",
      "date": "6 Nov 2017",
      "summary": "In this study, we improve upon the fast stylization method by Ulyanov et al. (2016) through a minor yet impactful modification: substituting batch normalization with instance normalization during both training and testing. This adjustment significantly enhances the quality of generated images and enables the training of efficient real-time image generation architectures. The code and full paper are available online."
    },
    {
      "title": "Layer Normalization",
      "url": "/2024-03-04-Layer Normalization.markdown",
      "date": "21 Jul 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    },
    {
      "title": "Rethinking Batch in BatchNorm",
      "url": "/2024-03-04-Rethinking Batch in BatchNorm.markdown",
      "date": "17 May 2021",
      "summary": "BatchNorm, essential in convolutional neural networks, behaves uniquely due to its batch-based operation, leading to performance issues in visual recognition tasks. This paper identifies these issues and suggests reevaluating the concept of batch in BatchNorm for better performance, aiming to guide researchers in its effective use."
    },
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/2024-03-04-Sigmoid Loss for Language Image Pre-Training.markdown",
      "date": "27 sept 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    }
  ],
  "semi": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/2024-03-03-A Simple Framework for Contrastive Learning of Visual Representations.markdown",
      "date": "1 Jul 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/2024-03-03-Bootstrap Your Own Latent A New Approach to Self-Supervised Learning.markdown",
      "date": "10 Sep 2020",
      "summary": "We introduce BYOL, a novel self-supervised learning method for image representation without using negative pairs. Utilizing two networks, BYOL trains one to predict the representation of an image as seen by the other under a different view, updating the latter with the former's slow-moving average. Surpassing state-of-the-art methods, it achieves up to 79.6% accuracy on ImageNet with ResNet models, performing equally or better on transfer and semi-supervised tasks. Code and models are available on GitHub."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/2024-03-04-RandAugment Practical automated data augmentation with a reduced search space.markdown",
      "date": "14 Nov 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    }
  ],
  "imagenet": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/2024-03-03-A Simple Framework for Contrastive Learning of Visual Representations.markdown",
      "date": "1 Jul 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/2024-03-03-Bootstrap Your Own Latent A New Approach to Self-Supervised Learning.markdown",
      "date": "10 Sep 2020",
      "summary": "We introduce BYOL, a novel self-supervised learning method for image representation without using negative pairs. Utilizing two networks, BYOL trains one to predict the representation of an image as seen by the other under a different view, updating the latter with the former's slow-moving average. Surpassing state-of-the-art methods, it achieves up to 79.6% accuracy on ImageNet with ResNet models, performing equally or better on transfer and semi-supervised tasks. Code and models are available on GitHub."
    },
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/2024-03-03-Contrastive Masked Autoencoders are Stronger Vision Learners.markdown",
      "date": "29 Jan 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/2024-03-03-Deep neural networks are robust to weight binarization and other non-linear distortions.markdown",
      "date": "7 Jun 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/2024-03-03-I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture.markdown",
      "date": "13 Apr 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/2024-03-03-Loss of Plasticity in Deep Continual Learning.markdown",
      "date": "18 Aug 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/2024-03-03-PatchDropout Economizing Vision Transformers Using Patch Dropou.markdown",
      "date": "4 Oct 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/2024-03-03-Vision Mamba Efficient Visual Representation Learning with Bidirectional State Space Model.markdown",
      "date": "10 Feb 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/2024-03-04-A downsampled variant of imagenet as an alternative to the cifar datasets.markdown",
      "date": "23 Aug 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/2024-03-04-Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.markdown",
      "date": "2 Mar 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/2024-03-04-Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations.markdown",
      "date": "27 Apr 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/2024-03-04-Delving Deep into Label Smoothing.markdown",
      "date": "22 Jul 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/2024-03-04-Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classificatio.markdown",
      "date": "6 Feb 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    },
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/2024-03-04-Dropout Reduces Underfittin.markdown",
      "date": "31 May 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/2024-03-04-ON INTERACTION BETWEEN AUGMENTATIONS AND CORRUPTIONS IN NATURAL CORRUPTION ROBUSTNESS.markdown",
      "date": "19 Nov 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/2024-03-04-RandAugment Practical automated data augmentation with a reduced search space.markdown",
      "date": "14 Nov 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/2024-03-04-Sigmoid Loss for Language Image Pre-Training.markdown",
      "date": "27 sept 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    },
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/2024-03-04-White-Box Transformers via Sparse Rate Reduction.markdown",
      "date": "1 Jun 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/2024-03-05-An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.markdown",
      "date": "3 Jun 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/2024-03-05-Colorization as a Proxy Task for Visual Understanding.markdown",
      "date": "13 Aug 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/2024-03-05-Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions.markdown",
      "date": "20 Apr 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learner",
      "url": "/2024-03-05-Masked Autoencoders Are Scalable Vision Learner.markdown",
      "date": "19 Dec 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/2024-03-05-Masked Image Residual Learning for Scaling Deeper Vision Transformers.markdown",
      "date": "15 Nov 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/2024-03-05-Pre-training Vision Transformers with Very Limited Synthesized Images.markdown",
      "date": "31 Jul 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/2024-03-06-(Certified!!) Adversarial Robustness for Free.markdown",
      "date": "6 Mar 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/2024-03-06-Denoising Masked Autoencoders Help Robust Classification.markdown",
      "date": "7 Mar 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    }
  ],
  "linear": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/2024-03-03-A Simple Framework for Contrastive Learning of Visual Representations.markdown",
      "date": "1 Jul 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/2024-03-03-Deep Networks Always Grok and Heres Why.markdown",
      "date": "23 Feb 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/2024-03-03-Deep neural networks are robust to weight binarization and other non-linear distortions.markdown",
      "date": "7 Jun 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/2024-03-03-Extreme Masking for Learning Instance and Distributed Visual Representations.markdown",
      "date": "8 Mar 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    },
    {
      "title": "Fast Feedforward Networks",
      "url": "/2024-03-03-Fast Feedforward Networks.markdown",
      "date": "18 Sep 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    },
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/2024-03-03-Loss of Plasticity in Deep Continual Learning.markdown",
      "date": "18 Aug 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/2024-03-03-Mamba Linear-Time Sequence Modeling with Selective State Spaces.markdown",
      "date": "1 Dec 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    },
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/2024-03-03-MambaByte Token-free Selective State Space Model.markdown",
      "date": "24 Jan 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/2024-03-03-VMamba Visual State Space Model.markdown",
      "date": "18 Jan 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/2024-03-04-Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classificatio.markdown",
      "date": "6 Feb 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/2024-03-04-Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution.markdown",
      "date": "25 Oct 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/2024-03-04-Understanding the difficulty of training deep feedforward neural networks.markdown",
      "date": "31 Mar 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/2024-03-04-What do Self-supervised vision transformers learn.markdown",
      "date": "1 May 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/2024-03-05-Designing Network Design Spaces.markdown",
      "date": "30 Mar 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    }
  ],
  "classifier": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/2024-03-03-A Simple Framework for Contrastive Learning of Visual Representations.markdown",
      "date": "1 Jul 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/2024-03-04-Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations.markdown",
      "date": "27 Apr 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/2024-03-06-(Certified!!) Adversarial Robustness for Free.markdown",
      "date": "6 Mar 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/2024-03-06-Denoising Masked Autoencoders Help Robust Classification.markdown",
      "date": "7 Mar 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    }
  ],
  "best": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/2024-03-03-A Simple Framework for Contrastive Learning of Visual Representations.markdown",
      "date": "1 Jul 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/2024-03-05-Adversarially Self-supervised Pre-training Improves Accuracy and Robustness.markdown",
      "date": "10 Mar 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    }
  ],
  "resnet": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/2024-03-03-A Simple Framework for Contrastive Learning of Visual Representations.markdown",
      "date": "1 Jul 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/2024-03-03-Bootstrap Your Own Latent A New Approach to Self-Supervised Learning.markdown",
      "date": "10 Sep 2020",
      "summary": "We introduce BYOL, a novel self-supervised learning method for image representation without using negative pairs. Utilizing two networks, BYOL trains one to predict the representation of an image as seen by the other under a different view, updating the latter with the former's slow-moving average. Surpassing state-of-the-art methods, it achieves up to 79.6% accuracy on ImageNet with ResNet models, performing equally or better on transfer and semi-supervised tasks. Code and models are available on GitHub."
    },
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/2024-03-03-Deep Networks Always Grok and Heres Why.markdown",
      "date": "23 Feb 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/2024-03-04-Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations.markdown",
      "date": "27 Apr 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    }
  ],
  "label": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/2024-03-03-A Simple Framework for Contrastive Learning of Visual Representations.markdown",
      "date": "1 Jul 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION",
      "url": "/2024-03-03-AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION.markdown",
      "date": "23 Oct 2020",
      "summary": "Label smoothing is hypothesized to mitigate overfitting and enhance generalization, with supporting empirical evidence. However, its mathematical underpinnings remain unclear. This paper introduces a theoretical framework explaining label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. We identify an optimal label smoothing value that minimizes generalization loss and validate our theory with comprehensive experiments. Our results aim to clarify label smoothing's utility for both theoreticians and practitioners in real-world applications"
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/2024-03-04-Delving Deep into Label Smoothing.markdown",
      "date": "22 Jul 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/2024-03-04-Reading Digits in Natural Images With Unsupervised Feature Learning.markdown",
      "date": "1 Jan 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/2024-03-05-Are Labels Necessary for Neural Architecture Search.markdown",
      "date": "3 Aug 2020",
      "summary": "This paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning if effective neural architectures can be discovered using only images without human-annotated labels. Through two experimental setups—sample-based and search-based—we investigate this possibility. In the sample-based approach, we evaluate 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. In the search-based experiments, we employ a recognized NAS algorithm, DARTS, with various unsupervised objectives, finding that architectures identified without labels perform competitively compared to those found with labels. These findings suggest that human-annotated labels may not be necessary for identifying efficient neural architectures, as image statistics alone could be sufficient."
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/2024-03-05-Colorization as a Proxy Task for Visual Understanding.markdown",
      "date": "13 Aug 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/2024-03-05-Localizing Objects with Self-Supervised Transformers and no Labels.markdown",
      "date": "29 Sept 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "outdo": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/2024-03-03-A Simple Framework for Contrastive Learning of Visual Representations.markdown",
      "date": "1 Jul 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    }
  ],
  "alexnet": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/2024-03-03-A Simple Framework for Contrastive Learning of Visual Representations.markdown",
      "date": "1 Jul 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/2024-03-04-Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations.markdown",
      "date": "27 Apr 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    }
  ],
  "time": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/2024-03-03-A Simple Framework for Contrastive Learning of Visual Representations.markdown",
      "date": "1 Jul 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/2024-03-03-Deep neural networks are robust to weight binarization and other non-linear distortions.markdown",
      "date": "7 Jun 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "Fast Feedforward Networks",
      "url": "/2024-03-03-Fast Feedforward Networks.markdown",
      "date": "18 Sep 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    },
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/2024-03-03-Loss of Plasticity in Deep Continual Learning.markdown",
      "date": "18 Aug 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/2024-03-03-Mamba Linear-Time Sequence Modeling with Selective State Spaces.markdown",
      "date": "1 Dec 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/2024-03-04-Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.markdown",
      "date": "2 Mar 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/2024-03-04-Data is Overrated Perceptual Metrics Can Lead Learning in the Absence of Training Data.markdown",
      "date": "6 Dec 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    },
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/2024-03-04-Dropout A Simple Way to Prevent Neural Networks from Overfitting.markdown",
      "date": "1 Jan 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    },
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/2024-03-04-Instance Normalization The Missing Ingredient for Fast Stylization.markdown",
      "date": "6 Nov 2017",
      "summary": "In this study, we improve upon the fast stylization method by Ulyanov et al. (2016) through a minor yet impactful modification: substituting batch normalization with instance normalization during both training and testing. This adjustment significantly enhances the quality of generated images and enables the training of efficient real-time image generation architectures. The code and full paper are available online."
    },
    {
      "title": "Layer Normalization",
      "url": "/2024-03-04-Layer Normalization.markdown",
      "date": "21 Jul 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    },
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/2024-03-04-On Warm-Starting Neural Network Training.markdown",
      "date": "31 Dec 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/2024-03-04-Understanding the difficulty of training deep feedforward neural networks.markdown",
      "date": "31 Mar 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/2024-03-05-Designing Network Design Spaces.markdown",
      "date": "30 Mar 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/2024-03-05-Masked Image Residual Learning for Scaling Deeper Vision Transformers.markdown",
      "date": "15 Nov 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/2024-03-05-Scaling Language-Image Pre-training via Masking.markdown",
      "date": "30 Mar 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "mask": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/2024-03-03-A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REP- RESENTATIONS.markdown",
      "date": "30 Oct 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    },
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/2024-03-03-Contrastive Masked Autoencoders are Stronger Vision Learners.markdown",
      "date": "29 Jan 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/2024-03-03-Extreme Masking for Learning Instance and Distributed Visual Representations.markdown",
      "date": "8 Mar 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/2024-03-03-I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture.markdown",
      "date": "13 Apr 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/2024-03-04-Understanding Masked Image Modeling via Learning Occlusion Invariant Feature.markdown",
      "date": "8 Aug 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/2024-03-04-What do Self-supervised vision transformers learn.markdown",
      "date": "1 May 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/2024-03-05-A Closer Look at Self-Supervised Lightweight Vision Transformers.markdown",
      "date": "3 May 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/2024-03-05-Architecture-Agnostic Masked Image Modeling – From ViT back to CNN.markdown",
      "date": "2 Jun 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/2024-03-05-Beyond Masking Demystifying Token-Based Pre-Training for Vision Transformers.markdown",
      "date": "27 Mar 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learner",
      "url": "/2024-03-05-Masked Autoencoders Are Scalable Vision Learner.markdown",
      "date": "19 Dec 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/2024-03-05-Masked Image Residual Learning for Scaling Deeper Vision Transformers.markdown",
      "date": "15 Nov 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/2024-03-05-Scaling Language-Image Pre-training via Masking.markdown",
      "date": "30 Mar 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    },
    {
      "title": "Segment Anything",
      "url": "/2024-03-05-Segment Anything.markdown",
      "date": "5 Apr 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/2024-03-05-Understanding Self-Supervised Pretraining with Part-Aware Representation Learning.markdown",
      "date": "23 Jan 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/2024-03-06-Denoising Masked Autoencoders Help Robust Classification.markdown",
      "date": "7 Mar 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/2024-03-06-Hard Patches Mining for Masked Image Modeling.markdown",
      "date": "12 Apr 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "autoencoder": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/2024-03-03-A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REP- RESENTATIONS.markdown",
      "date": "30 Oct 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    },
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/2024-03-03-Contrastive Masked Autoencoders are Stronger Vision Learners.markdown",
      "date": "29 Jan 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/2024-03-04-Data is Overrated Perceptual Metrics Can Lead Learning in the Absence of Training Data.markdown",
      "date": "6 Dec 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/2024-03-05-A Closer Look at Self-Supervised Lightweight Vision Transformers.markdown",
      "date": "3 May 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learner",
      "url": "/2024-03-05-Masked Autoencoders Are Scalable Vision Learner.markdown",
      "date": "19 Dec 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/2024-03-06-Denoising Masked Autoencoders Help Robust Classification.markdown",
      "date": "7 Mar 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    }
  ],
  "representations": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/2024-03-03-A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REP- RESENTATIONS.markdown",
      "date": "30 Oct 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    }
  ],
  "introuce": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/2024-03-03-A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REP- RESENTATIONS.markdown",
      "date": "30 Oct 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    }
  ],
  "can": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/2024-03-03-A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REP- RESENTATIONS.markdown",
      "date": "30 Oct 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    }
  ],
  "combination": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/2024-03-03-A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REP- RESENTATIONS.markdown",
      "date": "30 Oct 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    },
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/2024-03-03-DINOv2 Learning Robust Visual Features without Supervision.markdown",
      "date": "2 Feb 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/2024-03-03-Robust and Generalizable Visual Representation Learning via Random Convolutions.markdown",
      "date": "3 May 2021",
      "summary": "This study demonstrates that deep neural networks' vulnerability to texture shifts and perturbations can be mitigated by using random convolutions for data augmentation. Random convolutions, which approximately preserve shape while distorting local textures, create an infinite variety of new domains. We tested using these convolutions as new images or in combination with original images during training. Our method significantly enhances performance on unseen domains and domain generalization benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods. Additionally, it provides a more robust pretrained visual representation for downstream tasks."
    },
    {
      "title": "DropKey",
      "url": "/2024-03-04-DropKey.markdown",
      "date": "11 Apr 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/2024-03-05-Beyond Masking Demystifying Token-Based Pre-Training for Vision Transformers.markdown",
      "date": "27 Mar 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/2024-03-05-Understanding Self-Supervised Pretraining with Part-Aware Representation Learning.markdown",
      "date": "23 Jan 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/2024-03-05-Universal Adversarial Robustness of Texture and Shape-biased Models.markdown",
      "date": "31 Aug 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    }
  ],
  "noise": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/2024-03-03-A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REP- RESENTATIONS.markdown",
      "date": "30 Oct 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    },
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/2024-03-03-Deep neural networks are robust to weight binarization and other non-linear distortions.markdown",
      "date": "7 Jun 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/2024-03-04-Data is Overrated Perceptual Metrics Can Lead Learning in the Absence of Training Data.markdown",
      "date": "6 Dec 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    },
    {
      "title": "DROPOUT AS DATA AUGMENTATION",
      "url": "/2024-03-04-DROPOUT AS DATA AUGMENTATION.markdown",
      "date": "8 Jan 2016",
      "summary": "Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/2024-03-04-Impact of Noise on Calibration and Generalisation of Neural Networks.markdown",
      "date": "30 Jun 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. We examine different types of noise, such as activation and input augmentation noise, in both in-distribution and out-of-distribution scenarios. Activation noise significantly improves generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/2024-03-04-ON INTERACTION BETWEEN AUGMENTATIONS AND CORRUPTIONS IN NATURAL CORRUPTION ROBUSTNESS.markdown",
      "date": "19 Nov 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/2024-03-05-Deeper Insights into the Robustness of ViTs towards Common Corruptions.markdown",
      "date": "19 Aug 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/2024-03-05-Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions.markdown",
      "date": "20 Apr 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/2024-03-06-Denoising Masked Autoencoders Help Robust Classification.markdown",
      "date": "7 Mar 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    }
  ],
  "prediction": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/2024-03-03-A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REP- RESENTATIONS.markdown",
      "date": "30 Oct 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/2024-03-03-I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture.markdown",
      "date": "13 Apr 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/2024-03-04-Delving Deep into Label Smoothing.markdown",
      "date": "22 Jul 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/2024-03-05-A Closer Look at Self-Supervised Lightweight Vision Transformers.markdown",
      "date": "3 May 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/2024-03-05-Vision Transformers Need Register.markdown",
      "date": "28 Sep 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/2024-03-06-Hard Patches Mining for Masked Image Modeling.markdown",
      "date": "12 Apr 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "robustness": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/2024-03-03-A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REP- RESENTATIONS.markdown",
      "date": "30 Oct 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    },
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/2024-03-03-Deep Networks Always Grok and Heres Why.markdown",
      "date": "23 Feb 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/2024-03-03-Deep neural networks are robust to weight binarization and other non-linear distortions.markdown",
      "date": "7 Jun 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/2024-03-04-Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations.markdown",
      "date": "27 Apr 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/2024-03-04-Delving Deep into Label Smoothing.markdown",
      "date": "22 Jul 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/2024-03-04-Impact of Noise on Calibration and Generalisation of Neural Networks.markdown",
      "date": "30 Jun 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. We examine different types of noise, such as activation and input augmentation noise, in both in-distribution and out-of-distribution scenarios. Activation noise significantly improves generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/2024-03-04-ON INTERACTION BETWEEN AUGMENTATIONS AND CORRUPTIONS IN NATURAL CORRUPTION ROBUSTNESS.markdown",
      "date": "19 Nov 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/2024-03-04-RandAugment Practical automated data augmentation with a reduced search space.markdown",
      "date": "14 Nov 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/2024-03-05-Adversarially Self-supervised Pre-training Improves Accuracy and Robustness.markdown",
      "date": "10 Mar 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/2024-03-05-Deeper Insights into the Robustness of ViTs towards Common Corruptions.markdown",
      "date": "19 Aug 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/2024-03-05-Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions.markdown",
      "date": "20 Apr 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/2024-03-05-Universal Adversarial Robustness of Texture and Shape-biased Models.markdown",
      "date": "31 Aug 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/2024-03-06-(Certified!!) Adversarial Robustness for Free.markdown",
      "date": "6 Mar 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/2024-03-06-Denoising Masked Autoencoders Help Robust Classification.markdown",
      "date": "7 Mar 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    }
  ],
  "pretrain": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/2024-03-03-A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REP- RESENTATIONS.markdown",
      "date": "30 Oct 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    },
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/2024-03-04-Sigmoid Loss for Language Image Pre-Training.markdown",
      "date": "27 sept 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/2024-03-05-A Closer Look at Self-Supervised Lightweight Vision Transformers.markdown",
      "date": "3 May 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/2024-03-05-Adversarially Self-supervised Pre-training Improves Accuracy and Robustness.markdown",
      "date": "10 Mar 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/2024-03-05-Architecture-Agnostic Masked Image Modeling – From ViT back to CNN.markdown",
      "date": "2 Jun 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/2024-03-05-Beyond Masking Demystifying Token-Based Pre-Training for Vision Transformers.markdown",
      "date": "27 Mar 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/2024-03-05-Colorization as a Proxy Task for Visual Understanding.markdown",
      "date": "13 Aug 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learner",
      "url": "/2024-03-05-Masked Autoencoders Are Scalable Vision Learner.markdown",
      "date": "19 Dec 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/2024-03-05-Masked Image Residual Learning for Scaling Deeper Vision Transformers.markdown",
      "date": "15 Nov 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/2024-03-05-Pre-training Vision Transformers with Very Limited Synthesized Images.markdown",
      "date": "31 Jul 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/2024-03-05-Scaling Language-Image Pre-training via Masking.markdown",
      "date": "30 Mar 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/2024-03-05-Understanding Self-Supervised Pretraining with Part-Aware Representation Learning.markdown",
      "date": "23 Jan 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/2024-03-06-Hard Patches Mining for Masked Image Modeling.markdown",
      "date": "12 Apr 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "uncurate": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/2024-03-03-A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REP- RESENTATIONS.markdown",
      "date": "30 Oct 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    }
  ],
  "dataset": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/2024-03-03-A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REP- RESENTATIONS.markdown",
      "date": "30 Oct 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    },
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/2024-03-03-DINOv2 Learning Robust Visual Features without Supervision.markdown",
      "date": "2 Feb 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/2024-03-03-Loss of Plasticity in Deep Continual Learning.markdown",
      "date": "18 Aug 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/2024-03-03-PatchDropout Economizing Vision Transformers Using Patch Dropou.markdown",
      "date": "4 Oct 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/2024-03-04-A downsampled variant of imagenet as an alternative to the cifar datasets.markdown",
      "date": "23 Aug 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    },
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/2024-03-04-Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations.markdown",
      "date": "27 Apr 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/2024-03-04-Delving Deep into Label Smoothing.markdown",
      "date": "22 Jul 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/2024-03-04-Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classificatio.markdown",
      "date": "6 Feb 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    },
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/2024-03-04-Dropout Reduces Underfittin.markdown",
      "date": "31 May 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/2024-03-04-Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution.markdown",
      "date": "25 Oct 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/2024-03-04-RandAugment Practical automated data augmentation with a reduced search space.markdown",
      "date": "14 Nov 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/2024-03-04-Reading Digits in Natural Images With Unsupervised Feature Learning.markdown",
      "date": "1 Jan 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    },
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/2024-03-04-White-Box Transformers via Sparse Rate Reduction.markdown",
      "date": "1 Jun 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/2024-03-05-An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.markdown",
      "date": "3 Jun 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/2024-03-05-Pre-training Vision Transformers with Very Limited Synthesized Images.markdown",
      "date": "31 Jul 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Segment Anything",
      "url": "/2024-03-05-Segment Anything.markdown",
      "date": "5 Apr 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/2024-03-05-The geometry of hidden representations of large transformer models.markdown",
      "date": "30 Oct 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/2024-03-05-Transformer-Based Visual Segmentation A Surve.markdown",
      "date": "20 Dec 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/2024-03-06-Denoising Masked Autoencoders Help Robust Classification.markdown",
      "date": "7 Mar 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "load": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/2024-03-03-A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REP- RESENTATIONS.markdown",
      "date": "30 Oct 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    }
  ],
  "adam": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/2024-03-03-ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.markdown",
      "date": "30 Jan 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/2024-03-03-DECOUPLED WEIGHT DECAY REGULARIZATION.markdown",
      "date": "4 Jan 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "optimization": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/2024-03-03-ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.markdown",
      "date": "30 Jan 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/2024-03-03-DECOUPLED WEIGHT DECAY REGULARIZATION.markdown",
      "date": "4 Jan 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/2024-03-03-How Does Batch Normalization Help Optimization.markdown",
      "date": "15 Apr 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/2024-03-03-PatchDropout Economizing Vision Transformers Using Patch Dropou.markdown",
      "date": "4 Oct 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/2024-03-04-Scheduled DropHead A Regularization Method for Transformer Models.markdown",
      "date": "1 Nov 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    },
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/2024-03-04-White-Box Transformers via Sparse Rate Reduction.markdown",
      "date": "1 Jun 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "datum": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/2024-03-03-ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.markdown",
      "date": "30 Jan 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    }
  ],
  "parameter": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/2024-03-03-ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.markdown",
      "date": "30 Jan 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/2024-03-03-DINOv2 Learning Robust Visual Features without Supervision.markdown",
      "date": "2 Feb 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/2024-03-03-PatchDropout Economizing Vision Transformers Using Patch Dropou.markdown",
      "date": "4 Oct 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/2024-03-04-Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.markdown",
      "date": "2 Mar 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "DROPOUT AS DATA AUGMENTATION",
      "url": "/2024-03-04-DROPOUT AS DATA AUGMENTATION.markdown",
      "date": "8 Jan 2016",
      "summary": "Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/2024-03-05-Adversarially Self-supervised Pre-training Improves Accuracy and Robustness.markdown",
      "date": "10 Mar 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/2024-03-05-Deeper Insights into the Robustness of ViTs towards Common Corruptions.markdown",
      "date": "19 Aug 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/2024-03-06-(Certified!!) Adversarial Robustness for Free.markdown",
      "date": "6 Mar 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/2024-03-06-Denoising Masked Autoencoders Help Robust Classification.markdown",
      "date": "7 Mar 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    }
  ],
  "handle": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/2024-03-03-ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.markdown",
      "date": "30 Jan 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/2024-03-03-Vision Mamba Efficient Visual Representation Learning with Bidirectional State Space Model.markdown",
      "date": "10 Feb 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    }
  ],
  "moment": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/2024-03-03-ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.markdown",
      "date": "30 Jan 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    }
  ],
  "estimation": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/2024-03-03-ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.markdown",
      "date": "30 Jan 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "Group Normalization",
      "url": "/2024-03-04-Group Normalization.markdown",
      "date": "1 Jun 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "adaptive": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/2024-03-03-ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.markdown",
      "date": "30 Jan 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/2024-03-03-Loss of Plasticity in Deep Continual Learning.markdown",
      "date": "18 Aug 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    },
    {
      "title": "Layer Normalization",
      "url": "/2024-03-04-Layer Normalization.markdown",
      "date": "21 Jul 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/2024-03-04-RandAugment Practical automated data augmentation with a reduced search space.markdown",
      "date": "14 Nov 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    }
  ],
  "stationary": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/2024-03-03-ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.markdown",
      "date": "30 Jan 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    }
  ],
  "gradient": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/2024-03-03-ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.markdown",
      "date": "30 Jan 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/2024-03-03-DECOUPLED WEIGHT DECAY REGULARIZATION.markdown",
      "date": "4 Jan 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/2024-03-03-How Does Batch Normalization Help Optimization.markdown",
      "date": "15 Apr 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/2024-03-04-Dropout Reduces Underfittin.markdown",
      "date": "31 May 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/2024-03-04-Understanding the difficulty of training deep feedforward neural networks.markdown",
      "date": "31 Mar 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    }
  ],
  "hyper": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/2024-03-03-ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.markdown",
      "date": "30 Jan 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    }
  ],
  "connection": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/2024-03-03-ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.markdown",
      "date": "30 Jan 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/2024-03-04-Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution.markdown",
      "date": "25 Oct 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    }
  ],
  "convergence": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/2024-03-03-ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.markdown",
      "date": "30 Jan 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/2024-03-04-Understanding the difficulty of training deep feedforward neural networks.markdown",
      "date": "31 Mar 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "regret": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/2024-03-03-ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.markdown",
      "date": "30 Jan 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    }
  ],
  "bind": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/2024-03-03-ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.markdown",
      "date": "30 Jan 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/2024-03-06-(Certified!!) Adversarial Robustness for Free.markdown",
      "date": "6 Mar 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    }
  ],
  "adamax": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/2024-03-03-ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.markdown",
      "date": "30 Jan 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    }
  ],
  "infinity": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/2024-03-03-ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.markdown",
      "date": "30 Jan 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    }
  ],
  "normalization": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/2024-03-03-ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.markdown",
      "date": "30 Jan 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/2024-03-03-How Does Batch Normalization Help Optimization.markdown",
      "date": "15 Apr 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/2024-03-04-Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.markdown",
      "date": "2 Mar 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Group Normalization",
      "url": "/2024-03-04-Group Normalization.markdown",
      "date": "1 Jun 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    },
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/2024-03-04-Instance Normalization The Missing Ingredient for Fast Stylization.markdown",
      "date": "6 Nov 2017",
      "summary": "In this study, we improve upon the fast stylization method by Ulyanov et al. (2016) through a minor yet impactful modification: substituting batch normalization with instance normalization during both training and testing. This adjustment significantly enhances the quality of generated images and enables the training of efficient real-time image generation architectures. The code and full paper are available online."
    },
    {
      "title": "Layer Normalization",
      "url": "/2024-03-04-Layer Normalization.markdown",
      "date": "21 Jul 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/2024-03-04-Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution.markdown",
      "date": "25 Oct 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    },
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/2024-03-04-Sigmoid Loss for Language Image Pre-Training.markdown",
      "date": "27 sept 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/2024-03-05-Vision Transformers Need Register.markdown",
      "date": "28 Sep 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    }
  ],
  "smooth": [
    {
      "title": "AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION",
      "url": "/2024-03-03-AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION.markdown",
      "date": "23 Oct 2020",
      "summary": "Label smoothing is hypothesized to mitigate overfitting and enhance generalization, with supporting empirical evidence. However, its mathematical underpinnings remain unclear. This paper introduces a theoretical framework explaining label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. We identify an optimal label smoothing value that minimizes generalization loss and validate our theory with comprehensive experiments. Our results aim to clarify label smoothing's utility for both theoreticians and practitioners in real-world applications"
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/2024-03-04-Delving Deep into Label Smoothing.markdown",
      "date": "22 Jul 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/2024-03-06-(Certified!!) Adversarial Robustness for Free.markdown",
      "date": "6 Mar 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    }
  ],
  "generalization": [
    {
      "title": "AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION",
      "url": "/2024-03-03-AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION.markdown",
      "date": "23 Oct 2020",
      "summary": "Label smoothing is hypothesized to mitigate overfitting and enhance generalization, with supporting empirical evidence. However, its mathematical underpinnings remain unclear. This paper introduces a theoretical framework explaining label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. We identify an optimal label smoothing value that minimizes generalization loss and validate our theory with comprehensive experiments. Our results aim to clarify label smoothing's utility for both theoreticians and practitioners in real-world applications"
    },
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/2024-03-03-Deep Networks Always Grok and Heres Why.markdown",
      "date": "23 Feb 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/2024-03-03-Robust and Generalizable Visual Representation Learning via Random Convolutions.markdown",
      "date": "3 May 2021",
      "summary": "This study demonstrates that deep neural networks' vulnerability to texture shifts and perturbations can be mitigated by using random convolutions for data augmentation. Random convolutions, which approximately preserve shape while distorting local textures, create an infinite variety of new domains. We tested using these convolutions as new images or in combination with original images during training. Our method significantly enhances performance on unseen domains and domain generalization benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods. Additionally, it provides a more robust pretrained visual representation for downstream tasks."
    },
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/2024-03-04-Data is Overrated Perceptual Metrics Can Lead Learning in the Absence of Training Data.markdown",
      "date": "6 Dec 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    },
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/2024-03-04-Dropout Reduces Underfittin.markdown",
      "date": "31 May 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/2024-03-04-Impact of Noise on Calibration and Generalisation of Neural Networks.markdown",
      "date": "30 Jun 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. We examine different types of noise, such as activation and input augmentation noise, in both in-distribution and out-of-distribution scenarios. Activation noise significantly improves generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    },
    {
      "title": "Learning RelU Networks To High Uniform Accuracy Is Intractable",
      "url": "/2024-03-04-Learning Relu Networks To High Uniform Accuracy Is Intractable.markdown",
      "date": "28 Feb 2023",
      "summary": "Statistical learning theory provides guidelines on the number of training samples needed for achieving desired accuracy in learning problems, especially emphasizing generalization error. However, this is not always adequate, particularly in security-sensitive areas or computational sciences, where uniform accuracy across all inputs is necessary. This paper quantifies the training samples required for uniform accuracy in learning problems involving ReLU neural networks, revealing that the number of samples needed exponentially increases with the network's depth and input dimension."
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/2024-03-04-ON INTERACTION BETWEEN AUGMENTATIONS AND CORRUPTIONS IN NATURAL CORRUPTION ROBUSTNESS.markdown",
      "date": "19 Nov 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/2024-03-04-On Warm-Starting Neural Network Training.markdown",
      "date": "31 Dec 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/2024-03-04-Progressive Random Convolutions for Single Domain Generalization.markdown",
      "date": "2 Apr 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/2024-03-04-RandAugment Practical automated data augmentation with a reduced search space.markdown",
      "date": "14 Nov 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/2024-03-05-Architecture-Agnostic Masked Image Modeling – From ViT back to CNN.markdown",
      "date": "2 Jun 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/2024-03-05-Masked Image Residual Learning for Scaling Deeper Vision Transformers.markdown",
      "date": "15 Nov 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "smoothing": [
    {
      "title": "AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION",
      "url": "/2024-03-03-AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION.markdown",
      "date": "23 Oct 2020",
      "summary": "Label smoothing is hypothesized to mitigate overfitting and enhance generalization, with supporting empirical evidence. However, its mathematical underpinnings remain unclear. This paper introduces a theoretical framework explaining label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. We identify an optimal label smoothing value that minimizes generalization loss and validate our theory with comprehensive experiments. Our results aim to clarify label smoothing's utility for both theoreticians and practitioners in real-world applications"
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    }
  ],
  "underpinning": [
    {
      "title": "AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION",
      "url": "/2024-03-03-AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION.markdown",
      "date": "23 Oct 2020",
      "summary": "Label smoothing is hypothesized to mitigate overfitting and enhance generalization, with supporting empirical evidence. However, its mathematical underpinnings remain unclear. This paper introduces a theoretical framework explaining label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. We identify an optimal label smoothing value that minimizes generalization loss and validate our theory with comprehensive experiments. Our results aim to clarify label smoothing's utility for both theoreticians and practitioners in real-world applications"
    }
  ],
  "control": [
    {
      "title": "AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION",
      "url": "/2024-03-03-AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION.markdown",
      "date": "23 Oct 2020",
      "summary": "Label smoothing is hypothesized to mitigate overfitting and enhance generalization, with supporting empirical evidence. However, its mathematical underpinnings remain unclear. This paper introduces a theoretical framework explaining label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. We identify an optimal label smoothing value that minimizes generalization loss and validate our theory with comprehensive experiments. Our results aim to clarify label smoothing's utility for both theoreticians and practitioners in real-world applications"
    },
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/2024-03-03-How Does Batch Normalization Help Optimization.markdown",
      "date": "15 Apr 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/2024-03-04-Dropout Reduces Underfittin.markdown",
      "date": "31 May 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    }
  ],
  "value": [
    {
      "title": "AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION",
      "url": "/2024-03-03-AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION.markdown",
      "date": "23 Oct 2020",
      "summary": "Label smoothing is hypothesized to mitigate overfitting and enhance generalization, with supporting empirical evidence. However, its mathematical underpinnings remain unclear. This paper introduces a theoretical framework explaining label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. We identify an optimal label smoothing value that minimizes generalization loss and validate our theory with comprehensive experiments. Our results aim to clarify label smoothing's utility for both theoreticians and practitioners in real-world applications"
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/2024-03-04-Understanding the difficulty of training deep feedforward neural networks.markdown",
      "date": "31 Mar 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    }
  ],
  "utility": [
    {
      "title": "AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION",
      "url": "/2024-03-03-AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION.markdown",
      "date": "23 Oct 2020",
      "summary": "Label smoothing is hypothesized to mitigate overfitting and enhance generalization, with supporting empirical evidence. However, its mathematical underpinnings remain unclear. This paper introduces a theoretical framework explaining label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. We identify an optimal label smoothing value that minimizes generalization loss and validate our theory with comprehensive experiments. Our results aim to clarify label smoothing's utility for both theoreticians and practitioners in real-world applications"
    },
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/2024-03-04-On Warm-Starting Neural Network Training.markdown",
      "date": "31 Dec 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    }
  ],
  "theoretician": [
    {
      "title": "AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION",
      "url": "/2024-03-03-AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION.markdown",
      "date": "23 Oct 2020",
      "summary": "Label smoothing is hypothesized to mitigate overfitting and enhance generalization, with supporting empirical evidence. However, its mathematical underpinnings remain unclear. This paper introduces a theoretical framework explaining label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. We identify an optimal label smoothing value that minimizes generalization loss and validate our theory with comprehensive experiments. Our results aim to clarify label smoothing's utility for both theoreticians and practitioners in real-world applications"
    }
  ],
  "practitioner": [
    {
      "title": "AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION",
      "url": "/2024-03-03-AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION.markdown",
      "date": "23 Oct 2020",
      "summary": "Label smoothing is hypothesized to mitigate overfitting and enhance generalization, with supporting empirical evidence. However, its mathematical underpinnings remain unclear. This paper introduces a theoretical framework explaining label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. We identify an optimal label smoothing value that minimizes generalization loss and validate our theory with comprehensive experiments. Our results aim to clarify label smoothing's utility for both theoreticians and practitioners in real-world applications"
    }
  ],
  "world": [
    {
      "title": "AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION",
      "url": "/2024-03-03-AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION.markdown",
      "date": "23 Oct 2020",
      "summary": "Label smoothing is hypothesized to mitigate overfitting and enhance generalization, with supporting empirical evidence. However, its mathematical underpinnings remain unclear. This paper introduces a theoretical framework explaining label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. We identify an optimal label smoothing value that minimizes generalization loss and validate our theory with comprehensive experiments. Our results aim to clarify label smoothing's utility for both theoreticians and practitioners in real-world applications"
    }
  ],
  "bootstrap": [
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/2024-03-03-Bootstrap Your Own Latent A New Approach to Self-Supervised Learning.markdown",
      "date": "10 Sep 2020",
      "summary": "We introduce BYOL, a novel self-supervised learning method for image representation without using negative pairs. Utilizing two networks, BYOL trains one to predict the representation of an image as seen by the other under a different view, updating the latter with the former's slow-moving average. Surpassing state-of-the-art methods, it achieves up to 79.6% accuracy on ImageNet with ResNet models, performing equally or better on transfer and semi-supervised tasks. Code and models are available on GitHub."
    }
  ],
  "byol": [
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/2024-03-03-Bootstrap Your Own Latent A New Approach to Self-Supervised Learning.markdown",
      "date": "10 Sep 2020",
      "summary": "We introduce BYOL, a novel self-supervised learning method for image representation without using negative pairs. Utilizing two networks, BYOL trains one to predict the representation of an image as seen by the other under a different view, updating the latter with the former's slow-moving average. Surpassing state-of-the-art methods, it achieves up to 79.6% accuracy on ImageNet with ResNet models, performing equally or better on transfer and semi-supervised tasks. Code and models are available on GitHub."
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/2024-03-05-Adversarially Self-supervised Pre-training Improves Accuracy and Robustness.markdown",
      "date": "10 Mar 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    }
  ],
  "pair": [
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/2024-03-03-Bootstrap Your Own Latent A New Approach to Self-Supervised Learning.markdown",
      "date": "10 Sep 2020",
      "summary": "We introduce BYOL, a novel self-supervised learning method for image representation without using negative pairs. Utilizing two networks, BYOL trains one to predict the representation of an image as seen by the other under a different view, updating the latter with the former's slow-moving average. Surpassing state-of-the-art methods, it achieves up to 79.6% accuracy on ImageNet with ResNet models, performing equally or better on transfer and semi-supervised tasks. Code and models are available on GitHub."
    },
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/2024-03-04-Sigmoid Loss for Language Image Pre-Training.markdown",
      "date": "27 sept 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/2024-03-05-Scaling Language-Image Pre-training via Masking.markdown",
      "date": "30 Mar 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    }
  ],
  "see": [
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/2024-03-03-Bootstrap Your Own Latent A New Approach to Self-Supervised Learning.markdown",
      "date": "10 Sep 2020",
      "summary": "We introduce BYOL, a novel self-supervised learning method for image representation without using negative pairs. Utilizing two networks, BYOL trains one to predict the representation of an image as seen by the other under a different view, updating the latter with the former's slow-moving average. Surpassing state-of-the-art methods, it achieves up to 79.6% accuracy on ImageNet with ResNet models, performing equally or better on transfer and semi-supervised tasks. Code and models are available on GitHub."
    }
  ],
  "update": [
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/2024-03-03-Bootstrap Your Own Latent A New Approach to Self-Supervised Learning.markdown",
      "date": "10 Sep 2020",
      "summary": "We introduce BYOL, a novel self-supervised learning method for image representation without using negative pairs. Utilizing two networks, BYOL trains one to predict the representation of an image as seen by the other under a different view, updating the latter with the former's slow-moving average. Surpassing state-of-the-art methods, it achieves up to 79.6% accuracy on ImageNet with ResNet models, performing equally or better on transfer and semi-supervised tasks. Code and models are available on GitHub."
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/2024-03-04-Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.markdown",
      "date": "2 Mar 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    }
  ],
  "latter": [
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/2024-03-03-Bootstrap Your Own Latent A New Approach to Self-Supervised Learning.markdown",
      "date": "10 Sep 2020",
      "summary": "We introduce BYOL, a novel self-supervised learning method for image representation without using negative pairs. Utilizing two networks, BYOL trains one to predict the representation of an image as seen by the other under a different view, updating the latter with the former's slow-moving average. Surpassing state-of-the-art methods, it achieves up to 79.6% accuracy on ImageNet with ResNet models, performing equally or better on transfer and semi-supervised tasks. Code and models are available on GitHub."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "former": [
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/2024-03-03-Bootstrap Your Own Latent A New Approach to Self-Supervised Learning.markdown",
      "date": "10 Sep 2020",
      "summary": "We introduce BYOL, a novel self-supervised learning method for image representation without using negative pairs. Utilizing two networks, BYOL trains one to predict the representation of an image as seen by the other under a different view, updating the latter with the former's slow-moving average. Surpassing state-of-the-art methods, it achieves up to 79.6% accuracy on ImageNet with ResNet models, performing equally or better on transfer and semi-supervised tasks. Code and models are available on GitHub."
    }
  ],
  "github": [
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/2024-03-03-Bootstrap Your Own Latent A New Approach to Self-Supervised Learning.markdown",
      "date": "10 Sep 2020",
      "summary": "We introduce BYOL, a novel self-supervised learning method for image representation without using negative pairs. Utilizing two networks, BYOL trains one to predict the representation of an image as seen by the other under a different view, updating the latter with the former's slow-moving average. Surpassing state-of-the-art methods, it achieves up to 79.6% accuracy on ImageNet with ResNet models, performing equally or better on transfer and semi-supervised tasks. Code and models are available on GitHub."
    },
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/2024-03-04-Dropout Reduces Underfittin.markdown",
      "date": "31 May 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/2024-03-04-Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution.markdown",
      "date": "25 Oct 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    }
  ],
  "learner": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/2024-03-03-Contrastive Masked Autoencoders are Stronger Vision Learners.markdown",
      "date": "29 Jan 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learner",
      "url": "/2024-03-05-Masked Autoencoders Are Scalable Vision Learner.markdown",
      "date": "19 Dec 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    }
  ],
  "cmae": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/2024-03-03-Contrastive Masked Autoencoders are Stronger Vision Learners.markdown",
      "date": "29 Jan 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    }
  ],
  "modeling": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/2024-03-03-Contrastive Masked Autoencoders are Stronger Vision Learners.markdown",
      "date": "29 Jan 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/2024-03-03-Hungry Hungry Hippos Towards Language Modeling with State Space Models.markdown",
      "date": "29 Apr 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/2024-03-03-Mamba Linear-Time Sequence Modeling with Selective State Spaces.markdown",
      "date": "1 Dec 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    },
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/2024-03-03-MambaByte Token-free Selective State Space Model.markdown",
      "date": "24 Jan 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/2024-03-04-Understanding Masked Image Modeling via Learning Occlusion Invariant Feature.markdown",
      "date": "8 Aug 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/2024-03-04-What do Self-supervised vision transformers learn.markdown",
      "date": "1 May 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/2024-03-05-Architecture-Agnostic Masked Image Modeling – From ViT back to CNN.markdown",
      "date": "2 Jun 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/2024-03-05-Beyond Masking Demystifying Token-Based Pre-Training for Vision Transformers.markdown",
      "date": "27 Mar 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/2024-03-05-Masked Image Residual Learning for Scaling Deeper Vision Transformers.markdown",
      "date": "15 Nov 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/2024-03-05-Understanding Self-Supervised Pretraining with Part-Aware Representation Learning.markdown",
      "date": "23 Jan 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/2024-03-06-Hard Patches Mining for Masked Image Modeling.markdown",
      "date": "12 Apr 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "mim": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/2024-03-03-Contrastive Masked Autoencoders are Stronger Vision Learners.markdown",
      "date": "29 Jan 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/2024-03-04-Understanding Masked Image Modeling via Learning Occlusion Invariant Feature.markdown",
      "date": "8 Aug 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/2024-03-04-What do Self-supervised vision transformers learn.markdown",
      "date": "1 May 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/2024-03-05-Architecture-Agnostic Masked Image Modeling – From ViT back to CNN.markdown",
      "date": "2 Jun 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/2024-03-05-Beyond Masking Demystifying Token-Based Pre-Training for Vision Transformers.markdown",
      "date": "27 Mar 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/2024-03-05-Masked Image Residual Learning for Scaling Deeper Vision Transformers.markdown",
      "date": "15 Nov 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/2024-03-06-Hard Patches Mining for Masked Image Modeling.markdown",
      "date": "12 Apr 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "discriminability": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/2024-03-03-Contrastive Masked Autoencoders are Stronger Vision Learners.markdown",
      "date": "29 Jan 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    }
  ],
  "perceptibility": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/2024-03-03-Contrastive Masked Autoencoders are Stronger Vision Learners.markdown",
      "date": "29 Jan 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    }
  ],
  "branch": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/2024-03-03-Contrastive Masked Autoencoders are Stronger Vision Learners.markdown",
      "date": "29 Jan 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    }
  ],
  "encoder": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/2024-03-03-Contrastive Masked Autoencoders are Stronger Vision Learners.markdown",
      "date": "29 Jan 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learner",
      "url": "/2024-03-05-Masked Autoencoders Are Scalable Vision Learner.markdown",
      "date": "19 Dec 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/2024-03-05-Understanding Self-Supervised Pretraining with Part-Aware Representation Learning.markdown",
      "date": "23 Jan 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/2024-03-06-Denoising Masked Autoencoders Help Robust Classification.markdown",
      "date": "7 Mar 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "decoder": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/2024-03-03-Contrastive Masked Autoencoders are Stronger Vision Learners.markdown",
      "date": "29 Jan 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learner",
      "url": "/2024-03-05-Masked Autoencoders Are Scalable Vision Learner.markdown",
      "date": "19 Dec 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "momentum": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/2024-03-03-Contrastive Masked Autoencoders are Stronger Vision Learners.markdown",
      "date": "29 Jan 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/2024-03-03-DECOUPLED WEIGHT DECAY REGULARIZATION.markdown",
      "date": "4 Jan 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "pixel": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/2024-03-03-Contrastive Masked Autoencoders are Stronger Vision Learners.markdown",
      "date": "29 Jan 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learner",
      "url": "/2024-03-05-Masked Autoencoders Are Scalable Vision Learner.markdown",
      "date": "19 Dec 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    }
  ],
  "ensure": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/2024-03-03-Contrastive Masked Autoencoders are Stronger Vision Learners.markdown",
      "date": "29 Jan 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    }
  ],
  "compatibility": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/2024-03-03-Contrastive Masked Autoencoders are Stronger Vision Learners.markdown",
      "date": "29 Jan 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/2024-03-05-Architecture-Agnostic Masked Image Modeling – From ViT back to CNN.markdown",
      "date": "2 Jun 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    }
  ],
  "classification": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/2024-03-03-Contrastive Masked Autoencoders are Stronger Vision Learners.markdown",
      "date": "29 Jan 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/2024-03-03-DECOUPLED WEIGHT DECAY REGULARIZATION.markdown",
      "date": "4 Jan 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/2024-03-03-I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture.markdown",
      "date": "13 Apr 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/2024-03-03-PatchDropout Economizing Vision Transformers Using Patch Dropou.markdown",
      "date": "4 Oct 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/2024-03-04-Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.markdown",
      "date": "2 Mar 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/2024-03-04-Delving Deep into Label Smoothing.markdown",
      "date": "22 Jul 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/2024-03-04-Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classificatio.markdown",
      "date": "6 Feb 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    },
    {
      "title": "DropKey",
      "url": "/2024-03-04-DropKey.markdown",
      "date": "11 Apr 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/2024-03-04-Dropout A Simple Way to Prevent Neural Networks from Overfitting.markdown",
      "date": "1 Jan 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    },
    {
      "title": "Group Normalization",
      "url": "/2024-03-04-Group Normalization.markdown",
      "date": "1 Jun 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/2024-03-04-RandAugment Practical automated data augmentation with a reduced search space.markdown",
      "date": "14 Nov 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/2024-03-04-Scheduled DropHead A Regularization Method for Transformer Models.markdown",
      "date": "1 Nov 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/2024-03-05-A Closer Look at Self-Supervised Lightweight Vision Transformers.markdown",
      "date": "3 May 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/2024-03-05-An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.markdown",
      "date": "3 Jun 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/2024-03-05-Colorization as a Proxy Task for Visual Understanding.markdown",
      "date": "13 Aug 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/2024-03-05-Turbo Training with Token Dropout.markdown",
      "date": "10 Oct 2022",
      "summary": "This paper introduces Turbo training, an efficient training method for video-related tasks using Transformers. Turbo training offers three major contributions: Firstly, it presents a simple yet versatile training paradigm applicable to multiple video tasks. Secondly, it demonstrates Turbo training's effectiveness across action classification, video-language representation learning, and long-video activity classification, achieving competitive performance with up to 4× speed-up and reduced memory usage. Thirdly, it enables long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous methods that were resource-intensive."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/2024-03-06-Denoising Masked Autoencoders Help Robust Classification.markdown",
      "date": "7 Mar 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    }
  ],
  "miou": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/2024-03-03-Contrastive Masked Autoencoders are Stronger Vision Learners.markdown",
      "date": "29 Jan 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    }
  ],
  "ade20k": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/2024-03-03-Contrastive Masked Autoencoders are Stronger Vision Learners.markdown",
      "date": "29 Jan 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/2024-03-03-Vision Mamba Efficient Visual Representation Learning with Bidirectional State Space Model.markdown",
      "date": "10 Feb 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    }
  ],
  "httpsgithubcomzhichenghuangcmae": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/2024-03-03-Contrastive Masked Autoencoders are Stronger Vision Learners.markdown",
      "date": "29 Jan 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    }
  ],
  "adamw": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/2024-03-03-DECOUPLED WEIGHT DECAY REGULARIZATION.markdown",
      "date": "4 Jan 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "decouple": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/2024-03-03-DECOUPLED WEIGHT DECAY REGULARIZATION.markdown",
      "date": "4 Jan 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    }
  ],
  "weight": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/2024-03-03-DECOUPLED WEIGHT DECAY REGULARIZATION.markdown",
      "date": "4 Jan 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/2024-03-03-Deep neural networks are robust to weight binarization and other non-linear distortions.markdown",
      "date": "7 Jun 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/2024-03-03-Loss of Plasticity in Deep Continual Learning.markdown",
      "date": "18 Aug 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/2024-03-04-Delving Deep into Label Smoothing.markdown",
      "date": "22 Jul 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/2024-03-04-DropAttention A Regularization Method for Fully-Connected Self-Attention Networks.markdown",
      "date": "26 Jul 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. Unlike existing dropout techniques for other neural network layers, DropAttention specifically targets the unique challenges of self-attention mechanisms. Our experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    },
    {
      "title": "DropKey",
      "url": "/2024-03-04-DropKey.markdown",
      "date": "11 Apr 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/2024-03-04-Dropout A Simple Way to Prevent Neural Networks from Overfitting.markdown",
      "date": "1 Jan 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "decay": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/2024-03-03-DECOUPLED WEIGHT DECAY REGULARIZATION.markdown",
      "date": "4 Jan 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    }
  ],
  "regularization": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/2024-03-03-DECOUPLED WEIGHT DECAY REGULARIZATION.markdown",
      "date": "4 Jan 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/2024-03-04-Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.markdown",
      "date": "2 Mar 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/2024-03-04-Delving Deep into Label Smoothing.markdown",
      "date": "22 Jul 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/2024-03-04-DropAttention A Regularization Method for Fully-Connected Self-Attention Networks.markdown",
      "date": "26 Jul 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. Unlike existing dropout techniques for other neural network layers, DropAttention specifically targets the unique challenges of self-attention mechanisms. Our experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    },
    {
      "title": "DropKey",
      "url": "/2024-03-04-DropKey.markdown",
      "date": "11 Apr 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/2024-03-04-Dropout Reduces Underfittin.markdown",
      "date": "31 May 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/2024-03-04-RandAugment Practical automated data augmentation with a reduced search space.markdown",
      "date": "14 Nov 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/2024-03-04-Scheduled DropHead A Regularization Method for Transformer Models.markdown",
      "date": "1 Nov 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    }
  ],
  "l2": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/2024-03-03-DECOUPLED WEIGHT DECAY REGULARIZATION.markdown",
      "date": "4 Jan 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    }
  ],
  "sgd": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/2024-03-03-DECOUPLED WEIGHT DECAY REGULARIZATION.markdown",
      "date": "4 Jan 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/2024-03-04-Dropout Reduces Underfittin.markdown",
      "date": "31 May 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "inequivalence": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/2024-03-03-DECOUPLED WEIGHT DECAY REGULARIZATION.markdown",
      "date": "4 Jan 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    }
  ],
  "align": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/2024-03-03-DECOUPLED WEIGHT DECAY REGULARIZATION.markdown",
      "date": "4 Jan 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/2024-03-04-Dropout Reduces Underfittin.markdown",
      "date": "31 May 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    }
  ],
  "separate": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/2024-03-03-DECOUPLED WEIGHT DECAY REGULARIZATION.markdown",
      "date": "4 Jan 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    }
  ],
  "step": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/2024-03-03-DECOUPLED WEIGHT DECAY REGULARIZATION.markdown",
      "date": "4 Jan 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/2024-03-04-Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.markdown",
      "date": "2 Mar 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    }
  ],
  "factor": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/2024-03-03-DECOUPLED WEIGHT DECAY REGULARIZATION.markdown",
      "date": "4 Jan 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    }
  ],
  "match": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/2024-03-03-DECOUPLED WEIGHT DECAY REGULARIZATION.markdown",
      "date": "4 Jan 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/2024-03-03-Mamba Linear-Time Sequence Modeling with Selective State Spaces.markdown",
      "date": "1 Dec 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/2024-03-04-On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning.markdown",
      "date": "26 Jun 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/2024-03-05-A Closer Look at Self-Supervised Lightweight Vision Transformers.markdown",
      "date": "3 May 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/2024-03-05-Masked Image Residual Learning for Scaling Deeper Vision Transformers.markdown",
      "date": "15 Nov 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Segment Anything",
      "url": "/2024-03-05-Segment Anything.markdown",
      "date": "5 Apr 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    },
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/2024-03-05-Turbo Training with Token Dropout.markdown",
      "date": "10 Oct 2022",
      "summary": "This paper introduces Turbo training, an efficient training method for video-related tasks using Transformers. Turbo training offers three major contributions: Firstly, it presents a simple yet versatile training paradigm applicable to multiple video tasks. Secondly, it demonstrates Turbo training's effectiveness across action classification, video-language representation learning, and long-video activity classification, achieving competitive performance with up to 4× speed-up and reduced memory usage. Thirdly, it enables long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous methods that were resource-intensive."
    }
  ],
  "traction": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/2024-03-03-DECOUPLED WEIGHT DECAY REGULARIZATION.markdown",
      "date": "4 Jan 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    }
  ],
  "tensorflow": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/2024-03-03-DECOUPLED WEIGHT DECAY REGULARIZATION.markdown",
      "date": "4 Jan 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    }
  ],
  "pytorch": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/2024-03-03-DECOUPLED WEIGHT DECAY REGULARIZATION.markdown",
      "date": "4 Jan 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    }
  ],
  "httpsgithubcomloshchiladamwandsgdw": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/2024-03-03-DECOUPLED WEIGHT DECAY REGULARIZATION.markdown",
      "date": "4 Jan 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    }
  ],
  "grok": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/2024-03-03-Deep Networks Always Grok and Heres Why.markdown",
      "date": "23 Feb 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    }
  ],
  "here": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/2024-03-03-Deep Networks Always Grok and Heres Why.markdown",
      "date": "23 Feb 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    }
  ],
  "why": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/2024-03-03-Deep Networks Always Grok and Heres Why.markdown",
      "date": "23 Feb 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/2024-03-04-Understanding the difficulty of training deep feedforward neural networks.markdown",
      "date": "31 Mar 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    }
  ],
  "phenomenon": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/2024-03-03-Deep Networks Always Grok and Heres Why.markdown",
      "date": "23 Feb 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/2024-03-04-On Warm-Starting Neural Network Training.markdown",
      "date": "31 Dec 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    }
  ],
  "dnn": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/2024-03-03-Deep Networks Always Grok and Heres Why.markdown",
      "date": "23 Feb 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/2024-03-03-How Does Batch Normalization Help Optimization.markdown",
      "date": "15 Apr 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/2024-03-04-Delving Deep into Label Smoothing.markdown",
      "date": "22 Jul 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/2024-03-05-Universal Adversarial Robustness of Texture and Shape-biased Models.markdown",
      "date": "31 Aug 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    }
  ],
  "occur": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/2024-03-03-Deep Networks Always Grok and Heres Why.markdown",
      "date": "23 Feb 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    }
  ],
  "setting": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/2024-03-03-Deep Networks Always Grok and Heres Why.markdown",
      "date": "23 Feb 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/2024-03-04-RandAugment Practical automated data augmentation with a reduced search space.markdown",
      "date": "14 Nov 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/2024-03-05-A Closer Look at Self-Supervised Lightweight Vision Transformers.markdown",
      "date": "3 May 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "cifar10": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/2024-03-03-Deep Networks Always Grok and Heres Why.markdown",
      "date": "23 Feb 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "imagenette": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/2024-03-03-Deep Networks Always Grok and Heres Why.markdown",
      "date": "23 Feb 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    }
  ],
  "become": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/2024-03-03-Deep Networks Always Grok and Heres Why.markdown",
      "date": "23 Feb 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    }
  ],
  "emergence": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/2024-03-03-Deep Networks Always Grok and Heres Why.markdown",
      "date": "23 Feb 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/2024-03-05-Transformer-Based Visual Segmentation A Surve.markdown",
      "date": "20 Dec 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "complexity": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/2024-03-03-Deep Networks Always Grok and Heres Why.markdown",
      "date": "23 Feb 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/2024-03-03-VMamba Visual State Space Model.markdown",
      "date": "18 Jan 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/2024-03-04-RandAugment Practical automated data augmentation with a reduced search space.markdown",
      "date": "14 Nov 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    }
  ],
  "density": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/2024-03-03-Deep Networks Always Grok and Heres Why.markdown",
      "date": "23 Feb 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    }
  ],
  "region": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/2024-03-03-Deep Networks Always Grok and Heres Why.markdown",
      "date": "23 Feb 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    }
  ],
  "space": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/2024-03-03-Deep Networks Always Grok and Heres Why.markdown",
      "date": "23 Feb 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/2024-03-03-Efficiently Modeling Long Sequences with Structured State Spaces.markdown",
      "date": "5 Aug 2022",
      "summary": "he quest for a universal sequence model capable of handling data across different tasks and modalities, especially long-range dependencies, faces challenges with conventional models like RNNs, CNNs, and Transformers, which struggle with very long sequences. A promising approach using state space models (SSM) showed potential but had high computational demands. We introduce the Structured State Space sequence model (S4), a more efficient parameterization of SSM, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    },
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/2024-03-03-Hungry Hungry Hippos Towards Language Modeling with State Space Models.markdown",
      "date": "29 Apr 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/2024-03-03-Mamba Linear-Time Sequence Modeling with Selective State Spaces.markdown",
      "date": "1 Dec 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    },
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/2024-03-03-MambaByte Token-free Selective State Space Model.markdown",
      "date": "24 Jan 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/2024-03-03-Vision Mamba Efficient Visual Representation Learning with Bidirectional State Space Model.markdown",
      "date": "10 Feb 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/2024-03-03-VMamba Visual State Space Model.markdown",
      "date": "18 Jan 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "DROPOUT AS DATA AUGMENTATION",
      "url": "/2024-03-04-DROPOUT AS DATA AUGMENTATION.markdown",
      "date": "8 Jan 2016",
      "summary": "Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/2024-03-04-ON INTERACTION BETWEEN AUGMENTATIONS AND CORRUPTIONS IN NATURAL CORRUPTION ROBUSTNESS.markdown",
      "date": "19 Nov 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/2024-03-04-RandAugment Practical automated data augmentation with a reduced search space.markdown",
      "date": "14 Nov 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/2024-03-05-Designing Network Design Spaces.markdown",
      "date": "30 Mar 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "smoothness": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/2024-03-03-Deep Networks Always Grok and Heres Why.markdown",
      "date": "23 Feb 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/2024-03-03-How Does Batch Normalization Help Optimization.markdown",
      "date": "15 Apr 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    }
  ],
  "decision": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/2024-03-03-Deep Networks Always Grok and Heres Why.markdown",
      "date": "23 Feb 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    }
  ],
  "boundary": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/2024-03-03-Deep Networks Always Grok and Heres Why.markdown",
      "date": "23 Feb 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    }
  ],
  "partitioning": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/2024-03-03-Deep Networks Always Grok and Heres Why.markdown",
      "date": "23 Feb 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    }
  ],
  "binarization": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/2024-03-03-Deep neural networks are robust to weight binarization and other non-linear distortions.markdown",
      "date": "7 Jun 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    }
  ],
  "distortion": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/2024-03-03-Deep neural networks are robust to weight binarization and other non-linear distortions.markdown",
      "date": "7 Jun 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    }
  ],
  "quantize": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/2024-03-03-Deep neural networks are robust to weight binarization and other non-linear distortions.markdown",
      "date": "7 Jun 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/2024-03-05-Designing Network Design Spaces.markdown",
      "date": "30 Mar 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    }
  ],
  "beginning": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/2024-03-03-Deep neural networks are robust to weight binarization and other non-linear distortions.markdown",
      "date": "7 Jun 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    }
  ],
  "resilience": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/2024-03-03-Deep neural networks are robust to weight binarization and other non-linear distortions.markdown",
      "date": "7 Jun 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/2024-03-05-Deeper Insights into the Robustness of ViTs towards Common Corruptions.markdown",
      "date": "19 Aug 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/2024-03-05-Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions.markdown",
      "date": "20 Apr 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    }
  ],
  "projection": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/2024-03-03-Deep neural networks are robust to weight binarization and other non-linear distortions.markdown",
      "date": "7 Jun 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    }
  ],
  "quantization": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/2024-03-03-Deep neural networks are robust to weight binarization and other non-linear distortions.markdown",
      "date": "7 Jun 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    }
  ],
  "instance": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/2024-03-03-Deep neural networks are robust to weight binarization and other non-linear distortions.markdown",
      "date": "7 Jun 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/2024-03-03-Extreme Masking for Learning Instance and Distributed Visual Representations.markdown",
      "date": "8 Mar 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/2024-03-03-Vision Mamba Efficient Visual Representation Learning with Bidirectional State Space Model.markdown",
      "date": "10 Feb 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/2024-03-04-Instance Normalization The Missing Ingredient for Fast Stylization.markdown",
      "date": "6 Nov 2017",
      "summary": "In this study, we improve upon the fast stylization method by Ulyanov et al. (2016) through a minor yet impactful modification: substituting batch normalization with instance normalization during both training and testing. This adjustment significantly enhances the quality of generated images and enables the training of efficient real-time image generation architectures. The code and full paper are available online."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/2024-03-05-Pre-training Vision Transformers with Very Limited Synthesized Images.markdown",
      "date": "31 Jul 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    }
  ],
  "cifar": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/2024-03-03-Deep neural networks are robust to weight binarization and other non-linear distortions.markdown",
      "date": "7 Jun 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/2024-03-04-A downsampled variant of imagenet as an alternative to the cifar datasets.markdown",
      "date": "23 Aug 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/2024-03-04-Delving Deep into Label Smoothing.markdown",
      "date": "22 Jul 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/2024-03-05-An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.markdown",
      "date": "3 Jun 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/2024-03-06-Denoising Masked Autoencoders Help Robust Classification.markdown",
      "date": "7 Mar 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    }
  ],
  "bit": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/2024-03-03-Deep neural networks are robust to weight binarization and other non-linear distortions.markdown",
      "date": "7 Jun 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    }
  ],
  "backpropagation": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/2024-03-03-Deep neural networks are robust to weight binarization and other non-linear distortions.markdown",
      "date": "7 Jun 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/2024-03-03-Loss of Plasticity in Deep Continual Learning.markdown",
      "date": "18 Aug 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    }
  ],
  "omit": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/2024-03-03-Deep neural networks are robust to weight binarization and other non-linear distortions.markdown",
      "date": "7 Jun 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/2024-03-04-Dropout A Simple Way to Prevent Neural Networks from Overfitting.markdown",
      "date": "1 Jan 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    }
  ],
  "sacrificing": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/2024-03-03-Deep neural networks are robust to weight binarization and other non-linear distortions.markdown",
      "date": "7 Jun 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    }
  ],
  "rule": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/2024-03-03-Deep neural networks are robust to weight binarization and other non-linear distortions.markdown",
      "date": "7 Jun 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    }
  ],
  "dino": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "new": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "vit": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/2024-03-03-DINOv2 Learning Robust Visual Features without Supervision.markdown",
      "date": "2 Feb 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/2024-03-03-I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture.markdown",
      "date": "13 Apr 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/2024-03-03-PatchDropout Economizing Vision Transformers Using Patch Dropou.markdown",
      "date": "4 Oct 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/2024-03-03-VMamba Visual State Space Model.markdown",
      "date": "18 Jan 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/2024-03-04-What do Self-supervised vision transformers learn.markdown",
      "date": "1 May 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/2024-03-05-A Closer Look at Self-Supervised Lightweight Vision Transformers.markdown",
      "date": "3 May 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/2024-03-05-An Empirical Study of Training Self-Supervised Vision Transformers.markdown",
      "date": "16 Aug 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/2024-03-05-An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.markdown",
      "date": "3 Jun 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/2024-03-05-Architecture-Agnostic Masked Image Modeling – From ViT back to CNN.markdown",
      "date": "2 Jun 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/2024-03-05-Deeper Insights into the Robustness of ViTs towards Common Corruptions.markdown",
      "date": "19 Aug 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/2024-03-05-Masked Image Residual Learning for Scaling Deeper Vision Transformers.markdown",
      "date": "15 Nov 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/2024-03-05-Vision Transformers Need Register.markdown",
      "date": "28 Sep 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/2024-03-06-Denoising Masked Autoencoders Help Robust Classification.markdown",
      "date": "7 Mar 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    }
  ],
  "out": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "convnet": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "beyond": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "fact": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "following": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "contain": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "information": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/2024-03-04-Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution.markdown",
      "date": "25 Oct 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/2024-03-05-Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions.markdown",
      "date": "20 Apr 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/2024-03-05-The geometry of hidden representations of large transformer models.markdown",
      "date": "30 Oct 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "emerge": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "second": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "knn": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "underline": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "multicrop": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "patch": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/2024-03-03-PatchDropout Economizing Vision Transformers Using Patch Dropou.markdown",
      "date": "4 Oct 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/2024-03-05-An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.markdown",
      "date": "3 Jun 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/2024-03-05-Architecture-Agnostic Masked Image Modeling – From ViT back to CNN.markdown",
      "date": "2 Jun 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/2024-03-05-Deeper Insights into the Robustness of ViTs towards Common Corruptions.markdown",
      "date": "19 Aug 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learner",
      "url": "/2024-03-05-Masked Autoencoders Are Scalable Vision Learner.markdown",
      "date": "19 Dec 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/2024-03-05-Scaling Language-Image Pre-training via Masking.markdown",
      "date": "30 Mar 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/2024-03-05-Understanding Self-Supervised Pretraining with Part-Aware Representation Learning.markdown",
      "date": "23 Jan 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/2024-03-06-Denoising Masked Autoencoders Help Robust Classification.markdown",
      "date": "7 Mar 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/2024-03-06-Hard Patches Mining for Masked Image Modeling.markdown",
      "date": "12 Apr 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "form": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "distillation": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/2024-03-03-DINOv2 Learning Robust Visual Features without Supervision.markdown",
      "date": "2 Feb 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/2024-03-05-A Closer Look at Self-Supervised Lightweight Vision Transformers.markdown",
      "date": "3 May 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "synergy": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/2024-03-03-Dino Emerging Properties in Self-Supervised Vision Transformers.markdown",
      "date": "24 May 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "dinov2": [
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/2024-03-03-DINOv2 Learning Robust Visual Features without Supervision.markdown",
      "date": "2 Feb 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    }
  ],
  "creation": [
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/2024-03-03-DINOv2 Learning Robust Visual Features without Supervision.markdown",
      "date": "2 Feb 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    }
  ],
  "tuning": [
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/2024-03-03-DINOv2 Learning Robust Visual Features without Supervision.markdown",
      "date": "2 Feb 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/2024-03-03-PatchDropout Economizing Vision Transformers Using Patch Dropou.markdown",
      "date": "4 Oct 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/2024-03-04-On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning.markdown",
      "date": "26 Jun 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/2024-03-05-Adversarially Self-supervised Pre-training Improves Accuracy and Robustness.markdown",
      "date": "10 Mar 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/2024-03-05-Colorization as a Proxy Task for Visual Understanding.markdown",
      "date": "13 Aug 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/2024-03-05-Pre-training Vision Transformers with Very Limited Synthesized Images.markdown",
      "date": "31 Jul 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/2024-03-05-Transformer-Based Visual Segmentation A Surve.markdown",
      "date": "20 Dec 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/2024-03-06-(Certified!!) Adversarial Robustness for Free.markdown",
      "date": "6 Mar 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    }
  ],
  "curate": [
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/2024-03-03-DINOv2 Learning Robust Visual Features without Supervision.markdown",
      "date": "2 Feb 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    }
  ],
  "includes": [
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/2024-03-03-DINOv2 Learning Robust Visual Features without Supervision.markdown",
      "date": "2 Feb 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/2024-03-04-On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning.markdown",
      "date": "26 Jun 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    }
  ],
  "stability": [
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/2024-03-03-DINOv2 Learning Robust Visual Features without Supervision.markdown",
      "date": "2 Feb 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/2024-03-03-How Does Batch Normalization Help Optimization.markdown",
      "date": "15 Apr 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/2024-03-04-Dropout Reduces Underfittin.markdown",
      "date": "31 May 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/2024-03-05-An Empirical Study of Training Self-Supervised Vision Transformers.markdown",
      "date": "16 Aug 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    }
  ],
  "automate": [
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/2024-03-03-DINOv2 Learning Robust Visual Features without Supervision.markdown",
      "date": "2 Feb 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/2024-03-04-RandAugment Practical automated data augmentation with a reduced search space.markdown",
      "date": "14 Nov 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    }
  ],
  "pipeline": [
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/2024-03-03-DINOv2 Learning Robust Visual Features without Supervision.markdown",
      "date": "2 Feb 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    }
  ],
  "building": [
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/2024-03-03-DINOv2 Learning Robust Visual Features without Supervision.markdown",
      "date": "2 Feb 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/2024-03-04-ON INTERACTION BETWEEN AUGMENTATIONS AND CORRUPTIONS IN NATURAL CORRUPTION ROBUSTNESS.markdown",
      "date": "19 Nov 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    }
  ],
  "sequence": [
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/2024-03-03-Efficiently Modeling Long Sequences with Structured State Spaces.markdown",
      "date": "5 Aug 2022",
      "summary": "he quest for a universal sequence model capable of handling data across different tasks and modalities, especially long-range dependencies, faces challenges with conventional models like RNNs, CNNs, and Transformers, which struggle with very long sequences. A promising approach using state space models (SSM) showed potential but had high computational demands. We introduce the Structured State Space sequence model (S4), a more efficient parameterization of SSM, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    },
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/2024-03-03-Hungry Hungry Hippos Towards Language Modeling with State Space Models.markdown",
      "date": "29 Apr 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/2024-03-03-Mamba Linear-Time Sequence Modeling with Selective State Spaces.markdown",
      "date": "1 Dec 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    },
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/2024-03-03-MambaByte Token-free Selective State Space Model.markdown",
      "date": "24 Jan 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    },
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/2024-03-04-On Warm-Starting Neural Network Training.markdown",
      "date": "31 Dec 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/2024-03-05-An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.markdown",
      "date": "3 Jun 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/2024-03-05-Vision Transformers Need Register.markdown",
      "date": "28 Sep 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    }
  ],
  "structure": [
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/2024-03-03-Efficiently Modeling Long Sequences with Structured State Spaces.markdown",
      "date": "5 Aug 2022",
      "summary": "he quest for a universal sequence model capable of handling data across different tasks and modalities, especially long-range dependencies, faces challenges with conventional models like RNNs, CNNs, and Transformers, which struggle with very long sequences. A promising approach using state space models (SSM) showed potential but had high computational demands. We introduce the Structured State Space sequence model (S4), a more efficient parameterization of SSM, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    },
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/2024-03-04-Data is Overrated Perceptual Metrics Can Lead Learning in the Absence of Training Data.markdown",
      "date": "6 Dec 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/2024-03-04-Scheduled DropHead A Regularization Method for Transformer Models.markdown",
      "date": "1 Nov 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/2024-03-05-Designing Network Design Spaces.markdown",
      "date": "30 Mar 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/2024-03-05-The geometry of hidden representations of large transformer models.markdown",
      "date": "30 Oct 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "quest": [
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/2024-03-03-Efficiently Modeling Long Sequences with Structured State Spaces.markdown",
      "date": "5 Aug 2022",
      "summary": "he quest for a universal sequence model capable of handling data across different tasks and modalities, especially long-range dependencies, faces challenges with conventional models like RNNs, CNNs, and Transformers, which struggle with very long sequences. A promising approach using state space models (SSM) showed potential but had high computational demands. We introduce the Structured State Space sequence model (S4), a more efficient parameterization of SSM, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    }
  ],
  "handling": [
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/2024-03-03-Efficiently Modeling Long Sequences with Structured State Spaces.markdown",
      "date": "5 Aug 2022",
      "summary": "he quest for a universal sequence model capable of handling data across different tasks and modalities, especially long-range dependencies, faces challenges with conventional models like RNNs, CNNs, and Transformers, which struggle with very long sequences. A promising approach using state space models (SSM) showed potential but had high computational demands. We introduce the Structured State Space sequence model (S4), a more efficient parameterization of SSM, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/2024-03-03-Mamba Linear-Time Sequence Modeling with Selective State Spaces.markdown",
      "date": "1 Dec 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    }
  ],
  "modality": [
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/2024-03-03-Efficiently Modeling Long Sequences with Structured State Spaces.markdown",
      "date": "5 Aug 2022",
      "summary": "he quest for a universal sequence model capable of handling data across different tasks and modalities, especially long-range dependencies, faces challenges with conventional models like RNNs, CNNs, and Transformers, which struggle with very long sequences. A promising approach using state space models (SSM) showed potential but had high computational demands. We introduce the Structured State Space sequence model (S4), a more efficient parameterization of SSM, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    }
  ],
  "dependency": [
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/2024-03-03-Efficiently Modeling Long Sequences with Structured State Spaces.markdown",
      "date": "5 Aug 2022",
      "summary": "he quest for a universal sequence model capable of handling data across different tasks and modalities, especially long-range dependencies, faces challenges with conventional models like RNNs, CNNs, and Transformers, which struggle with very long sequences. A promising approach using state space models (SSM) showed potential but had high computational demands. We introduce the Structured State Space sequence model (S4), a more efficient parameterization of SSM, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    }
  ],
  "face": [
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/2024-03-03-Efficiently Modeling Long Sequences with Structured State Spaces.markdown",
      "date": "5 Aug 2022",
      "summary": "he quest for a universal sequence model capable of handling data across different tasks and modalities, especially long-range dependencies, faces challenges with conventional models like RNNs, CNNs, and Transformers, which struggle with very long sequences. A promising approach using state space models (SSM) showed potential but had high computational demands. We introduce the Structured State Space sequence model (S4), a more efficient parameterization of SSM, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    },
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/2024-03-04-Dropout A Simple Way to Prevent Neural Networks from Overfitting.markdown",
      "date": "1 Jan 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "rnns": [
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/2024-03-03-Efficiently Modeling Long Sequences with Structured State Spaces.markdown",
      "date": "5 Aug 2022",
      "summary": "he quest for a universal sequence model capable of handling data across different tasks and modalities, especially long-range dependencies, faces challenges with conventional models like RNNs, CNNs, and Transformers, which struggle with very long sequences. A promising approach using state space models (SSM) showed potential but had high computational demands. We introduce the Structured State Space sequence model (S4), a more efficient parameterization of SSM, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    }
  ],
  "ssm": [
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/2024-03-03-Efficiently Modeling Long Sequences with Structured State Spaces.markdown",
      "date": "5 Aug 2022",
      "summary": "he quest for a universal sequence model capable of handling data across different tasks and modalities, especially long-range dependencies, faces challenges with conventional models like RNNs, CNNs, and Transformers, which struggle with very long sequences. A promising approach using state space models (SSM) showed potential but had high computational demands. We introduce the Structured State Space sequence model (S4), a more efficient parameterization of SSM, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    },
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/2024-03-03-Hungry Hungry Hippos Towards Language Modeling with State Space Models.markdown",
      "date": "29 Apr 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/2024-03-03-Mamba Linear-Time Sequence Modeling with Selective State Spaces.markdown",
      "date": "1 Dec 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    }
  ],
  "s4": [
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/2024-03-03-Efficiently Modeling Long Sequences with Structured State Spaces.markdown",
      "date": "5 Aug 2022",
      "summary": "he quest for a universal sequence model capable of handling data across different tasks and modalities, especially long-range dependencies, faces challenges with conventional models like RNNs, CNNs, and Transformers, which struggle with very long sequences. A promising approach using state space models (SSM) showed potential but had high computational demands. We introduce the Structured State Space sequence model (S4), a more efficient parameterization of SSM, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    }
  ],
  "parameterization": [
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/2024-03-03-Efficiently Modeling Long Sequences with Structured State Spaces.markdown",
      "date": "5 Aug 2022",
      "summary": "he quest for a universal sequence model capable of handling data across different tasks and modalities, especially long-range dependencies, faces challenges with conventional models like RNNs, CNNs, and Transformers, which struggle with very long sequences. A promising approach using state space models (SSM) showed potential but had high computational demands. We introduce the Structured State Space sequence model (S4), a more efficient parameterization of SSM, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    }
  ],
  "distribute": [
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/2024-03-03-Extreme Masking for Learning Instance and Distributed Visual Representations.markdown",
      "date": "8 Mar 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    }
  ],
  "extrema": [
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/2024-03-03-Extreme Masking for Learning Instance and Distributed Visual Representations.markdown",
      "date": "8 Mar 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    }
  ],
  "attention": [
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/2024-03-03-Extreme Masking for Learning Instance and Distributed Visual Representations.markdown",
      "date": "8 Mar 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    },
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/2024-03-03-Hungry Hungry Hippos Towards Language Modeling with State Space Models.markdown",
      "date": "29 Apr 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/2024-03-03-Vision Mamba Efficient Visual Representation Learning with Bidirectional State Space Model.markdown",
      "date": "10 Feb 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/2024-03-04-DropAttention A Regularization Method for Fully-Connected Self-Attention Networks.markdown",
      "date": "26 Jul 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. Unlike existing dropout techniques for other neural network layers, DropAttention specifically targets the unique challenges of self-attention mechanisms. Our experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    },
    {
      "title": "DropKey",
      "url": "/2024-03-04-DropKey.markdown",
      "date": "11 Apr 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/2024-03-04-Scheduled DropHead A Regularization Method for Transformer Models.markdown",
      "date": "1 Nov 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/2024-03-04-What do Self-supervised vision transformers learn.markdown",
      "date": "1 May 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/2024-03-05-A Closer Look at Self-Supervised Lightweight Vision Transformers.markdown",
      "date": "3 May 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/2024-03-05-An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.markdown",
      "date": "3 Jun 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/2024-03-05-Transformer-Based Visual Segmentation A Surve.markdown",
      "date": "20 Dec 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/2024-03-05-Vision Transformers Need Register.markdown",
      "date": "28 Sep 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    }
  ],
  "cross": [
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/2024-03-03-Extreme Masking for Learning Instance and Distributed Visual Representations.markdown",
      "date": "8 Mar 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/2024-03-03-VMamba Visual State Space Model.markdown",
      "date": "18 Jan 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "block": [
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/2024-03-03-Extreme Masking for Learning Instance and Distributed Visual Representations.markdown",
      "date": "8 Mar 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/2024-03-03-I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture.markdown",
      "date": "13 Apr 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/2024-03-03-Vision Mamba Efficient Visual Representation Learning with Bidirectional State Space Model.markdown",
      "date": "10 Feb 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/2024-03-04-Progressive Random Convolutions for Single Domain Generalization.markdown",
      "date": "2 Apr 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/2024-03-04-White-Box Transformers via Sparse Rate Reduction.markdown",
      "date": "1 Jun 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "seek": [
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/2024-03-03-Extreme Masking for Learning Instance and Distributed Visual Representations.markdown",
      "date": "8 Mar 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    }
  ],
  "invariance": [
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/2024-03-03-Extreme Masking for Learning Instance and Distributed Visual Representations.markdown",
      "date": "8 Mar 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/2024-03-04-ON INTERACTION BETWEEN AUGMENTATIONS AND CORRUPTIONS IN NATURAL CORRUPTION ROBUSTNESS.markdown",
      "date": "19 Nov 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/2024-03-04-Understanding Masked Image Modeling via Learning Occlusion Invariant Feature.markdown",
      "date": "8 Aug 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    }
  ],
  "siamese": [
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/2024-03-03-Extreme Masking for Learning Instance and Distributed Visual Representations.markdown",
      "date": "8 Mar 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/2024-03-04-Understanding Masked Image Modeling via Learning Occlusion Invariant Feature.markdown",
      "date": "8 Aug 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    }
  ],
  "accelerates": [
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/2024-03-03-Extreme Masking for Learning Instance and Distributed Visual Representations.markdown",
      "date": "8 Mar 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    }
  ],
  "feedforward": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/2024-03-03-Fast Feedforward Networks.markdown",
      "date": "18 Sep 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/2024-03-04-Understanding the difficulty of training deep feedforward neural networks.markdown",
      "date": "31 Mar 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    }
  ],
  "break": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/2024-03-03-Fast Feedforward Networks.markdown",
      "date": "18 Sep 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    }
  ],
  "link": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/2024-03-03-Fast Feedforward Networks.markdown",
      "date": "18 Sep 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/2024-03-05-Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions.markdown",
      "date": "20 Apr 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    }
  ],
  "layer": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/2024-03-03-Fast Feedforward Networks.markdown",
      "date": "18 Sep 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    },
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/2024-03-03-How Does Batch Normalization Help Optimization.markdown",
      "date": "15 Apr 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/2024-03-03-Hungry Hungry Hippos Towards Language Modeling with State Space Models.markdown",
      "date": "29 Apr 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/2024-03-04-Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.markdown",
      "date": "2 Mar 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/2024-03-04-DropAttention A Regularization Method for Fully-Connected Self-Attention Networks.markdown",
      "date": "26 Jul 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. Unlike existing dropout techniques for other neural network layers, DropAttention specifically targets the unique challenges of self-attention mechanisms. Our experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    },
    {
      "title": "DropKey",
      "url": "/2024-03-04-DropKey.markdown",
      "date": "11 Apr 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Layer Normalization",
      "url": "/2024-03-04-Layer Normalization.markdown",
      "date": "21 Jul 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/2024-03-04-Progressive Random Convolutions for Single Domain Generalization.markdown",
      "date": "2 Apr 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/2024-03-04-Understanding the difficulty of training deep feedforward neural networks.markdown",
      "date": "31 Mar 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/2024-03-04-What do Self-supervised vision transformers learn.markdown",
      "date": "1 May 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/2024-03-05-A Closer Look at Self-Supervised Lightweight Vision Transformers.markdown",
      "date": "3 May 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/2024-03-05-Adversarially Self-supervised Pre-training Improves Accuracy and Robustness.markdown",
      "date": "10 Mar 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/2024-03-05-Masked Image Residual Learning for Scaling Deeper Vision Transformers.markdown",
      "date": "15 Nov 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/2024-03-05-The geometry of hidden representations of large transformer models.markdown",
      "date": "30 Oct 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "cost": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/2024-03-03-Fast Feedforward Networks.markdown",
      "date": "18 Sep 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    },
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/2024-03-04-A downsampled variant of imagenet as an alternative to the cifar datasets.markdown",
      "date": "23 Aug 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    },
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/2024-03-04-Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classificatio.markdown",
      "date": "6 Feb 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/2024-03-04-RandAugment Practical automated data augmentation with a reduced search space.markdown",
      "date": "14 Nov 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/2024-03-05-An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.markdown",
      "date": "3 Jun 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/2024-03-05-Masked Image Residual Learning for Scaling Deeper Vision Transformers.markdown",
      "date": "15 Nov 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "feedforward1": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/2024-03-03-Fast Feedforward Networks.markdown",
      "date": "18 Sep 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    }
  ],
  "fff": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/2024-03-03-Fast Feedforward Networks.markdown",
      "date": "18 Sep 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    }
  ],
  "log": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/2024-03-03-Fast Feedforward Networks.markdown",
      "date": "18 Sep 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    }
  ],
  "x": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/2024-03-03-Fast Feedforward Networks.markdown",
      "date": "18 Sep 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    }
  ],
  "mixture": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/2024-03-03-Fast Feedforward Networks.markdown",
      "date": "18 Sep 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    }
  ],
  "expert": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/2024-03-03-Fast Feedforward Networks.markdown",
      "date": "18 Sep 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    }
  ],
  "thank": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/2024-03-03-Fast Feedforward Networks.markdown",
      "date": "18 Sep 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    }
  ],
  "push": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/2024-03-03-Fast Feedforward Networks.markdown",
      "date": "18 Sep 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    },
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/2024-03-04-Sigmoid Loss for Language Image Pre-Training.markdown",
      "date": "27 sept 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    }
  ],
  "neuron": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/2024-03-03-Fast Feedforward Networks.markdown",
      "date": "18 Sep 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    },
    {
      "title": "Layer Normalization",
      "url": "/2024-03-04-Layer Normalization.markdown",
      "date": "21 Jul 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    }
  ],
  "help": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/2024-03-03-How Does Batch Normalization Help Optimization.markdown",
      "date": "15 Apr 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "Group Normalization",
      "url": "/2024-03-04-Group Normalization.markdown",
      "date": "1 Jun 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/2024-03-06-Denoising Masked Autoencoders Help Robust Classification.markdown",
      "date": "7 Mar 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    }
  ],
  "batch normalization": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/2024-03-03-How Does Batch Normalization Help Optimization.markdown",
      "date": "15 Apr 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "Rethinking Batch in BatchNorm",
      "url": "/2024-03-04-Rethinking Batch in BatchNorm.markdown",
      "date": "17 May 2021",
      "summary": "BatchNorm, essential in convolutional neural networks, behaves uniquely due to its batch-based operation, leading to performance issues in visual recognition tasks. This paper identifies these issues and suggests reevaluating the concept of batch in BatchNorm for better performance, aiming to guide researchers in its effective use."
    }
  ],
  "adopt": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/2024-03-03-How Does Batch Normalization Help Optimization.markdown",
      "date": "15 Apr 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/2024-03-05-Pre-training Vision Transformers with Very Limited Synthesized Images.markdown",
      "date": "31 Jul 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    }
  ],
  "pervasiveness": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/2024-03-03-How Does Batch Normalization Help Optimization.markdown",
      "date": "15 Apr 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    }
  ],
  "reason": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/2024-03-03-How Does Batch Normalization Help Optimization.markdown",
      "date": "15 Apr 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    }
  ],
  "belief": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/2024-03-03-How Does Batch Normalization Help Optimization.markdown",
      "date": "15 Apr 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    }
  ],
  "stem": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/2024-03-03-How Does Batch Normalization Help Optimization.markdown",
      "date": "15 Apr 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    }
  ],
  "layers": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/2024-03-03-How Does Batch Normalization Help Optimization.markdown",
      "date": "15 Apr 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/2024-03-04-DropAttention A Regularization Method for Fully-Connected Self-Attention Networks.markdown",
      "date": "26 Jul 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. Unlike existing dropout techniques for other neural network layers, DropAttention specifically targets the unique challenges of self-attention mechanisms. Our experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    }
  ],
  "distribution": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/2024-03-03-How Does Batch Normalization Help Optimization.markdown",
      "date": "15 Apr 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/2024-03-04-Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.markdown",
      "date": "2 Mar 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/2024-03-04-Delving Deep into Label Smoothing.markdown",
      "date": "22 Jul 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/2024-03-04-Impact of Noise on Calibration and Generalisation of Neural Networks.markdown",
      "date": "30 Jun 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. We examine different types of noise, such as activation and input augmentation noise, in both in-distribution and out-of-distribution scenarios. Activation noise significantly improves generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/2024-03-04-White-Box Transformers via Sparse Rate Reduction.markdown",
      "date": "1 Jun 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/2024-03-05-Adversarially Self-supervised Pre-training Improves Accuracy and Robustness.markdown",
      "date": "10 Mar 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/2024-03-05-Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions.markdown",
      "date": "20 Apr 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Segment Anything",
      "url": "/2024-03-05-Segment Anything.markdown",
      "date": "5 Apr 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    }
  ],
  "covariate": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/2024-03-03-How Does Batch Normalization Help Optimization.markdown",
      "date": "15 Apr 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/2024-03-04-Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.markdown",
      "date": "2 Mar 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    }
  ],
  "uncover": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/2024-03-03-How Does Batch Normalization Help Optimization.markdown",
      "date": "15 Apr 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    }
  ],
  "landscape": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/2024-03-03-How Does Batch Normalization Help Optimization.markdown",
      "date": "15 Apr 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    }
  ],
  "induce": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/2024-03-03-How Does Batch Normalization Help Optimization.markdown",
      "date": "15 Apr 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    }
  ],
  "behavior": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/2024-03-03-How Does Batch Normalization Help Optimization.markdown",
      "date": "15 Apr 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/2024-03-04-Data is Overrated Perceptual Metrics Can Lead Learning in the Absence of Training Data.markdown",
      "date": "6 Dec 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    }
  ],
  "hippo": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/2024-03-03-Hungry Hungry Hippos Towards Language Modeling with State Space Models.markdown",
      "date": "29 Apr 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    }
  ],
  "gap": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/2024-03-03-Hungry Hungry Hippos Towards Language Modeling with State Space Models.markdown",
      "date": "29 Apr 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/2024-03-04-On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning.markdown",
      "date": "26 Jun 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    }
  ],
  "length": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/2024-03-03-Hungry Hungry Hippos Towards Language Modeling with State Space Models.markdown",
      "date": "29 Apr 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    }
  ],
  "underperform": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/2024-03-03-Hungry Hungry Hippos Towards Language Modeling with State Space Models.markdown",
      "date": "29 Apr 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    }
  ],
  "suffer": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/2024-03-03-Hungry Hungry Hippos Towards Language Modeling with State Space Models.markdown",
      "date": "29 Apr 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/2024-03-03-Mamba Linear-Time Sequence Modeling with Selective State Spaces.markdown",
      "date": "1 Dec 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    }
  ],
  "hardware": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/2024-03-03-Hungry Hungry Hippos Towards Language Modeling with State Space Models.markdown",
      "date": "29 Apr 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    }
  ],
  "utilization": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/2024-03-03-Hungry Hungry Hippos Towards Language Modeling with State Space Models.markdown",
      "date": "29 Apr 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    }
  ],
  "h3": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/2024-03-03-Hungry Hungry Hippos Towards Language Modeling with State Space Models.markdown",
      "date": "29 Apr 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    }
  ],
  "recall": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/2024-03-03-Hungry Hungry Hippos Towards Language Modeling with State Space Models.markdown",
      "date": "29 Apr 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    }
  ],
  "comparison": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/2024-03-03-Hungry Hungry Hippos Towards Language Modeling with State Space Models.markdown",
      "date": "29 Apr 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/2024-03-05-Understanding Self-Supervised Pretraining with Part-Aware Representation Learning.markdown",
      "date": "23 Jan 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    }
  ],
  "narrow": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/2024-03-03-Hungry Hungry Hippos Towards Language Modeling with State Space Models.markdown",
      "date": "29 Apr 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    }
  ],
  "h3attention": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/2024-03-03-Hungry Hungry Hippos Towards Language Modeling with State Space Models.markdown",
      "date": "29 Apr 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    }
  ],
  "openwebtext": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/2024-03-03-Hungry Hungry Hippos Towards Language Modeling with State Space Models.markdown",
      "date": "29 Apr 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    }
  ],
  "flashconv": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/2024-03-03-Hungry Hungry Hippos Towards Language Modeling with State Space Models.markdown",
      "date": "29 Apr 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    }
  ],
  "speeds": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/2024-03-03-Hungry Hungry Hippos Towards Language Modeling with State Space Models.markdown",
      "date": "29 Apr 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    }
  ],
  "and": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/2024-03-03-Hungry Hungry Hippos Towards Language Modeling with State Space Models.markdown",
      "date": "29 Apr 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    }
  ],
  "enables": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/2024-03-03-Hungry Hungry Hippos Towards Language Modeling with State Space Models.markdown",
      "date": "29 Apr 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    }
  ],
  "perplexity": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/2024-03-03-Hungry Hungry Hippos Towards Language Modeling with State Space Models.markdown",
      "date": "29 Apr 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "shot": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/2024-03-03-Hungry Hungry Hippos Towards Language Modeling with State Space Models.markdown",
      "date": "29 Apr 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    }
  ],
  "ijepa": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/2024-03-03-I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture.markdown",
      "date": "13 Apr 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    }
  ],
  "joint": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/2024-03-03-I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture.markdown",
      "date": "13 Apr 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    }
  ],
  "embedding": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/2024-03-03-I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture.markdown",
      "date": "13 Apr 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/2024-03-03-Vision Mamba Efficient Visual Representation Learning with Bidirectional State Space Model.markdown",
      "date": "10 Feb 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    }
  ],
  "generate": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/2024-03-03-I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture.markdown",
      "date": "13 Apr 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/2024-03-04-Delving Deep into Label Smoothing.markdown",
      "date": "22 Jul 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "DROPOUT AS DATA AUGMENTATION",
      "url": "/2024-03-04-DROPOUT AS DATA AUGMENTATION.markdown",
      "date": "8 Jan 2016",
      "summary": "Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/2024-03-05-Deeper Insights into the Robustness of ViTs towards Common Corruptions.markdown",
      "date": "19 Aug 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    }
  ],
  "scalability": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/2024-03-03-I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture.markdown",
      "date": "13 Apr 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/2024-03-03-Mamba Linear-Time Sequence Modeling with Selective State Spaces.markdown",
      "date": "1 Dec 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    }
  ],
  "depth": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/2024-03-03-I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture.markdown",
      "date": "13 Apr 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "Learning RelU Networks To High Uniform Accuracy Is Intractable",
      "url": "/2024-03-04-Learning Relu Networks To High Uniform Accuracy Is Intractable.markdown",
      "date": "28 Feb 2023",
      "summary": "Statistical learning theory provides guidelines on the number of training samples needed for achieving desired accuracy in learning problems, especially emphasizing generalization error. However, this is not always adequate, particularly in security-sensitive areas or computational sciences, where uniform accuracy across all inputs is necessary. This paper quantifies the training samples required for uniform accuracy in learning problems involving ReLU neural networks, revealing that the number of samples needed exponentially increases with the network's depth and input dimension."
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/2024-03-04-Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution.markdown",
      "date": "25 Oct 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/2024-03-05-Designing Network Design Spaces.markdown",
      "date": "30 Mar 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/2024-03-05-Masked Image Residual Learning for Scaling Deeper Vision Transformers.markdown",
      "date": "15 Nov 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    }
  ],
  "huge14": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/2024-03-03-I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture.markdown",
      "date": "13 Apr 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    }
  ],
  "a100": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/2024-03-03-I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture.markdown",
      "date": "13 Apr 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    }
  ],
  "gpus": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/2024-03-03-I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture.markdown",
      "date": "13 Apr 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    }
  ],
  "plasticity": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/2024-03-03-Loss of Plasticity in Deep Continual Learning.markdown",
      "date": "18 Aug 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    }
  ],
  "environment": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/2024-03-03-Loss of Plasticity in Deep Continual Learning.markdown",
      "date": "18 Aug 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    }
  ],
  "forget": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/2024-03-03-Loss of Plasticity in Deep Continual Learning.markdown",
      "date": "18 Aug 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    }
  ],
  "ability": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/2024-03-03-Loss of Plasticity in Deep Continual Learning.markdown",
      "date": "18 Aug 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/2024-03-06-Hard Patches Mining for Masked Image Modeling.markdown",
      "date": "12 Apr 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "know": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/2024-03-03-Loss of Plasticity in Deep Continual Learning.markdown",
      "date": "18 Aug 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/2024-03-04-Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.markdown",
      "date": "2 Mar 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    }
  ],
  "mnist": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/2024-03-03-Loss of Plasticity in Deep Continual Learning.markdown",
      "date": "18 Aug 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    }
  ],
  "fall": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/2024-03-03-Loss of Plasticity in Deep Continual Learning.markdown",
      "date": "18 Aug 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/2024-03-05-Deeper Insights into the Robustness of ViTs towards Common Corruptions.markdown",
      "date": "19 Aug 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    }
  ],
  "l2regularization": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/2024-03-03-Loss of Plasticity in Deep Continual Learning.markdown",
      "date": "18 Aug 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    }
  ],
  "perturbation": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/2024-03-03-Loss of Plasticity in Deep Continual Learning.markdown",
      "date": "18 Aug 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    },
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/2024-03-03-Robust and Generalizable Visual Representation Learning via Random Convolutions.markdown",
      "date": "3 May 2021",
      "summary": "This study demonstrates that deep neural networks' vulnerability to texture shifts and perturbations can be mitigated by using random convolutions for data augmentation. Random convolutions, which approximately preserve shape while distorting local textures, create an infinite variety of new domains. We tested using these convolutions as new images or in combination with original images during training. Our method significantly enhances performance on unseen domains and domain generalization benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods. Additionally, it provides a more robust pretrained visual representation for downstream tasks."
    },
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/2024-03-04-Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations.markdown",
      "date": "27 Apr 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/2024-03-05-Adversarially Self-supervised Pre-training Improves Accuracy and Robustness.markdown",
      "date": "10 Mar 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/2024-03-05-Universal Adversarial Robustness of Texture and Shape-biased Models.markdown",
      "date": "31 Aug 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/2024-03-06-(Certified!!) Adversarial Robustness for Free.markdown",
      "date": "6 Mar 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    }
  ],
  "alter": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/2024-03-03-Loss of Plasticity in Deep Continual Learning.markdown",
      "date": "18 Aug 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    }
  ],
  "reinitialize": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/2024-03-03-Loss of Plasticity in Deep Continual Learning.markdown",
      "date": "18 Aug 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    }
  ],
  "promise": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/2024-03-03-Loss of Plasticity in Deep Continual Learning.markdown",
      "date": "18 Aug 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    }
  ],
  "mamba": [
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/2024-03-03-Mamba Linear-Time Sequence Modeling with Selective State Spaces.markdown",
      "date": "1 Dec 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/2024-03-03-Vision Mamba Efficient Visual Representation Learning with Bidirectional State Space Model.markdown",
      "date": "10 Feb 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    }
  ],
  "inefficiency": [
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/2024-03-03-Mamba Linear-Time Sequence Modeling with Selective State Spaces.markdown",
      "date": "1 Dec 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    }
  ],
  "reasoning": [
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/2024-03-03-Mamba Linear-Time Sequence Modeling with Selective State Spaces.markdown",
      "date": "1 Dec 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    }
  ],
  "improving": [
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/2024-03-03-Mamba Linear-Time Sequence Modeling with Selective State Spaces.markdown",
      "date": "1 Dec 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    }
  ],
  "domain": [
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/2024-03-03-Mamba Linear-Time Sequence Modeling with Selective State Spaces.markdown",
      "date": "1 Dec 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    },
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/2024-03-03-Robust and Generalizable Visual Representation Learning via Random Convolutions.markdown",
      "date": "3 May 2021",
      "summary": "This study demonstrates that deep neural networks' vulnerability to texture shifts and perturbations can be mitigated by using random convolutions for data augmentation. Random convolutions, which approximately preserve shape while distorting local textures, create an infinite variety of new domains. We tested using these convolutions as new images or in combination with original images during training. Our method significantly enhances performance on unseen domains and domain generalization benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods. Additionally, it provides a more robust pretrained visual representation for downstream tasks."
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/2024-03-03-VMamba Visual State Space Model.markdown",
      "date": "18 Jan 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/2024-03-04-Data is Overrated Perceptual Metrics Can Lead Learning in the Absence of Training Data.markdown",
      "date": "6 Dec 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    },
    {
      "title": "DROPOUT AS DATA AUGMENTATION",
      "url": "/2024-03-04-DROPOUT AS DATA AUGMENTATION.markdown",
      "date": "8 Jan 2016",
      "summary": "Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/2024-03-04-Progressive Random Convolutions for Single Domain Generalization.markdown",
      "date": "2 Apr 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/2024-03-05-Deeper Insights into the Robustness of ViTs towards Common Corruptions.markdown",
      "date": "19 Aug 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    }
  ],
  "audio": [
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/2024-03-03-Mamba Linear-Time Sequence Modeling with Selective State Spaces.markdown",
      "date": "1 Dec 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    },
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/2024-03-04-Data is Overrated Perceptual Metrics Can Lead Learning in the Absence of Training Data.markdown",
      "date": "6 Dec 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    }
  ],
  "genomic": [
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/2024-03-03-Mamba Linear-Time Sequence Modeling with Selective State Spaces.markdown",
      "date": "1 Dec 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    }
  ],
  "involve": [
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/2024-03-03-Mamba Linear-Time Sequence Modeling with Selective State Spaces.markdown",
      "date": "1 Dec 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    },
    {
      "title": "Learning RelU Networks To High Uniform Accuracy Is Intractable",
      "url": "/2024-03-04-Learning Relu Networks To High Uniform Accuracy Is Intractable.markdown",
      "date": "28 Feb 2023",
      "summary": "Statistical learning theory provides guidelines on the number of training samples needed for achieving desired accuracy in learning problems, especially emphasizing generalization error. However, this is not always adequate, particularly in security-sensitive areas or computational sciences, where uniform accuracy across all inputs is necessary. This paper quantifies the training samples required for uniform accuracy in learning problems involving ReLU neural networks, revealing that the number of samples needed exponentially increases with the network's depth and input dimension."
    }
  ],
  "mambabyte": [
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/2024-03-03-MambaByte Token-free Selective State Space Model.markdown",
      "date": "24 Jan 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    }
  ],
  "operate": [
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/2024-03-03-MambaByte Token-free Selective State Space Model.markdown",
      "date": "24 Jan 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "byte": [
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/2024-03-03-MambaByte Token-free Selective State Space Model.markdown",
      "date": "24 Jan 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    }
  ],
  "subword": [
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/2024-03-03-MambaByte Token-free Selective State Space Model.markdown",
      "date": "24 Jan 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    }
  ],
  "tokenization": [
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/2024-03-03-MambaByte Token-free Selective State Space Model.markdown",
      "date": "24 Jan 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    }
  ],
  "bias": [
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/2024-03-03-MambaByte Token-free Selective State Space Model.markdown",
      "date": "24 Jan 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Layer Normalization",
      "url": "/2024-03-04-Layer Normalization.markdown",
      "date": "21 Jul 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/2024-03-05-Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions.markdown",
      "date": "20 Apr 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/2024-03-05-Universal Adversarial Robustness of Texture and Shape-biased Models.markdown",
      "date": "31 Aug 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    }
  ],
  "patchdropout": [
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/2024-03-03-PatchDropout Economizing Vision Transformers Using Patch Dropou.markdown",
      "date": "4 Oct 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    }
  ],
  "economize": [
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/2024-03-03-PatchDropout Economizing Vision Transformers Using Patch Dropou.markdown",
      "date": "4 Oct 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    }
  ],
  "dropout": [
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/2024-03-03-PatchDropout Economizing Vision Transformers Using Patch Dropou.markdown",
      "date": "4 Oct 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/2024-03-04-Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.markdown",
      "date": "2 Mar 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/2024-03-04-DropAttention A Regularization Method for Fully-Connected Self-Attention Networks.markdown",
      "date": "26 Jul 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. Unlike existing dropout techniques for other neural network layers, DropAttention specifically targets the unique challenges of self-attention mechanisms. Our experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    },
    {
      "title": "DropKey",
      "url": "/2024-03-04-DropKey.markdown",
      "date": "11 Apr 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/2024-03-04-Dropout A Simple Way to Prevent Neural Networks from Overfitting.markdown",
      "date": "1 Jan 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    },
    {
      "title": "DROPOUT AS DATA AUGMENTATION",
      "url": "/2024-03-04-DROPOUT AS DATA AUGMENTATION.markdown",
      "date": "8 Jan 2016",
      "summary": "Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/2024-03-04-Dropout Reduces Underfittin.markdown",
      "date": "31 May 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/2024-03-04-Scheduled DropHead A Regularization Method for Transformer Models.markdown",
      "date": "1 Nov 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    },
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/2024-03-05-Turbo Training with Token Dropout.markdown",
      "date": "10 Oct 2022",
      "summary": "This paper introduces Turbo training, an efficient training method for video-related tasks using Transformers. Turbo training offers three major contributions: Firstly, it presents a simple yet versatile training paradigm applicable to multiple video tasks. Secondly, it demonstrates Turbo training's effectiveness across action classification, video-language representation learning, and long-video activity classification, achieving competitive performance with up to 4× speed-up and reduced memory usage. Thirdly, it enables long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous methods that were resource-intensive."
    }
  ],
  "resolution": [
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/2024-03-03-PatchDropout Economizing Vision Transformers Using Patch Dropou.markdown",
      "date": "4 Oct 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/2024-03-03-Vision Mamba Efficient Visual Representation Learning with Bidirectional State Space Model.markdown",
      "date": "10 Feb 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/2024-03-03-VMamba Visual State Space Model.markdown",
      "date": "18 Jan 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/2024-03-04-A downsampled variant of imagenet as an alternative to the cifar datasets.markdown",
      "date": "23 Aug 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/2024-03-04-Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution.markdown",
      "date": "25 Oct 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "requiring": [
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/2024-03-03-PatchDropout Economizing Vision Transformers Using Patch Dropou.markdown",
      "date": "4 Oct 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    }
  ],
  "drop": [
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/2024-03-03-PatchDropout Economizing Vision Transformers Using Patch Dropou.markdown",
      "date": "4 Oct 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "DropKey",
      "url": "/2024-03-04-DropKey.markdown",
      "date": "11 Apr 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/2024-03-04-Scheduled DropHead A Regularization Method for Transformer Models.markdown",
      "date": "1 Nov 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    }
  ],
  "csaw": [
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/2024-03-03-PatchDropout Economizing Vision Transformers Using Patch Dropou.markdown",
      "date": "4 Oct 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    }
  ],
  "budget": [
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/2024-03-03-PatchDropout Economizing Vision Transformers Using Patch Dropou.markdown",
      "date": "4 Oct 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/2024-03-05-Designing Network Design Spaces.markdown",
      "date": "30 Mar 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    }
  ],
  "vulnerability": [
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/2024-03-03-Robust and Generalizable Visual Representation Learning via Random Convolutions.markdown",
      "date": "3 May 2021",
      "summary": "This study demonstrates that deep neural networks' vulnerability to texture shifts and perturbations can be mitigated by using random convolutions for data augmentation. Random convolutions, which approximately preserve shape while distorting local textures, create an infinite variety of new domains. We tested using these convolutions as new images or in combination with original images during training. Our method significantly enhances performance on unseen domains and domain generalization benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods. Additionally, it provides a more robust pretrained visual representation for downstream tasks."
    }
  ],
  "texture": [
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/2024-03-03-Robust and Generalizable Visual Representation Learning via Random Convolutions.markdown",
      "date": "3 May 2021",
      "summary": "This study demonstrates that deep neural networks' vulnerability to texture shifts and perturbations can be mitigated by using random convolutions for data augmentation. Random convolutions, which approximately preserve shape while distorting local textures, create an infinite variety of new domains. We tested using these convolutions as new images or in combination with original images during training. Our method significantly enhances performance on unseen domains and domain generalization benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods. Additionally, it provides a more robust pretrained visual representation for downstream tasks."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/2024-03-04-Progressive Random Convolutions for Single Domain Generalization.markdown",
      "date": "2 Apr 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/2024-03-05-Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions.markdown",
      "date": "20 Apr 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/2024-03-05-Universal Adversarial Robustness of Texture and Shape-biased Models.markdown",
      "date": "31 Aug 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "shape": [
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/2024-03-03-Robust and Generalizable Visual Representation Learning via Random Convolutions.markdown",
      "date": "3 May 2021",
      "summary": "This study demonstrates that deep neural networks' vulnerability to texture shifts and perturbations can be mitigated by using random convolutions for data augmentation. Random convolutions, which approximately preserve shape while distorting local textures, create an infinite variety of new domains. We tested using these convolutions as new images or in combination with original images during training. Our method significantly enhances performance on unseen domains and domain generalization benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods. Additionally, it provides a more robust pretrained visual representation for downstream tasks."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/2024-03-04-What do Self-supervised vision transformers learn.markdown",
      "date": "1 May 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/2024-03-05-Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions.markdown",
      "date": "20 Apr 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/2024-03-05-Universal Adversarial Robustness of Texture and Shape-biased Models.markdown",
      "date": "31 Aug 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    }
  ],
  "distort": [
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/2024-03-03-Robust and Generalizable Visual Representation Learning via Random Convolutions.markdown",
      "date": "3 May 2021",
      "summary": "This study demonstrates that deep neural networks' vulnerability to texture shifts and perturbations can be mitigated by using random convolutions for data augmentation. Random convolutions, which approximately preserve shape while distorting local textures, create an infinite variety of new domains. We tested using these convolutions as new images or in combination with original images during training. Our method significantly enhances performance on unseen domains and domain generalization benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods. Additionally, it provides a more robust pretrained visual representation for downstream tasks."
    }
  ],
  "variety": [
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/2024-03-03-Robust and Generalizable Visual Representation Learning via Random Convolutions.markdown",
      "date": "3 May 2021",
      "summary": "This study demonstrates that deep neural networks' vulnerability to texture shifts and perturbations can be mitigated by using random convolutions for data augmentation. Random convolutions, which approximately preserve shape while distorting local textures, create an infinite variety of new domains. We tested using these convolutions as new images or in combination with original images during training. Our method significantly enhances performance on unseen domains and domain generalization benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods. Additionally, it provides a more robust pretrained visual representation for downstream tasks."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "sketch": [
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/2024-03-03-Robust and Generalizable Visual Representation Learning via Random Convolutions.markdown",
      "date": "3 May 2021",
      "summary": "This study demonstrates that deep neural networks' vulnerability to texture shifts and perturbations can be mitigated by using random convolutions for data augmentation. Random convolutions, which approximately preserve shape while distorting local textures, create an infinite variety of new domains. We tested using these convolutions as new images or in combination with original images during training. Our method significantly enhances performance on unseen domains and domain generalization benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods. Additionally, it provides a more robust pretrained visual representation for downstream tasks."
    }
  ],
  "downstream": [
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/2024-03-03-Robust and Generalizable Visual Representation Learning via Random Convolutions.markdown",
      "date": "3 May 2021",
      "summary": "This study demonstrates that deep neural networks' vulnerability to texture shifts and perturbations can be mitigated by using random convolutions for data augmentation. Random convolutions, which approximately preserve shape while distorting local textures, create an infinite variety of new domains. We tested using these convolutions as new images or in combination with original images during training. Our method significantly enhances performance on unseen domains and domain generalization benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods. Additionally, it provides a more robust pretrained visual representation for downstream tasks."
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/2024-03-04-What do Self-supervised vision transformers learn.markdown",
      "date": "1 May 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/2024-03-05-A Closer Look at Self-Supervised Lightweight Vision Transformers.markdown",
      "date": "3 May 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/2024-03-05-Beyond Masking Demystifying Token-Based Pre-Training for Vision Transformers.markdown",
      "date": "27 Mar 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learner",
      "url": "/2024-03-05-Masked Autoencoders Are Scalable Vision Learner.markdown",
      "date": "19 Dec 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/2024-03-05-Masked Image Residual Learning for Scaling Deeper Vision Transformers.markdown",
      "date": "15 Nov 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/2024-03-05-Pre-training Vision Transformers with Very Limited Synthesized Images.markdown",
      "date": "31 Jul 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/2024-03-05-Scaling Language-Image Pre-training via Masking.markdown",
      "date": "30 Mar 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/2024-03-05-The geometry of hidden representations of large transformer models.markdown",
      "date": "30 Oct 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/2024-03-05-Vision Transformers Need Register.markdown",
      "date": "28 Sep 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    }
  ],
  "mode": [
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/2024-03-03-Vision Mamba Efficient Visual Representation Learning with Bidirectional State Space Model.markdown",
      "date": "10 Feb 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    }
  ],
  "vim": [
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/2024-03-03-Vision Mamba Efficient Visual Representation Learning with Bidirectional State Space Model.markdown",
      "date": "10 Feb 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    }
  ],
  "backbone": [
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/2024-03-03-Vision Mamba Efficient Visual Representation Learning with Bidirectional State Space Model.markdown",
      "date": "10 Feb 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    }
  ],
  "position": [
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/2024-03-03-Vision Mamba Efficient Visual Representation Learning with Bidirectional State Space Model.markdown",
      "date": "10 Feb 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    }
  ],
  "sensitivity": [
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/2024-03-03-Vision Mamba Efficient Visual Representation Learning with Bidirectional State Space Model.markdown",
      "date": "10 Feb 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/2024-03-03-VMamba Visual State Space Model.markdown",
      "date": "18 Jan 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    }
  ],
  "coco": [
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/2024-03-03-Vision Mamba Efficient Visual Representation Learning with Bidirectional State Space Model.markdown",
      "date": "10 Feb 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    }
  ],
  "deit": [
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/2024-03-03-Vision Mamba Efficient Visual Representation Learning with Bidirectional State Space Model.markdown",
      "date": "10 Feb 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    }
  ],
  "gpu": [
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/2024-03-03-Vision Mamba Efficient Visual Representation Learning with Bidirectional State Space Model.markdown",
      "date": "10 Feb 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/2024-03-05-Designing Network Design Spaces.markdown",
      "date": "30 Mar 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    }
  ],
  "generation": [
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/2024-03-03-Vision Mamba Efficient Visual Representation Learning with Bidirectional State Space Model.markdown",
      "date": "10 Feb 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/2024-03-04-Instance Normalization The Missing Ingredient for Fast Stylization.markdown",
      "date": "6 Nov 2017",
      "summary": "In this study, we improve upon the fast stylization method by Ulyanov et al. (2016) through a minor yet impactful modification: substituting batch normalization with instance normalization during both training and testing. This adjustment significantly enhances the quality of generated images and enables the training of efficient real-time image generation architectures. The code and full paper are available online."
    }
  ],
  "vmamba": [
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/2024-03-03-VMamba Visual State Space Model.markdown",
      "date": "18 Jan 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    }
  ],
  "offering": [
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/2024-03-03-VMamba Visual State Space Model.markdown",
      "date": "18 Jan 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    }
  ],
  "fit": [
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/2024-03-03-VMamba Visual State Space Model.markdown",
      "date": "18 Jan 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/2024-03-04-Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classificatio.markdown",
      "date": "6 Feb 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    }
  ],
  "inspire": [
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/2024-03-03-VMamba Visual State Space Model.markdown",
      "date": "18 Jan 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "fields": [
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/2024-03-03-VMamba Visual State Space Model.markdown",
      "date": "18 Jan 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    }
  ],
  "addition": [
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/2024-03-03-VMamba Visual State Space Model.markdown",
      "date": "18 Jan 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/2024-03-05-Beyond Masking Demystifying Token-Based Pre-Training for Vision Transformers.markdown",
      "date": "27 Mar 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    }
  ],
  "scan": [
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/2024-03-03-VMamba Visual State Space Model.markdown",
      "date": "18 Jan 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    }
  ],
  "csm": [
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/2024-03-03-VMamba Visual State Space Model.markdown",
      "date": "18 Jan 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    }
  ],
  "traversal": [
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/2024-03-03-VMamba Visual State Space Model.markdown",
      "date": "18 Jan 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    }
  ],
  "downsample": [
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/2024-03-04-A downsampled variant of imagenet as an alternative to the cifar datasets.markdown",
      "date": "23 Aug 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    }
  ],
  "version": [
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/2024-03-04-A downsampled variant of imagenet as an alternative to the cifar datasets.markdown",
      "date": "23 Aug 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    },
    {
      "title": "DROPOUT AS DATA AUGMENTATION",
      "url": "/2024-03-04-DROPOUT AS DATA AUGMENTATION.markdown",
      "date": "8 Jan 2016",
      "summary": "Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "imagenet32x32": [
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/2024-03-04-A downsampled variant of imagenet as an alternative to the cifar datasets.markdown",
      "date": "23 Aug 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    }
  ],
  "variants": [
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/2024-03-04-A downsampled variant of imagenet as an alternative to the cifar datasets.markdown",
      "date": "23 Aug 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    }
  ],
  "imagenet64x64": [
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/2024-03-04-A downsampled variant of imagenet as an alternative to the cifar datasets.markdown",
      "date": "23 Aug 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    }
  ],
  "imagenet16x16": [
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/2024-03-04-A downsampled variant of imagenet as an alternative to the cifar datasets.markdown",
      "date": "23 Aug 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    }
  ],
  "class": [
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/2024-03-04-A downsampled variant of imagenet as an alternative to the cifar datasets.markdown",
      "date": "23 Aug 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "hyperparameters": [
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/2024-03-04-A downsampled variant of imagenet as an alternative to the cifar datasets.markdown",
      "date": "23 Aug 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    }
  ],
  "characteristic": [
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/2024-03-04-A downsampled variant of imagenet as an alternative to the cifar datasets.markdown",
      "date": "23 Aug 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    }
  ],
  "script": [
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/2024-03-04-A downsampled variant of imagenet as an alternative to the cifar datasets.markdown",
      "date": "23 Aug 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    }
  ],
  "cause": [
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/2024-03-04-Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.markdown",
      "date": "2 Mar 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    }
  ],
  "slows": [
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/2024-03-04-Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.markdown",
      "date": "2 Mar 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    }
  ],
  "initialization": [
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/2024-03-04-Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.markdown",
      "date": "2 Mar 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/2024-03-04-Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classificatio.markdown",
      "date": "6 Feb 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/2024-03-04-Understanding Masked Image Modeling via Learning Occlusion Invariant Feature.markdown",
      "date": "8 Aug 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/2024-03-04-Understanding the difficulty of training deep feedforward neural networks.markdown",
      "date": "31 Mar 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    }
  ],
  "nonlinearity": [
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/2024-03-04-Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.markdown",
      "date": "2 Mar 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    }
  ],
  "normalize": [
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/2024-03-04-Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.markdown",
      "date": "2 Mar 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Layer Normalization",
      "url": "/2024-03-04-Layer Normalization.markdown",
      "date": "21 Jul 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "improves": [
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/2024-03-04-Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.markdown",
      "date": "2 Mar 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/2024-03-05-Architecture-Agnostic Masked Image Modeling – From ViT back to CNN.markdown",
      "date": "2 Jun 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    }
  ],
  "remove": [
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/2024-03-04-Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.markdown",
      "date": "2 Mar 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    }
  ],
  "ensemble": [
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/2024-03-04-Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.markdown",
      "date": "2 Mar 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/2024-03-05-Universal Adversarial Robustness of Texture and Shape-biased Models.markdown",
      "date": "31 Aug 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    }
  ],
  "record": [
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/2024-03-04-Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.markdown",
      "date": "2 Mar 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    }
  ],
  "surpassing": [
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/2024-03-04-Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.markdown",
      "date": "2 Mar 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/2024-03-04-Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classificatio.markdown",
      "date": "6 Feb 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    }
  ],
  "human": [
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/2024-03-04-Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.markdown",
      "date": "2 Mar 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/2024-03-04-Data is Overrated Perceptual Metrics Can Lead Learning in the Absence of Training Data.markdown",
      "date": "6 Dec 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    },
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/2024-03-04-Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classificatio.markdown",
      "date": "6 Feb 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    },
    {
      "title": "DropKey",
      "url": "/2024-03-04-DropKey.markdown",
      "date": "11 Apr 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/2024-03-04-Reading Digits in Natural Images With Unsupervised Feature Learning.markdown",
      "date": "1 Jan 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    }
  ],
  "corruption": [
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/2024-03-04-Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations.markdown",
      "date": "27 Apr 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/2024-03-04-ON INTERACTION BETWEEN AUGMENTATIONS AND CORRUPTIONS IN NATURAL CORRUPTION ROBUSTNESS.markdown",
      "date": "19 Nov 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/2024-03-04-RandAugment Practical automated data augmentation with a reduced search space.markdown",
      "date": "14 Nov 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/2024-03-05-Deeper Insights into the Robustness of ViTs towards Common Corruptions.markdown",
      "date": "19 Aug 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/2024-03-05-Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions.markdown",
      "date": "20 Apr 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    }
  ],
  "peturbation": [
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/2024-03-04-Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations.markdown",
      "date": "27 Apr 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    }
  ],
  "role": [
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/2024-03-04-Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations.markdown",
      "date": "27 Apr 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    }
  ],
  "p": [
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/2024-03-04-Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations.markdown",
      "date": "27 Apr 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    }
  ],
  "difference": [
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/2024-03-04-Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations.markdown",
      "date": "27 Apr 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/2024-03-04-On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning.markdown",
      "date": "26 Jun 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    }
  ],
  "bypass": [
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/2024-03-04-Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations.markdown",
      "date": "27 Apr 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    }
  ],
  "defense": [
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/2024-03-04-Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations.markdown",
      "date": "27 Apr 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/2024-03-05-Adversarially Self-supervised Pre-training Improves Accuracy and Robustness.markdown",
      "date": "10 Mar 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    }
  ],
  "overrate": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/2024-03-04-Data is Overrated Perceptual Metrics Can Lead Learning in the Absence of Training Data.markdown",
      "date": "6 Dec 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    }
  ],
  "metric": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/2024-03-04-Data is Overrated Perceptual Metrics Can Lead Learning in the Absence of Training Data.markdown",
      "date": "6 Dec 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/2024-03-06-Hard Patches Mining for Masked Image Modeling.markdown",
      "date": "12 Apr 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "absence": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/2024-03-04-Data is Overrated Perceptual Metrics Can Lead Learning in the Absence of Training Data.markdown",
      "date": "6 Dec 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    }
  ],
  "uniform": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/2024-03-04-Data is Overrated Perceptual Metrics Can Lead Learning in the Absence of Training Data.markdown",
      "date": "6 Dec 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/2024-03-04-Delving Deep into Label Smoothing.markdown",
      "date": "22 Jul 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "Learning RelU Networks To High Uniform Accuracy Is Intractable",
      "url": "/2024-03-04-Learning Relu Networks To High Uniform Accuracy Is Intractable.markdown",
      "date": "28 Feb 2023",
      "summary": "Statistical learning theory provides guidelines on the number of training samples needed for achieving desired accuracy in learning problems, especially emphasizing generalization error. However, this is not always adequate, particularly in security-sensitive areas or computational sciences, where uniform accuracy across all inputs is necessary. This paper quantifies the training samples required for uniform accuracy in learning problems involving ReLU neural networks, revealing that the number of samples needed exponentially increases with the network's depth and input dimension."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    }
  ],
  "reconstruction": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/2024-03-04-Data is Overrated Perceptual Metrics Can Lead Learning in the Absence of Training Data.markdown",
      "date": "6 Dec 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/2024-03-06-Hard Patches Mining for Masked Image Modeling.markdown",
      "date": "12 Apr 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "spectrogram": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/2024-03-04-Data is Overrated Perceptual Metrics Can Lead Learning in the Absence of Training Data.markdown",
      "date": "6 Dec 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    }
  ],
  "resynthesize": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/2024-03-04-Data is Overrated Perceptual Metrics Can Lead Learning in the Absence of Training Data.markdown",
      "date": "6 Dec 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    }
  ],
  "euclidean": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/2024-03-04-Data is Overrated Perceptual Metrics Can Lead Learning in the Absence of Training Data.markdown",
      "date": "6 Dec 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    }
  ],
  "delve": [
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/2024-03-04-Delving Deep into Label Smoothing.markdown",
      "date": "22 Jul 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/2024-03-04-Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classificatio.markdown",
      "date": "6 Feb 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    }
  ],
  "blend": [
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/2024-03-04-Delving Deep into Label Smoothing.markdown",
      "date": "22 Jul 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    }
  ],
  "ols": [
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/2024-03-04-Delving Deep into Label Smoothing.markdown",
      "date": "22 Jul 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    }
  ],
  "statistics": [
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/2024-03-04-Delving Deep into Label Smoothing.markdown",
      "date": "22 Jul 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "Group Normalization",
      "url": "/2024-03-04-Group Normalization.markdown",
      "date": "1 Jun 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    },
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/2024-03-05-Are Labels Necessary for Neural Architecture Search.markdown",
      "date": "3 Aug 2020",
      "summary": "This paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning if effective neural architectures can be discovered using only images without human-annotated labels. Through two experimental setups—sample-based and search-based—we investigate this possibility. In the sample-based approach, we evaluate 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. In the search-based experiments, we employ a recognized NAS algorithm, DARTS, with various unsupervised objectives, finding that architectures identified without labels perform competitively compared to those found with labels. These findings suggest that human-annotated labels may not be necessary for identifying efficient neural architectures, as image statistics alone could be sufficient."
    }
  ],
  "probability": [
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/2024-03-04-Delving Deep into Label Smoothing.markdown",
      "date": "22 Jul 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "DropKey",
      "url": "/2024-03-04-DropKey.markdown",
      "date": "11 Apr 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    }
  ],
  "exist": [
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/2024-03-04-Delving Deep into Label Smoothing.markdown",
      "date": "22 Jul 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/2024-03-04-DropAttention A Regularization Method for Fully-Connected Self-Attention Networks.markdown",
      "date": "26 Jul 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. Unlike existing dropout techniques for other neural network layers, DropAttention specifically targets the unique challenges of self-attention mechanisms. Our experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    }
  ],
  "rectifiers": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/2024-03-04-Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classificatio.markdown",
      "date": "6 Feb 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    }
  ],
  "classificatio": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/2024-03-04-Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classificatio.markdown",
      "date": "6 Feb 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    }
  ],
  "rectifier": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/2024-03-04-Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classificatio.markdown",
      "date": "6 Feb 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    }
  ],
  "rectify": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/2024-03-04-Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classificatio.markdown",
      "date": "6 Feb 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    }
  ],
  "prelu": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/2024-03-04-Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classificatio.markdown",
      "date": "6 Feb 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    }
  ],
  "tailor": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/2024-03-04-Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classificatio.markdown",
      "date": "6 Feb 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/2024-03-04-Scheduled DropHead A Regularization Method for Transformer Models.markdown",
      "date": "1 Nov 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    }
  ],
  "linearity": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/2024-03-04-Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classificatio.markdown",
      "date": "6 Feb 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/2024-03-04-Understanding the difficulty of training deep feedforward neural networks.markdown",
      "date": "31 Mar 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    }
  ],
  "scratch": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/2024-03-04-Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classificatio.markdown",
      "date": "6 Feb 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    }
  ],
  "dropattention": [
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/2024-03-04-DropAttention A Regularization Method for Fully-Connected Self-Attention Networks.markdown",
      "date": "26 Jul 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. Unlike existing dropout techniques for other neural network layers, DropAttention specifically targets the unique challenges of self-attention mechanisms. Our experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    }
  ],
  "providing": [
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/2024-03-04-DropAttention A Regularization Method for Fully-Connected Self-Attention Networks.markdown",
      "date": "26 Jul 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. Unlike existing dropout techniques for other neural network layers, DropAttention specifically targets the unique challenges of self-attention mechanisms. Our experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    }
  ],
  "dropkey": [
    {
      "title": "DropKey",
      "url": "/2024-03-04-DropKey.markdown",
      "date": "11 Apr 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    }
  ],
  "ignore": [
    {
      "title": "DropKey",
      "url": "/2024-03-04-DropKey.markdown",
      "date": "11 Apr 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    }
  ],
  "softmax": [
    {
      "title": "DropKey",
      "url": "/2024-03-04-DropKey.markdown",
      "date": "11 Apr 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    }
  ],
  "matrix": [
    {
      "title": "DropKey",
      "url": "/2024-03-04-DropKey.markdown",
      "date": "11 Apr 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "ratio": [
    {
      "title": "DropKey",
      "url": "/2024-03-04-DropKey.markdown",
      "date": "11 Apr 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/2024-03-04-Sigmoid Loss for Language Image Pre-Training.markdown",
      "date": "27 sept 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    }
  ],
  "schedule": [
    {
      "title": "DropKey",
      "url": "/2024-03-04-DropKey.markdown",
      "date": "11 Apr 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/2024-03-04-Scheduled DropHead A Regularization Method for Transformer Models.markdown",
      "date": "1 Nov 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    },
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/2024-03-05-Turbo Training with Token Dropout.markdown",
      "date": "10 Oct 2022",
      "summary": "This paper introduces Turbo training, an efficient training method for video-related tasks using Transformers. Turbo training offers three major contributions: Firstly, it presents a simple yet versatile training paradigm applicable to multiple video tasks. Secondly, it demonstrates Turbo training's effectiveness across action classification, video-language representation learning, and long-video activity classification, achieving competitive performance with up to 4× speed-up and reduced memory usage. Thirdly, it enables long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous methods that were resource-intensive."
    }
  ],
  "balance": [
    {
      "title": "DropKey",
      "url": "/2024-03-04-DropKey.markdown",
      "date": "11 Apr 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    }
  ],
  "retention": [
    {
      "title": "DropKey",
      "url": "/2024-03-04-DropKey.markdown",
      "date": "11 Apr 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    }
  ],
  "structured": [
    {
      "title": "DropKey",
      "url": "/2024-03-04-DropKey.markdown",
      "date": "11 Apr 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    }
  ],
  "way": [
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/2024-03-04-Dropout A Simple Way to Prevent Neural Networks from Overfitting.markdown",
      "date": "1 Jan 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/2024-03-05-Beyond Masking Demystifying Token-Based Pre-Training for Vision Transformers.markdown",
      "date": "27 Mar 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    }
  ],
  "overfit": [
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/2024-03-04-Dropout A Simple Way to Prevent Neural Networks from Overfitting.markdown",
      "date": "1 Jan 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    },
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/2024-03-04-Dropout Reduces Underfittin.markdown",
      "date": "31 May 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/2024-03-06-Hard Patches Mining for Masked Image Modeling.markdown",
      "date": "12 Apr 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "usage": [
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/2024-03-04-Dropout A Simple Way to Prevent Neural Networks from Overfitting.markdown",
      "date": "1 Jan 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/2024-03-05-Scaling Language-Image Pre-training via Masking.markdown",
      "date": "30 Mar 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    },
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/2024-03-05-Turbo Training with Token Dropout.markdown",
      "date": "10 Oct 2022",
      "summary": "This paper introduces Turbo training, an efficient training method for video-related tasks using Transformers. Turbo training offers three major contributions: Firstly, it presents a simple yet versatile training paradigm applicable to multiple video tasks. Secondly, it demonstrates Turbo training's effectiveness across action classification, video-language representation learning, and long-video activity classification, achieving competitive performance with up to 4× speed-up and reduced memory usage. Thirdly, it enables long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous methods that were resource-intensive."
    }
  ],
  "combats": [
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/2024-03-04-Dropout A Simple Way to Prevent Neural Networks from Overfitting.markdown",
      "date": "1 Jan 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    }
  ],
  "coadaptation": [
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/2024-03-04-Dropout A Simple Way to Prevent Neural Networks from Overfitting.markdown",
      "date": "1 Jan 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    }
  ],
  "speech": [
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/2024-03-04-Dropout A Simple Way to Prevent Neural Networks from Overfitting.markdown",
      "date": "1 Jan 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    }
  ],
  "recognition": [
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/2024-03-04-Dropout A Simple Way to Prevent Neural Networks from Overfitting.markdown",
      "date": "1 Jan 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/2024-03-04-Reading Digits in Natural Images With Unsupervised Feature Learning.markdown",
      "date": "1 Jan 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    },
    {
      "title": "Rethinking Batch in BatchNorm",
      "url": "/2024-03-04-Rethinking Batch in BatchNorm.markdown",
      "date": "17 May 2021",
      "summary": "BatchNorm, essential in convolutional neural networks, behaves uniquely due to its batch-based operation, leading to performance issues in visual recognition tasks. This paper identifies these issues and suggests reevaluating the concept of batch in BatchNorm for better performance, aiming to guide researchers in its effective use."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/2024-03-04-Understanding Masked Image Modeling via Learning Occlusion Invariant Feature.markdown",
      "date": "8 Aug 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/2024-03-05-An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.markdown",
      "date": "3 Jun 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/2024-03-05-Beyond Masking Demystifying Token-Based Pre-Training for Vision Transformers.markdown",
      "date": "27 Mar 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/2024-03-05-Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions.markdown",
      "date": "20 Apr 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/2024-03-05-Understanding Self-Supervised Pretraining with Part-Aware Representation Learning.markdown",
      "date": "23 Jan 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    }
  ],
  "document": [
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/2024-03-04-Dropout A Simple Way to Prevent Neural Networks from Overfitting.markdown",
      "date": "1 Jan 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    },
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/2024-03-04-Reading Digits in Natural Images With Unsupervised Feature Learning.markdown",
      "date": "1 Jan 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    }
  ],
  "bagging": [
    {
      "title": "DROPOUT AS DATA AUGMENTATION",
      "url": "/2024-03-04-DROPOUT AS DATA AUGMENTATION.markdown",
      "date": "8 Jan 2016",
      "summary": "Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cos"
    }
  ],
  "share": [
    {
      "title": "DROPOUT AS DATA AUGMENTATION",
      "url": "/2024-03-04-DROPOUT AS DATA AUGMENTATION.markdown",
      "date": "8 Jan 2016",
      "summary": "Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/2024-03-05-A Closer Look at Self-Supervised Lightweight Vision Transformers.markdown",
      "date": "3 May 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/2024-03-06-Denoising Masked Autoencoders Help Robust Classification.markdown",
      "date": "7 Mar 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    }
  ],
  "kind": [
    {
      "title": "DROPOUT AS DATA AUGMENTATION",
      "url": "/2024-03-04-DROPOUT AS DATA AUGMENTATION.markdown",
      "date": "8 Jan 2016",
      "summary": "Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cos"
    }
  ],
  "back": [
    {
      "title": "DROPOUT AS DATA AUGMENTATION",
      "url": "/2024-03-04-DROPOUT AS DATA AUGMENTATION.markdown",
      "date": "8 Jan 2016",
      "summary": "Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/2024-03-05-Architecture-Agnostic Masked Image Modeling – From ViT back to CNN.markdown",
      "date": "2 Jun 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    }
  ],
  "yield": [
    {
      "title": "DROPOUT AS DATA AUGMENTATION",
      "url": "/2024-03-04-DROPOUT AS DATA AUGMENTATION.markdown",
      "date": "8 Jan 2016",
      "summary": "Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/2024-03-04-White-Box Transformers via Sparse Rate Reduction.markdown",
      "date": "1 Jun 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/2024-03-05-Scaling Language-Image Pre-training via Masking.markdown",
      "date": "30 Mar 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    }
  ],
  "add": [
    {
      "title": "DROPOUT AS DATA AUGMENTATION",
      "url": "/2024-03-04-DROPOUT AS DATA AUGMENTATION.markdown",
      "date": "8 Jan 2016",
      "summary": "Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/2024-03-05-Vision Transformers Need Register.markdown",
      "date": "28 Sep 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    }
  ],
  "underfittin": [
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/2024-03-04-Dropout Reduces Underfittin.markdown",
      "date": "31 May 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    }
  ],
  "hinton": [
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/2024-03-04-Dropout Reduces Underfittin.markdown",
      "date": "31 May 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    }
  ],
  "et": [
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/2024-03-04-Dropout Reduces Underfittin.markdown",
      "date": "31 May 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/2024-03-04-Instance Normalization The Missing Ingredient for Fast Stylization.markdown",
      "date": "6 Nov 2017",
      "summary": "In this study, we improve upon the fast stylization method by Ulyanov et al. (2016) through a minor yet impactful modification: substituting batch normalization with instance normalization during both training and testing. This adjustment significantly enhances the quality of generated images and enables the training of efficient real-time image generation architectures. The code and full paper are available online."
    }
  ],
  "al": [
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/2024-03-04-Dropout Reduces Underfittin.markdown",
      "date": "31 May 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/2024-03-04-Instance Normalization The Missing Ingredient for Fast Stylization.markdown",
      "date": "6 Nov 2017",
      "summary": "In this study, we improve upon the fast stylization method by Ulyanov et al. (2016) through a minor yet impactful modification: substituting batch normalization with instance normalization during both training and testing. This adjustment significantly enhances the quality of generated images and enables the training of efficient real-time image generation architectures. The code and full paper are available online."
    }
  ],
  "well": [
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/2024-03-04-Dropout Reduces Underfittin.markdown",
      "date": "31 May 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/2024-03-05-An Empirical Study of Training Self-Supervised Vision Transformers.markdown",
      "date": "16 Aug 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    }
  ],
  "variance": [
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/2024-03-04-Dropout Reduces Underfittin.markdown",
      "date": "31 May 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "phase": [
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/2024-03-04-Dropout Reduces Underfittin.markdown",
      "date": "31 May 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "Layer Normalization",
      "url": "/2024-03-04-Layer Normalization.markdown",
      "date": "21 Jul 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/2024-03-04-RandAugment Practical automated data augmentation with a reduced search space.markdown",
      "date": "14 Nov 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    }
  ],
  "activate": [
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/2024-03-04-Dropout Reduces Underfittin.markdown",
      "date": "31 May 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    }
  ],
  "stage": [
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/2024-03-04-Dropout Reduces Underfittin.markdown",
      "date": "31 May 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "group": [
    {
      "title": "Group Normalization",
      "url": "/2024-03-04-Group Normalization.markdown",
      "date": "1 Jun 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    }
  ],
  "bn": [
    {
      "title": "Group Normalization",
      "url": "/2024-03-04-Group Normalization.markdown",
      "date": "1 Jun 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    }
  ],
  "constrain": [
    {
      "title": "Group Normalization",
      "url": "/2024-03-04-Group Normalization.markdown",
      "date": "1 Jun 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    }
  ],
  "gn": [
    {
      "title": "Group Normalization",
      "url": "/2024-03-04-Group Normalization.markdown",
      "date": "1 Jun 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    }
  ],
  "divide": [
    {
      "title": "Group Normalization",
      "url": "/2024-03-04-Group Normalization.markdown",
      "date": "1 Jun 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/2024-03-04-On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning.markdown",
      "date": "26 Jun 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    }
  ],
  "channel": [
    {
      "title": "Group Normalization",
      "url": "/2024-03-04-Group Normalization.markdown",
      "date": "1 Jun 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "video": [
    {
      "title": "Group Normalization",
      "url": "/2024-03-04-Group Normalization.markdown",
      "date": "1 Jun 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/2024-03-05-Turbo Training with Token Dropout.markdown",
      "date": "10 Oct 2022",
      "summary": "This paper introduces Turbo training, an efficient training method for video-related tasks using Transformers. Turbo training offers three major contributions: Firstly, it presents a simple yet versatile training paradigm applicable to multiple video tasks. Secondly, it demonstrates Turbo training's effectiveness across action classification, video-language representation learning, and long-video activity classification, achieving competitive performance with up to 4× speed-up and reduced memory usage. Thirdly, it enables long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous methods that were resource-intensive."
    }
  ],
  "replacement": [
    {
      "title": "Group Normalization",
      "url": "/2024-03-04-Group Normalization.markdown",
      "date": "1 Jun 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    }
  ],
  "library": [
    {
      "title": "Group Normalization",
      "url": "/2024-03-04-Group Normalization.markdown",
      "date": "1 Jun 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    }
  ],
  "calibration": [
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/2024-03-04-Impact of Noise on Calibration and Generalisation of Neural Networks.markdown",
      "date": "30 Jun 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. We examine different types of noise, such as activation and input augmentation noise, in both in-distribution and out-of-distribution scenarios. Activation noise significantly improves generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    }
  ],
  "generalisation": [
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/2024-03-04-Impact of Noise on Calibration and Generalisation of Neural Networks.markdown",
      "date": "30 Jun 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. We examine different types of noise, such as activation and input augmentation noise, in both in-distribution and out-of-distribution scenarios. Activation noise significantly improves generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    }
  ],
  "injection": [
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/2024-03-04-Impact of Noise on Calibration and Generalisation of Neural Networks.markdown",
      "date": "30 Jun 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. We examine different types of noise, such as activation and input augmentation noise, in both in-distribution and out-of-distribution scenarios. Activation noise significantly improves generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    }
  ],
  "nn": [
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/2024-03-04-Impact of Noise on Calibration and Generalisation of Neural Networks.markdown",
      "date": "30 Jun 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. We examine different types of noise, such as activation and input augmentation noise, in both in-distribution and out-of-distribution scenarios. Activation noise significantly improves generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    }
  ],
  "activation": [
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/2024-03-04-Impact of Noise on Calibration and Generalisation of Neural Networks.markdown",
      "date": "30 Jun 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. We examine different types of noise, such as activation and input augmentation noise, in both in-distribution and out-of-distribution scenarios. Activation noise significantly improves generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/2024-03-04-Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution.markdown",
      "date": "25 Oct 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/2024-03-04-Understanding the difficulty of training deep feedforward neural networks.markdown",
      "date": "31 Mar 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    },
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/2024-03-05-Localizing Objects with Self-Supervised Transformers and no Labels.markdown",
      "date": "29 Sept 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    }
  ],
  "ingredient": [
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/2024-03-04-Instance Normalization The Missing Ingredient for Fast Stylization.markdown",
      "date": "6 Nov 2017",
      "summary": "In this study, we improve upon the fast stylization method by Ulyanov et al. (2016) through a minor yet impactful modification: substituting batch normalization with instance normalization during both training and testing. This adjustment significantly enhances the quality of generated images and enables the training of efficient real-time image generation architectures. The code and full paper are available online."
    }
  ],
  "stylization": [
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/2024-03-04-Instance Normalization The Missing Ingredient for Fast Stylization.markdown",
      "date": "6 Nov 2017",
      "summary": "In this study, we improve upon the fast stylization method by Ulyanov et al. (2016) through a minor yet impactful modification: substituting batch normalization with instance normalization during both training and testing. This adjustment significantly enhances the quality of generated images and enables the training of efficient real-time image generation architectures. The code and full paper are available online."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/2024-03-05-Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions.markdown",
      "date": "20 Apr 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    }
  ],
  "ulyanov": [
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/2024-03-04-Instance Normalization The Missing Ingredient for Fast Stylization.markdown",
      "date": "6 Nov 2017",
      "summary": "In this study, we improve upon the fast stylization method by Ulyanov et al. (2016) through a minor yet impactful modification: substituting batch normalization with instance normalization during both training and testing. This adjustment significantly enhances the quality of generated images and enables the training of efficient real-time image generation architectures. The code and full paper are available online."
    }
  ],
  "substitute": [
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/2024-03-04-Instance Normalization The Missing Ingredient for Fast Stylization.markdown",
      "date": "6 Nov 2017",
      "summary": "In this study, we improve upon the fast stylization method by Ulyanov et al. (2016) through a minor yet impactful modification: substituting batch normalization with instance normalization during both training and testing. This adjustment significantly enhances the quality of generated images and enables the training of efficient real-time image generation architectures. The code and full paper are available online."
    }
  ],
  "generated": [
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/2024-03-04-Instance Normalization The Missing Ingredient for Fast Stylization.markdown",
      "date": "6 Nov 2017",
      "summary": "In this study, we improve upon the fast stylization method by Ulyanov et al. (2016) through a minor yet impactful modification: substituting batch normalization with instance normalization during both training and testing. This adjustment significantly enhances the quality of generated images and enables the training of efficient real-time image generation architectures. The code and full paper are available online."
    }
  ],
  "flexibility": [
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    }
  ],
  "aid": [
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/2024-03-04-What do Self-supervised vision transformers learn.markdown",
      "date": "1 May 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    }
  ],
  "occlusions": [
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    }
  ],
  "family": [
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/2024-03-04-On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning.markdown",
      "date": "26 Jun 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    }
  ],
  "advantage": [
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    }
  ],
  "acces": [
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    }
  ],
  "httpsgitiojs15x": [
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/2024-03-04-Intriguing Properties of Vision Transformers.markdown",
      "date": "25 Nov 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    }
  ],
  "feed": [
    {
      "title": "Layer Normalization",
      "url": "/2024-03-04-Layer Normalization.markdown",
      "date": "21 Jul 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/2024-03-05-Deeper Insights into the Robustness of ViTs towards Common Corruptions.markdown",
      "date": "19 Aug 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    }
  ],
  "forward": [
    {
      "title": "Layer Normalization",
      "url": "/2024-03-04-Layer Normalization.markdown",
      "date": "21 Jul 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/2024-03-05-Deeper Insights into the Robustness of ViTs towards Common Corruptions.markdown",
      "date": "19 Aug 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "vary": [
    {
      "title": "Layer Normalization",
      "url": "/2024-03-04-Layer Normalization.markdown",
      "date": "21 Jul 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    }
  ],
  "rnn": [
    {
      "title": "Layer Normalization",
      "url": "/2024-03-04-Layer Normalization.markdown",
      "date": "21 Jul 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    }
  ],
  "case": [
    {
      "title": "Layer Normalization",
      "url": "/2024-03-04-Layer Normalization.markdown",
      "date": "21 Jul 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    }
  ],
  "stabilizes": [
    {
      "title": "Layer Normalization",
      "url": "/2024-03-04-Layer Normalization.markdown",
      "date": "21 Jul 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    }
  ],
  "dynamic": [
    {
      "title": "Layer Normalization",
      "url": "/2024-03-04-Layer Normalization.markdown",
      "date": "21 Jul 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    }
  ],
  "cut": [
    {
      "title": "Layer Normalization",
      "url": "/2024-03-04-Layer Normalization.markdown",
      "date": "21 Jul 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "relu": [
    {
      "title": "Learning RelU Networks To High Uniform Accuracy Is Intractable",
      "url": "/2024-03-04-Learning Relu Networks To High Uniform Accuracy Is Intractable.markdown",
      "date": "28 Feb 2023",
      "summary": "Statistical learning theory provides guidelines on the number of training samples needed for achieving desired accuracy in learning problems, especially emphasizing generalization error. However, this is not always adequate, particularly in security-sensitive areas or computational sciences, where uniform accuracy across all inputs is necessary. This paper quantifies the training samples required for uniform accuracy in learning problems involving ReLU neural networks, revealing that the number of samples needed exponentially increases with the network's depth and input dimension."
    }
  ],
  "achieving": [
    {
      "title": "Learning RelU Networks To High Uniform Accuracy Is Intractable",
      "url": "/2024-03-04-Learning Relu Networks To High Uniform Accuracy Is Intractable.markdown",
      "date": "28 Feb 2023",
      "summary": "Statistical learning theory provides guidelines on the number of training samples needed for achieving desired accuracy in learning problems, especially emphasizing generalization error. However, this is not always adequate, particularly in security-sensitive areas or computational sciences, where uniform accuracy across all inputs is necessary. This paper quantifies the training samples required for uniform accuracy in learning problems involving ReLU neural networks, revealing that the number of samples needed exponentially increases with the network's depth and input dimension."
    }
  ],
  "desire": [
    {
      "title": "Learning RelU Networks To High Uniform Accuracy Is Intractable",
      "url": "/2024-03-04-Learning Relu Networks To High Uniform Accuracy Is Intractable.markdown",
      "date": "28 Feb 2023",
      "summary": "Statistical learning theory provides guidelines on the number of training samples needed for achieving desired accuracy in learning problems, especially emphasizing generalization error. However, this is not always adequate, particularly in security-sensitive areas or computational sciences, where uniform accuracy across all inputs is necessary. This paper quantifies the training samples required for uniform accuracy in learning problems involving ReLU neural networks, revealing that the number of samples needed exponentially increases with the network's depth and input dimension."
    }
  ],
  "science": [
    {
      "title": "Learning RelU Networks To High Uniform Accuracy Is Intractable",
      "url": "/2024-03-04-Learning Relu Networks To High Uniform Accuracy Is Intractable.markdown",
      "date": "28 Feb 2023",
      "summary": "Statistical learning theory provides guidelines on the number of training samples needed for achieving desired accuracy in learning problems, especially emphasizing generalization error. However, this is not always adequate, particularly in security-sensitive areas or computational sciences, where uniform accuracy across all inputs is necessary. This paper quantifies the training samples required for uniform accuracy in learning problems involving ReLU neural networks, revealing that the number of samples needed exponentially increases with the network's depth and input dimension."
    }
  ],
  "quantify": [
    {
      "title": "Learning RelU Networks To High Uniform Accuracy Is Intractable",
      "url": "/2024-03-04-Learning Relu Networks To High Uniform Accuracy Is Intractable.markdown",
      "date": "28 Feb 2023",
      "summary": "Statistical learning theory provides guidelines on the number of training samples needed for achieving desired accuracy in learning problems, especially emphasizing generalization error. However, this is not always adequate, particularly in security-sensitive areas or computational sciences, where uniform accuracy across all inputs is necessary. This paper quantifies the training samples required for uniform accuracy in learning problems involving ReLU neural networks, revealing that the number of samples needed exponentially increases with the network's depth and input dimension."
    }
  ],
  "dimension": [
    {
      "title": "Learning RelU Networks To High Uniform Accuracy Is Intractable",
      "url": "/2024-03-04-Learning Relu Networks To High Uniform Accuracy Is Intractable.markdown",
      "date": "28 Feb 2023",
      "summary": "Statistical learning theory provides guidelines on the number of training samples needed for achieving desired accuracy in learning problems, especially emphasizing generalization error. However, this is not always adequate, particularly in security-sensitive areas or computational sciences, where uniform accuracy across all inputs is necessary. This paper quantifies the training samples required for uniform accuracy in learning problems involving ReLU neural networks, revealing that the number of samples needed exponentially increases with the network's depth and input dimension."
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/2024-03-04-On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning.markdown",
      "date": "26 Jun 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/2024-03-05-The geometry of hidden representations of large transformer models.markdown",
      "date": "30 Oct 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "warp": [
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/2024-03-04-ON INTERACTION BETWEEN AUGMENTATIONS AND CORRUPTIONS IN NATURAL CORRUPTION ROBUSTNESS.markdown",
      "date": "19 Nov 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    }
  ],
  "color": [
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/2024-03-04-ON INTERACTION BETWEEN AUGMENTATIONS AND CORRUPTIONS IN NATURAL CORRUPTION ROBUSTNESS.markdown",
      "date": "19 Nov 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "correlation": [
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/2024-03-04-ON INTERACTION BETWEEN AUGMENTATIONS AND CORRUPTIONS IN NATURAL CORRUPTION ROBUSTNESS.markdown",
      "date": "19 Nov 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/2024-03-05-Are Labels Necessary for Neural Architecture Search.markdown",
      "date": "3 Aug 2020",
      "summary": "This paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning if effective neural architectures can be discovered using only images without human-annotated labels. Through two experimental setups—sample-based and search-based—we investigate this possibility. In the sample-based approach, we evaluate 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. In the search-based experiments, we employ a recognized NAS algorithm, DARTS, with various unsupervised objectives, finding that architectures identified without labels perform competitively compared to those found with labels. These findings suggest that human-annotated labels may not be necessary for identifying efficient neural architectures, as image statistics alone could be sufficient."
    }
  ],
  "transform": [
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/2024-03-04-ON INTERACTION BETWEEN AUGMENTATIONS AND CORRUPTIONS IN NATURAL CORRUPTION ROBUSTNESS.markdown",
      "date": "19 Nov 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/2024-03-05-Understanding Self-Supervised Pretraining with Part-Aware Representation Learning.markdown",
      "date": "23 Jan 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    }
  ],
  "distance": [
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/2024-03-04-ON INTERACTION BETWEEN AUGMENTATIONS AND CORRUPTIONS IN NATURAL CORRUPTION ROBUSTNESS.markdown",
      "date": "19 Nov 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    }
  ],
  "similarity": [
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/2024-03-04-ON INTERACTION BETWEEN AUGMENTATIONS AND CORRUPTIONS IN NATURAL CORRUPTION ROBUSTNESS.markdown",
      "date": "19 Nov 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/2024-03-04-On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning.markdown",
      "date": "26 Jun 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    },
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/2024-03-04-Sigmoid Loss for Language Image Pre-Training.markdown",
      "date": "27 sept 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/2024-03-04-Understanding Masked Image Modeling via Learning Occlusion Invariant Feature.markdown",
      "date": "8 Aug 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "tool": [
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/2024-03-04-ON INTERACTION BETWEEN AUGMENTATIONS AND CORRUPTIONS IN NATURAL CORRUPTION ROBUSTNESS.markdown",
      "date": "19 Nov 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/2024-03-04-Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution.markdown",
      "date": "25 Oct 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/2024-03-05-A Closer Look at Self-Supervised Lightweight Vision Transformers.markdown",
      "date": "3 May 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    }
  ],
  "httpsgithubcomfacebookresearchaugmentationcorruption": [
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/2024-03-04-ON INTERACTION BETWEEN AUGMENTATIONS AND CORRUPTIONS IN NATURAL CORRUPTION ROBUSTNESS.markdown",
      "date": "19 Nov 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    }
  ],
  "duality": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/2024-03-04-On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning.markdown",
      "date": "26 Jun 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    }
  ],
  "covariance": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/2024-03-04-On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning.markdown",
      "date": "26 Jun 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    }
  ],
  "algebraically": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/2024-03-04-On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning.markdown",
      "date": "26 Jun 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    }
  ],
  "condition": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/2024-03-04-On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning.markdown",
      "date": "26 Jun 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    },
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/2024-03-05-An Empirical Study of Training Self-Supervised Vision Transformers.markdown",
      "date": "16 Aug 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/2024-03-05-Designing Network Design Spaces.markdown",
      "date": "30 Mar 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    }
  ],
  "relationship": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/2024-03-04-On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning.markdown",
      "date": "26 Jun 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    }
  ],
  "connect": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/2024-03-04-On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning.markdown",
      "date": "26 Jun 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    }
  ],
  "vicreg": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/2024-03-04-On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning.markdown",
      "date": "26 Jun 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    }
  ],
  "hyperparameter": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/2024-03-04-On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning.markdown",
      "date": "26 Jun 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    },
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/2024-03-04-On Warm-Starting Neural Network Training.markdown",
      "date": "31 Dec 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/2024-03-04-RandAugment Practical automated data augmentation with a reduced search space.markdown",
      "date": "14 Nov 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "assumption": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/2024-03-04-On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning.markdown",
      "date": "26 Jun 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    }
  ],
  "shrink": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/2024-03-04-On Warm-Starting Neural Network Training.markdown",
      "date": "31 Dec 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    }
  ],
  "perturb": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/2024-03-04-On Warm-Starting Neural Network Training.markdown",
      "date": "31 Dec 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    }
  ],
  "starting": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/2024-03-04-On Warm-Starting Neural Network Training.markdown",
      "date": "31 Dec 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    }
  ],
  "machine": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/2024-03-04-On Warm-Starting Neural Network Training.markdown",
      "date": "31 Dec 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "arrive": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/2024-03-04-On Warm-Starting Neural Network Training.markdown",
      "date": "31 Dec 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    }
  ],
  "nature": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/2024-03-04-On Warm-Starting Neural Network Training.markdown",
      "date": "31 Dec 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "selection": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/2024-03-04-On Warm-Starting Neural Network Training.markdown",
      "date": "31 Dec 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    }
  ],
  "initialize": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/2024-03-04-On Warm-Starting Neural Network Training.markdown",
      "date": "31 Dec 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    }
  ],
  "save": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/2024-03-04-On Warm-Starting Neural Network Training.markdown",
      "date": "31 Dec 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    }
  ],
  "discrepancy": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/2024-03-04-On Warm-Starting Neural Network Training.markdown",
      "date": "31 Dec 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/2024-03-05-Vision Transformers Need Register.markdown",
      "date": "28 Sep 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    }
  ],
  "persist": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/2024-03-04-On Warm-Starting Neural Network Training.markdown",
      "date": "31 Dec 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    }
  ],
  "expense": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/2024-03-04-On Warm-Starting Neural Network Training.markdown",
      "date": "31 Dec 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    }
  ],
  "introduces": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/2024-03-04-On Warm-Starting Neural Network Training.markdown",
      "date": "31 Dec 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/2024-03-06-Denoising Masked Autoencoders Help Robust Classification.markdown",
      "date": "7 Mar 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    }
  ],
  "imaging": [
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/2024-03-04-Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution.markdown",
      "date": "25 Oct 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    }
  ],
  "sensing": [
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/2024-03-04-Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution.markdown",
      "date": "25 Oct 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    }
  ],
  "organize": [
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/2024-03-04-Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution.markdown",
      "date": "25 Oct 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    }
  ],
  "sonn": [
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/2024-03-04-Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution.markdown",
      "date": "25 Oct 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    }
  ],
  "filter": [
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/2024-03-04-Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution.markdown",
      "date": "25 Oct 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    }
  ],
  "parameters": [
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/2024-03-04-Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution.markdown",
      "date": "25 Oct 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    }
  ],
  "equivalent": [
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/2024-03-04-Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution.markdown",
      "date": "25 Oct 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    }
  ],
  "limitation": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/2024-03-04-Progressive Random Convolutions for Single Domain Generalization.markdown",
      "date": "2 Apr 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/2024-03-05-A Closer Look at Self-Supervised Lightweight Vision Transformers.markdown",
      "date": "3 May 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    }
  ],
  "convolutions": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/2024-03-04-Progressive Random Convolutions for Single Domain Generalization.markdown",
      "date": "2 Apr 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    }
  ],
  "randconv": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/2024-03-04-Progressive Random Convolutions for Single Domain Generalization.markdown",
      "date": "2 Apr 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    }
  ],
  "kernel": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/2024-03-04-Progressive Random Convolutions for Single Domain Generalization.markdown",
      "date": "2 Apr 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    }
  ],
  "integrity": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/2024-03-04-Progressive Random Convolutions for Single Domain Generalization.markdown",
      "date": "2 Apr 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    }
  ],
  "style": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/2024-03-04-Progressive Random Convolutions for Single Domain Generalization.markdown",
      "date": "2 Apr 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/2024-03-05-Beyond Masking Demystifying Token-Based Pre-Training for Vision Transformers.markdown",
      "date": "27 Mar 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/2024-03-05-Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions.markdown",
      "date": "20 Apr 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    }
  ],
  "diversity": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/2024-03-04-Progressive Random Convolutions for Single Domain Generalization.markdown",
      "date": "2 Apr 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "affine": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/2024-03-04-Progressive Random Convolutions for Single Domain Generalization.markdown",
      "date": "2 Apr 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    }
  ],
  "contrast": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/2024-03-04-Progressive Random Convolutions for Single Domain Generalization.markdown",
      "date": "2 Apr 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/2024-03-05-Scaling Language-Image Pre-training via Masking.markdown",
      "date": "30 Mar 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    }
  ],
  "diversification": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/2024-03-04-Progressive Random Convolutions for Single Domain Generalization.markdown",
      "date": "2 Apr 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    }
  ],
  "generator": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/2024-03-04-Progressive Random Convolutions for Single Domain Generalization.markdown",
      "date": "2 Apr 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    }
  ],
  "randaugment": [
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/2024-03-04-RandAugment Practical automated data augmentation with a reduced search space.markdown",
      "date": "14 Nov 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    }
  ],
  "search": [
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/2024-03-04-RandAugment Practical automated data augmentation with a reduced search space.markdown",
      "date": "14 Nov 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/2024-03-05-Are Labels Necessary for Neural Architecture Search.markdown",
      "date": "3 Aug 2020",
      "summary": "This paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning if effective neural architectures can be discovered using only images without human-annotated labels. Through two experimental setups—sample-based and search-based—we investigate this possibility. In the sample-based approach, we evaluate 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. In the search-based experiments, we employ a recognized NAS algorithm, DARTS, with various unsupervised objectives, finding that architectures identified without labels perform competitively compared to those found with labels. These findings suggest that human-annotated labels may not be necessary for identifying efficient neural architectures, as image statistics alone could be sufficient."
    }
  ],
  "strategies": [
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/2024-03-04-RandAugment Practical automated data augmentation with a reduced search space.markdown",
      "date": "14 Nov 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    }
  ],
  "adoption": [
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/2024-03-04-RandAugment Practical automated data augmentation with a reduced search space.markdown",
      "date": "14 Nov 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    }
  ],
  "hinder": [
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/2024-03-04-RandAugment Practical automated data augmentation with a reduced search space.markdown",
      "date": "14 Nov 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    }
  ],
  "proxy": [
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/2024-03-04-RandAugment Practical automated data augmentation with a reduced search space.markdown",
      "date": "14 Nov 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/2024-03-05-Colorization as a Proxy Task for Visual Understanding.markdown",
      "date": "13 Aug 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    }
  ],
  "customization": [
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/2024-03-04-RandAugment Practical automated data augmentation with a reduced search space.markdown",
      "date": "14 Nov 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    }
  ],
  "read": [
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/2024-03-04-Reading Digits in Natural Images With Unsupervised Feature Learning.markdown",
      "date": "1 Jan 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    }
  ],
  "digit": [
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/2024-03-04-Reading Digits in Natural Images With Unsupervised Feature Learning.markdown",
      "date": "1 Jan 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    }
  ],
  "text": [
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/2024-03-04-Reading Digits in Natural Images With Unsupervised Feature Learning.markdown",
      "date": "1 Jan 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/2024-03-04-Scheduled DropHead A Regularization Method for Transformer Models.markdown",
      "date": "1 Nov 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/2024-03-05-Scaling Language-Image Pre-training via Masking.markdown",
      "date": "30 Mar 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/2024-03-05-The geometry of hidden representations of large transformer models.markdown",
      "date": "30 Oct 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "character": [
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/2024-03-04-Reading Digits in Natural Images With Unsupervised Feature Learning.markdown",
      "date": "1 Jan 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    }
  ],
  "recognizing": [
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/2024-03-04-Reading Digits in Natural Images With Unsupervised Feature Learning.markdown",
      "date": "1 Jan 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    }
  ],
  "complex": [
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/2024-03-04-Reading Digits in Natural Images With Unsupervised Feature Learning.markdown",
      "date": "1 Jan 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    }
  ],
  "scene": [
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/2024-03-04-Reading Digits in Natural Images With Unsupervised Feature Learning.markdown",
      "date": "1 Jan 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    }
  ],
  "photograph": [
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/2024-03-04-Reading Digits in Natural Images With Unsupervised Feature Learning.markdown",
      "date": "1 Jan 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    }
  ],
  "trail": [
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/2024-03-04-Reading Digits in Natural Images With Unsupervised Feature Learning.markdown",
      "date": "1 Jan 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    }
  ],
  "street": [
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/2024-03-04-Reading Digits in Natural Images With Unsupervised Feature Learning.markdown",
      "date": "1 Jan 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    }
  ],
  "photo": [
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/2024-03-04-Reading Digits in Natural Images With Unsupervised Feature Learning.markdown",
      "date": "1 Jan 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "rethink": [
    {
      "title": "Rethinking Batch in BatchNorm",
      "url": "/2024-03-04-Rethinking Batch in BatchNorm.markdown",
      "date": "17 May 2021",
      "summary": "BatchNorm, essential in convolutional neural networks, behaves uniquely due to its batch-based operation, leading to performance issues in visual recognition tasks. This paper identifies these issues and suggests reevaluating the concept of batch in BatchNorm for better performance, aiming to guide researchers in its effective use."
    }
  ],
  "behave": [
    {
      "title": "Rethinking Batch in BatchNorm",
      "url": "/2024-03-04-Rethinking Batch in BatchNorm.markdown",
      "date": "17 May 2021",
      "summary": "BatchNorm, essential in convolutional neural networks, behaves uniquely due to its batch-based operation, leading to performance issues in visual recognition tasks. This paper identifies these issues and suggests reevaluating the concept of batch in BatchNorm for better performance, aiming to guide researchers in its effective use."
    }
  ],
  "suggests": [
    {
      "title": "Rethinking Batch in BatchNorm",
      "url": "/2024-03-04-Rethinking Batch in BatchNorm.markdown",
      "date": "17 May 2021",
      "summary": "BatchNorm, essential in convolutional neural networks, behaves uniquely due to its batch-based operation, leading to performance issues in visual recognition tasks. This paper identifies these issues and suggests reevaluating the concept of batch in BatchNorm for better performance, aiming to guide researchers in its effective use."
    }
  ],
  "reevaluate": [
    {
      "title": "Rethinking Batch in BatchNorm",
      "url": "/2024-03-04-Rethinking Batch in BatchNorm.markdown",
      "date": "17 May 2021",
      "summary": "BatchNorm, essential in convolutional neural networks, behaves uniquely due to its batch-based operation, leading to performance issues in visual recognition tasks. This paper identifies these issues and suggests reevaluating the concept of batch in BatchNorm for better performance, aiming to guide researchers in its effective use."
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/2024-03-05-Transformer-Based Visual Segmentation A Surve.markdown",
      "date": "20 Dec 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "researcher": [
    {
      "title": "Rethinking Batch in BatchNorm",
      "url": "/2024-03-04-Rethinking Batch in BatchNorm.markdown",
      "date": "17 May 2021",
      "summary": "BatchNorm, essential in convolutional neural networks, behaves uniquely due to its batch-based operation, leading to performance issues in visual recognition tasks. This paper identifies these issues and suggests reevaluating the concept of batch in BatchNorm for better performance, aiming to guide researchers in its effective use."
    }
  ],
  "drophead": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/2024-03-04-Scheduled DropHead A Regularization Method for Transformer Models.markdown",
      "date": "1 Nov 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    }
  ],
  "multihead": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/2024-03-04-Scheduled DropHead A Regularization Method for Transformer Models.markdown",
      "date": "1 Nov 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    }
  ],
  "head": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/2024-03-04-Scheduled DropHead A Regularization Method for Transformer Models.markdown",
      "date": "1 Nov 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    }
  ],
  "avoid": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/2024-03-04-Scheduled DropHead A Regularization Method for Transformer Models.markdown",
      "date": "1 Nov 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/2024-03-06-Hard Patches Mining for Masked Image Modeling.markdown",
      "date": "12 Apr 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "dominance": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/2024-03-04-Scheduled DropHead A Regularization Method for Transformer Models.markdown",
      "date": "1 Nov 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    }
  ],
  "scheduler": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/2024-03-04-Scheduled DropHead A Regularization Method for Transformer Models.markdown",
      "date": "1 Nov 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    }
  ],
  "bleu": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/2024-03-04-Scheduled DropHead A Regularization Method for Transformer Models.markdown",
      "date": "1 Nov 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "score": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/2024-03-04-Scheduled DropHead A Regularization Method for Transformer Models.markdown",
      "date": "1 Nov 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "wmt14": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/2024-03-04-Scheduled DropHead A Regularization Method for Transformer Models.markdown",
      "date": "1 Nov 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    }
  ],
  "en": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/2024-03-04-Scheduled DropHead A Regularization Method for Transformer Models.markdown",
      "date": "1 Nov 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    }
  ],
  "translation": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/2024-03-04-Scheduled DropHead A Regularization Method for Transformer Models.markdown",
      "date": "1 Nov 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    }
  ],
  "sigmoid": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/2024-03-04-Sigmoid Loss for Language Image Pre-Training.markdown",
      "date": "27 sept 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/2024-03-04-Understanding the difficulty of training deep feedforward neural networks.markdown",
      "date": "31 Mar 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    }
  ],
  "pairwise": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/2024-03-04-Sigmoid Loss for Language Image Pre-Training.markdown",
      "date": "27 sept 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    }
  ],
  "siglip": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/2024-03-04-Sigmoid Loss for Language Image Pre-Training.markdown",
      "date": "27 sept 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    }
  ],
  "outperforms": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/2024-03-04-Sigmoid Loss for Language Image Pre-Training.markdown",
      "date": "27 sept 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    }
  ],
  "tpuv4": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/2024-03-04-Sigmoid Loss for Language Image Pre-Training.markdown",
      "date": "27 sept 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    }
  ],
  "chip": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/2024-03-04-Sigmoid Loss for Language Image Pre-Training.markdown",
      "date": "27 sept 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    }
  ],
  "siglit": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/2024-03-04-Sigmoid Loss for Language Image Pre-Training.markdown",
      "date": "27 sept 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    }
  ],
  "disentanglement": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/2024-03-04-Sigmoid Loss for Language Image Pre-Training.markdown",
      "date": "27 sept 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    }
  ],
  "count": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/2024-03-04-Sigmoid Loss for Language Image Pre-Training.markdown",
      "date": "27 sept 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    }
  ],
  "diminish": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/2024-03-04-Sigmoid Loss for Language Image Pre-Training.markdown",
      "date": "27 sept 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    }
  ],
  "return": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/2024-03-04-Sigmoid Loss for Language Image Pre-Training.markdown",
      "date": "27 sept 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    }
  ],
  "release": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/2024-03-04-Sigmoid Loss for Language Image Pre-Training.markdown",
      "date": "27 sept 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    }
  ],
  "url": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/2024-03-04-Sigmoid Loss for Language Image Pre-Training.markdown",
      "date": "27 sept 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    }
  ],
  "occlusion": [
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/2024-03-04-Understanding Masked Image Modeling via Learning Occlusion Invariant Feature.markdown",
      "date": "8 Aug 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    }
  ],
  "working": [
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/2024-03-04-Understanding Masked Image Modeling via Learning Occlusion Invariant Feature.markdown",
      "date": "8 Aug 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    }
  ],
  "unclear": [
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/2024-03-04-Understanding Masked Image Modeling via Learning Occlusion Invariant Feature.markdown",
      "date": "8 Aug 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    }
  ],
  "perspective": [
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/2024-03-04-Understanding Masked Image Modeling via Learning Occlusion Invariant Feature.markdown",
      "date": "8 Aug 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    }
  ],
  "differ": [
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/2024-03-04-Understanding Masked Image Modeling via Learning Occlusion Invariant Feature.markdown",
      "date": "8 Aug 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    }
  ],
  "measurement": [
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/2024-03-04-Understanding Masked Image Modeling via Learning Occlusion Invariant Feature.markdown",
      "date": "8 Aug 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    }
  ],
  "mae": [
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/2024-03-04-Understanding Masked Image Modeling via Learning Occlusion Invariant Feature.markdown",
      "date": "8 Aug 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/2024-03-05-Adversarially Self-supervised Pre-training Improves Accuracy and Robustness.markdown",
      "date": "10 Mar 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learner",
      "url": "/2024-03-05-Masked Autoencoders Are Scalable Vision Learner.markdown",
      "date": "19 Dec 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    }
  ],
  "difficulty": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/2024-03-04-Understanding the difficulty of training deep feedforward neural networks.markdown",
      "date": "31 Mar 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/2024-03-06-Hard Patches Mining for Masked Image Modeling.markdown",
      "date": "12 Apr 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "multilayer": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/2024-03-04-Understanding the difficulty of training deep feedforward neural networks.markdown",
      "date": "31 Mar 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    }
  ],
  "descent": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/2024-03-04-Understanding the difficulty of training deep feedforward neural networks.markdown",
      "date": "31 Mar 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    }
  ],
  "saturation": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/2024-03-04-Understanding the difficulty of training deep feedforward neural networks.markdown",
      "date": "31 Mar 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    }
  ],
  "hide": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/2024-03-04-Understanding the difficulty of training deep feedforward neural networks.markdown",
      "date": "31 Mar 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    }
  ],
  "discover": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/2024-03-04-Understanding the difficulty of training deep feedforward neural networks.markdown",
      "date": "31 Mar 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    }
  ],
  "desaturate": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/2024-03-04-Understanding the difficulty of training deep feedforward neural networks.markdown",
      "date": "31 Mar 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "explains": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/2024-03-04-Understanding the difficulty of training deep feedforward neural networks.markdown",
      "date": "31 Mar 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    }
  ],
  "plateaus": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/2024-03-04-Understanding the difficulty of training deep feedforward neural networks.markdown",
      "date": "31 Mar 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    }
  ],
  "saturate": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/2024-03-04-Understanding the difficulty of training deep feedforward neural networks.markdown",
      "date": "31 Mar 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    }
  ],
  "jacobian": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/2024-03-04-Understanding the difficulty of training deep feedforward neural networks.markdown",
      "date": "31 Mar 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    }
  ],
  "associate": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/2024-03-04-Understanding the difficulty of training deep feedforward neural networks.markdown",
      "date": "31 Mar 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    }
  ],
  "what": [
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/2024-03-04-What do Self-supervised vision transformers learn.markdown",
      "date": "1 May 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    }
  ],
  "cl": [
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/2024-03-04-What do Self-supervised vision transformers learn.markdown",
      "date": "1 May 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    }
  ],
  "pattern": [
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/2024-03-04-What do Self-supervised vision transformers learn.markdown",
      "date": "1 May 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/2024-03-05-The geometry of hidden representations of large transformer models.markdown",
      "date": "30 Oct 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "orient": [
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/2024-03-04-What do Self-supervised vision transformers learn.markdown",
      "date": "1 May 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "separation": [
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/2024-03-04-What do Self-supervised vision transformers learn.markdown",
      "date": "1 May 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    }
  ],
  "frequency": [
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/2024-03-04-What do Self-supervised vision transformers learn.markdown",
      "date": "1 May 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    }
  ],
  "harmonize": [
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/2024-03-04-What do Self-supervised vision transformers learn.markdown",
      "date": "1 May 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    }
  ],
  "pace": [
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    }
  ],
  "multiclass": [
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    }
  ],
  "popularity": [
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    }
  ],
  "underlying": [
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    }
  ],
  "beam": [
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    }
  ],
  "teacher": [
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/2024-03-06-Hard Patches Mining for Masked Image Modeling.markdown",
      "date": "12 Apr 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "clustering": [
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "penultimate": [
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    }
  ],
  "resemblance": [
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/2024-03-04-When Does Label Smoothing Help.markdown",
      "date": "10 Jun 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    }
  ],
  "box": [
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/2024-03-04-White-Box Transformers via Sparse Rate Reduction.markdown",
      "date": "1 Jun 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    }
  ],
  "argue": [
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/2024-03-04-White-Box Transformers via Sparse Rate Reduction.markdown",
      "date": "1 Jun 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    }
  ],
  "compress": [
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/2024-03-04-White-Box Transformers via Sparse Rate Reduction.markdown",
      "date": "1 Jun 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    }
  ],
  "gaussian": [
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/2024-03-04-White-Box Transformers via Sparse Rate Reduction.markdown",
      "date": "1 Jun 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/2024-03-06-Denoising Masked Autoencoders Help Robust Classification.markdown",
      "date": "7 Mar 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    }
  ],
  "optimizer": [
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/2024-03-04-White-Box Transformers via Sparse Rate Reduction.markdown",
      "date": "1 Jun 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "alternate": [
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/2024-03-04-White-Box Transformers via Sparse Rate Reduction.markdown",
      "date": "1 Jun 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    }
  ],
  "sparsify": [
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/2024-03-04-White-Box Transformers via Sparse Rate Reduction.markdown",
      "date": "1 Jun 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "transformerlike": [
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/2024-03-04-White-Box Transformers via Sparse Rate Reduction.markdown",
      "date": "1 Jun 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    }
  ],
  "compres": [
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/2024-03-04-White-Box Transformers via Sparse Rate Reduction.markdown",
      "date": "1 Jun 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    }
  ],
  "look": [
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/2024-03-05-A Closer Look at Self-Supervised Lightweight Vision Transformers.markdown",
      "date": "3 May 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    }
  ],
  "notion": [
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/2024-03-05-A Closer Look at Self-Supervised Lightweight Vision Transformers.markdown",
      "date": "3 May 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    }
  ],
  "pretraining": [
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/2024-03-05-A Closer Look at Self-Supervised Lightweight Vision Transformers.markdown",
      "date": "3 May 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/2024-03-05-Masked Image Residual Learning for Scaling Deeper Vision Transformers.markdown",
      "date": "15 Nov 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "sota": [
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/2024-03-05-A Closer Look at Self-Supervised Lightweight Vision Transformers.markdown",
      "date": "3 May 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    }
  ],
  "lack": [
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/2024-03-05-A Closer Look at Self-Supervised Lightweight Vision Transformers.markdown",
      "date": "3 May 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    }
  ],
  "map": [
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/2024-03-05-A Closer Look at Self-Supervised Lightweight Vision Transformers.markdown",
      "date": "3 May 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/2024-03-05-Vision Transformers Need Register.markdown",
      "date": "28 Sep 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    }
  ],
  "enhancing": [
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/2024-03-05-A Closer Look at Self-Supervised Lightweight Vision Transformers.markdown",
      "date": "3 May 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    }
  ],
  "maebase": [
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/2024-03-05-A Closer Look at Self-Supervised Lightweight Vision Transformers.markdown",
      "date": "3 May 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    }
  ],
  "httpsgithubcomwangsr126maelite": [
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/2024-03-05-A Closer Look at Self-Supervised Lightweight Vision Transformers.markdown",
      "date": "3 May 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    }
  ],
  "adahessian": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "order": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/2024-03-05-Architecture-Agnostic Masked Image Modeling – From ViT back to CNN.markdown",
      "date": "2 Jun 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    }
  ],
  "incorporate": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/2024-03-06-Hard Patches Mining for Masked Image Modeling.markdown",
      "date": "12 Apr 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "curvature": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "hessian": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "approximation": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "square": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "averaging": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "cv": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "nlp": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/2024-03-05-An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.markdown",
      "date": "3 Jun 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    }
  ],
  "achievement": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "ppl": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "glue": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "criteo": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "ad": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "kaggle": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "adagrad": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "iteration": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "sourced": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/2024-03-05-AdaHessian An Adaptive Second Order Optimizer for Machine Learning.markdown",
      "date": "29 Apr 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "explores": [
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/2024-03-05-Adversarially Self-supervised Pre-training Improves Accuracy and Robustness.markdown",
      "date": "10 Mar 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    }
  ],
  "rotnet": [
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/2024-03-05-Adversarially Self-supervised Pre-training Improves Accuracy and Robustness.markdown",
      "date": "10 Mar 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    }
  ],
  "radius": [
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/2024-03-05-Adversarially Self-supervised Pre-training Improves Accuracy and Robustness.markdown",
      "date": "10 Mar 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/2024-03-06-Denoising Masked Autoencoders Help Robust Classification.markdown",
      "date": "7 Mar 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    }
  ],
  "instability": [
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/2024-03-05-An Empirical Study of Training Self-Supervised Vision Transformers.markdown",
      "date": "16 Aug 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    }
  ],
  "undermine": [
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/2024-03-05-An Empirical Study of Training Self-Supervised Vision Transformers.markdown",
      "date": "16 Aug 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    }
  ],
  "outcome": [
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/2024-03-05-An Empirical Study of Training Self-Supervised Vision Transformers.markdown",
      "date": "16 Aug 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/2024-03-05-Scaling Language-Image Pre-training via Masking.markdown",
      "date": "30 Mar 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    }
  ],
  "ablation": [
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/2024-03-05-An Empirical Study of Training Self-Supervised Vision Transformers.markdown",
      "date": "16 Aug 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    }
  ],
  "moco": [
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/2024-03-05-An Empirical Study of Training Self-Supervised Vision Transformers.markdown",
      "date": "16 Aug 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    }
  ],
  "v3": [
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/2024-03-05-An Empirical Study of Training Self-Supervised Vision Transformers.markdown",
      "date": "16 Aug 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    }
  ],
  "experience": [
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/2024-03-05-An Empirical Study of Training Self-Supervised Vision Transformers.markdown",
      "date": "16 Aug 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    }
  ],
  "worth": [
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/2024-03-05-An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.markdown",
      "date": "3 Jun 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    }
  ],
  "16x16": [
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/2024-03-05-An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.markdown",
      "date": "3 Jun 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    }
  ],
  "words": [
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/2024-03-05-An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.markdown",
      "date": "3 Jun 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    }
  ],
  "abandon": [
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/2024-03-05-An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.markdown",
      "date": "3 Jun 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    }
  ],
  "reliance": [
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/2024-03-05-An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.markdown",
      "date": "3 Jun 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    }
  ],
  "refer": [
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/2024-03-05-An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.markdown",
      "date": "3 Jun 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    }
  ],
  "vitcan": [
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/2024-03-05-An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.markdown",
      "date": "3 Jun 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    }
  ],
  "vtab": [
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/2024-03-05-An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.markdown",
      "date": "3 Jun 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    }
  ],
  "part": [
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/2024-03-05-Architecture-Agnostic Masked Image Modeling – From ViT back to CNN.markdown",
      "date": "2 Jun 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/2024-03-05-Beyond Masking Demystifying Token-Based Pre-Training for Vision Transformers.markdown",
      "date": "27 Mar 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/2024-03-05-Understanding Self-Supervised Pretraining with Part-Aware Representation Learning.markdown",
      "date": "23 Jan 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    }
  ],
  "reconstruct": [
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/2024-03-05-Architecture-Agnostic Masked Image Modeling – From ViT back to CNN.markdown",
      "date": "2 Jun 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learner",
      "url": "/2024-03-05-Masked Autoencoders Are Scalable Vision Learner.markdown",
      "date": "19 Dec 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/2024-03-06-Denoising Masked Autoencoders Help Robust Classification.markdown",
      "date": "7 Mar 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/2024-03-06-Hard Patches Mining for Masked Image Modeling.markdown",
      "date": "12 Apr 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "a2mim": [
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/2024-03-05-Architecture-Agnostic Masked Image Modeling – From ViT back to CNN.markdown",
      "date": "2 Jun 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    }
  ],
  "transferability": [
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/2024-03-05-Architecture-Agnostic Masked Image Modeling – From ViT back to CNN.markdown",
      "date": "2 Jun 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/2024-03-06-Denoising Masked Autoencoders Help Robust Classification.markdown",
      "date": "7 Mar 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    }
  ],
  "unna": [
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/2024-03-05-Are Labels Necessary for Neural Architecture Search.markdown",
      "date": "3 Aug 2020",
      "summary": "This paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning if effective neural architectures can be discovered using only images without human-annotated labels. Through two experimental setups—sample-based and search-based—we investigate this possibility. In the sample-based approach, we evaluate 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. In the search-based experiments, we employ a recognized NAS algorithm, DARTS, with various unsupervised objectives, finding that architectures identified without labels perform competitively compared to those found with labels. These findings suggest that human-annotated labels may not be necessary for identifying efficient neural architectures, as image statistics alone could be sufficient."
    }
  ],
  "setup": [
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/2024-03-05-Are Labels Necessary for Neural Architecture Search.markdown",
      "date": "3 Aug 2020",
      "summary": "This paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning if effective neural architectures can be discovered using only images without human-annotated labels. Through two experimental setups—sample-based and search-based—we investigate this possibility. In the sample-based approach, we evaluate 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. In the search-based experiments, we employ a recognized NAS algorithm, DARTS, with various unsupervised objectives, finding that architectures identified without labels perform competitively compared to those found with labels. These findings suggest that human-annotated labels may not be necessary for identifying efficient neural architectures, as image statistics alone could be sufficient."
    }
  ],
  "possibility": [
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/2024-03-05-Are Labels Necessary for Neural Architecture Search.markdown",
      "date": "3 Aug 2020",
      "summary": "This paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning if effective neural architectures can be discovered using only images without human-annotated labels. Through two experimental setups—sample-based and search-based—we investigate this possibility. In the sample-based approach, we evaluate 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. In the search-based experiments, we employ a recognized NAS algorithm, DARTS, with various unsupervised objectives, finding that architectures identified without labels perform competitively compared to those found with labels. These findings suggest that human-annotated labels may not be necessary for identifying efficient neural architectures, as image statistics alone could be sufficient."
    }
  ],
  "ranking": [
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/2024-03-05-Are Labels Necessary for Neural Architecture Search.markdown",
      "date": "3 Aug 2020",
      "summary": "This paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning if effective neural architectures can be discovered using only images without human-annotated labels. Through two experimental setups—sample-based and search-based—we investigate this possibility. In the sample-based approach, we evaluate 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. In the search-based experiments, we employ a recognized NAS algorithm, DARTS, with various unsupervised objectives, finding that architectures identified without labels perform competitively compared to those found with labels. These findings suggest that human-annotated labels may not be necessary for identifying efficient neural architectures, as image statistics alone could be sufficient."
    }
  ],
  "recognize": [
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/2024-03-05-Are Labels Necessary for Neural Architecture Search.markdown",
      "date": "3 Aug 2020",
      "summary": "This paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning if effective neural architectures can be discovered using only images without human-annotated labels. Through two experimental setups—sample-based and search-based—we investigate this possibility. In the sample-based approach, we evaluate 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. In the search-based experiments, we employ a recognized NAS algorithm, DARTS, with various unsupervised objectives, finding that architectures identified without labels perform competitively compared to those found with labels. These findings suggest that human-annotated labels may not be necessary for identifying efficient neural architectures, as image statistics alone could be sufficient."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/2024-03-05-Understanding Self-Supervised Pretraining with Part-Aware Representation Learning.markdown",
      "date": "23 Jan 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    }
  ],
  "nas": [
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/2024-03-05-Are Labels Necessary for Neural Architecture Search.markdown",
      "date": "3 Aug 2020",
      "summary": "This paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning if effective neural architectures can be discovered using only images without human-annotated labels. Through two experimental setups—sample-based and search-based—we investigate this possibility. In the sample-based approach, we evaluate 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. In the search-based experiments, we employ a recognized NAS algorithm, DARTS, with various unsupervised objectives, finding that architectures identified without labels perform competitively compared to those found with labels. These findings suggest that human-annotated labels may not be necessary for identifying efficient neural architectures, as image statistics alone could be sufficient."
    }
  ],
  "dart": [
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/2024-03-05-Are Labels Necessary for Neural Architecture Search.markdown",
      "date": "3 Aug 2020",
      "summary": "This paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning if effective neural architectures can be discovered using only images without human-annotated labels. Through two experimental setups—sample-based and search-based—we investigate this possibility. In the sample-based approach, we evaluate 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. In the search-based experiments, we employ a recognized NAS algorithm, DARTS, with various unsupervised objectives, finding that architectures identified without labels perform competitively compared to those found with labels. These findings suggest that human-annotated labels may not be necessary for identifying efficient neural architectures, as image statistics alone could be sufficient."
    }
  ],
  "demystify": [
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/2024-03-05-Beyond Masking Demystifying Token-Based Pre-Training for Vision Transformers.markdown",
      "date": "27 Mar 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    }
  ],
  "degrade": [
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/2024-03-05-Beyond Masking Demystifying Token-Based Pre-Training for Vision Transformers.markdown",
      "date": "27 Mar 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    }
  ],
  "misalignment": [
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/2024-03-05-Beyond Masking Demystifying Token-Based Pre-Training for Vision Transformers.markdown",
      "date": "27 Mar 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    }
  ],
  "httpsgithubcomsunsmarterjiebeyond": [
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/2024-03-05-Beyond Masking Demystifying Token-Based Pre-Training for Vision Transformers.markdown",
      "date": "27 Mar 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    }
  ],
  "colorization": [
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/2024-03-05-Colorization as a Proxy Task for Visual Understanding.markdown",
      "date": "13 Aug 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "colorize": [
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "grayscale": [
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "underconstrain": [
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "user": [
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "producing": [
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "frame": [
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "rebalance": [
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "pass": [
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "turing": [
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "participant": [
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "distinguish": [
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "illustrate": [
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "serve": [
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/2024-03-06-Denoising Masked Autoencoders Help Robust Classification.markdown",
      "date": "7 Mar 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    }
  ],
  "pretext": [
    {
      "title": "Colorful Image Colorization",
      "url": "/2024-03-05-Colorful Image Colorization.markdown",
      "date": "5 Oct 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "unlabel": [
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/2024-03-05-Colorization as a Proxy Task for Visual Understanding.markdown",
      "date": "13 Aug 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "voc": [
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/2024-03-05-Colorization as a Proxy Task for Visual Understanding.markdown",
      "date": "13 Aug 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    },
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/2024-03-05-Localizing Objects with Self-Supervised Transformers and no Labels.markdown",
      "date": "29 Sept 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    }
  ],
  "formulation": [
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/2024-03-05-Colorization as a Proxy Task for Visual Understanding.markdown",
      "date": "13 Aug 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    }
  ],
  "specific": [
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/2024-03-05-Colorization as a Proxy Task for Visual Understanding.markdown",
      "date": "13 Aug 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    }
  ],
  "revisit": [
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/2024-03-05-Colorization as a Proxy Task for Visual Understanding.markdown",
      "date": "13 Aug 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    }
  ],
  "volume": [
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/2024-03-05-Colorization as a Proxy Task for Visual Understanding.markdown",
      "date": "13 Aug 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    }
  ],
  "quantity": [
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/2024-03-05-Colorization as a Proxy Task for Visual Understanding.markdown",
      "date": "13 Aug 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    }
  ],
  "adaptability": [
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/2024-03-05-Colorization as a Proxy Task for Visual Understanding.markdown",
      "date": "13 Aug 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    }
  ],
  "power": [
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/2024-03-05-Colorization as a Proxy Task for Visual Understanding.markdown",
      "date": "13 Aug 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    }
  ],
  "mark": [
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/2024-03-05-Deeper Insights into the Robustness of ViTs towards Common Corruptions.markdown",
      "date": "19 Aug 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/2024-03-06-(Certified!!) Adversarial Robustness for Free.markdown",
      "date": "6 Mar 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    }
  ],
  "embed": [
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/2024-03-05-Deeper Insights into the Robustness of ViTs towards Common Corruptions.markdown",
      "date": "19 Aug 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    }
  ],
  "ffn": [
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/2024-03-05-Deeper Insights into the Robustness of ViTs towards Common Corruptions.markdown",
      "date": "19 Aug 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    }
  ],
  "scrutinize": [
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/2024-03-05-Deeper Insights into the Robustness of ViTs towards Common Corruptions.markdown",
      "date": "19 Aug 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    }
  ],
  "fourier": [
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/2024-03-05-Deeper Insights into the Robustness of ViTs towards Common Corruptions.markdown",
      "date": "19 Aug 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    }
  ],
  "parameterize": [
    {
      "title": "Designing Network Design Spaces",
      "url": "/2024-03-05-Designing Network Design Spaces.markdown",
      "date": "30 Mar 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    }
  ],
  "populations": [
    {
      "title": "Designing Network Design Spaces",
      "url": "/2024-03-05-Designing Network Design Spaces.markdown",
      "date": "30 Mar 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    }
  ],
  "akin": [
    {
      "title": "Designing Network Design Spaces",
      "url": "/2024-03-05-Designing Network Design Spaces.markdown",
      "date": "30 Mar 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/2024-03-06-Hard Patches Mining for Masked Image Modeling.markdown",
      "date": "12 Apr 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "regnet": [
    {
      "title": "Designing Network Design Spaces",
      "url": "/2024-03-05-Designing Network Design Spaces.markdown",
      "date": "30 Mar 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    }
  ],
  "width": [
    {
      "title": "Designing Network Design Spaces",
      "url": "/2024-03-05-Designing Network Design Spaces.markdown",
      "date": "30 Mar 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    }
  ],
  "efficientnet": [
    {
      "title": "Designing Network Design Spaces",
      "url": "/2024-03-05-Designing Network Design Spaces.markdown",
      "date": "30 Mar 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    }
  ],
  "blur": [
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/2024-03-05-Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions.markdown",
      "date": "20 Apr 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    }
  ],
  "edge": [
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/2024-03-05-Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions.markdown",
      "date": "20 Apr 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "bia": [
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/2024-03-05-Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions.markdown",
      "date": "20 Apr 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    }
  ],
  "description": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    }
  ],
  "internet": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    }
  ],
  "caption": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    }
  ],
  "eliminate": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/2024-03-05-Vision Transformers Need Register.markdown",
      "date": "28 Sep 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    }
  ],
  "ocr": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    }
  ],
  "geo": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    }
  ],
  "localization": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/2024-03-05-Localizing Objects with Self-Supervised Transformers and no Labels.markdown",
      "date": "29 Sept 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    }
  ],
  "baseline": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    }
  ],
  "httpsgithubcomopenaiclip": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/2024-03-05-Learning Transferable Visual Models From Natural Language Supervision.markdown",
      "date": "26 Feb 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    }
  ],
  "localize": [
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/2024-03-05-Localizing Objects with Self-Supervised Transformers and no Labels.markdown",
      "date": "29 Sept 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    }
  ],
  "collection": [
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/2024-03-05-Localizing Objects with Self-Supervised Transformers and no Labels.markdown",
      "date": "29 Sept 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    }
  ],
  "manner": [
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/2024-03-05-Localizing Objects with Self-Supervised Transformers and no Labels.markdown",
      "date": "29 Sept 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    }
  ],
  "corloc": [
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/2024-03-05-Localizing Objects with Self-Supervised Transformers and no Labels.markdown",
      "date": "29 Sept 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    }
  ],
  "point": [
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/2024-03-05-Localizing Objects with Self-Supervised Transformers and no Labels.markdown",
      "date": "29 Sept 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/2024-03-05-Transformer-Based Visual Segmentation A Surve.markdown",
      "date": "20 Dec 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/2024-03-06-(Certified!!) Adversarial Robustness for Free.markdown",
      "date": "6 Mar 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    }
  ],
  "pascal": [
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/2024-03-05-Localizing Objects with Self-Supervised Transformers and no Labels.markdown",
      "date": "29 Sept 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    }
  ],
  "detector": [
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/2024-03-05-Localizing Objects with Self-Supervised Transformers and no Labels.markdown",
      "date": "29 Sept 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "httpsgithubcomvaleoailost": [
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/2024-03-05-Localizing Objects with Self-Supervised Transformers and no Labels.markdown",
      "date": "29 Sept 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    }
  ],
  "presents": [
    {
      "title": "Masked Autoencoders Are Scalable Vision Learner",
      "url": "/2024-03-05-Masked Autoencoders Are Scalable Vision Learner.markdown",
      "date": "19 Dec 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    }
  ],
  "underpin": [
    {
      "title": "Masked Autoencoders Are Scalable Vision Learner",
      "url": "/2024-03-05-Masked Autoencoders Are Scalable Vision Learner.markdown",
      "date": "19 Dec 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    }
  ],
  "token": [
    {
      "title": "Masked Autoencoders Are Scalable Vision Learner",
      "url": "/2024-03-05-Masked Autoencoders Are Scalable Vision Learner.markdown",
      "date": "19 Dec 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/2024-03-05-Vision Transformers Need Register.markdown",
      "date": "28 Sep 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    }
  ],
  "portion": [
    {
      "title": "Masked Autoencoders Are Scalable Vision Learner",
      "url": "/2024-03-05-Masked Autoencoders Are Scalable Vision Learner.markdown",
      "date": "19 Dec 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/2024-03-05-Scaling Language-Image Pre-training via Masking.markdown",
      "date": "30 Mar 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    }
  ],
  "triple": [
    {
      "title": "Masked Autoencoders Are Scalable Vision Learner",
      "url": "/2024-03-05-Masked Autoencoders Are Scalable Vision Learner.markdown",
      "date": "19 Dec 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    }
  ],
  "standout": [
    {
      "title": "Masked Autoencoders Are Scalable Vision Learner",
      "url": "/2024-03-05-Masked Autoencoders Are Scalable Vision Learner.markdown",
      "date": "19 Dec 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    }
  ],
  "degradation": [
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/2024-03-05-Masked Image Residual Learning for Scaling Deeper Vision Transformers.markdown",
      "date": "15 Nov 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    }
  ],
  "mirl": [
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/2024-03-05-Masked Image Residual Learning for Scaling Deeper Vision Transformers.markdown",
      "date": "15 Nov 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    }
  ],
  "alleviate": [
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/2024-03-05-Masked Image Residual Learning for Scaling Deeper Vision Transformers.markdown",
      "date": "15 Nov 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    }
  ],
  "redefine": [
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/2024-03-05-Masked Image Residual Learning for Scaling Deeper Vision Transformers.markdown",
      "date": "15 Nov 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    }
  ],
  "recover": [
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/2024-03-05-Masked Image Residual Learning for Scaling Deeper Vision Transformers.markdown",
      "date": "15 Nov 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    }
  ],
  "s": [
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/2024-03-05-Masked Image Residual Learning for Scaling Deeper Vision Transformers.markdown",
      "date": "15 Nov 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    }
  ],
  "httpsgithubcomrussellllaputamirl": [
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/2024-03-05-Masked Image Residual Learning for Scaling Deeper Vision Transformers.markdown",
      "date": "15 Nov 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    }
  ],
  "formula": [
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/2024-03-05-Pre-training Vision Transformers with Very Limited Synthesized Images.markdown",
      "date": "31 Jul 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "fdsl": [
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/2024-03-05-Pre-training Vision Transformers with Very Limited Synthesized Images.markdown",
      "date": "31 Jul 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "fractal": [
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/2024-03-05-Pre-training Vision Transformers with Very Limited Synthesized Images.markdown",
      "date": "31 Jul 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    }
  ],
  "generating": [
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/2024-03-05-Pre-training Vision Transformers with Very Limited Synthesized Images.markdown",
      "date": "31 Jul 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    }
  ],
  "act": [
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/2024-03-05-Pre-training Vision Transformers with Very Limited Synthesized Images.markdown",
      "date": "31 Jul 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    }
  ],
  "database": [
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/2024-03-05-Pre-training Vision Transformers with Very Limited Synthesized Images.markdown",
      "date": "31 Jul 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    }
  ],
  "ofdb": [
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/2024-03-05-Pre-training Vision Transformers with Very Limited Synthesized Images.markdown",
      "date": "31 Jul 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    }
  ],
  "outperforming": [
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/2024-03-05-Pre-training Vision Transformers with Very Limited Synthesized Images.markdown",
      "date": "31 Jul 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    }
  ],
  "ofdbs": [
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/2024-03-05-Pre-training Vision Transformers with Very Limited Synthesized Images.markdown",
      "date": "31 Jul 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    }
  ],
  "replacing": [
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    }
  ],
  "contour": [
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "circumvent": [
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    }
  ],
  "privacycopyright": [
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "concern": [
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "inaccuracy": [
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    }
  ],
  "biases": [
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    }
  ],
  "avenue": [
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    }
  ],
  "hypothes": [
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/2024-03-05-Replacing Labeled Real-image Datasets with Auto-generated Contours.markdown",
      "date": "18 Jun 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    }
  ],
  "flip": [
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/2024-03-05-Scaling Language-Image Pre-training via Masking.markdown",
      "date": "30 Mar 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    }
  ],
  "clip": [
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/2024-03-05-Scaling Language-Image Pre-training via Masking.markdown",
      "date": "30 Mar 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    }
  ],
  "amount": [
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/2024-03-05-Scaling Language-Image Pre-training via Masking.markdown",
      "date": "30 Mar 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    }
  ],
  "speedup": [
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/2024-03-05-Scaling Language-Image Pre-training via Masking.markdown",
      "date": "30 Mar 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    }
  ],
  "period": [
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/2024-03-05-Scaling Language-Image Pre-training via Masking.markdown",
      "date": "30 Mar 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    }
  ],
  "anything": [
    {
      "title": "Segment Anything",
      "url": "/2024-03-05-Segment Anything.markdown",
      "date": "5 Apr 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    }
  ],
  "sa": [
    {
      "title": "Segment Anything",
      "url": "/2024-03-05-Segment Anything.markdown",
      "date": "5 Apr 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    }
  ],
  "date": [
    {
      "title": "Segment Anything",
      "url": "/2024-03-05-Segment Anything.markdown",
      "date": "5 Apr 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    }
  ],
  "privacy": [
    {
      "title": "Segment Anything",
      "url": "/2024-03-05-Segment Anything.markdown",
      "date": "5 Apr 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    }
  ],
  "respecting": [
    {
      "title": "Segment Anything",
      "url": "/2024-03-05-Segment Anything.markdown",
      "date": "5 Apr 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    }
  ],
  "engineer": [
    {
      "title": "Segment Anything",
      "url": "/2024-03-05-Segment Anything.markdown",
      "date": "5 Apr 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    }
  ],
  "promptability": [
    {
      "title": "Segment Anything",
      "url": "/2024-03-05-Segment Anything.markdown",
      "date": "5 Apr 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    }
  ],
  "sam": [
    {
      "title": "Segment Anything",
      "url": "/2024-03-05-Segment Anything.markdown",
      "date": "5 Apr 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    }
  ],
  "httpssegmentanythingcom": [
    {
      "title": "Segment Anything",
      "url": "/2024-03-05-Segment Anything.markdown",
      "date": "5 Apr 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    }
  ],
  "using": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "treat": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "node": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "graph": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "represent": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "eigendecomposition": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "foreground": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "eigenvector": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "indication": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "likelihood": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "belong": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "margin": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "voc07": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "voc12": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "coco20k": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "cad": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "saliency": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "iou": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "ecssd": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "duts": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "dut": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "omron": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "cub": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "httpswwwmpsifrpaperstokencut2022": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/2024-03-05-Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut.markdown",
      "date": "24 Mar 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "geometry": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/2024-03-05-The geometry of hidden representations of large transformer models.markdown",
      "date": "30 Oct 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "hidden": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/2024-03-05-The geometry of hidden representations of large transformer models.markdown",
      "date": "30 Oct 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "protein": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/2024-03-05-The geometry of hidden representations of large transformer models.markdown",
      "date": "30 Oct 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "sequences": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/2024-03-05-The geometry of hidden representations of large transformer models.markdown",
      "date": "30 Oct 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "evolution": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/2024-03-05-The geometry of hidden representations of large transformer models.markdown",
      "date": "30 Oct 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/2024-03-05-Transformer-Based Visual Segmentation A Surve.markdown",
      "date": "20 Dec 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "manifold": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/2024-03-05-The geometry of hidden representations of large transformer models.markdown",
      "date": "30 Oct 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "contract": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/2024-03-05-The geometry of hidden representations of large transformer models.markdown",
      "date": "30 Oct 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "id": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/2024-03-05-The geometry of hidden representations of large transformer models.markdown",
      "date": "30 Oct 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "stabilize": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/2024-03-05-The geometry of hidden representations of large transformer models.markdown",
      "date": "30 Oct 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "peak": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/2024-03-05-The geometry of hidden representations of large transformer models.markdown",
      "date": "30 Oct 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "expansion": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/2024-03-05-The geometry of hidden representations of large transformer models.markdown",
      "date": "30 Oct 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "trend": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/2024-03-05-The geometry of hidden representations of large transformer models.markdown",
      "date": "30 Oct 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "pinpoint": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/2024-03-05-The geometry of hidden representations of large transformer models.markdown",
      "date": "30 Oct 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "tinyvit": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "series": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "compact": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "device": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "distil": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "logit": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "store": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "counterpart": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "comparable": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "swin": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "l": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/2024-03-05-TinyViT Fast Pretraining Distillation for Small Vision Transformer.markdown",
      "date": "21 Jul 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "driving": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/2024-03-05-Transformer-Based Visual Segmentation A Surve.markdown",
      "date": "20 Dec 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "edit": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/2024-03-05-Transformer-Based Visual Segmentation A Surve.markdown",
      "date": "20 Dec 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "robot": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/2024-03-05-Transformer-Based Visual Segmentation A Surve.markdown",
      "date": "20 Dec 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "sense": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/2024-03-05-Transformer-Based Visual Segmentation A Surve.markdown",
      "date": "20 Dec 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "advent": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/2024-03-05-Transformer-Based Visual Segmentation A Surve.markdown",
      "date": "20 Dec 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "there": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/2024-03-05-Transformer-Based Visual Segmentation A Surve.markdown",
      "date": "20 Dec 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "cover": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/2024-03-05-Transformer-Based Visual Segmentation A Surve.markdown",
      "date": "20 Dec 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "simplifies": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/2024-03-05-Transformer-Based Visual Segmentation A Surve.markdown",
      "date": "20 Dec 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "d": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/2024-03-05-Transformer-Based Visual Segmentation A Surve.markdown",
      "date": "20 Dec 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "cloud": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/2024-03-05-Transformer-Based Visual Segmentation A Surve.markdown",
      "date": "20 Dec 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "foundation": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/2024-03-05-Transformer-Based Visual Segmentation A Surve.markdown",
      "date": "20 Dec 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "outline": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/2024-03-05-Transformer-Based Visual Segmentation A Surve.markdown",
      "date": "20 Dec 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "httpsgithubcomlxtghawesomesegmentationwithtransformer": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/2024-03-05-Transformer-Based Visual Segmentation A Surve.markdown",
      "date": "20 Dec 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "turbo": [
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/2024-03-05-Turbo Training with Token Dropout.markdown",
      "date": "10 Oct 2022",
      "summary": "This paper introduces Turbo training, an efficient training method for video-related tasks using Transformers. Turbo training offers three major contributions: Firstly, it presents a simple yet versatile training paradigm applicable to multiple video tasks. Secondly, it demonstrates Turbo training's effectiveness across action classification, video-language representation learning, and long-video activity classification, achieving competitive performance with up to 4× speed-up and reduced memory usage. Thirdly, it enables long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous methods that were resource-intensive."
    }
  ],
  "relate": [
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/2024-03-05-Turbo Training with Token Dropout.markdown",
      "date": "10 Oct 2022",
      "summary": "This paper introduces Turbo training, an efficient training method for video-related tasks using Transformers. Turbo training offers three major contributions: Firstly, it presents a simple yet versatile training paradigm applicable to multiple video tasks. Secondly, it demonstrates Turbo training's effectiveness across action classification, video-language representation learning, and long-video activity classification, achieving competitive performance with up to 4× speed-up and reduced memory usage. Thirdly, it enables long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous methods that were resource-intensive."
    }
  ],
  "motivate": [
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/2024-03-05-Understanding Self-Supervised Pretraining with Part-Aware Representation Learning.markdown",
      "date": "23 Jan 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    }
  ],
  "inferring": [
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/2024-03-05-Understanding Self-Supervised Pretraining with Part-Aware Representation Learning.markdown",
      "date": "23 Jan 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    }
  ],
  "predispose": [
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/2024-03-05-Understanding Self-Supervised Pretraining with Part-Aware Representation Learning.markdown",
      "date": "23 Jan 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    }
  ],
  "shelf": [
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/2024-03-05-Understanding Self-Supervised Pretraining with Part-Aware Representation Learning.markdown",
      "date": "23 Jan 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    }
  ],
  "uap": [
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/2024-03-05-Universal Adversarial Robustness of Texture and Shape-biased Models.markdown",
      "date": "31 Aug 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    }
  ],
  "retain": [
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/2024-03-05-Universal Adversarial Robustness of Texture and Shape-biased Models.markdown",
      "date": "31 Aug 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    }
  ],
  "register": [
    {
      "title": "Vision Transformers Need Registers",
      "url": "/2024-03-05-Vision Transformers Need Register.markdown",
      "date": "28 Sep 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    }
  ],
  "artifact": [
    {
      "title": "Vision Transformers Need Registers",
      "url": "/2024-03-05-Vision Transformers Need Register.markdown",
      "date": "28 Sep 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    }
  ],
  "appear": [
    {
      "title": "Vision Transformers Need Registers",
      "url": "/2024-03-05-Vision Transformers Need Register.markdown",
      "date": "28 Sep 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    }
  ],
  "resolve": [
    {
      "title": "Vision Transformers Need Registers",
      "url": "/2024-03-05-Vision Transformers Need Register.markdown",
      "date": "28 Sep 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    }
  ],
  "atom": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "waves": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "exfractaldb": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "harmonic": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "visualatom": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "near": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "jft": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "m": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "static": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "costserror": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "biase": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/2024-03-05-Visual Atoms Pre-training Vision Transformers with Sinusoidal Waves.markdown",
      "date": "2 Mar 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "free": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/2024-03-06-(Certified!!) Adversarial Robustness for Free.markdown",
      "date": "6 Mar 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    }
  ],
  "demonstrates": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/2024-03-06-(Certified!!) Adversarial Robustness for Free.markdown",
      "date": "6 Mar 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    }
  ],
  "certify": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/2024-03-06-(Certified!!) Adversarial Robustness for Free.markdown",
      "date": "6 Mar 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    }
  ],
  "2norm": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/2024-03-06-(Certified!!) Adversarial Robustness for Free.markdown",
      "date": "6 Mar 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    }
  ],
  "denoise": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/2024-03-06-(Certified!!) Adversarial Robustness for Free.markdown",
      "date": "6 Mar 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/2024-03-06-Denoising Masked Autoencoders Help Robust Classification.markdown",
      "date": "7 Mar 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "denoising": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/2024-03-06-(Certified!!) Adversarial Robustness for Free.markdown",
      "date": "6 Mar 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/2024-03-06-Denoising Masked Autoencoders Help Robust Classification.markdown",
      "date": "7 Mar 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    }
  ],
  "diffusion": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/2024-03-06-(Certified!!) Adversarial Robustness for Free.markdown",
      "date": "6 Mar 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    }
  ],
  "constraint": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/2024-03-06-(Certified!!) Adversarial Robustness for Free.markdown",
      "date": "6 Mar 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    }
  ],
  "percentage": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/2024-03-06-(Certified!!) Adversarial Robustness for Free.markdown",
      "date": "6 Mar 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    }
  ],
  "over": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/2024-03-06-(Certified!!) Adversarial Robustness for Free.markdown",
      "date": "6 Mar 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    }
  ],
  "without": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/2024-03-06-(Certified!!) Adversarial Robustness for Free.markdown",
      "date": "6 Mar 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    }
  ],
  "retrain": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/2024-03-06-(Certified!!) Adversarial Robustness for Free.markdown",
      "date": "6 Mar 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    }
  ],
  "dmae": [
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/2024-03-06-Denoising Masked Autoencoders Help Robust Classification.markdown",
      "date": "7 Mar 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    }
  ],
  "semantic": [
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/2024-03-06-Denoising Masked Autoencoders Help Robust Classification.markdown",
      "date": "7 Mar 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    }
  ],
  "applicability": [
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/2024-03-06-Denoising Masked Autoencoders Help Robust Classification.markdown",
      "date": "7 Mar 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    }
  ],
  "mapping": [
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "auxiliary": [
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/2024-03-06-Hard Patches Mining for Masked Image Modeling.markdown",
      "date": "12 Apr 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "ebm": [
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "energy": [
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "restoration": [
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "assign": [
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "restore": [
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "sort": [
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "epoch": [
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/2024-03-06-Energy-inspired Self-supervised Pretraining For Vision Models.markdown",
      "date": "2 Feb 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "mine": [
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/2024-03-06-Hard Patches Mining for Masked Image Modeling.markdown",
      "date": "12 Apr 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "mining": [
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/2024-03-06-Hard Patches Mining for Masked Image Modeling.markdown",
      "date": "12 Apr 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "hpm": [
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/2024-03-06-Hard Patches Mining for Masked Image Modeling.markdown",
      "date": "12 Apr 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "go": [
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/2024-03-06-Hard Patches Mining for Masked Image Modeling.markdown",
      "date": "12 Apr 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "solving": [
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/2024-03-06-Hard Patches Mining for Masked Image Modeling.markdown",
      "date": "12 Apr 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "student": [
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/2024-03-06-Hard Patches Mining for Masked Image Modeling.markdown",
      "date": "12 Apr 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "predictor": [
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/2024-03-06-Hard Patches Mining for Masked Image Modeling.markdown",
      "date": "12 Apr 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ]
}
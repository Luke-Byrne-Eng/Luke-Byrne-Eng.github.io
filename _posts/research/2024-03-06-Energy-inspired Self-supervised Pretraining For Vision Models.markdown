---
layout: paper
title:  "Energy-inspired Self-supervised Pretraining For Vision Models"
date:   2 Feb 2023
categories: research
paper_url: https://arxiv.org/pdf/2302.01384
code_url: 
summary: "This paper introduces a self-supervised vision model pretraining framework inspired by energy-based models (EBMs), leveraging symmetric mappings in deep networks without auxiliary components. The framework models energy estimation and data restoration through the network's forward and backward passes, assigning low energy to unlabeled dataset samples and using gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting various pretext tasks. Experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
---


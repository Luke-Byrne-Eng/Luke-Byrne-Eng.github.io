{
  "convolution": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "This paper shows self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs or convnets. Also proposes a modification to BYOL self-supervised learning with a momentum encoder, multi-crop training, and feature sharping for the teacher network."
    },
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/research/2021/05/03/Robust-and-Generalizable-Visual-Representation-Learning-via-Random-Convolutions.html",
      "date": "May 3, 2021",
      "summary": "This study introduces random convolutions for data augmentation. Random convolutions approximately preserve shape while distorting local textures. This method significantly enhances performance on domain generalization and robustness / corruption benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods."
    },
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a method for dropout in Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. The authors find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. "
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets."
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, this paper introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, the authors enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. This simple method surpasses the current state-of-the-art in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, enhances vision tasks using Vision transformers by masking and reconstructing parts of an image. The compatibility of MIM with CNNs and its operational principle are unclear. this study reveals that MIM improves generalized feature extraction through middle-order patch interactions and introduces Architecture-Agnostic Masked Image Modeling (A2MIM), compatible with both Transformers and CNNs. Extensive testing demonstrates A2MIM's ability to enhance representation learning and transferability to various tasks without specialized modifications."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results. The method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. A convolutional neural network (CNN) was trained on over a million color images, operating as a feed-forward pass at test time. The algorithm's effectiveness was validated through a colorization Turing test, where it deceived participants in 32% of trials. The paper also illustrates how colorization can serve as an effective pretraining task for self-supervised feature learning, achieving state-of-the-art results on several benchmarks."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Incorporating diverse image styles into CNN training data reduces texture bias, enhances shape recognition, and improves resilience against common image corruptions. This is typically explained as decreasing model reliance on high frequency texture information. This paper challenges this explaniation through large scale testing with natural images, edge information, and stylization, finding no direct link between shape bias and robustness. The enhanced corruption robustness is instead attributed to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "This study challenges the traditional understanding of how Convolutional Neural Networks (CNNs) recognize objects, revealing that they are more biased towards recognizing textures rather than learning complex shapes. The authors found that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, training a ResNet-50 on a stylized version of ImageNet, designed to emphasize shape, aligned the network's performance more closely with human behavior. This shape-based training matched human performance in controlled experiments and enhanced object detection and robustness against image distortions, highlighting the benefits of shape-based representations in visual recognition systems."
    },
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "This paper introduces the Kinetics Human Action Video dataset. The authors note that the limited number of videos in existing datasets like UCF-101 and HMDB-51 hinders the ability to effectively evaluate architectures due to similar performance across these small benchmarks. Kinetics contains 400 classes with over 400 video clips per class from challenging YouTube videos. The paper analyzes the impact of this larger dataset on the performance of existing architectures as well as the benefits of pre-training models on Kinetics. The authors introduce the Two-Stream Inflated 3D ConvNet (I3D), an extension of 2D ConvNets to 3D for improved video feature extraction. When pre-trained on Kinetics, this I3D model sets new benchmarks for action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    },
    {
      "title": "Are Convolutional Neural Networks or Transformers more like human vision?",
      "url": "/research/2021/07/01/Are-Convolutional-Neural-Networks-or-Transformers-more-like-human-vision.html",
      "date": "July 1, 2021",
      "summary": "This study investigates the performance of Convolutional Neural Networks (CNNs) and attention-based Vision Transformers (ViTs) in computer vision tasks. While CNNs excel in accuracy, ViTs offer a different approach with weaker inductive biases. By analyzing error patterns, we find that ViTs exhibit consistency with human errors, suggesting potential for more human-like vision models and insights into human object recognition."
    },
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    },
    {
      "title": "Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding",
      "url": "/research/2021/11/16/Improved-Robustness-of-Vision-Transformer-via-PreLayerNorm-in-Patch-Embedding.html",
      "date": "November 16, 2021",
      "summary": "Recent advancements in vision transformers (ViTs) have shown superior performance across diverse visual tasks, surpassing convolutional neural networks (CNNs). Given ViT's distinct architecture, understanding its behavior and reliability is imperative. This paper investigates ViT's robustness by comparing it with CNNs under various image corruptions relevant to real-world vision tasks. While ViT generally exhibits comparable or improved robustness over CNNs, it consistently underperforms in contrast enhancement tasks. Analysis suggests that positional embedding in ViT's patch embedding may malfunction with color scale changes. We propose PreLayerNorm, a modified patch embedding structure, to address this issue and ensure scale-invariant behavior in ViT. ViT with PreLayerNorm demonstrates enhanced robustness across various corruptions, particularly in contrast-varying environments."
    },
    {
      "title": "Language Modeling with Gated Convolutional Networks",
      "url": "/research/2017/09/08/Language-Modeling-with-Gated-Convolutional-Networks.html",
      "date": "September 8, 2017",
      "summary": "This paper introduces a novel language modeling approach using stacked convolutions that enable efficient parallel processing, in contrast to the traditionally used recurrent neural networks known for handling unbounded context. The proposed method features a simplified gating mechanism that outperforms previous models and demonstrates superior performance on the WikiText-103 benchmark, showing its capability to manage long-term dependencies. It also delivers competitive results on the Google Billion Words benchmark and significantly reduces sentence scoring latency compared to recurrent models. This marks the first instance of a non-recurrent model achieving comparable success to strong recurrent models on major language tasks."
    },
    {
      "title": "Do Vision Transformers See Like Convolutional Neural Networks?",
      "url": "/research/2022/03/03/Do-Vision-Transformers-See-Like-Convolutional-Neural-Networks.html",
      "date": "March 3, 2022",
      "summary": "Convolutional Neural Networks (CNNs) have been prominent in processing visual data, but recent studies show Vision Transformers (ViTs) can match or outperform CNNs in image classification. This work examines how ViTs solve classification tasks, discovering significant differences from CNNs, including uniformity in ViT representations across layers due to self-attention and residual connections that enhance feature propagation. ViTs also maintain spatial information effectively, influenced by classification methods. The study further explores how dataset scale affects ViT features and their transferability, linking these findings to new architectures like MLP-Mixer."
    },
    {
      "title": "On the Relationship Between Self-Attention And Convolutional Layers",
      "url": "/research/2020/01/10/On-the-Relationship-Between-Self-Attention-And-Convolutional-Layers.html",
      "date": "January 10, 2020",
      "summary": "Recent studies have shown that attention mechanisms can rival or even outperform convolutional layers in vision tasks, challenging their dominance. Ramachandran et al. (2019) demonstrated that attention could entirely replace convolution, achieving top results. This work investigates whether attention layers function like convolutional layers, finding that multi-head self-attention layers, with enough heads, match the expressiveness of convolutional layers. Numerical experiments confirm that self-attention layers learn to focus on pixel-grid patterns akin to convolutional layers, supporting our findings."
    }
  ],
  "roi": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation."
    }
  ],
  "pool": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation."
    }
  ],
  "module": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation."
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "This paper introduces the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a multi-directional Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "This paper introduces DiffBIR, a framework that utilizes pretrained text-to-image diffusion models for blind image restoration. The two-stage pipeline involves pretraining a restoration module on various degradations to enhance real-world applicability, followed by the use of latent diffusion models for realistic restoration. The authors introduce a novel injective modulation sub-network, LAControlNet, for fine-tuning and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, a controllable module allows users to adjust quality and fidelity during the denoising process. Extensive tests demonstrate DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets."
    },
    {
      "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
      "url": "/research/2024/02/28/CL-MAE:-Curriculum-Learned-Masked-Autoencoders.html",
      "date": "February 28, 2024",
      "summary": "This paper proposes a curriculum learning approach that updates the masking strategy of Masked Auto Encoders (MAE) to progressively increase the complexity of the self-supervised reconstruction task. To achieve this, a novel learnable masking module is introduced, capable of generating masks of varying complexities, and integrated into masked autoencoders (MAE). This module is jointly trained with the MAE, adjusting its behavior from partner (optimizing the same reconstruction loss) to adversary (optimizing the opposite loss), with a smooth transition regulated by a factor multiplied with the reconstruction loss. This training procedure creates an easy-to-hard curriculum. The Curriculum-Learned Masked Autoencoder (CL-MAE) is trained on ImageNet and demonstrates superior representation learning compared to MAE. Empirical results on five downstream tasks confirm that curriculum learning can successfully self-supervise masked autoencoders."
    }
  ],
  "transformation": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation."
    },
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a method for contrastive learning of visual representations. Authors find that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch size with more training steps improve representation quality. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, this paper introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, the authors enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. This simple method surpasses the current state-of-the-art in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. The Authors show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper investigates how self-supervised pretraining methods learn part-aware representations. The authors describe contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting that these methods predispose encoders to recognize object parts. Through empirical comparison, they find that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "This paper introduces Mask-Reconstruct Augmentation (MRA), an image augmentation technique that leverages self-supervised masked autoencoders to generate distorted inputs and address the issue of overfitting in deep neural networks. Inspired by the success of masked image modeling in self-supervised learning, MRA uses nonlinear transformations for regularization, as opposed to current hand-crafted linear techniques like scale, flip, and color jitter. Extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks."
    },
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    },
    {
      "title": "FNet: Mixing Tokens with Fourier Transforms",
      "url": "/research/2022/05/26/FNet-Mixing-Tokens-with-Fourier-Transforms.html",
      "date": "May 26, 2022",
      "summary": "We demonstrate that Transformer encoder architectures can be accelerated with minimal impact on accuracy by substituting self-attention sublayers with simple linear transformations. Remarkably, using a standard Fourier Transform instead of the self-attention sublayer in a Transformer encoder achieves 92-97% of BERT models' accuracy on the GLUE benchmark, while training 80% faster on GPUs and 70% faster on TPUs for standard 512 input lengths. Our FNet model significantly outperforms in speed at longer input lengths, matching the accuracy of the most accurate models on the Long Range Arena benchmark and surpassing the fastest models in speed across all sequence lengths on GPUs and shorter lengths on TPUs. Additionally, FNet is lightweight in memory use and exceptionally efficient in smaller sizes, outperforming Transformer models under the same speed and accuracy constraints."
    },
    {
      "title": "Learning and Leveraging World Models in Visual Representation Learning",
      "url": "/research/2024/03/01/Learning-and-Leveraging-World-Models-in-Visual-Representation-Learning.html",
      "date": "March 1, 2024",
      "summary": "The Joint-Embedding Predictive Architecture (JEPA) is a self-supervised method previously used for predicting missing input parts. This study expands JEPA to predict a wider range of corruptions by introducing Image World Models (IWM), which learn the effects of global photometric transformations in latent space. Key to effective IWM learning are conditioning, prediction difficulty, and model capacity. The study demonstrates that IWM's predictive world model, when fine-tuned, can tackle diverse tasks and either matches or outperforms existing self-supervised techniques. Additionally, it enables control over the abstraction level of learned representations, achieving either invariant or equivariant representations, similar to contrastive methods or masked image modeling, respectively."
    },
    {
      "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum",
      "url": "/research/2023/09/30/When-to-Learn-What:-Model-Adaptive-Data-Augmentation-Curriculum.html",
      "date": "September 30, 2023",
      "summary": "Data augmentation (DA) improves neural network generalization by enforcing invariances and symmetries to predefined transformations applied to input data. However, fixed augmentation policies affect samples differently at various training stages, and existing approaches cannot adapt policies to individual samples and the training model. This paper proposes Model-Adaptive Data Augmentation (MADAug) which trains an augmentation policy network to determine when to learn what. Unlike previous work, MADAug selects augmentation operators for each input image with a model-adaptive policy that varies between training stages, creating an optimized data augmentation curriculum. The policy is trained using a bi-level optimization scheme to minimize validation-set loss of a model trained with policy-produced augmentations. Extensive evaluations on multiple image classification tasks and network architectures show that MADAug outperforms or matches existing DA approaches, enhances fairness by improving all classes, particularly the difficult ones, and performs better when transferred to fine-grained datasets. Additionally, the auto-optimized policy in MADAug gradually increases perturbations, forming an easy-to-hard curriculum."
    }
  ],
  "augmentation": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation."
    },
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a method for contrastive learning of visual representations. Authors find that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch size with more training steps improve representation quality. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/research/2020/09/10/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning.html",
      "date": "September 10, 2020",
      "summary": "This paper introduces BYOL, a self-supervised learning method that does not need negative pairs. BYOL uses a teacher student pair of networks, where the teacher is the EMA of the student. The teacher and student are given two differently augmented versionos the input, and the loss is the cross entropy difference between the outputs."
    },
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights. This paper shows deep networks have significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. The authors propose a stochastic projection rule that sets a new benchmark CIFAR-10 without data augmentation."
    },
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/research/2023/03/08/Extreme-Masking-for-Learning-Instance-and-Distributed-Visual-Representations.html",
      "date": "March 8, 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance."
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. This self-supervised method uses multiple crops of an image. Given a context crop the network is trained to predict the embeddings of several target crops. When applied to Vision Transformers, I-JEPA acheived strong performance in various tasks like image classification and depth prediction, outperforming Masked Auto Encoding (MAE) in linear probing when controlling for compute."
    },
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/research/2021/05/03/Robust-and-Generalizable-Visual-Representation-Learning-via-Random-Convolutions.html",
      "date": "May 3, 2021",
      "summary": "This study introduces random convolutions for data augmentation. Random convolutions approximately preserve shape while distorting local textures. This method significantly enhances performance on domain generalization and robustness / corruption benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods."
    },
    {
      "title": "Dropout as Data Augmentation",
      "url": "/research/2016/01/08/DROPOUT-AS-DATA-AUGMENTATION.html",
      "date": "January 8, 2016",
      "summary": "Dropout is typically interpreted as turning the test time network into an ensemble of the thinner training networks. This paper argues that dropout can also be interpreted as a kind of data augmentation in image space. Authors present an approach to project the dropout noise within a network back into the input space, visualising the augmented versions of the training data, and show that training a deterministic network on the augmented samples yields similar results. Authors then propose a new dropout noise scheme and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/research/2023/06/30/Impact-of-Noise-on-Calibration-and-Generalisation-of-Neural-Networks.html",
      "date": "June 30, 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. Activation noise is shown to significantly improve generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/ON-INTERACTION-BETWEEN-AUGMENTATIONS-AND-CORRUPTIONS-IN-NATURAL-CORRUPTION-ROBUSTNESS.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, the correlation between data augmentations and test-time corruptions remains unclear. This paper introduces Minimal Sample Distance, and demonstrates a strong correlation between augmentation-corruption similarity and performance. Authors argue that training with augmentations that are perceptually similar to corruptions enhances test error."
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, this paper introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, the authors enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. This simple method surpasses the current state-of-the-art in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Data augmentation improves generalization, and robustness against image corruptions. However, the use of complex augmentation pipelines typically involves tuning of an enormous number of hyper parameters. RandAugment addresses this by reducing the search space. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of Vision Transformer (ViT) variants against common corruptions. The benchmarking reveals that overlapping patch embedding and convolutional feed-forward networks (FFNs) significantly enhance ViT robustness. The study also scrutinizes the effectiveness of CNN-based data augmentation strategies when applied to ViTs, finding that adversarial noise training is effective, while fourier-domain augmentation falls short. A new conditional method for generating dynamic augmentation parameters based on input images is proposed, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Incorporating diverse image styles into CNN training data reduces texture bias, enhances shape recognition, and improves resilience against common image corruptions. This is typically explained as decreasing model reliance on high frequency texture information. This paper challenges this explaniation through large scale testing with natural images, edge information, and stylization, finding no direct link between shape bias and robustness. The enhanced corruption robustness is instead attributed to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) uses synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. Given this perspective, a one-instance fractal database (OFDB) is developed where only a single image per category is present. Despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M comparable or superior results are obtaine to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "This paper explores the interactions between training data amount, model regularization or data augmentation (AugReg), model size, and compute budget for Vision Transformers (ViT) in various vision tasks. The authors find that using more compute and AugReg can achieve the same performance as training with significantly more data. They demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "This paper introduces Mask-Reconstruct Augmentation (MRA), an image augmentation technique that leverages self-supervised masked autoencoders to generate distorted inputs and address the issue of overfitting in deep neural networks. Inspired by the success of masked image modeling in self-supervised learning, MRA uses nonlinear transformations for regularization, as opposed to current hand-crafted linear techniques like scale, flip, and color jitter. Extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks."
    },
    {
      "title": "Better plain ViT baselines for ImageNet-1k",
      "url": "/research/2022/05/02/Better-plain-ViT-baselines-for-ImageNet-1k.html",
      "date": "May 2, 2022",
      "summary": "This paper challenges the common belief that the Vision Transformer (ViT) model requires sophisticated regularization techniques to excel on ImageNet-1k scale data. The authors present minor modifications to the original ViT vanilla training setting that significantly improve the performance of plain ViT models. They find that standard data augmentation is sufficient, with 90 epochs of training surpassing 76% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reaching 80% in less than one day."
    },
    {
      "title": "DeiT III: Revenge of the ViT",
      "url": "/research/2023/01/18/DeiT-III-Revenge-of-the-ViT.html",
      "date": "January 18, 2023",
      "summary": "This paper explores the supervised training of Vision Transformers (ViTs) using a simplified training approach adapted from ResNet-50 that includes a novel data-augmentation method with just 3 augmentations. The study demonstrates that this method significantly improves ViTs' performance in image classification, transfer learning, and semantic segmentation over previous supervised training techniques. Moreover, it shows ViTs' performance can match newer architectures, providing a new benchmark for evaluating self-supervised methods on ViTs."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "This paper introduces FixMatch, a streamlined semi-supervised learning (SSL) algorithm. FixMatch generates pseudo-labels using a model's predictions on weakly-augmented unlabeled images, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness, and the code is publicly available."
    },
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This paper investigates using high-quality rendering software and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle. This synthetic dataset is used to augment limited real training data for image classification and semantic segmentation tasks. While models trained solely on synthetic images showed low accuracy on real validation data, including even small amounts of real data significantly improved performance. Augmenting real training data with synthetic images outperformed using only real images. Furthermore, pretraining models on the synthetic dataset before transfer learning significantly reduced training costs by allowing most of the training to be completed upfront using the synthetic data."
    },
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "The paper introduces TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method that applies a single augmentation to each image. Despite its minimal complexity and cost, TrivialAugment outperforms existing methods across various image classification scenarios, as validated through extensive experiments against state-of-the-art methods and multiple ablation studies involving different augmentation spaces and methods. The work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Noting a stagnation in automatic augmentation research, the authors conclude by proposing best practices for future advancements in the field."
    },
    {
      "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
      "url": "/research/2021/07/24/The-Many-Faces-of-Robustness-A-Critical-Analysis-of-Out-of-Distribution-Generalization.html",
      "date": "July 24, 2021",
      "summary": "We present four novel real-world distribution shift datasets encompassing changes in image style, blurriness, location, camera settings, and more. Evaluating existing methods for enhancing out-of-distribution robustness, we discover that employing larger models and artificial data augmentations can enhance robustness against real-world distribution shifts, contradicting prior claims. Our findings demonstrate that improvements in artificial robustness benchmarks can indeed transfer to real-world distribution shifts, contrary to prior assumptions. Additionally, we introduce a novel data augmentation technique that surpasses models pretrained with significantly more labeled data, emphasizing its efficacy in addressing real-world distribution shifts. While some methods consistently mitigate texture and local image statistics shifts, they fail to address other shifts like geographic changes. Our results underscore the necessity for future research to examine multiple distribution shifts concurrently, as no single method consistently improves robustness across all evaluated scenarios."
    },
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    },
    {
      "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum",
      "url": "/research/2023/09/30/When-to-Learn-What:-Model-Adaptive-Data-Augmentation-Curriculum.html",
      "date": "September 30, 2023",
      "summary": "Data augmentation (DA) improves neural network generalization by enforcing invariances and symmetries to predefined transformations applied to input data. However, fixed augmentation policies affect samples differently at various training stages, and existing approaches cannot adapt policies to individual samples and the training model. This paper proposes Model-Adaptive Data Augmentation (MADAug) which trains an augmentation policy network to determine when to learn what. Unlike previous work, MADAug selects augmentation operators for each input image with a model-adaptive policy that varies between training stages, creating an optimized data augmentation curriculum. The policy is trained using a bi-level optimization scheme to minimize validation-set loss of a model trained with policy-produced augmentations. Extensive evaluations on multiple image classification tasks and network architectures show that MADAug outperforms or matches existing DA approaches, enhances fairness by improving all classes, particularly the difficult ones, and performs better when transferred to fine-grained datasets. Additionally, the auto-optimized policy in MADAug gradually increases perturbations, forming an easy-to-hard curriculum."
    }
  ],
  "sampling": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation."
    }
  ],
  "offset": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation."
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, this paper introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, the authors enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. This simple method surpasses the current state-of-the-art in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    }
  ],
  "supervision": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation."
    },
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load."
    },
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a method for contrastive learning of visual representations. Authors find that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch size with more training steps improve representation quality. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/research/2020/09/10/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning.html",
      "date": "September 10, 2020",
      "summary": "This paper introduces BYOL, a self-supervised learning method that does not need negative pairs. BYOL uses a teacher student pair of networks, where the teacher is the EMA of the student. The teacher and student are given two differently augmented versionos the input, and the loss is the cross entropy difference between the outputs."
    },
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "This paper introdices an automated pipeline for building high-quality, diverse image datasets. Also proposes minor mofications to the archetecture and loss of a DINO ViT. A large ViT model with 1 billion parameters was trained and distilled into smaller models."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "This paper shows self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs or convnets. Also proposes a modification to BYOL self-supervised learning with a momentum encoder, multi-crop training, and feature sharping for the teacher network."
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. This self-supervised method uses multiple crops of an image. Given a context crop the network is trained to predict the embeddings of several target crops. When applied to Vision Transformers, I-JEPA acheived strong performance in various tasks like image classification and depth prediction, outperforming Masked Auto Encoding (MAE) in linear probing when controlling for compute."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. The authors find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. "
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, the authors demonstrate the close relationship between these two families. This analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. The Authors show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study looks at self-supervised pre-training methods on small scale or lightweight Vision Transformers (ViTs). Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods."
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, enhances vision tasks using Vision transformers by masking and reconstructing parts of an image. The compatibility of MIM with CNNs and its operational principle are unclear. this study reveals that MIM improves generalized feature extraction through middle-order patch interactions and introduces Architecture-Agnostic Masked Image Modeling (A2MIM), compatible with both Transformers and CNNs. Extensive testing demonstrates A2MIM's ability to enhance representation learning and transferability to various tasks without specialized modifications."
    },
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/research/2020/08/03/Are-Labels-Necessary-for-Neural-Architecture-Search.html",
      "date": "August 3, 2020",
      "summary": "The paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning the necessity of human-annotated labels for discovering effective neural architectures. Two experimental setups sample-based and search-based were investigated. The sample-based approach evaluated 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. The search-based experiments employed DARTS, a recognized NAS algorithm, with various unsupervised objectives. The architectures identified without labels performed competitively compared to those found with labels. The findings suggested that image statistics alone could be sufficient for identifying efficient neural architectures, potentially eliminating the need for human-annotated labels."
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM) for self-supervised visual representation in vision transformers ViT. Five different learning objectives were proposed, which degraded the input image in various ways similar to masking. Design principles are proposed for  token-based pre-training of vision transformers. The most effective strategy combined preserving the original image style with spatial misalignment and masking. This approach outperformed traditional MIM on downstream recognition tasks without increasing computational demands."
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores self-supervision through automatic colorization as an alternative to ImageNet pretraining. Self-supervised training achieved state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. This paper highlights the significance of loss formulation, training specifics, and network architecture when pretraining through colorization. It also revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and feature adaptability upon fine-tuning. The findings suggest that colorization offers a better supervisory signal comparable to various types of ImageNet pretraining."
    },
    {
      "title": "CLIP: Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns from raw text descriptions of images, moving beyond training on fixed object categories. By pre-training on 400 million (image, text) pairs using caption-image matching, the method achieves state-of-the-art image representations from scratch. This allows for zero-shot transfer to various downstream tasks using natural language, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse datasets, showing competitive results against fully supervised baselines without task-specific training, and matching the accuracy of ResNet-50 on ImageNet zero-shot without using its training examples."
    },
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "This paper introdices LOST, a method for unsupervised object localization in images. LOST utilizes activation features from a self-supervised pre-trained vision transformer. Unlike other methods, LOST works on individual images without relying on external object proposals or image collection exploration. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, demonstrating its effectiveness in unsupervised object discovery."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. MAE involves masking random patches of an input image and reconstructing the missing pixels using an asymmetric encoder-decoder architecture and a lightweight decoder. Masking a significant portion of the input image creates a challenging yet informative self-supervisory task. These innovations allow for efficient training of large models, tripling training speed and enhancing accuracy. A vanilla ViT-Huge model reaches top accuracy (87.8%) on ImageNet-1K data among similar methods. The model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating promising scaling potential."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. In this paper Masked Image Residual Learning (MIRL), a self-supervised learning framework, was introduced to alleviate the degradation issue and enable effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Extensive testing shows that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) uses synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. Given this perspective, a one-instance fractal database (OFDB) is developed where only a single image per category is present. Despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M comparable or superior results are obtaine to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. A ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training. Synthetic images generated by FDSL circumvent issues associated with real images, offering a promising avenue for pre-training general models. Investigations suggest that simple object contours can match fractal-based dataset performance and increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model consists of two parallel transformer encoders (text prompt and image) and a shared mask decoder. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. "
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "This paper investigates the geometric and statistical properties of representations across layers in large transformers used for self-supervised learning on various data types. The authors observe common evolution patterns, with data manifolds expanding initially, contracting at intermediate layers, and the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. They find that semantic information peaks after initial expansion, a trend consistent across models and datasets. The study proposes an unsupervised method to identify layers richest in semantic content, suggesting that those at a relative ID minimum are optimal for downstream tasks."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper investigates how self-supervised pretraining methods learn part-aware representations. The authors describe contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting that these methods predispose encoders to recognize object parts. Through empirical comparison, they find that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This paper is effectively a follow-up to DinoV2. The study addresses the issue of artifacts in feature maps of Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. The authors introduce a straightforward strategy of adding extra tokens to the input sequence of the Vision Transformer, which eliminates these artifacts for both supervised and self-supervised models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks, facilitates object discovery with larger models, and results in smoother feature and attention maps for downstream visual processing tasks."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets for formula-driven supervised learning (FDSL). The authors identify the optimal FDSL parameters and maximize synthetic image variety, which is crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. The authors demonstrate FDSL's potential for continuous improvement and its ability to avoid issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "This paper introduces a self-supervised vision model pretraining framework inspired by energy-based models (EBMs), leveraging symmetric mappings in deep networks without auxiliary components. The framework models energy estimation and data restoration through the network's forward and backward passes, assigning low energy to unlabeled dataset samples and using gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting various pretext tasks. Experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "This research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. The authors validate these approaches using the ImageNet-1k dataset and confirm their findings with the ImageNet-v2 test set, evaluating transfer performance across six additional datasets."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "This paper introduces Mask-Reconstruct Augmentation (MRA), an image augmentation technique that leverages self-supervised masked autoencoders to generate distorted inputs and address the issue of overfitting in deep neural networks. Inspired by the success of masked image modeling in self-supervised learning, MRA uses nonlinear transformations for regularization, as opposed to current hand-crafted linear techniques like scale, flip, and color jitter. Extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks."
    },
    {
      "title": "DeiT III: Revenge of the ViT",
      "url": "/research/2023/01/18/DeiT-III-Revenge-of-the-ViT.html",
      "date": "January 18, 2023",
      "summary": "This paper explores the supervised training of Vision Transformers (ViTs) using a simplified training approach adapted from ResNet-50 that includes a novel data-augmentation method with just 3 augmentations. The study demonstrates that this method significantly improves ViTs' performance in image classification, transfer learning, and semantic segmentation over previous supervised training techniques. Moreover, it shows ViTs' performance can match newer architectures, providing a new benchmark for evaluating self-supervised methods on ViTs."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "This paper introduces FixMatch, a streamlined semi-supervised learning (SSL) algorithm. FixMatch generates pseudo-labels using a model's predictions on weakly-augmented unlabeled images, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness, and the code is publicly available."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    },
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    },
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    },
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    },
    {
      "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
      "url": "/research/2024/02/28/CL-MAE:-Curriculum-Learned-Masked-Autoencoders.html",
      "date": "February 28, 2024",
      "summary": "This paper proposes a curriculum learning approach that updates the masking strategy of Masked Auto Encoders (MAE) to progressively increase the complexity of the self-supervised reconstruction task. To achieve this, a novel learnable masking module is introduced, capable of generating masks of varying complexities, and integrated into masked autoencoders (MAE). This module is jointly trained with the MAE, adjusting its behavior from partner (optimizing the same reconstruction loss) to adversary (optimizing the opposite loss), with a smooth transition regulated by a factor multiplied with the reconstruction loss. This training procedure creates an easy-to-hard curriculum. The Curriculum-Learned Masked Autoencoder (CL-MAE) is trained on ImageNet and demonstrates superior representation learning compared to MAE. Empirical results on five downstream tasks confirm that curriculum learning can successfully self-supervise masked autoencoders."
    },
    {
      "title": "Non-negative Contrastive Learning",
      "url": "/research/2024/03/19/Non-negative-Contrastive-Learning.html",
      "date": "March 19, 2024",
      "summary": "Deep representations offer promising performance for downstream tasks but lack interpretability, posing a significant challenge. In this paper, we introduce Non-negative Contrastive Learning (NCL), a refinement of Non-negative Matrix Factorization (NMF) aimed at generating interpretable features. NCL enforces non-negativity constraints, akin to NMF, resulting in sparse and disentangled representations, unlike standard contrastive learning (CL). We establish theoretical guarantees on NCL's identifiability and downstream generalization. Empirically, NCL outperforms CL in feature disentanglement, selection, and downstream classification tasks. Moreover, NCL can be extended to other learning scenarios, benefiting supervised learning."
    },
    {
      "title": "Towards a Unified Theoretical Understanding of Non-contrastive Learning via Rank Differential Mechanism",
      "url": "/research/2023/03/04/Towards-a-Unified-Theoretical-Understanding-of-Non-contrastive-Learning-via-Rank-Differential-Mechanism.html",
      "date": "March 4, 2023",
      "summary": "Recent non-contrastive learning methods like BYOL, SimSiam, SwAV, and DINO have shown that asymmetric architectural designs can achieve good self-supervised visual learning performance by aligning positive pairs alone, despite a lack of unified theoretical understanding on how these designs prevent feature collapse. This study introduces the Rank Differential Mechanism (RDM) as a unified theoretical framework for non-contrastive learning. RDM demonstrates that these asymmetric designs maintain a consistent rank difference in output features, enhancing effective dimensionality and preventing feature collapse. Unlike previous theories, RDM applies to designs both with and without a predictor, offering a comprehensive understanding of non-contrastive learning methods and guiding the development of new variants. Experiments confirm that these variants perform comparably, if not better than, existing methods on benchmark datasets."
    }
  ],
  "vision": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation."
    },
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a self-supervised method that adds a contrastive loss to the Masked Autoencoder. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "This paper shows self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs or convnets. Also proposes a modification to BYOL self-supervised learning with a momentum encoder, multi-crop training, and feature sharping for the teacher network."
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. This self-supervised method uses multiple crops of an image. Given a context crop the network is trained to predict the embeddings of several target crops. When applied to Vision Transformers, I-JEPA acheived strong performance in various tasks like image classification and depth prediction, outperforming Masked Auto Encoding (MAE) in linear probing when controlling for compute."
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "This paper introduces PatchDropout, a simple training technique for Vision Transformers ViT that drops random image patches, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a Vision archetecture that uses bidirectional Mamba blocks, challenging the necessity of self-attention in vision. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient."
    },
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a method for dropout in Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. The authors find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. "
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/ON-INTERACTION-BETWEEN-AUGMENTATIONS-AND-CORRUPTIONS-IN-NATURAL-CORRUPTION-ROBUSTNESS.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, the correlation between data augmentations and test-time corruptions remains unclear. This paper introduces Minimal Sample Distance, and demonstrates a strong correlation between augmentation-corruption similarity and performance. Authors argue that training with augmentations that are perceptually similar to corruptions enhances test error."
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study looks at self-supervised pre-training methods on small scale or lightweight Vision Transformers (ViTs). Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods."
    },
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/research/2021/08/16/An-Empirical-Study-of-Training-Self-Supervised-Vision-Transformers.html",
      "date": "August 16, 2021",
      "summary": "This study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "This study introduces the Vision Transformer ViT archetecture. The paper shows that a pure transformer applied directly to sequences of image patches can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, enhances vision tasks using Vision transformers by masking and reconstructing parts of an image. The compatibility of MIM with CNNs and its operational principle are unclear. this study reveals that MIM improves generalized feature extraction through middle-order patch interactions and introduces Architecture-Agnostic Masked Image Modeling (A2MIM), compatible with both Transformers and CNNs. Extensive testing demonstrates A2MIM's ability to enhance representation learning and transferability to various tasks without specialized modifications."
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM) for self-supervised visual representation in vision transformers ViT. Five different learning objectives were proposed, which degraded the input image in various ways similar to masking. Design principles are proposed for  token-based pre-training of vision transformers. The most effective strategy combined preserving the original image style with spatial misalignment and masking. This approach outperformed traditional MIM on downstream recognition tasks without increasing computational demands."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of Vision Transformer (ViT) variants against common corruptions. The benchmarking reveals that overlapping patch embedding and convolutional feed-forward networks (FFNs) significantly enhance ViT robustness. The study also scrutinizes the effectiveness of CNN-based data augmentation strategies when applied to ViTs, finding that adversarial noise training is effective, while fourier-domain augmentation falls short. A new conditional method for generating dynamic augmentation parameters based on input images is proposed, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "CLIP: Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns from raw text descriptions of images, moving beyond training on fixed object categories. By pre-training on 400 million (image, text) pairs using caption-image matching, the method achieves state-of-the-art image representations from scratch. This allows for zero-shot transfer to various downstream tasks using natural language, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse datasets, showing competitive results against fully supervised baselines without task-specific training, and matching the accuracy of ResNet-50 on ImageNet zero-shot without using its training examples."
    },
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "This paper introdices LOST, a method for unsupervised object localization in images. LOST utilizes activation features from a self-supervised pre-trained vision transformer. Unlike other methods, LOST works on individual images without relying on external object proposals or image collection exploration. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, demonstrating its effectiveness in unsupervised object discovery."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. MAE involves masking random patches of an input image and reconstructing the missing pixels using an asymmetric encoder-decoder architecture and a lightweight decoder. Masking a significant portion of the input image creates a challenging yet informative self-supervisory task. These innovations allow for efficient training of large models, tripling training speed and enhancing accuracy. A vanilla ViT-Huge model reaches top accuracy (87.8%) on ImageNet-1K data among similar methods. The model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating promising scaling potential."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. In this paper Masked Image Residual Learning (MIRL), a self-supervised learning framework, was introduced to alleviate the degradation issue and enable effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Extensive testing shows that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) uses synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. Given this perspective, a one-instance fractal database (OFDB) is developed where only a single image per category is present. Despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M comparable or superior results are obtaine to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. A ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training. Synthetic images generated by FDSL circumvent issues associated with real images, offering a promising avenue for pre-training general models. Investigations suggest that simple object contours can match fractal-based dataset performance and increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces TinyViT, a series of compact and efficient vision transformers (ViTs) designed for devices with limited resources. The authors employ a fast distillation framework to transfer knowledge from large pretrained models to smaller ones during pretraining, using sparsified logits from teacher models to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. With increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters, and demonstrates strong transferability across various downstream tasks."
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey provides a comprehensive review of transformer-based visual segmentation, a crucial field for various applications. The authors cover the evolution from convolutional methods to vision transformers, offering a unified framework to simplify understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas such as 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. The authors re-evaluate these methods on established datasets, outline current challenges, and suggest future research directions."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This paper is effectively a follow-up to DinoV2. The study addresses the issue of artifacts in feature maps of Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. The authors introduce a straightforward strategy of adding extra tokens to the input sequence of the Vision Transformer, which eliminates these artifacts for both supervised and self-supervised models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks, facilitates object discovery with larger models, and results in smoother feature and attention maps for downstream visual processing tasks."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets for formula-driven supervised learning (FDSL). The authors identify the optimal FDSL parameters and maximize synthetic image variety, which is crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. The authors demonstrate FDSL's potential for continuous improvement and its ability to avoid issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "This paper introduces a self-supervised vision model pretraining framework inspired by energy-based models (EBMs), leveraging symmetric mappings in deep networks without auxiliary components. The framework models energy estimation and data restoration through the network's forward and backward passes, assigning low energy to unlabeled dataset samples and using gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting various pretext tasks. Experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "This paper explores the interactions between training data amount, model regularization or data augmentation (AugReg), model size, and compute budget for Vision Transformers (ViT) in various vision tasks. The authors find that using more compute and AugReg can achieve the same performance as training with significantly more data. They demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "This research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. The authors validate these approaches using the ImageNet-1k dataset and confirm their findings with the ImageNet-v2 test set, evaluating transfer performance across six additional datasets."
    },
    {
      "title": "Training data-efficient image transformers & distillation through attention",
      "url": "/research/2021/01/15/Training-data-efficient-image-transformers-&-distillation-through-attention.html",
      "date": "January 15, 2021",
      "summary": "This paper introduces DeiT, an efficient method for training vision transformers. The authors present a unique teacher-student strategy for transformers, leveraging a distillation token for efficient learning from a convolutional network teacher. This method achieves performance on par with convolutional networks, with up to 85.2% accuracy on ImageNet, and demonstrates effective transferability to other tasks."
    },
    {
      "title": "Better plain ViT baselines for ImageNet-1k",
      "url": "/research/2022/05/02/Better-plain-ViT-baselines-for-ImageNet-1k.html",
      "date": "May 2, 2022",
      "summary": "This paper challenges the common belief that the Vision Transformer (ViT) model requires sophisticated regularization techniques to excel on ImageNet-1k scale data. The authors present minor modifications to the original ViT vanilla training setting that significantly improve the performance of plain ViT models. They find that standard data augmentation is sufficient, with 90 epochs of training surpassing 76% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reaching 80% in less than one day."
    },
    {
      "title": "DeiT III: Revenge of the ViT",
      "url": "/research/2023/01/18/DeiT-III-Revenge-of-the-ViT.html",
      "date": "January 18, 2023",
      "summary": "This paper explores the supervised training of Vision Transformers (ViTs) using a simplified training approach adapted from ResNet-50 that includes a novel data-augmentation method with just 3 augmentations. The study demonstrates that this method significantly improves ViTs' performance in image classification, transfer learning, and semantic segmentation over previous supervised training techniques. Moreover, it shows ViTs' performance can match newer architectures, providing a new benchmark for evaluating self-supervised methods on ViTs."
    },
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "This paper introduces the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. The authors' empirical evaluation across computer vision, natural language processing, and speech tasks demonstrates that GELU offers performance improvements over ReLU and ELU activations."
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of Vision Transformers (ViTs) without relying on large-scale pre-training datasets like ImageNet and JFT or requiring modifications to the architecture or loss functions. The authors show that this approach improves ViT performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, while maintaining ViT's attention to relevant regions and robustness, despite ViT's inherent lack of inductive biases and typical dependence on large-scale pre-training."
    },
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "The paper investigates the use of synthetic data generated by graphics simulators for pre-training computer vision models. The authors find that model performance on downstream tasks varies with different simulation parameters used to generate the synthetic data. They introduce Task2Sim, a model that maps the requirements of a downstream task to the optimal simulation parameters for generating synthetic pre-training data."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article provides an extensive review of vision transformer techniques applied specifically to human action recognition. These action transformers are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also examines optimizing spatio-temporal attention in transformers for longer sequences and different learning strategies like self-supervised and zero-shot learning with their respective loss functions. Additionally, it assesses progress in benchmark evaluation scores and discusses challenges and future directions in this research area."
    },
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "The paper focuses on evaluating the performance of recent advancements in image transformers, such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, on the Tiny ImageNet dataset for transfer learning tasks. While these models are typically trained on large datasets like ImageNet-21k and then fine-tuned on ImageNet-1k, their assessments often overlook the Tiny ImageNet benchmark. The study updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. This underscores the importance of evaluating these models on the Tiny ImageNet dataset for transfer learning performance."
    },
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    },
    {
      "title": "Are Convolutional Neural Networks or Transformers more like human vision?",
      "url": "/research/2021/07/01/Are-Convolutional-Neural-Networks-or-Transformers-more-like-human-vision.html",
      "date": "July 1, 2021",
      "summary": "This study investigates the performance of Convolutional Neural Networks (CNNs) and attention-based Vision Transformers (ViTs) in computer vision tasks. While CNNs excel in accuracy, ViTs offer a different approach with weaker inductive biases. By analyzing error patterns, we find that ViTs exhibit consistency with human errors, suggesting potential for more human-like vision models and insights into human object recognition."
    },
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding",
      "url": "/research/2021/11/16/Improved-Robustness-of-Vision-Transformer-via-PreLayerNorm-in-Patch-Embedding.html",
      "date": "November 16, 2021",
      "summary": "Recent advancements in vision transformers (ViTs) have shown superior performance across diverse visual tasks, surpassing convolutional neural networks (CNNs). Given ViT's distinct architecture, understanding its behavior and reliability is imperative. This paper investigates ViT's robustness by comparing it with CNNs under various image corruptions relevant to real-world vision tasks. While ViT generally exhibits comparable or improved robustness over CNNs, it consistently underperforms in contrast enhancement tasks. Analysis suggests that positional embedding in ViT's patch embedding may malfunction with color scale changes. We propose PreLayerNorm, a modified patch embedding structure, to address this issue and ensure scale-invariant behavior in ViT. ViT with PreLayerNorm demonstrates enhanced robustness across various corruptions, particularly in contrast-varying environments."
    },
    {
      "title": "Scaling Vision Transformers to 22 Billion Parameters",
      "url": "/research/2023/02/10/Scaling-Vision-Transformers-to-22-Billion-Parameters.html",
      "date": "February 10, 2023",
      "summary": "We introduce a method for efficiently training a 22B-parameter Vision Transformer (ViT-22B) and conduct various experiments to assess its performance. Compared to existing models, ViT-22B shows improved scalability and benefits such as enhanced fairness-performance tradeoff, alignment with human visual perception, and increased robustness. Our findings suggest promising avenues for achieving large-scale language model-like capabilities in vision tasks."
    },
    {
      "title": "Symbolic Discovery of Optimization Algorithms",
      "url": "/research/2023/05/08/Symbolic-Discovery-of-Optimization-Algorithms.html",
      "date": "May 8, 2023",
      "summary": "We propose a method for algorithm discovery via program search, focusing on optimizing deep neural network training. Our approach, Lion (EvoLved Sign Momentum), is memory-efficient and achieves comparable or superior performance to widely-used optimizers such as Adam and Adafactor across various tasks. Specifically, Lion enhances accuracy on tasks like image classification and vision-language contrastive learning, while reducing training compute. Notably, Lion exhibits improved performance with larger batch sizes and requires smaller learning rates compared to Adam. Despite its effectiveness, Lion has limitations, which we analyze, and provide insights into its deployment and performance. Our implementation is publicly available and has been successfully utilized in Google's search ads CTR model."
    },
    {
      "title": "Do Vision Transformers See Like Convolutional Neural Networks?",
      "url": "/research/2022/03/03/Do-Vision-Transformers-See-Like-Convolutional-Neural-Networks.html",
      "date": "March 3, 2022",
      "summary": "Convolutional Neural Networks (CNNs) have been prominent in processing visual data, but recent studies show Vision Transformers (ViTs) can match or outperform CNNs in image classification. This work examines how ViTs solve classification tasks, discovering significant differences from CNNs, including uniformity in ViT representations across layers due to self-attention and residual connections that enhance feature propagation. ViTs also maintain spatial information effectively, influenced by classification methods. The study further explores how dataset scale affects ViT features and their transferability, linking these findings to new architectures like MLP-Mixer."
    },
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    },
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    },
    {
      "title": "On the Relationship Between Self-Attention And Convolutional Layers",
      "url": "/research/2020/01/10/On-the-Relationship-Between-Self-Attention-And-Convolutional-Layers.html",
      "date": "January 10, 2020",
      "summary": "Recent studies have shown that attention mechanisms can rival or even outperform convolutional layers in vision tasks, challenging their dominance. Ramachandran et al. (2019) demonstrated that attention could entirely replace convolution, achieving top results. This work investigates whether attention layers function like convolutional layers, finding that multi-head self-attention layers, with enough heads, match the expressiveness of convolutional layers. Numerical experiments confirm that self-attention layers learn to focus on pixel-grid patterns akin to convolutional layers, supporting our findings."
    },
    {
      "title": "The Platonic Representation Hypothesis",
      "url": "/research/2024/05/13/The-Platonic-Representation-Hypothesis.html",
      "date": "May 13, 2024",
      "summary": "This paper argues that AI model representations, especially in deep networks, are converging. It surveys examples in the literature showing how neural network representations align over time and across domains. It demonstrates convergence across data modalities, noting that as vision and language models grow, they measure distances between data points similarly. The authors hypothesize that this convergence leads to a shared statistical model of reality, termed the platonic representation and discuss possible selective pressures toward it. The paper also addresses the implications, limitations, and counterexamples to this trend."
    }
  ],
  "object": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation."
    },
    {
      "title": "CLIP: Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns from raw text descriptions of images, moving beyond training on fixed object categories. By pre-training on 400 million (image, text) pairs using caption-image matching, the method achieves state-of-the-art image representations from scratch. This allows for zero-shot transfer to various downstream tasks using natural language, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse datasets, showing competitive results against fully supervised baselines without task-specific training, and matching the accuracy of ResNet-50 on ImageNet zero-shot without using its training examples."
    },
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "This paper introdices LOST, a method for unsupervised object localization in images. LOST utilizes activation features from a self-supervised pre-trained vision transformer. Unlike other methods, LOST works on individual images without relying on external object proposals or image collection exploration. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, demonstrating its effectiveness in unsupervised object discovery."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. A ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training. Synthetic images generated by FDSL circumvent issues associated with real images, offering a promising avenue for pre-training general models. Investigations suggest that simple object contours can match fractal-based dataset performance and increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper investigates how self-supervised pretraining methods learn part-aware representations. The authors describe contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting that these methods predispose encoders to recognize object parts. Through empirical comparison, they find that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This paper is effectively a follow-up to DinoV2. The study addresses the issue of artifacts in feature maps of Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. The authors introduce a straightforward strategy of adding extra tokens to the input sequence of the Vision Transformer, which eliminates these artifacts for both supervised and self-supervised models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks, facilitates object discovery with larger models, and results in smoother feature and attention maps for downstream visual processing tasks."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "This study challenges the traditional understanding of how Convolutional Neural Networks (CNNs) recognize objects, revealing that they are more biased towards recognizing textures rather than learning complex shapes. The authors found that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, training a ResNet-50 on a stylized version of ImageNet, designed to emphasize shape, aligned the network's performance more closely with human behavior. This shape-based training matched human performance in controlled experiments and enhanced object detection and robustness against image distortions, highlighting the benefits of shape-based representations in visual recognition systems."
    },
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "This paper introduces a curriculum learning framework for image-caption pretraining, inspired by children's language learning from cognitive science, to address the challenges of aligning multiple concepts from captions to objects in images. The method starts with simple image-caption pairs and gradually increases complexity by adding more concepts, leveraging knowledge from each phase for subsequent learning. This approach outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios."
    },
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces an iterative self-training framework for sim-to-real 6D object pose estimation to enable cost-effective robotic bin-picking. The authors create a photo-realistic simulator to synthesize virtual data for training an initial pose estimation network (teacher model). This teacher predicts poses on unlabeled real data, and an adaptive selection scheme filters reliable predictions to generate pseudo-labels for training a student model on real data. Iteratively refining the teacher with the student improves pseudo-label quality. Evaluated on public benchmarks and a new dataset, their method shows 11.49% and 22.62% ADD(-S) improvements and a 19.54% increase in bin-picking success, demonstrating the effectiveness of this iterative sim-to-real approach."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "The paper introduces a system that trains deep neural networks for object detection using only synthetic images generated with domain randomization. This involves randomizing non-realistic simulator parameters like lighting and object textures, allowing the network to identify essential object features. Their findings show that networks can achieve impressive performance using just synthetic data, and further improve with fine-tuning on real data. This suggests the potential of using cost-effective synthetic data for training, avoiding the need to acquire vast amounts of real-world data or create detailed synthetic environments. They validate their method on car bounding box detection using the KITTI dataset."
    },
    {
      "title": "Are Convolutional Neural Networks or Transformers more like human vision?",
      "url": "/research/2021/07/01/Are-Convolutional-Neural-Networks-or-Transformers-more-like-human-vision.html",
      "date": "July 1, 2021",
      "summary": "This study investigates the performance of Convolutional Neural Networks (CNNs) and attention-based Vision Transformers (ViTs) in computer vision tasks. While CNNs excel in accuracy, ViTs offer a different approach with weaker inductive biases. By analyzing error patterns, we find that ViTs exhibit consistency with human errors, suggesting potential for more human-like vision models and insights into human object recognition."
    },
    {
      "title": "Deep Residual Learning for Image Recognition",
      "url": "/research/2015/12/10/Deep-Residual-Learning-for-Image-Recognition.html",
      "date": "December 10, 2015",
      "summary": "We introduce a residual learning framework to train deeper neural networks effectively. By reformulating layers to learn residual functions relative to inputs, our approach facilitates optimization and achieves higher accuracy with increased depth. Evaluations on ImageNet demonstrate the effectiveness of residual networks up to 152 layers deep, outperforming previous architectures. Our method achieves 3.57% error on the ImageNet test set, winning 1st place in the ILSVRC 2015 classification task. Additionally, our deep representations lead to a 28% improvement on the COCO object detection dataset. In the ILSVRC & COCO 2015 competitions, our approach secured 1st place in ImageNet detection, localization, COCO detection, and segmentation tasks."
    },
    {
      "title": "Is Cosine-Similarity of Embeddings Really About Similarity?",
      "url": "/research/2024/03/08/Is-Cosine-Similarity-of-Embeddings-Really-About-Similarity.html",
      "date": "March 8, 2024",
      "summary": "Cosine similarity, often used to gauge semantic similarity between high-dimensional objects by comparing low-dimensional feature embeddings, can yield variable results compared to unnormalized dot products. We investigate this phenomenon by analyzing embeddings from regularized linear models, revealing that cosine similarity can produce arbitrary and even meaningless similarities. This applies not only to linear models but also to deep models due to the implicit effects of various regularizations. Consequently, we advise against relying solely on cosine similarity and suggest exploring alternative approaches."
    }
  ],
  "detection": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "This study challenges the traditional understanding of how Convolutional Neural Networks (CNNs) recognize objects, revealing that they are more biased towards recognizing textures rather than learning complex shapes. The authors found that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, training a ResNet-50 on a stylized version of ImageNet, designed to emphasize shape, aligned the network's performance more closely with human behavior. This shape-based training matched human performance in controlled experiments and enhanced object detection and robustness against image distortions, highlighting the benefits of shape-based representations in visual recognition systems."
    },
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "This paper introduces a novel approach called mixed-order self-paced curriculum learning (Mo-SCL) to address the challenges faced by self-paced curriculum learning (SCL) in medical image analysis tasks, such as universal lesion detection. These challenges include inaccurate difficulty estimation and the under-utilization of hard samples. Mo-SCL combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches. Through theoretical analysis and experiments on the DeepLesion dataset, the authors demonstrate that Mo-SCL enhances lesion detection accuracy in state-of-the-art methods without requiring additional network modifications."
    },
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "This paper introduces SlowFast networks, a new convolutional architecture for video recognition tasks. These networks have two pathways: a Slow pathway operating at low frame rates to capture spatial semantics, and a Fast pathway working at high frame rates with reduced channel capacity to efficiently capture motion details. The SlowFast models achieve top accuracy on major video benchmarks like Kinetics, Charades, and AVA, significantly improving performance for action classification and detection tasks."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "The paper introduces a system that trains deep neural networks for object detection using only synthetic images generated with domain randomization. This involves randomizing non-realistic simulator parameters like lighting and object textures, allowing the network to identify essential object features. Their findings show that networks can achieve impressive performance using just synthetic data, and further improve with fine-tuning on real data. This suggests the potential of using cost-effective synthetic data for training, avoiding the need to acquire vast amounts of real-world data or create detailed synthetic environments. They validate their method on car bounding box detection using the KITTI dataset."
    },
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "The study presents a novel approach for detection identification of Holstein-Friesian cattle. This technique offers a completely hands-off solution for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training when new cattle are introduced. The system utilizes convolutional neural networks and deep metric learning, achieving high accuracy with a 93.8% success rate in identifying cattle unseen during training, using only half of the population."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    },
    {
      "title": "Deep Residual Learning for Image Recognition",
      "url": "/research/2015/12/10/Deep-Residual-Learning-for-Image-Recognition.html",
      "date": "December 10, 2015",
      "summary": "We introduce a residual learning framework to train deeper neural networks effectively. By reformulating layers to learn residual functions relative to inputs, our approach facilitates optimization and achieves higher accuracy with increased depth. Evaluations on ImageNet demonstrate the effectiveness of residual networks up to 152 layers deep, outperforming previous architectures. Our method achieves 3.57% error on the ImageNet test set, winning 1st place in the ILSVRC 2015 classification task. Additionally, our deep representations lead to a 28% improvement on the COCO object detection dataset. In the ILSVRC & COCO 2015 competitions, our approach secured 1st place in ImageNet detection, localization, COCO detection, and segmentation tasks."
    }
  ],
  "segmentation": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "This paper shows self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs or convnets. Also proposes a modification to BYOL self-supervised learning with a momentum encoder, multi-crop training, and feature sharping for the teacher network."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. The authors find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. "
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores self-supervision through automatic colorization as an alternative to ImageNet pretraining. Self-supervised training achieved state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. This paper highlights the significance of loss formulation, training specifics, and network architecture when pretraining through colorization. It also revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and feature adaptability upon fine-tuning. The findings suggest that colorization offers a better supervisory signal comparable to various types of ImageNet pretraining."
    },
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model consists of two parallel transformer encoders (text prompt and image) and a shared mask decoder. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. "
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey provides a comprehensive review of transformer-based visual segmentation, a crucial field for various applications. The authors cover the evolution from convolutional methods to vision transformers, offering a unified framework to simplify understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas such as 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. The authors re-evaluate these methods on established datasets, outline current challenges, and suggest future research directions."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "DeiT III: Revenge of the ViT",
      "url": "/research/2023/01/18/DeiT-III-Revenge-of-the-ViT.html",
      "date": "January 18, 2023",
      "summary": "This paper explores the supervised training of Vision Transformers (ViTs) using a simplified training approach adapted from ResNet-50 that includes a novel data-augmentation method with just 3 augmentations. The study demonstrates that this method significantly improves ViTs' performance in image classification, transfer learning, and semantic segmentation over previous supervised training techniques. Moreover, it shows ViTs' performance can match newer architectures, providing a new benchmark for evaluating self-supervised methods on ViTs."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "This paper introduces Online Normalization, a novel method for normalizing neural network hidden activations that offers a batch-independent alternative with comparable accuracy to Batch Normalization. Online Normalization addresses Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. The method is applicable to recurrent, fully connected networks, and those with high activation memory requirements. The authors demonstrate its effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    },
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This paper investigates using high-quality rendering software and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle. This synthetic dataset is used to augment limited real training data for image classification and semantic segmentation tasks. While models trained solely on synthetic images showed low accuracy on real validation data, including even small amounts of real data significantly improved performance. Augmenting real training data with synthetic images outperformed using only real images. Furthermore, pretraining models on the synthetic dataset before transfer learning significantly reduced training costs by allowing most of the training to be completed upfront using the synthetic data."
    },
    {
      "title": "Deep Residual Learning for Image Recognition",
      "url": "/research/2015/12/10/Deep-Residual-Learning-for-Image-Recognition.html",
      "date": "December 10, 2015",
      "summary": "We introduce a residual learning framework to train deeper neural networks effectively. By reformulating layers to learn residual functions relative to inputs, our approach facilitates optimization and achieves higher accuracy with increased depth. Evaluations on ImageNet demonstrate the effectiveness of residual networks up to 152 layers deep, outperforming previous architectures. Our method achieves 3.57% error on the ImageNet test set, winning 1st place in the ILSVRC 2015 classification task. Additionally, our deep representations lead to a 28% improvement on the COCO object detection dataset. In the ILSVRC & COCO 2015 competitions, our approach secured 1st place in ImageNet detection, localization, COCO detection, and segmentation tasks."
    }
  ],
  "transformer": [
    {
      "title": "Do Transformer Modifications Transfer Across Implementations and Applications?",
      "url": "/research/2021/09/10/Do_Transformer_Modifications_Transfer_Across_Implementations_and_Applications.html",
      "date": "September 10, 2021",
      "summary": "This paper evaluates numerous Transformer architecture modifications in a unified experimental framework, focusing on common natural language processing (NLP) applications. Surprisingly, it finds that most modifications do not significantly enhance performance. The beneficial variants are mostly minor or developed in the same code base used for testing. The study suggests that performance gains may largely depend on implementation details and offers recommendations for improving the generalizability of experimental results."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "This paper shows self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs or convnets. Also proposes a modification to BYOL self-supervised learning with a momentum encoder, multi-crop training, and feature sharping for the teacher network."
    },
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "State Space Models (SSM), despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. To improve SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing."
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. This self-supervised method uses multiple crops of an image. Given a context crop the network is trained to predict the embeddings of several target crops. When applied to Vision Transformers, I-JEPA acheived strong performance in various tasks like image classification and depth prediction, outperforming Masked Auto Encoding (MAE) in linear probing when controlling for compute."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "This paper introduces the Mamba archetecture, a selective state space model (SSSM) with input dependance. Mamba has better speed and scalability compared to transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences. Authors also present a hardware efficient scanning mechanism that is used in Mamba."
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "This paper introduces PatchDropout, a simple training technique for Vision Transformers ViT that drops random image patches, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a Vision archetecture that uses bidirectional Mamba blocks, challenging the necessity of self-attention in vision. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient."
    },
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/research/2019/07/26/DropAttention-A-Regularization-Method-for-Fully-Connected-Self-Attention-Networks.html",
      "date": "July 26, 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. DropAttention uses a mask to zero out elements in the attention matrix. Experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    },
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a method for dropout in Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. The authors find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. "
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "This paper introduces DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. Authors also propose a dropout rate scheduler to optimize training."
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets transformers as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study looks at self-supervised pre-training methods on small scale or lightweight Vision Transformers (ViTs). Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods."
    },
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/research/2021/08/16/An-Empirical-Study-of-Training-Self-Supervised-Vision-Transformers.html",
      "date": "August 16, 2021",
      "summary": "This study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "This study introduces the Vision Transformer ViT archetecture. The paper shows that a pure transformer applied directly to sequences of image patches can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, enhances vision tasks using Vision transformers by masking and reconstructing parts of an image. The compatibility of MIM with CNNs and its operational principle are unclear. this study reveals that MIM improves generalized feature extraction through middle-order patch interactions and introduces Architecture-Agnostic Masked Image Modeling (A2MIM), compatible with both Transformers and CNNs. Extensive testing demonstrates A2MIM's ability to enhance representation learning and transferability to various tasks without specialized modifications."
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM) for self-supervised visual representation in vision transformers ViT. Five different learning objectives were proposed, which degraded the input image in various ways similar to masking. Design principles are proposed for  token-based pre-training of vision transformers. The most effective strategy combined preserving the original image style with spatial misalignment and masking. This approach outperformed traditional MIM on downstream recognition tasks without increasing computational demands."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of Vision Transformer (ViT) variants against common corruptions. The benchmarking reveals that overlapping patch embedding and convolutional feed-forward networks (FFNs) significantly enhance ViT robustness. The study also scrutinizes the effectiveness of CNN-based data augmentation strategies when applied to ViTs, finding that adversarial noise training is effective, while fourier-domain augmentation falls short. A new conditional method for generating dynamic augmentation parameters based on input images is proposed, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "This paper introdices LOST, a method for unsupervised object localization in images. LOST utilizes activation features from a self-supervised pre-trained vision transformer. Unlike other methods, LOST works on individual images without relying on external object proposals or image collection exploration. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, demonstrating its effectiveness in unsupervised object discovery."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. In this paper Masked Image Residual Learning (MIRL), a self-supervised learning framework, was introduced to alleviate the degradation issue and enable effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Extensive testing shows that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) uses synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. Given this perspective, a one-instance fractal database (OFDB) is developed where only a single image per category is present. Despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M comparable or superior results are obtaine to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. A ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training. Synthetic images generated by FDSL circumvent issues associated with real images, offering a promising avenue for pre-training general models. Investigations suggest that simple object contours can match fractal-based dataset performance and increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model consists of two parallel transformer encoders (text prompt and image) and a shared mask decoder. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. "
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "This paper investigates the geometric and statistical properties of representations across layers in large transformers used for self-supervised learning on various data types. The authors observe common evolution patterns, with data manifolds expanding initially, contracting at intermediate layers, and the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. They find that semantic information peaks after initial expansion, a trend consistent across models and datasets. The study proposes an unsupervised method to identify layers richest in semantic content, suggesting that those at a relative ID minimum are optimal for downstream tasks."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces TinyViT, a series of compact and efficient vision transformers (ViTs) designed for devices with limited resources. The authors employ a fast distillation framework to transfer knowledge from large pretrained models to smaller ones during pretraining, using sparsified logits from teacher models to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. With increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters, and demonstrates strong transferability across various downstream tasks."
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey provides a comprehensive review of transformer-based visual segmentation, a crucial field for various applications. The authors cover the evolution from convolutional methods to vision transformers, offering a unified framework to simplify understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas such as 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. The authors re-evaluate these methods on established datasets, outline current challenges, and suggest future research directions."
    },
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/research/2022/10/10/Turbo-Training-with-Token-Dropout.html",
      "date": "October 10, 2022",
      "summary": "This paper introduces Turbo training, an efficient method for training Transformers on video-related tasks. The authors present a simple yet versatile training paradigm applicable to multiple video tasks and demonstrate its effectiveness across action classification, video-language representation learning, and long-video activity classification. Turbo training achieves competitive performance with up to 4× speed-up and reduced memory usage, enabling long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous resource-intensive methods.."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This paper is effectively a follow-up to DinoV2. The study addresses the issue of artifacts in feature maps of Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. The authors introduce a straightforward strategy of adding extra tokens to the input sequence of the Vision Transformer, which eliminates these artifacts for both supervised and self-supervised models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks, facilitates object discovery with larger models, and results in smoother feature and attention maps for downstream visual processing tasks."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets for formula-driven supervised learning (FDSL). The authors identify the optimal FDSL parameters and maximize synthetic image variety, which is crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. The authors demonstrate FDSL's potential for continuous improvement and its ability to avoid issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "This paper explores the interactions between training data amount, model regularization or data augmentation (AugReg), model size, and compute budget for Vision Transformers (ViT) in various vision tasks. The authors find that using more compute and AugReg can achieve the same performance as training with significantly more data. They demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel normalization technique for self-supervised transformer training that normalizes the [CLS] token and regular tokens separately. The authors find that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks compared to previous models like masked autoencoders (MAE) that use a single normalization layer for all tokens."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "This research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. The authors validate these approaches using the ImageNet-1k dataset and confirm their findings with the ImageNet-v2 test set, evaluating transfer performance across six additional datasets."
    },
    {
      "title": "Training data-efficient image transformers & distillation through attention",
      "url": "/research/2021/01/15/Training-data-efficient-image-transformers-&-distillation-through-attention.html",
      "date": "January 15, 2021",
      "summary": "This paper introduces DeiT, an efficient method for training vision transformers. The authors present a unique teacher-student strategy for transformers, leveraging a distillation token for efficient learning from a convolutional network teacher. This method achieves performance on par with convolutional networks, with up to 85.2% accuracy on ImageNet, and demonstrates effective transferability to other tasks."
    },
    {
      "title": "Better plain ViT baselines for ImageNet-1k",
      "url": "/research/2022/05/02/Better-plain-ViT-baselines-for-ImageNet-1k.html",
      "date": "May 2, 2022",
      "summary": "This paper challenges the common belief that the Vision Transformer (ViT) model requires sophisticated regularization techniques to excel on ImageNet-1k scale data. The authors present minor modifications to the original ViT vanilla training setting that significantly improve the performance of plain ViT models. They find that standard data augmentation is sufficient, with 90 epochs of training surpassing 76% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reaching 80% in less than one day."
    },
    {
      "title": "DeiT III: Revenge of the ViT",
      "url": "/research/2023/01/18/DeiT-III-Revenge-of-the-ViT.html",
      "date": "January 18, 2023",
      "summary": "This paper explores the supervised training of Vision Transformers (ViTs) using a simplified training approach adapted from ResNet-50 that includes a novel data-augmentation method with just 3 augmentations. The study demonstrates that this method significantly improves ViTs' performance in image classification, transfer learning, and semantic segmentation over previous supervised training techniques. Moreover, it shows ViTs' performance can match newer architectures, providing a new benchmark for evaluating self-supervised methods on ViTs."
    },
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "This paper explores Gated Linear Units (GLUs), which involve the component-wise product of two linear projections, with one undergoing a sigmoid function. The authors investigate GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers. They find that certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of Vision Transformers (ViTs) without relying on large-scale pre-training datasets like ImageNet and JFT or requiring modifications to the architecture or loss functions. The authors show that this approach improves ViT performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, while maintaining ViT's attention to relevant regions and robustness, despite ViT's inherent lack of inductive biases and typical dependence on large-scale pre-training."
    },
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "This paper investigates the importance of the warm-up stage in training transformers, and the impact of layer normalization placement. Using mean field theory, the authors demonstrate that the original Post-LN Transformer's design results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    },
    {
      "title": "Stable Diffusion 3: Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "This paper focuses on improving diffusion models for high-dimensional data like images. The authors enhance noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Their large-scale study shows superior performance over established diffusion methods in high-resolution text-to-image synthesis. A new transformer-based architecture is introduced for better text comprehension, typography, and human preference in text-to-image generation. The findings demonstrate predictable scaling trends, with lower validation loss tied to improved synthesis. The largest models outperform state-of-the-art approaches."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "The paper proposes three normalization-centric modifications to improve Transformer training: PRENORM which introduces pre-norm residual connections and smaller initializations, enabling warmup-free, validation-based training with large learning rates; SCALENORM which suggests L2 normalization with a single scale parameter for faster training and better performance; and FIXNORM which reaffirms the efficacy of normalizing word embeddings to a fixed length."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article provides an extensive review of vision transformer techniques applied specifically to human action recognition. These action transformers are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also examines optimizing spatio-temporal attention in transformers for longer sequences and different learning strategies like self-supervised and zero-shot learning with their respective loss functions. Additionally, it assesses progress in benchmark evaluation scores and discusses challenges and future directions in this research area."
    },
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "The paper focuses on evaluating the performance of recent advancements in image transformers, such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, on the Tiny ImageNet dataset for transfer learning tasks. While these models are typically trained on large datasets like ImageNet-21k and then fine-tuned on ImageNet-1k, their assessments often overlook the Tiny ImageNet benchmark. The study updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. This underscores the importance of evaluating these models on the Tiny ImageNet dataset for transfer learning performance."
    },
    {
      "title": "Are Convolutional Neural Networks or Transformers more like human vision?",
      "url": "/research/2021/07/01/Are-Convolutional-Neural-Networks-or-Transformers-more-like-human-vision.html",
      "date": "July 1, 2021",
      "summary": "This study investigates the performance of Convolutional Neural Networks (CNNs) and attention-based Vision Transformers (ViTs) in computer vision tasks. While CNNs excel in accuracy, ViTs offer a different approach with weaker inductive biases. By analyzing error patterns, we find that ViTs exhibit consistency with human errors, suggesting potential for more human-like vision models and insights into human object recognition."
    },
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding",
      "url": "/research/2021/11/16/Improved-Robustness-of-Vision-Transformer-via-PreLayerNorm-in-Patch-Embedding.html",
      "date": "November 16, 2021",
      "summary": "Recent advancements in vision transformers (ViTs) have shown superior performance across diverse visual tasks, surpassing convolutional neural networks (CNNs). Given ViT's distinct architecture, understanding its behavior and reliability is imperative. This paper investigates ViT's robustness by comparing it with CNNs under various image corruptions relevant to real-world vision tasks. While ViT generally exhibits comparable or improved robustness over CNNs, it consistently underperforms in contrast enhancement tasks. Analysis suggests that positional embedding in ViT's patch embedding may malfunction with color scale changes. We propose PreLayerNorm, a modified patch embedding structure, to address this issue and ensure scale-invariant behavior in ViT. ViT with PreLayerNorm demonstrates enhanced robustness across various corruptions, particularly in contrast-varying environments."
    },
    {
      "title": "Normalized Attention Without Probability Cage",
      "url": "/research/2020/05/19/Normalized-Attention-Without-Probability-Cage.html",
      "date": "May 19, 2020",
      "summary": "Softmax-attention architectures, especially popularized by Transformers, have seen significant advancements in various tasks. However, the geometric implications of softmax-attention remain underexplored. In this study, we demonstrate limitations arising from constraining attention weights to the probability simplex and its impact on the convex hull of value vectors. We reveal sequence length-dependent biases in Transformers towards token isolation at initialization and compare them with max- and sum-pooling, which are strong but often overlooked baselines. To address these issues, we propose a novel approach of replacing softmax with normalization in self-attention, resulting in a robust and widely applicable architecture. Our findings are supported by empirical results from over 25,000 trained models, and all results and implementations are publicly available."
    },
    {
      "title": "On Layer Normalization in the Transformer Architecture",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architecture.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is widely used in natural language processing. Training it typically requires a carefully designed learning rate warm-up stage, crucial for performance but slowing optimization and requiring more hyperparameter tuning. We provide theoretical insights into the necessity of the warm-up stage, demonstrating that the location of layer normalization affects gradient behavior. For Post-LN Transformers, with layer normalization between residual blocks, large gradients near the output layer at initialization make training unstable without warm-up. Conversely, for Pre-LN Transformers, with layer normalization inside residual blocks, gradients are well-behaved, suggesting the removal of the warm-up stage. Experimental results show that Pre-LN Transformers without warm-up achieve comparable performance with less training time and hyperparameter tuning."
    },
    {
      "title": "Scaling Vision Transformers to 22 Billion Parameters",
      "url": "/research/2023/02/10/Scaling-Vision-Transformers-to-22-Billion-Parameters.html",
      "date": "February 10, 2023",
      "summary": "We introduce a method for efficiently training a 22B-parameter Vision Transformer (ViT-22B) and conduct various experiments to assess its performance. Compared to existing models, ViT-22B shows improved scalability and benefits such as enhanced fairness-performance tradeoff, alignment with human visual perception, and increased robustness. Our findings suggest promising avenues for achieving large-scale language model-like capabilities in vision tasks."
    },
    {
      "title": "DeepNet: Scaling Transformers to 1,000 Layers",
      "url": "/research/2022/03/01/DeepNet-Scaling-Transformers-to-1,000-Layers.html",
      "date": "March 1, 2022",
      "summary": "This paper introduces DEEPNORM, a novel method to stabilize extremely deep Transformers through a new normalization function and a theoretically derived initialization for the residual connection. DEEPNORM offers a stable and efficient training regime, blending the benefits of Post-LN's performance with Pre-LN's stability, allowing for the scaling of Transformers up to 1,000 layers without difficulty. This represents a significant advancement over previous models, with a 200-layer, 3.2B parameter model outperforming a 48-layer, 12B parameter state-of-the-art model by 5 BLEU points on a multilingual benchmark with 7,482 translation directions, suggesting a promising direction for scaling."
    },
    {
      "title": "Do Vision Transformers See Like Convolutional Neural Networks?",
      "url": "/research/2022/03/03/Do-Vision-Transformers-See-Like-Convolutional-Neural-Networks.html",
      "date": "March 3, 2022",
      "summary": "Convolutional Neural Networks (CNNs) have been prominent in processing visual data, but recent studies show Vision Transformers (ViTs) can match or outperform CNNs in image classification. This work examines how ViTs solve classification tasks, discovering significant differences from CNNs, including uniformity in ViT representations across layers due to self-attention and residual connections that enhance feature propagation. ViTs also maintain spatial information effectively, influenced by classification methods. The study further explores how dataset scale affects ViT features and their transferability, linking these findings to new architectures like MLP-Mixer."
    },
    {
      "title": "FNet: Mixing Tokens with Fourier Transforms",
      "url": "/research/2022/05/26/FNet-Mixing-Tokens-with-Fourier-Transforms.html",
      "date": "May 26, 2022",
      "summary": "We demonstrate that Transformer encoder architectures can be accelerated with minimal impact on accuracy by substituting self-attention sublayers with simple linear transformations. Remarkably, using a standard Fourier Transform instead of the self-attention sublayer in a Transformer encoder achieves 92-97% of BERT models' accuracy on the GLUE benchmark, while training 80% faster on GPUs and 70% faster on TPUs for standard 512 input lengths. Our FNet model significantly outperforms in speed at longer input lengths, matching the accuracy of the most accurate models on the Long Range Arena benchmark and surpassing the fastest models in speed across all sequence lengths on GPUs and shorter lengths on TPUs. Additionally, FNet is lightweight in memory use and exceptionally efficient in smaller sizes, outperforming Transformer models under the same speed and accuracy constraints."
    },
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    },
    {
      "title": "MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts",
      "url": "/research/2024/02/26/MoE-Mamba-Efficient-Selective-State-Space-Models-with-Mixture-of-Experts.html",
      "date": "February 26, 2024",
      "summary": "State Space Models (SSMs) are emerging as strong competitors in sequential modeling, challenging Transformers. Integrating Mixture of Experts (MoE) has enhanced Transformer-based models, including cutting-edge open models. We suggest combining SSMs with MoE to further unlock their scaling potential. Our model, MoE-Mamba, based on the SSM model Mamba, exceeds the performance of both Mamba and traditional Transformer-MoE models. Notably, MoE-Mamba achieves Mamba's performance with 2.35 times fewer training steps, maintaining Mamba's inference advantages over Transformers."
    },
    {
      "title": "Transformers with Learnable Activation Functions",
      "url": "/research/2023/02/14/Transformers-with-Learnable-Activation-Functions.html",
      "date": "February 14, 2023",
      "summary": "Activation functions significantly affect model performance by reducing data complexity, yet their selection in Transformer-based language models is often overlooked. This paper explores the impact of using rational activation functions (RAFs), which unlike fixed activation functions (FAFs), can learn optimal functions from data. Our experiments demonstrate that Transformer models with RAFs (RAFT) outperform those with FAFs (FAFT), achieving a 5.71 point higher score on the GLUE benchmark with only 100 training examples and a 2.05 point increase on SQuAD with full data. The varied shapes of learned RAFs across layers and tasks suggest a new method for analyzing and understanding large pre-trained language models."
    },
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "transfer": [
    {
      "title": "Do Transformer Modifications Transfer Across Implementations and Applications?",
      "url": "/research/2021/09/10/Do_Transformer_Modifications_Transfer_Across_Implementations_and_Applications.html",
      "date": "September 10, 2021",
      "summary": "This paper evaluates numerous Transformer architecture modifications in a unified experimental framework, focusing on common natural language processing (NLP) applications. Surprisingly, it finds that most modifications do not significantly enhance performance. The beneficial variants are mostly minor or developed in the same code base used for testing. The study suggests that performance gains may largely depend on implementation details and offers recommendations for improving the generalizability of experimental results."
    },
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load."
    },
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/research/2017/11/06/Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization.html",
      "date": "November 6, 2017",
      "summary": "This paper itroduces Instance Normalization, an online normalization technique that calculates the mean and std across individual data instances. This is shown to work well in situations where there is very high vatiation within batches, such as in style transfer."
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, enhances vision tasks using Vision transformers by masking and reconstructing parts of an image. The compatibility of MIM with CNNs and its operational principle are unclear. this study reveals that MIM improves generalized feature extraction through middle-order patch interactions and introduces Architecture-Agnostic Masked Image Modeling (A2MIM), compatible with both Transformers and CNNs. Extensive testing demonstrates A2MIM's ability to enhance representation learning and transferability to various tasks without specialized modifications."
    },
    {
      "title": "CLIP: Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns from raw text descriptions of images, moving beyond training on fixed object categories. By pre-training on 400 million (image, text) pairs using caption-image matching, the method achieves state-of-the-art image representations from scratch. This allows for zero-shot transfer to various downstream tasks using natural language, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse datasets, showing competitive results against fully supervised baselines without task-specific training, and matching the accuracy of ResNet-50 on ImageNet zero-shot without using its training examples."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. MAE involves masking random patches of an input image and reconstructing the missing pixels using an asymmetric encoder-decoder architecture and a lightweight decoder. Masking a significant portion of the input image creates a challenging yet informative self-supervisory task. These innovations allow for efficient training of large models, tripling training speed and enhancing accuracy. A vanilla ViT-Huge model reaches top accuracy (87.8%) on ImageNet-1K data among similar methods. The model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating promising scaling potential."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces TinyViT, a series of compact and efficient vision transformers (ViTs) designed for devices with limited resources. The authors employ a fast distillation framework to transfer knowledge from large pretrained models to smaller ones during pretraining, using sparsified logits from teacher models to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. With increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters, and demonstrates strong transferability across various downstream tasks."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. This encoder serves as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The DMAE ViT-Base model achieves comparable or superior certified accuracy with fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on ImageNet. The model also shows high transferability to CIFAR-10, indicating its broad applicability."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "This research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. The authors validate these approaches using the ImageNet-1k dataset and confirm their findings with the ImageNet-v2 test set, evaluating transfer performance across six additional datasets."
    },
    {
      "title": "Training data-efficient image transformers & distillation through attention",
      "url": "/research/2021/01/15/Training-data-efficient-image-transformers-&-distillation-through-attention.html",
      "date": "January 15, 2021",
      "summary": "This paper introduces DeiT, an efficient method for training vision transformers. The authors present a unique teacher-student strategy for transformers, leveraging a distillation token for efficient learning from a convolutional network teacher. This method achieves performance on par with convolutional networks, with up to 85.2% accuracy on ImageNet, and demonstrates effective transferability to other tasks."
    },
    {
      "title": "DeiT III: Revenge of the ViT",
      "url": "/research/2023/01/18/DeiT-III-Revenge-of-the-ViT.html",
      "date": "January 18, 2023",
      "summary": "This paper explores the supervised training of Vision Transformers (ViTs) using a simplified training approach adapted from ResNet-50 that includes a novel data-augmentation method with just 3 augmentations. The study demonstrates that this method significantly improves ViTs' performance in image classification, transfer learning, and semantic segmentation over previous supervised training techniques. Moreover, it shows ViTs' performance can match newer architectures, providing a new benchmark for evaluating self-supervised methods on ViTs."
    },
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "The paper investigates the use of synthetic data generated by graphics simulators for pre-training computer vision models. The authors find that model performance on downstream tasks varies with different simulation parameters used to generate the synthetic data. They introduce Task2Sim, a model that maps the requirements of a downstream task to the optimal simulation parameters for generating synthetic pre-training data."
    },
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "This paper introduces tools for training monocular depth estimation models with multiple datasets despite incompatible annotations. Their approach uses a robust training objective, multi-objective learning for data integration, and pretraining encoders on auxiliary tasks. Tested across five datasets, including 3D films as a novel data source, their methods achieve superior zero-shot cross-dataset generalization, outperforming existing benchmarks through principled multi-dataset training."
    },
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "The paper focuses on evaluating the performance of recent advancements in image transformers, such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, on the Tiny ImageNet dataset for transfer learning tasks. While these models are typically trained on large datasets like ImageNet-21k and then fine-tuned on ImageNet-1k, their assessments often overlook the Tiny ImageNet benchmark. The study updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. This underscores the importance of evaluating these models on the Tiny ImageNet dataset for transfer learning performance."
    },
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    },
    {
      "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
      "url": "/research/2021/07/24/The-Many-Faces-of-Robustness-A-Critical-Analysis-of-Out-of-Distribution-Generalization.html",
      "date": "July 24, 2021",
      "summary": "We present four novel real-world distribution shift datasets encompassing changes in image style, blurriness, location, camera settings, and more. Evaluating existing methods for enhancing out-of-distribution robustness, we discover that employing larger models and artificial data augmentations can enhance robustness against real-world distribution shifts, contradicting prior claims. Our findings demonstrate that improvements in artificial robustness benchmarks can indeed transfer to real-world distribution shifts, contrary to prior assumptions. Additionally, we introduce a novel data augmentation technique that surpasses models pretrained with significantly more labeled data, emphasizing its efficacy in addressing real-world distribution shifts. While some methods consistently mitigate texture and local image statistics shifts, they fail to address other shifts like geographic changes. Our results underscore the necessity for future research to examine multiple distribution shifts concurrently, as no single method consistently improves robustness across all evaluated scenarios."
    },
    {
      "title": "Do Vision Transformers See Like Convolutional Neural Networks?",
      "url": "/research/2022/03/03/Do-Vision-Transformers-See-Like-Convolutional-Neural-Networks.html",
      "date": "March 3, 2022",
      "summary": "Convolutional Neural Networks (CNNs) have been prominent in processing visual data, but recent studies show Vision Transformers (ViTs) can match or outperform CNNs in image classification. This work examines how ViTs solve classification tasks, discovering significant differences from CNNs, including uniformity in ViT representations across layers due to self-attention and residual connections that enhance feature propagation. ViTs also maintain spatial information effectively, influenced by classification methods. The study further explores how dataset scale affects ViT features and their transferability, linking these findings to new architectures like MLP-Mixer."
    },
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    },
    {
      "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum",
      "url": "/research/2023/09/30/When-to-Learn-What:-Model-Adaptive-Data-Augmentation-Curriculum.html",
      "date": "September 30, 2023",
      "summary": "Data augmentation (DA) improves neural network generalization by enforcing invariances and symmetries to predefined transformations applied to input data. However, fixed augmentation policies affect samples differently at various training stages, and existing approaches cannot adapt policies to individual samples and the training model. This paper proposes Model-Adaptive Data Augmentation (MADAug) which trains an augmentation policy network to determine when to learn what. Unlike previous work, MADAug selects augmentation operators for each input image with a model-adaptive policy that varies between training stages, creating an optimized data augmentation curriculum. The policy is trained using a bi-level optimization scheme to minimize validation-set loss of a model trained with policy-produced augmentations. Extensive evaluations on multiple image classification tasks and network architectures show that MADAug outperforms or matches existing DA approaches, enhances fairness by improving all classes, particularly the difficult ones, and performs better when transferred to fine-grained datasets. Additionally, the auto-optimized policy in MADAug gradually increases perturbations, forming an easy-to-hard curriculum."
    }
  ],
  "language": [
    {
      "title": "Do Transformer Modifications Transfer Across Implementations and Applications?",
      "url": "/research/2021/09/10/Do_Transformer_Modifications_Transfer_Across_Implementations_and_Applications.html",
      "date": "September 10, 2021",
      "summary": "This paper evaluates numerous Transformer architecture modifications in a unified experimental framework, focusing on common natural language processing (NLP) applications. Surprisingly, it finds that most modifications do not significantly enhance performance. The beneficial variants are mostly minor or developed in the same code base used for testing. The study suggests that performance gains may largely depend on implementation details and offers recommendations for improving the generalizability of experimental results."
    },
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "State Space Models (SSM), despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. To improve SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "This paper introduces the Mamba archetecture, a selective state space model (SSSM) with input dependance. Mamba has better speed and scalability compared to transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences. Authors also present a hardware efficient scanning mechanism that is used in Mamba."
    },
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/research/2024/01/24/MambaByte-Token-free-Selective-State-Space-Model.html",
      "date": "January 24, 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    },
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "This paper introduces a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. This decouples batch size from the loss, and allows the authors to increase the batch size and also improves performance even with smaller batches. Pushing the batch size to one million showed diminishing returns, with 32k being a good middle ground."
    },
    {
      "title": "CLIP: Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns from raw text descriptions of images, moving beyond training on fixed object categories. By pre-training on 400 million (image, text) pairs using caption-image matching, the method achieves state-of-the-art image representations from scratch. This allows for zero-shot transfer to various downstream tasks using natural language, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse datasets, showing competitive results against fully supervised baselines without task-specific training, and matching the accuracy of ResNet-50 on ImageNet zero-shot without using its training examples."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "This paper introduces Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperformed CLIP models on various downstream tasks. The speedup from this method enables experimentation with larger model sizes, more data, or longer training periods."
    },
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/research/2022/10/10/Turbo-Training-with-Token-Dropout.html",
      "date": "October 10, 2022",
      "summary": "This paper introduces Turbo training, an efficient method for training Transformers on video-related tasks. The authors present a simple yet versatile training paradigm applicable to multiple video tasks and demonstrate its effectiveness across action classification, video-language representation learning, and long-video activity classification. Turbo training achieves competitive performance with up to 4× speed-up and reduced memory usage, enabling long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous resource-intensive methods.."
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel normalization technique for self-supervised transformer training that normalizes the [CLS] token and regular tokens separately. The authors find that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks compared to previous models like masked autoencoders (MAE) that use a single normalization layer for all tokens."
    },
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "This paper introduces the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. The authors' empirical evaluation across computer vision, natural language processing, and speech tasks demonstrates that GELU offers performance improvements over ReLU and ELU activations."
    },
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "This paper introduces a curriculum learning framework for image-caption pretraining, inspired by children's language learning from cognitive science, to address the challenges of aligning multiple concepts from captions to objects in images. The method starts with simple image-caption pairs and gradually increases complexity by adding more concepts, leveraging knowledge from each phase for subsequent learning. This approach outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "This paper introduces Online Normalization, a novel method for normalizing neural network hidden activations that offers a batch-independent alternative with comparable accuracy to Batch Normalization. Online Normalization addresses Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. The method is applicable to recurrent, fully connected networks, and those with high activation memory requirements. The authors demonstrate its effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    },
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "This paper introduces QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU while maintaining 16-bit task performance. QLORA utilizes 4-bit quantized language models and Low Rank Adapters (LoRA) and incorporates several memory-saving innovations without compromising performance. The authors' best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. The authors applied QLORA to finetune over 1,000 models and analyzed performance across various datasets, model types, and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Their findings suggest that GPT-4 evaluations are a viable substitute for human assessments and question the reliability of current chatbot benchmarks. The authors make their models and 4-bit training code public."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    },
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "Language Modeling with Gated Convolutional Networks",
      "url": "/research/2017/09/08/Language-Modeling-with-Gated-Convolutional-Networks.html",
      "date": "September 8, 2017",
      "summary": "This paper introduces a novel language modeling approach using stacked convolutions that enable efficient parallel processing, in contrast to the traditionally used recurrent neural networks known for handling unbounded context. The proposed method features a simplified gating mechanism that outperforms previous models and demonstrates superior performance on the WikiText-103 benchmark, showing its capability to manage long-term dependencies. It also delivers competitive results on the Google Billion Words benchmark and significantly reduces sentence scoring latency compared to recurrent models. This marks the first instance of a non-recurrent model achieving comparable success to strong recurrent models on major language tasks."
    },
    {
      "title": "On Layer Normalization in the Transformer Architecture",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architecture.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is widely used in natural language processing. Training it typically requires a carefully designed learning rate warm-up stage, crucial for performance but slowing optimization and requiring more hyperparameter tuning. We provide theoretical insights into the necessity of the warm-up stage, demonstrating that the location of layer normalization affects gradient behavior. For Post-LN Transformers, with layer normalization between residual blocks, large gradients near the output layer at initialization make training unstable without warm-up. Conversely, for Pre-LN Transformers, with layer normalization inside residual blocks, gradients are well-behaved, suggesting the removal of the warm-up stage. Experimental results show that Pre-LN Transformers without warm-up achieve comparable performance with less training time and hyperparameter tuning."
    },
    {
      "title": "Scaling Vision Transformers to 22 Billion Parameters",
      "url": "/research/2023/02/10/Scaling-Vision-Transformers-to-22-Billion-Parameters.html",
      "date": "February 10, 2023",
      "summary": "We introduce a method for efficiently training a 22B-parameter Vision Transformer (ViT-22B) and conduct various experiments to assess its performance. Compared to existing models, ViT-22B shows improved scalability and benefits such as enhanced fairness-performance tradeoff, alignment with human visual perception, and increased robustness. Our findings suggest promising avenues for achieving large-scale language model-like capabilities in vision tasks."
    },
    {
      "title": "Symbolic Discovery of Optimization Algorithms",
      "url": "/research/2023/05/08/Symbolic-Discovery-of-Optimization-Algorithms.html",
      "date": "May 8, 2023",
      "summary": "We propose a method for algorithm discovery via program search, focusing on optimizing deep neural network training. Our approach, Lion (EvoLved Sign Momentum), is memory-efficient and achieves comparable or superior performance to widely-used optimizers such as Adam and Adafactor across various tasks. Specifically, Lion enhances accuracy on tasks like image classification and vision-language contrastive learning, while reducing training compute. Notably, Lion exhibits improved performance with larger batch sizes and requires smaller learning rates compared to Adam. Despite its effectiveness, Lion has limitations, which we analyze, and provide insights into its deployment and performance. Our implementation is publicly available and has been successfully utilized in Google's search ads CTR model."
    },
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    },
    {
      "title": "Chronos: Learning the Language of Time Series",
      "url": "/research/2024/03/12/Chronos-Learning-the-Language-of-Time-Series.html",
      "date": "March 12, 2024",
      "summary": "Chronos is a framework that enhances pretrained probabilistic time series models by tokenizing time series data for training with transformer-based architectures, notably the T5 family. Pretrained on a mix of public and synthetic datasets created via Gaussian processes, Chronos models outshine others in 42 benchmark datasets by showing superior performance on familiar datasets and competitive or better zero-shot capabilities on new datasets. This illustrates Chronos's ability to generalize across different domains, offering a simpler approach to forecasting tasks."
    },
    {
      "title": "Exponentially Faster Language Modeling",
      "url": "/research/2023/11/21/Exponentially-Faster-Language-Modeling.html",
      "date": "November 21, 2023",
      "summary": "UltraFastBERT, a BERT variant, operates with just 0.3% of its neurons—selectively using 12 out of 4095 neurons per layer—for inference, matching the performance of similar models. It replaces conventional feedforward networks with fast feedforward networks (FFFs) to achieve this efficiency. Although fully efficient conditional neural execution isn't yet practical, we offer a high-level CPU code that achieves a 78x speedup and a PyTorch implementation with a 40x speedup over standard batched feedforward inference."
    },
    {
      "title": "Neural Networks Learn Statistics of Increasing Complexity",
      "url": "/research/2024/02/13/Neural-Networks-Learn-Statistics-of-Increasing-Complexity.html",
      "date": "February 13, 2024",
      "summary": "The distributional simplicity bias (DSB) theory suggests neural networks first learn basic patterns in data before understanding more complex correlations. We provide new evidence supporting DSB, showing networks initially excel with data matching training set's simple statistics but this ability diminishes later. Extending DSB to discrete domains, we demonstrate an equivalence between n-gram frequencies and vector moments, also observing this bias in large language models (LLMs). Additionally, by adjusting low-level statistics of images to resemble another class, we reveal that networks in early training phases misclassify these edited images as if they belonged to the target class."
    },
    {
      "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
      "url": "/research/2024/03/13/Simple-and-Scalable-Strategies-to-Continually-Pre-train-Large-Language-Models.html",
      "date": "March 13, 2024",
      "summary": "Large language models (LLMs) often require re-training on new data, consuming extensive computational resources. We present an efficient method that combines learning rate re-warming, re-decaying, and data replay, effectively maintaining performance without full re-training. This approach works well across different data distributions, including minor shifts (English→English) and significant shifts (English→German), tested up to 405M and 10B parameter models. Our findings suggest that continual learning strategies can update LLMs with minimal computational cost, rivaling traditional re-training methods. Additionally, we propose alternatives to the cosine learning rate schedule to reduce forgetting, offering more flexibility in learning without a fixed token budget."
    },
    {
      "title": "Suppressing Pink Elephants with Direct Principle Feedback",
      "url": "/research/2024/02/13/Suppressing-Pink-Elephants-with-Direct-Principle-Feedback.html",
      "date": "February 13, 2024",
      "summary": "Existing methods for controlling language models focus on training desired behaviors, but often lack the flexibility needed for diverse applications. The authors address this with the Pink Elephant Problem, demonstrating the need for language models to adapt to different contexts by avoiding certain topics (the Pink Elephant) in favor of others (Grey Elephant). The authors introduce Direct Principle Feedback (DPF), an adaptation of Constitutional AI that improves control by directly applying critiques and revisions without ranking responses. Our study shows that a 13B LLaMA 2 model fine-tuned with DPF on a synthetic dataset outperforms existing models and matches GPT-4 in managing the Pink Elephant Problem."
    },
    {
      "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
      "url": "/research/2024/02/27/The-Era-of-1-bit-LLMs-All-Large-Language-Models-are-in-1.58-Bits.html",
      "date": "February 27, 2024",
      "summary": "Recent research introduces BitNet b1.58, a ternary (values of -1, 0, 1) 1-bit Large Language Model (LLM) that matches traditional full-precision LLMs in performance and perplexity but is significantly more efficient in terms of latency, memory, throughput, and energy. This work establishes a new scaling law and training approach for future high-performance, cost-effective LLMs, while also facilitating the design of specialized hardware optimized for 1-bit LLMs."
    },
    {
      "title": "Transformers with Learnable Activation Functions",
      "url": "/research/2023/02/14/Transformers-with-Learnable-Activation-Functions.html",
      "date": "February 14, 2023",
      "summary": "Activation functions significantly affect model performance by reducing data complexity, yet their selection in Transformer-based language models is often overlooked. This paper explores the impact of using rational activation functions (RAFs), which unlike fixed activation functions (FAFs), can learn optimal functions from data. Our experiments demonstrate that Transformer models with RAFs (RAFT) outperform those with FAFs (FAFT), achieving a 5.71 point higher score on the GLUE benchmark with only 100 training examples and a 2.05 point increase on SQuAD with full data. The varied shapes of learned RAFs across layers and tasks suggest a new method for analyzing and understanding large pre-trained language models."
    },
    {
      "title": "V-Jepa: Latent Video Prediction For Visual Representation Learning",
      "url": "/research/2023/02/15/V-Jepa-Latent-Video-Prediction-For-Visual-Representation-Learning.html",
      "date": "February 15, 2023",
      "summary": "This paper introduces V-JEPA, a self-supervised video learning method that predicts masked spatio-temporal regions in latent space, effectively applying the masked-modelling principle of large language models to video. This approach generates visual features useful across various image and video tasks without needing model adjustment, achieving significant improvements on Kinetics-400 (82.1%) and Something-Something-v2 (71.2%) benchmarks, outperforming prior video models. V-JEPA also excels in motion understanding tasks, surpassing leading image models like DINOv2 and OpenCLIP, and achieves 77.9% on ImageNet classification with video training alone, setting a new standard for video models."
    },
    {
      "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
      "url": "/research/2024/01/24/Better-Call-GPT,-Comparing-Large-Language-Models-Against-Lawyers.html",
      "date": "January 24, 2024",
      "summary": "This paper compares Large Language Models (LLMs) with traditional legal contract reviewers—Junior Lawyers and Legal Process Outsourcers (LPOs). It evaluates whether LLMs outperform humans in accuracy, speed, and cost-efficiency during contract review. Empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, revealing that advanced models match or exceed human accuracy in identifying legal issues. LLMs complete reviews in seconds, vastly outpacing the hours required by humans. Cost-wise, LLMs achieve a 99.97 percent reduction compared to traditional methods. These findings indicate a transformative shift in legal practice, with LLMs enhancing the accessibility and efficiency of legal services. The research suggests that LLM dominance in legal contract review challenges the status quo, necessitating a reimagined future for legal workflows."
    },
    {
      "title": "The Platonic Representation Hypothesis",
      "url": "/research/2024/05/13/The-Platonic-Representation-Hypothesis.html",
      "date": "May 13, 2024",
      "summary": "This paper argues that AI model representations, especially in deep networks, are converging. It surveys examples in the literature showing how neural network representations align over time and across domains. It demonstrates convergence across data modalities, noting that as vision and language models grow, they measure distances between data points similarly. The authors hypothesize that this convergence leads to a shared statistical model of reality, termed the platonic representation and discuss possible selective pressures toward it. The paper also addresses the implications, limitations, and counterexamples to this trend."
    },
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "nlp": [
    {
      "title": "Do Transformer Modifications Transfer Across Implementations and Applications?",
      "url": "/research/2021/09/10/Do_Transformer_Modifications_Transfer_Across_Implementations_and_Applications.html",
      "date": "September 10, 2021",
      "summary": "This paper evaluates numerous Transformer architecture modifications in a unified experimental framework, focusing on common natural language processing (NLP) applications. Surprisingly, it finds that most modifications do not significantly enhance performance. The beneficial variants are mostly minor or developed in the same code base used for testing. The study suggests that performance gains may largely depend on implementation details and offers recommendations for improving the generalizability of experimental results."
    },
    {
      "title": "AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks",
      "url": "/research/2023/03/01/AdaSAM-Boosting-Sharpness-Aware-Minimization-with-Adaptive-Learning-Rate-and-Momentum-for-Training-Deep-Neural-Networks.html",
      "date": "March 1, 2023",
      "summary": "The Sharpness Aware Minimization (SAM) optimizer, known for enhancing the generalization of deep neural networks by introducing extra perturbation steps, is further developed into AdaSAM by integrating adaptive learning rates and momentum acceleration. Although AdaSAM has been empirically applied to large-scale networks, a theoretical understanding of its performance, considering the complexity of its components, was lacking. This study presents a theoretical analysis of AdaSAM's convergence in stochastic non-convex settings, demonstrating a convergence rate of O(1/√bT) that scales linearly with mini-batch size. By introducing a delayed second-order momentum term, the study successfully decouples and analyzes the intertwined effects of stochastic gradients, adaptive learning rates, and perturbations. This is the first work to offer a detailed convergence rate for SAM with adaptive mechanisms. Experimental results on various NLP tasks indicate AdaSAM's superior performance over SGD, AMSGrad, and SAM optimizers."
    }
  ],
  "depend": [
    {
      "title": "Do Transformer Modifications Transfer Across Implementations and Applications?",
      "url": "/research/2021/09/10/Do_Transformer_Modifications_Transfer_Across_Implementations_and_Applications.html",
      "date": "September 10, 2021",
      "summary": "This paper evaluates numerous Transformer architecture modifications in a unified experimental framework, focusing on common natural language processing (NLP) applications. Surprisingly, it finds that most modifications do not significantly enhance performance. The beneficial variants are mostly minor or developed in the same code base used for testing. The study suggests that performance gains may largely depend on implementation details and offers recommendations for improving the generalizability of experimental results."
    }
  ],
  "generalizability": [
    {
      "title": "Do Transformer Modifications Transfer Across Implementations and Applications?",
      "url": "/research/2021/09/10/Do_Transformer_Modifications_Transfer_Across_Implementations_and_Applications.html",
      "date": "September 10, 2021",
      "summary": "This paper evaluates numerous Transformer architecture modifications in a unified experimental framework, focusing on common natural language processing (NLP) applications. Surprisingly, it finds that most modifications do not significantly enhance performance. The beneficial variants are mostly minor or developed in the same code base used for testing. The study suggests that performance gains may largely depend on implementation details and offers recommendations for improving the generalizability of experimental results."
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "This study introduces a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. The method involves creating spaces for network design that parameterize multiple network populations. A low-dimensional, efficient design space called RegNet is developed, based on modeling network widths and depths through a quantized linear function. The analysis of the RegNet space challenges existing design practices, offering simpler and faster networks effective across various computational budgets. RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs under similar conditions."
    }
  ],
  "mask": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load."
    },
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a self-supervised method that adds a contrastive loss to the Masked Autoencoder. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning."
    },
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/research/2023/03/08/Extreme-Masking-for-Learning-Instance-and-Distributed-Visual-Representations.html",
      "date": "March 8, 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance."
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. This self-supervised method uses multiple crops of an image. Given a context crop the network is trained to predict the embeddings of several target crops. When applied to Vision Transformers, I-JEPA acheived strong performance in various tasks like image classification and depth prediction, outperforming Masked Auto Encoding (MAE) in linear probing when controlling for compute."
    },
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/research/2019/07/26/DropAttention-A-Regularization-Method-for-Fully-Connected-Self-Attention-Networks.html",
      "date": "July 26, 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. DropAttention uses a mask to zero out elements in the attention matrix. Experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. The Authors show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements."
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study looks at self-supervised pre-training methods on small scale or lightweight Vision Transformers (ViTs). Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, enhances vision tasks using Vision transformers by masking and reconstructing parts of an image. The compatibility of MIM with CNNs and its operational principle are unclear. this study reveals that MIM improves generalized feature extraction through middle-order patch interactions and introduces Architecture-Agnostic Masked Image Modeling (A2MIM), compatible with both Transformers and CNNs. Extensive testing demonstrates A2MIM's ability to enhance representation learning and transferability to various tasks without specialized modifications."
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM) for self-supervised visual representation in vision transformers ViT. Five different learning objectives were proposed, which degraded the input image in various ways similar to masking. Design principles are proposed for  token-based pre-training of vision transformers. The most effective strategy combined preserving the original image style with spatial misalignment and masking. This approach outperformed traditional MIM on downstream recognition tasks without increasing computational demands."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. MAE involves masking random patches of an input image and reconstructing the missing pixels using an asymmetric encoder-decoder architecture and a lightweight decoder. Masking a significant portion of the input image creates a challenging yet informative self-supervisory task. These innovations allow for efficient training of large models, tripling training speed and enhancing accuracy. A vanilla ViT-Huge model reaches top accuracy (87.8%) on ImageNet-1K data among similar methods. The model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating promising scaling potential."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. In this paper Masked Image Residual Learning (MIRL), a self-supervised learning framework, was introduced to alleviate the degradation issue and enable effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Extensive testing shows that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "This paper introduces Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperformed CLIP models on various downstream tasks. The speedup from this method enables experimentation with larger model sizes, more data, or longer training periods."
    },
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model consists of two parallel transformer encoders (text prompt and image) and a shared mask decoder. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. "
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper investigates how self-supervised pretraining methods learn part-aware representations. The authors describe contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting that these methods predispose encoders to recognize object parts. Through empirical comparison, they find that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. This encoder serves as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The DMAE ViT-Base model achieves comparable or superior certified accuracy with fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on ImageNet. The model also shows high transferability to CIFAR-10, indicating its broad applicability."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "This paper proposes Hard Patches Mining (HPM), a novel framework for masked image modeling (MIM) pre-training that goes beyond simply solving given problems. HPM aims for the model to generate more challenging tasks for itself, using reconstruction loss as a metric for task difficulty. It incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel normalization technique for self-supervised transformer training that normalizes the [CLS] token and regular tokens separately. The authors find that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks compared to previous models like masked autoencoders (MAE) that use a single normalization layer for all tokens."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "This research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. The authors validate these approaches using the ImageNet-1k dataset and confirm their findings with the ImageNet-v2 test set, evaluating transfer performance across six additional datasets."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "This paper introduces Mask-Reconstruct Augmentation (MRA), an image augmentation technique that leverages self-supervised masked autoencoders to generate distorted inputs and address the issue of overfitting in deep neural networks. Inspired by the success of masked image modeling in self-supervised learning, MRA uses nonlinear transformations for regularization, as opposed to current hand-crafted linear techniques like scale, flip, and color jitter. Extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks."
    },
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "The authors introduce VideoMAE, a data-efficient self-supervised video pre-training (SSVP) approach that utilizes video masked autoencoders with a novel high-ratio video tube masking technique inspired by ImageMAE. Key findings include the effectiveness of high masking ratios (90-95%) due to video's temporal redundancy, strong performance on small datasets (~3k-4k videos) without extra data highlighting the importance of high-level structure learning, and data quality being more crucial than quantity with domain shift being significant. Notably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone without extra data."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    },
    {
      "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
      "url": "/research/2024/02/28/CL-MAE:-Curriculum-Learned-Masked-Autoencoders.html",
      "date": "February 28, 2024",
      "summary": "This paper proposes a curriculum learning approach that updates the masking strategy of Masked Auto Encoders (MAE) to progressively increase the complexity of the self-supervised reconstruction task. To achieve this, a novel learnable masking module is introduced, capable of generating masks of varying complexities, and integrated into masked autoencoders (MAE). This module is jointly trained with the MAE, adjusting its behavior from partner (optimizing the same reconstruction loss) to adversary (optimizing the opposite loss), with a smooth transition regulated by a factor multiplied with the reconstruction loss. This training procedure creates an easy-to-hard curriculum. The Curriculum-Learned Masked Autoencoder (CL-MAE) is trained on ImageNet and demonstrates superior representation learning compared to MAE. Empirical results on five downstream tasks confirm that curriculum learning can successfully self-supervise masked autoencoders."
    },
    {
      "title": "V-Jepa: Latent Video Prediction For Visual Representation Learning",
      "url": "/research/2023/02/15/V-Jepa-Latent-Video-Prediction-For-Visual-Representation-Learning.html",
      "date": "February 15, 2023",
      "summary": "This paper introduces V-JEPA, a self-supervised video learning method that predicts masked spatio-temporal regions in latent space, effectively applying the masked-modelling principle of large language models to video. This approach generates visual features useful across various image and video tasks without needing model adjustment, achieving significant improvements on Kinetics-400 (82.1%) and Something-Something-v2 (71.2%) benchmarks, outperforming prior video models. V-JEPA also excels in motion understanding tasks, surpassing leading image models like DINOv2 and OpenCLIP, and achieves 77.9% on ImageNet classification with video training alone, setting a new standard for video models."
    }
  ],
  "autoencoder": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load."
    },
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a self-supervised method that adds a contrastive loss to the Masked Autoencoder. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning."
    },
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. Taking this to the extreme, this study trains a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss even when trained on pure noise."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study looks at self-supervised pre-training methods on small scale or lightweight Vision Transformers (ViTs). Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. MAE involves masking random patches of an input image and reconstructing the missing pixels using an asymmetric encoder-decoder architecture and a lightweight decoder. Masking a significant portion of the input image creates a challenging yet informative self-supervisory task. These innovations allow for efficient training of large models, tripling training speed and enhancing accuracy. A vanilla ViT-Huge model reaches top accuracy (87.8%) on ImageNet-1K data among similar methods. The model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating promising scaling potential."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. This encoder serves as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The DMAE ViT-Base model achieves comparable or superior certified accuracy with fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on ImageNet. The model also shows high transferability to CIFAR-10, indicating its broad applicability."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel normalization technique for self-supervised transformer training that normalizes the [CLS] token and regular tokens separately. The authors find that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks compared to previous models like masked autoencoders (MAE) that use a single normalization layer for all tokens."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "This paper introduces Mask-Reconstruct Augmentation (MRA), an image augmentation technique that leverages self-supervised masked autoencoders to generate distorted inputs and address the issue of overfitting in deep neural networks. Inspired by the success of masked image modeling in self-supervised learning, MRA uses nonlinear transformations for regularization, as opposed to current hand-crafted linear techniques like scale, flip, and color jitter. Extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks."
    },
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "The authors introduce VideoMAE, a data-efficient self-supervised video pre-training (SSVP) approach that utilizes video masked autoencoders with a novel high-ratio video tube masking technique inspired by ImageMAE. Key findings include the effectiveness of high masking ratios (90-95%) due to video's temporal redundancy, strong performance on small datasets (~3k-4k videos) without extra data highlighting the importance of high-level structure learning, and data quality being more crucial than quantity with domain shift being significant. Notably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone without extra data."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    },
    {
      "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
      "url": "/research/2024/02/28/CL-MAE:-Curriculum-Learned-Masked-Autoencoders.html",
      "date": "February 28, 2024",
      "summary": "This paper proposes a curriculum learning approach that updates the masking strategy of Masked Auto Encoders (MAE) to progressively increase the complexity of the self-supervised reconstruction task. To achieve this, a novel learnable masking module is introduced, capable of generating masks of varying complexities, and integrated into masked autoencoders (MAE). This module is jointly trained with the MAE, adjusting its behavior from partner (optimizing the same reconstruction loss) to adversary (optimizing the opposite loss), with a smooth transition regulated by a factor multiplied with the reconstruction loss. This training procedure creates an easy-to-hard curriculum. The Curriculum-Learned Masked Autoencoder (CL-MAE) is trained on ImageNet and demonstrates superior representation learning compared to MAE. Empirical results on five downstream tasks confirm that curriculum learning can successfully self-supervise masked autoencoders."
    },
    {
      "title": "Neural Network Diffusion",
      "url": "/research/2024/02/20/Neural-Network-Diffusion.html",
      "date": "February 20, 2024",
      "summary": "Diffusion models, known for their success in image and video generation, can also generate high-performing neural network parameters, as demonstrated in this study. By employing a simple combination of an autoencoder and a standard latent diffusion model, our method involves extracting latent representations of network parameters, which are then synthesized from random noise by the diffusion model. These new representations, processed through the autoencoder's decoder, serve as fresh network parameters. Tested across various architectures and datasets, this diffusion approach consistently produces models with comparable or superior performance to traditionally trained networks at minimal extra cost. Importantly, the generated models show distinct performance differences from the trained ones, suggesting further exploration into the versatile applications of diffusion models."
    }
  ],
  "representations": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load."
    }
  ],
  "introuce": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load."
    }
  ],
  "can": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load."
    },
    {
      "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
      "url": "/research/2024/02/28/CL-MAE:-Curriculum-Learned-Masked-Autoencoders.html",
      "date": "February 28, 2024",
      "summary": "This paper proposes a curriculum learning approach that updates the masking strategy of Masked Auto Encoders (MAE) to progressively increase the complexity of the self-supervised reconstruction task. To achieve this, a novel learnable masking module is introduced, capable of generating masks of varying complexities, and integrated into masked autoencoders (MAE). This module is jointly trained with the MAE, adjusting its behavior from partner (optimizing the same reconstruction loss) to adversary (optimizing the opposite loss), with a smooth transition regulated by a factor multiplied with the reconstruction loss. This training procedure creates an easy-to-hard curriculum. The Curriculum-Learned Masked Autoencoder (CL-MAE) is trained on ImageNet and demonstrates superior representation learning compared to MAE. Empirical results on five downstream tasks confirm that curriculum learning can successfully self-supervise masked autoencoders."
    },
    {
      "title": "On the Relationship Between Self-Attention And Convolutional Layers",
      "url": "/research/2020/01/10/On-the-Relationship-Between-Self-Attention-And-Convolutional-Layers.html",
      "date": "January 10, 2020",
      "summary": "Recent studies have shown that attention mechanisms can rival or even outperform convolutional layers in vision tasks, challenging their dominance. Ramachandran et al. (2019) demonstrated that attention could entirely replace convolution, achieving top results. This work investigates whether attention layers function like convolutional layers, finding that multi-head self-attention layers, with enough heads, match the expressiveness of convolutional layers. Numerical experiments confirm that self-attention layers learn to focus on pixel-grid patterns akin to convolutional layers, supporting our findings."
    }
  ],
  "noise": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load."
    },
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights. This paper shows deep networks have significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. The authors propose a stochastic projection rule that sets a new benchmark CIFAR-10 without data augmentation."
    },
    {
      "title": "Dropout as Data Augmentation",
      "url": "/research/2016/01/08/DROPOUT-AS-DATA-AUGMENTATION.html",
      "date": "January 8, 2016",
      "summary": "Dropout is typically interpreted as turning the test time network into an ensemble of the thinner training networks. This paper argues that dropout can also be interpreted as a kind of data augmentation in image space. Authors present an approach to project the dropout noise within a network back into the input space, visualising the augmented versions of the training data, and show that training a deterministic network on the augmented samples yields similar results. Authors then propose a new dropout noise scheme and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. Taking this to the extreme, this study trains a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss even when trained on pure noise."
    },
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/research/2023/06/30/Impact-of-Noise-on-Calibration-and-Generalisation-of-Neural-Networks.html",
      "date": "June 30, 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. Activation noise is shown to significantly improve generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/ON-INTERACTION-BETWEEN-AUGMENTATIONS-AND-CORRUPTIONS-IN-NATURAL-CORRUPTION-ROBUSTNESS.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, the correlation between data augmentations and test-time corruptions remains unclear. This paper introduces Minimal Sample Distance, and demonstrates a strong correlation between augmentation-corruption similarity and performance. Authors argue that training with augmentations that are perceptually similar to corruptions enhances test error."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of Vision Transformer (ViT) variants against common corruptions. The benchmarking reveals that overlapping patch embedding and convolutional feed-forward networks (FFNs) significantly enhance ViT robustness. The study also scrutinizes the effectiveness of CNN-based data augmentation strategies when applied to ViTs, finding that adversarial noise training is effective, while fourier-domain augmentation falls short. A new conditional method for generating dynamic augmentation parameters based on input images is proposed, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. This encoder serves as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The DMAE ViT-Base model achieves comparable or superior certified accuracy with fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on ImageNet. The model also shows high transferability to CIFAR-10, indicating its broad applicability."
    },
    {
      "title": "Stable Diffusion 3: Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "This paper focuses on improving diffusion models for high-dimensional data like images. The authors enhance noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Their large-scale study shows superior performance over established diffusion methods in high-resolution text-to-image synthesis. A new transformer-based architecture is introduced for better text comprehension, typography, and human preference in text-to-image generation. The findings demonstrate predictable scaling trends, with lower validation loss tied to improved synthesis. The largest models outperform state-of-the-art approaches."
    },
    {
      "title": "Understanding Deep Learning Requires Re- Thinking Generalization",
      "url": "/research/2017/02/26/Understanding-Deep-Learning-Requires-Re--Thinking-Generalization.html",
      "date": "February 26, 2017",
      "summary": "Extensive experiments reveal that traditional explanations for the strong generalization performance of large neural networks fall short. Despite conventional wisdom attributing success to model properties or regularization techniques, our findings show that state-of-the-art convolutional networks for image classification can easily overfit random labels. This phenomenon persists regardless of explicit regularization or substituting true images with random noise. Theoretical analysis suggests that shallow neural networks achieve perfect finite sample expressivity when parameters outnumber data points, as commonly observed. We interpret these results in contrast to traditional models."
    },
    {
      "title": "Neural Network Diffusion",
      "url": "/research/2024/02/20/Neural-Network-Diffusion.html",
      "date": "February 20, 2024",
      "summary": "Diffusion models, known for their success in image and video generation, can also generate high-performing neural network parameters, as demonstrated in this study. By employing a simple combination of an autoencoder and a standard latent diffusion model, our method involves extracting latent representations of network parameters, which are then synthesized from random noise by the diffusion model. These new representations, processed through the autoencoder's decoder, serve as fresh network parameters. Tested across various architectures and datasets, this diffusion approach consistently produces models with comparable or superior performance to traditionally trained networks at minimal extra cost. Importantly, the generated models show distinct performance differences from the trained ones, suggesting further exploration into the versatile applications of diffusion models."
    }
  ],
  "prediction": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load."
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. This self-supervised method uses multiple crops of an image. Given a context crop the network is trained to predict the embeddings of several target crops. When applied to Vision Transformers, I-JEPA acheived strong performance in various tasks like image classification and depth prediction, outperforming Masked Auto Encoding (MAE) in linear probing when controlling for compute."
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing is a regularization technique that reduces overfitting and enhances classification performance. Label smothing involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution. This significantly improves classification accuracy and model robustness to noisy labels."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This study suggest that label smoothing not only boosts generalization but also enhances model calibration. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. Label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This paper is effectively a follow-up to DinoV2. The study addresses the issue of artifacts in feature maps of Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. The authors introduce a straightforward strategy of adding extra tokens to the input sequence of the Vision Transformer, which eliminates these artifacts for both supervised and self-supervised models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks, facilitates object discovery with larger models, and results in smoother feature and attention maps for downstream visual processing tasks."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "This paper proposes Hard Patches Mining (HPM), a novel framework for masked image modeling (MIM) pre-training that goes beyond simply solving given problems. HPM aims for the model to generate more challenging tasks for itself, using reconstruction loss as a metric for task difficulty. It incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "This paper introduces FixMatch, a streamlined semi-supervised learning (SSL) algorithm. FixMatch generates pseudo-labels using a model's predictions on weakly-augmented unlabeled images, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness, and the code is publicly available."
    },
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces an iterative self-training framework for sim-to-real 6D object pose estimation to enable cost-effective robotic bin-picking. The authors create a photo-realistic simulator to synthesize virtual data for training an initial pose estimation network (teacher model). This teacher predicts poses on unlabeled real data, and an adaptive selection scheme filters reliable predictions to generate pseudo-labels for training a student model on real data. Iteratively refining the teacher with the student improves pseudo-label quality. Evaluated on public benchmarks and a new dataset, their method shows 11.49% and 22.62% ADD(-S) improvements and a 19.54% increase in bin-picking success, demonstrating the effectiveness of this iterative sim-to-real approach."
    },
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "This paper introduces an algorithm that identifies simple and highly generalizable neural networks by searching for extensive flat minima regions in the error function, where the error rate is relatively stable. Such flat minima are associated with lower overfitting risks based on minimum description length (MDL) principles. Despite requiring second-order derivative calculations, the algorithm has a complexity level comparable to backpropagation. When tested on feedforward and recurrent networks, as well as stock market prediction tasks, this algorithm outperformed traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    },
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    },
    {
      "title": "Learning and Leveraging World Models in Visual Representation Learning",
      "url": "/research/2024/03/01/Learning-and-Leveraging-World-Models-in-Visual-Representation-Learning.html",
      "date": "March 1, 2024",
      "summary": "The Joint-Embedding Predictive Architecture (JEPA) is a self-supervised method previously used for predicting missing input parts. This study expands JEPA to predict a wider range of corruptions by introducing Image World Models (IWM), which learn the effects of global photometric transformations in latent space. Key to effective IWM learning are conditioning, prediction difficulty, and model capacity. The study demonstrates that IWM's predictive world model, when fine-tuned, can tackle diverse tasks and either matches or outperforms existing self-supervised techniques. Additionally, it enables control over the abstraction level of learned representations, achieving either invariant or equivariant representations, similar to contrastive methods or masked image modeling, respectively."
    },
    {
      "title": "V-Jepa: Latent Video Prediction For Visual Representation Learning",
      "url": "/research/2023/02/15/V-Jepa-Latent-Video-Prediction-For-Visual-Representation-Learning.html",
      "date": "February 15, 2023",
      "summary": "This paper introduces V-JEPA, a self-supervised video learning method that predicts masked spatio-temporal regions in latent space, effectively applying the masked-modelling principle of large language models to video. This approach generates visual features useful across various image and video tasks without needing model adjustment, achieving significant improvements on Kinetics-400 (82.1%) and Something-Something-v2 (71.2%) benchmarks, outperforming prior video models. V-JEPA also excels in motion understanding tasks, surpassing leading image models like DINOv2 and OpenCLIP, and achieves 77.9% on ImageNet classification with video training alone, setting a new standard for video models."
    }
  ],
  "robustness": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load."
    },
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "This paper introduces the concept of delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a measure of local complexity, analyzing the density of linear regions in the DNN input space. The authors find that during training linear regions emerge near training samples with the nonlinearities being forced towards decision boundries."
    },
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights. This paper shows deep networks have significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. The authors propose a stochastic projection rule that sets a new benchmark CIFAR-10 without data augmentation."
    },
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/research/2021/05/03/Robust-and-Generalizable-Visual-Representation-Learning-via-Random-Convolutions.html",
      "date": "May 3, 2021",
      "summary": "This study introduces random convolutions for data augmentation. Random convolutions approximately preserve shape while distorting local textures. This method significantly enhances performance on domain generalization and robustness / corruption benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods."
    },
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/research/2019/04/27/Benchmarking-Neural-Network-Robustness-to-Common-Corruptions-and-Surface-Variations.html",
      "date": "April 27, 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and IMAGENET-P, which assesses perturbation robustness."
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing is a regularization technique that reduces overfitting and enhances classification performance. Label smothing involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution. This significantly improves classification accuracy and model robustness to noisy labels."
    },
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/research/2023/06/30/Impact-of-Noise-on-Calibration-and-Generalisation-of-Neural-Networks.html",
      "date": "June 30, 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. Activation noise is shown to significantly improve generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/ON-INTERACTION-BETWEEN-AUGMENTATIONS-AND-CORRUPTIONS-IN-NATURAL-CORRUPTION-ROBUSTNESS.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, the correlation between data augmentations and test-time corruptions remains unclear. This paper introduces Minimal Sample Distance, and demonstrates a strong correlation between augmentation-corruption similarity and performance. Authors argue that training with augmentations that are perceptually similar to corruptions enhances test error."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Data augmentation improves generalization, and robustness against image corruptions. However, the use of complex augmentation pipelines typically involves tuning of an enormous number of hyper parameters. RandAugment addresses this by reducing the search space. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets."
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of Vision Transformer (ViT) variants against common corruptions. The benchmarking reveals that overlapping patch embedding and convolutional feed-forward networks (FFNs) significantly enhance ViT robustness. The study also scrutinizes the effectiveness of CNN-based data augmentation strategies when applied to ViTs, finding that adversarial noise training is effective, while fourier-domain augmentation falls short. A new conditional method for generating dynamic augmentation parameters based on input images is proposed, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Incorporating diverse image styles into CNN training data reduces texture bias, enhances shape recognition, and improves resilience against common image corruptions. This is typically explained as decreasing model reliance on high frequency texture information. This paper challenges this explaniation through large scale testing with natural images, edge information, and stylization, finding no direct link between shape bias and robustness. The enhanced corruption robustness is instead attributed to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/research/2021/08/31/Universal-Adversarial-Robustness-of-Texture-and-Shape-biased-Models.html",
      "date": "August 31, 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates how to achieve state-of-the-art certified adversarial robustness to ℓ2-norm bounded perturbations using only off-the-shelf pretrained models. The authors instantiate the denoised smoothing approach by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This method certifies 71% accuracy on ImageNet under adversarial perturbations constrained to an ℓ2 norm of ε = 0.5, improving upon the prior certified state-of-the-art by 14 percentage points and denoised smoothing by 30 percentage points, without requiring any fine-tuning or retraining of model parameters."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. This encoder serves as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The DMAE ViT-Base model achieves comparable or superior certified accuracy with fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on ImageNet. The model also shows high transferability to CIFAR-10, indicating its broad applicability."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of Vision Transformers (ViTs) without relying on large-scale pre-training datasets like ImageNet and JFT or requiring modifications to the architecture or loss functions. The authors show that this approach improves ViT performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, while maintaining ViT's attention to relevant regions and robustness, despite ViT's inherent lack of inductive biases and typical dependence on large-scale pre-training."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "This study challenges the traditional understanding of how Convolutional Neural Networks (CNNs) recognize objects, revealing that they are more biased towards recognizing textures rather than learning complex shapes. The authors found that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, training a ResNet-50 on a stylized version of ImageNet, designed to emphasize shape, aligned the network's performance more closely with human behavior. This shape-based training matched human performance in controlled experiments and enhanced object detection and robustness against image distortions, highlighting the benefits of shape-based representations in visual recognition systems."
    },
    {
      "title": "SinLU: Sinu-Sigmoidal Linear Uni",
      "url": "/research/2021/12/02/SinLU-Sinu-Sigmoidal-Linear-Uni.html",
      "date": "December 2, 2021",
      "summary": "This paper introduces a non-linear activation function called the Sinu-sigmoidal Linear Unit (SinLU), formulated as SinLU(x) = (x + a sin bx) · σ(x). It incorporates a sine wave for added functionality over traditional linear units, with two trainable parameters controlling the sinusoidal participation. The authors compare SinLU's performance to ReLU, GELU, and SiLU across various domains, models, and standard datasets. The results demonstrate SinLU's robustness and superior performance due to the incorporation of the trainable sine wave parameters, facilitating easy training and fast convergence."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks",
      "url": "/research/2020/06/11/Batch-Normalization-Provably-Avoids-Rank-Collapse-for-Randomly-Initialised-Deep-Networks.html",
      "date": "June 11, 2020",
      "summary": "We investigate the challenge of training randomly initialized deep neural networks due to spectral instabilities in products of random matrices. Batch normalization emerges as an effective solution to prevent rank collapse in both linear and ReLU networks. Leveraging tools from Markov chain theory, we establish a lower rank bound for deep linear networks. Empirical findings show that this rank robustness extends to ReLU networks. Our experiments on real-world datasets underscore the significance of rank stability in training modern deep neural architectures."
    },
    {
      "title": "How to decay your learning rate",
      "url": "/research/2021/05/23/How-to-decay-your-learning-rate.html",
      "date": "May 23, 2021",
      "summary": "Empirical findings suggest that typical fine-tuned learning rate schedules decay the learning rate following weight norm fluctuations. This led to the development of ABEL, an automatic scheduler that adjusts the learning rate based on weight norm changes. ABEL performs comparably to tuned schedules but demonstrates greater robustness to parameter variations. Extensive experiments across various domains reveal that when the weight norm remains stable, simplified schedules yield equivalent performance to complex ones, resembling a constant learning rate with decay towards the end of training."
    },
    {
      "title": "Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding",
      "url": "/research/2021/11/16/Improved-Robustness-of-Vision-Transformer-via-PreLayerNorm-in-Patch-Embedding.html",
      "date": "November 16, 2021",
      "summary": "Recent advancements in vision transformers (ViTs) have shown superior performance across diverse visual tasks, surpassing convolutional neural networks (CNNs). Given ViT's distinct architecture, understanding its behavior and reliability is imperative. This paper investigates ViT's robustness by comparing it with CNNs under various image corruptions relevant to real-world vision tasks. While ViT generally exhibits comparable or improved robustness over CNNs, it consistently underperforms in contrast enhancement tasks. Analysis suggests that positional embedding in ViT's patch embedding may malfunction with color scale changes. We propose PreLayerNorm, a modified patch embedding structure, to address this issue and ensure scale-invariant behavior in ViT. ViT with PreLayerNorm demonstrates enhanced robustness across various corruptions, particularly in contrast-varying environments."
    },
    {
      "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
      "url": "/research/2021/07/24/The-Many-Faces-of-Robustness-A-Critical-Analysis-of-Out-of-Distribution-Generalization.html",
      "date": "July 24, 2021",
      "summary": "We present four novel real-world distribution shift datasets encompassing changes in image style, blurriness, location, camera settings, and more. Evaluating existing methods for enhancing out-of-distribution robustness, we discover that employing larger models and artificial data augmentations can enhance robustness against real-world distribution shifts, contradicting prior claims. Our findings demonstrate that improvements in artificial robustness benchmarks can indeed transfer to real-world distribution shifts, contrary to prior assumptions. Additionally, we introduce a novel data augmentation technique that surpasses models pretrained with significantly more labeled data, emphasizing its efficacy in addressing real-world distribution shifts. While some methods consistently mitigate texture and local image statistics shifts, they fail to address other shifts like geographic changes. Our results underscore the necessity for future research to examine multiple distribution shifts concurrently, as no single method consistently improves robustness across all evaluated scenarios."
    },
    {
      "title": "ImageNet-D: A new challenging robustness dataset inspired by domain adaptation",
      "url": "/research/2023/05/05/ImageNet-D-A-new-challenging-robustness-dataset-inspired-by-domain-adaptation.html",
      "date": "May 5, 2023",
      "summary": "We introduce ImageNet-D, a novel dataset designed to evaluate the robustness of ImageNet-trained models across various domains. With six distinct domains including Real, Painting, Clipart, Sketch, Infograph, and Quick-draw, ImageNet-D challenges even state-of-the-art models, revealing interpretable errors. For instance, our leading EfficientNet-L2 model exhibits a significant performance decrease, dropping from 11.6% on clean ImageNet to 29.2% on the Real domain."
    },
    {
      "title": "Scaling Vision Transformers to 22 Billion Parameters",
      "url": "/research/2023/02/10/Scaling-Vision-Transformers-to-22-Billion-Parameters.html",
      "date": "February 10, 2023",
      "summary": "We introduce a method for efficiently training a 22B-parameter Vision Transformer (ViT-22B) and conduct various experiments to assess its performance. Compared to existing models, ViT-22B shows improved scalability and benefits such as enhanced fairness-performance tradeoff, alignment with human visual perception, and increased robustness. Our findings suggest promising avenues for achieving large-scale language model-like capabilities in vision tasks."
    },
    {
      "title": "SAM as an Optimal Relaxation of Bayes",
      "url": "/research/2023/12/10/SAM-as-an-Optimal-Relaxation-of-Bayes.html",
      "date": "December 10, 2023",
      "summary": "We introduce SAM, a method enhancing generalization in deep learning, as a relaxation of the Bayes objective. SAM replaces the expected negative-loss with an optimal convex lower bound derived using the Fenchel biconjugate. This connection enables an Adam-like extension of SAM, offering automatic uncertainty estimates and potential accuracy improvements. Bridging adversarial and Bayesian methods, our work paves the way for robustness enhancement."
    },
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    }
  ],
  "pretraining": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load."
    },
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "This paper introduces a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. This decouples batch size from the loss, and allows the authors to increase the batch size and also improves performance even with smaller batches. Pushing the batch size to one million showed diminishing returns, with 32k being a good middle ground."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study looks at self-supervised pre-training methods on small scale or lightweight Vision Transformers (ViTs). Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods."
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, enhances vision tasks using Vision transformers by masking and reconstructing parts of an image. The compatibility of MIM with CNNs and its operational principle are unclear. this study reveals that MIM improves generalized feature extraction through middle-order patch interactions and introduces Architecture-Agnostic Masked Image Modeling (A2MIM), compatible with both Transformers and CNNs. Extensive testing demonstrates A2MIM's ability to enhance representation learning and transferability to various tasks without specialized modifications."
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM) for self-supervised visual representation in vision transformers ViT. Five different learning objectives were proposed, which degraded the input image in various ways similar to masking. Design principles are proposed for  token-based pre-training of vision transformers. The most effective strategy combined preserving the original image style with spatial misalignment and masking. This approach outperformed traditional MIM on downstream recognition tasks without increasing computational demands."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results. The method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. A convolutional neural network (CNN) was trained on over a million color images, operating as a feed-forward pass at test time. The algorithm's effectiveness was validated through a colorization Turing test, where it deceived participants in 32% of trials. The paper also illustrates how colorization can serve as an effective pretraining task for self-supervised feature learning, achieving state-of-the-art results on several benchmarks."
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores self-supervision through automatic colorization as an alternative to ImageNet pretraining. Self-supervised training achieved state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. This paper highlights the significance of loss formulation, training specifics, and network architecture when pretraining through colorization. It also revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and feature adaptability upon fine-tuning. The findings suggest that colorization offers a better supervisory signal comparable to various types of ImageNet pretraining."
    },
    {
      "title": "CLIP: Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns from raw text descriptions of images, moving beyond training on fixed object categories. By pre-training on 400 million (image, text) pairs using caption-image matching, the method achieves state-of-the-art image representations from scratch. This allows for zero-shot transfer to various downstream tasks using natural language, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse datasets, showing competitive results against fully supervised baselines without task-specific training, and matching the accuracy of ResNet-50 on ImageNet zero-shot without using its training examples."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. MAE involves masking random patches of an input image and reconstructing the missing pixels using an asymmetric encoder-decoder architecture and a lightweight decoder. Masking a significant portion of the input image creates a challenging yet informative self-supervisory task. These innovations allow for efficient training of large models, tripling training speed and enhancing accuracy. A vanilla ViT-Huge model reaches top accuracy (87.8%) on ImageNet-1K data among similar methods. The model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating promising scaling potential."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. In this paper Masked Image Residual Learning (MIRL), a self-supervised learning framework, was introduced to alleviate the degradation issue and enable effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Extensive testing shows that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) uses synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. Given this perspective, a one-instance fractal database (OFDB) is developed where only a single image per category is present. Despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M comparable or superior results are obtaine to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. A ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training. Synthetic images generated by FDSL circumvent issues associated with real images, offering a promising avenue for pre-training general models. Investigations suggest that simple object contours can match fractal-based dataset performance and increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "This paper introduces Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperformed CLIP models on various downstream tasks. The speedup from this method enables experimentation with larger model sizes, more data, or longer training periods."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces TinyViT, a series of compact and efficient vision transformers (ViTs) designed for devices with limited resources. The authors employ a fast distillation framework to transfer knowledge from large pretrained models to smaller ones during pretraining, using sparsified logits from teacher models to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. With increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters, and demonstrates strong transferability across various downstream tasks."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper investigates how self-supervised pretraining methods learn part-aware representations. The authors describe contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting that these methods predispose encoders to recognize object parts. Through empirical comparison, they find that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets for formula-driven supervised learning (FDSL). The authors identify the optimal FDSL parameters and maximize synthetic image variety, which is crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. The authors demonstrate FDSL's potential for continuous improvement and its ability to avoid issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "This paper introduces a self-supervised vision model pretraining framework inspired by energy-based models (EBMs), leveraging symmetric mappings in deep networks without auxiliary components. The framework models energy estimation and data restoration through the network's forward and backward passes, assigning low energy to unlabeled dataset samples and using gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting various pretext tasks. Experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "This paper proposes Hard Patches Mining (HPM), a novel framework for masked image modeling (MIM) pre-training that goes beyond simply solving given problems. HPM aims for the model to generate more challenging tasks for itself, using reconstruction loss as a metric for task difficulty. It incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "This paper introduces DiffBIR, a framework that utilizes pretrained text-to-image diffusion models for blind image restoration. The two-stage pipeline involves pretraining a restoration module on various degradations to enhance real-world applicability, followed by the use of latent diffusion models for realistic restoration. The authors introduce a novel injective modulation sub-network, LAControlNet, for fine-tuning and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, a controllable module allows users to adjust quality and fidelity during the denoising process. Extensive tests demonstrate DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets."
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of Vision Transformers (ViTs) without relying on large-scale pre-training datasets like ImageNet and JFT or requiring modifications to the architecture or loss functions. The authors show that this approach improves ViT performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, while maintaining ViT's attention to relevant regions and robustness, despite ViT's inherent lack of inductive biases and typical dependence on large-scale pre-training."
    },
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "This paper introduces a curriculum learning framework for image-caption pretraining, inspired by children's language learning from cognitive science, to address the challenges of aligning multiple concepts from captions to objects in images. The method starts with simple image-caption pairs and gradually increases complexity by adding more concepts, leveraging knowledge from each phase for subsequent learning. This approach outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios."
    },
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "This paper introduces the Kinetics Human Action Video dataset. The authors note that the limited number of videos in existing datasets like UCF-101 and HMDB-51 hinders the ability to effectively evaluate architectures due to similar performance across these small benchmarks. Kinetics contains 400 classes with over 400 video clips per class from challenging YouTube videos. The paper analyzes the impact of this larger dataset on the performance of existing architectures as well as the benefits of pre-training models on Kinetics. The authors introduce the Two-Stream Inflated 3D ConvNet (I3D), an extension of 2D ConvNets to 3D for improved video feature extraction. When pre-trained on Kinetics, this I3D model sets new benchmarks for action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    },
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This paper investigates using high-quality rendering software and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle. This synthetic dataset is used to augment limited real training data for image classification and semantic segmentation tasks. While models trained solely on synthetic images showed low accuracy on real validation data, including even small amounts of real data significantly improved performance. Augmenting real training data with synthetic images outperformed using only real images. Furthermore, pretraining models on the synthetic dataset before transfer learning significantly reduced training costs by allowing most of the training to be completed upfront using the synthetic data."
    },
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "The paper investigates the use of synthetic data generated by graphics simulators for pre-training computer vision models. The authors find that model performance on downstream tasks varies with different simulation parameters used to generate the synthetic data. They introduce Task2Sim, a model that maps the requirements of a downstream task to the optimal simulation parameters for generating synthetic pre-training data."
    },
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "This paper introduces tools for training monocular depth estimation models with multiple datasets despite incompatible annotations. Their approach uses a robust training objective, multi-objective learning for data integration, and pretraining encoders on auxiliary tasks. Tested across five datasets, including 3D films as a novel data source, their methods achieve superior zero-shot cross-dataset generalization, outperforming existing benchmarks through principled multi-dataset training."
    },
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "The authors introduce VideoMAE, a data-efficient self-supervised video pre-training (SSVP) approach that utilizes video masked autoencoders with a novel high-ratio video tube masking technique inspired by ImageMAE. Key findings include the effectiveness of high masking ratios (90-95%) due to video's temporal redundancy, strong performance on small datasets (~3k-4k videos) without extra data highlighting the importance of high-level structure learning, and data quality being more crucial than quantity with domain shift being significant. Notably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone without extra data."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    },
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    },
    {
      "title": "Deep learning via Hessian-free optimization",
      "url": "/research/2010/06/21/Deep-learning-via-Hessian-free-optimization.html",
      "date": "June 21, 2010",
      "summary": "We present a novel 2nd-order optimization method inspired by the Hessian-free approach, applied to deep auto-encoders. Achieving superior results without pre-training compared to Hinton & Salakhutdinov (2006), our method is practical, user-friendly, scalable to large datasets, and versatile across various model classes. Additionally, we address the challenge of pathological curvature in deep learning, highlighting how our method effectively mitigates this issue."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "url": "/research/2014/02/19/Exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks.html",
      "date": "February 19, 2014",
      "summary": "This study aims to enhance our comprehension of deep learning dynamics by analyzing deep linear neural networks. Despite their linear input-output mapping, these networks display nonlinear gradient descent dynamics, resulting in phenomena like prolonged plateaus and swift transitions to improved solutions. Through analytical exploration, we reveal that as network depth increases indefinitely, learning speed can remain finite under specific initial conditions. Unsupervised pretraining, under certain data conditions, can discover these favorable initial conditions, unlike random Gaussian initializations. We introduce a novel class of random orthogonal initial conditions that, akin to unsupervised pretraining, enable depth-independent learning. These conditions also ensure gradient propagation in deep nonlinear networks, particularly when operating at the edge of chaos."
    },
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    },
    {
      "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
      "url": "/research/2024/03/13/Simple-and-Scalable-Strategies-to-Continually-Pre-train-Large-Language-Models.html",
      "date": "March 13, 2024",
      "summary": "Large language models (LLMs) often require re-training on new data, consuming extensive computational resources. We present an efficient method that combines learning rate re-warming, re-decaying, and data replay, effectively maintaining performance without full re-training. This approach works well across different data distributions, including minor shifts (English→English) and significant shifts (English→German), tested up to 405M and 10B parameter models. Our findings suggest that continual learning strategies can update LLMs with minimal computational cost, rivaling traditional re-training methods. Additionally, we propose alternatives to the cosine learning rate schedule to reduce forgetting, offering more flexibility in learning without a fixed token budget."
    },
    {
      "title": "VideoMamba: State Space Model for Efficient Video Understanding",
      "url": "/research/2024/03/12/VideoMamba-State-Space-Model-for-Efficient-Video-Understanding.html",
      "date": "March 12, 2024",
      "summary": "This work introduces VideoMamba, an innovative approach that addresses local redundancy and global dependencies in video understanding by adapting the Mamba framework to video analysis. VideoMamba outperforms existing 3D convolutional neural networks and video transformers through its linear-complexity operator, which facilitates efficient long-term modeling for high-resolution, long-duration videos. Its effectiveness is demonstrated across four main areas: scalability in the visual domain without needing extensive dataset pretraining, thanks to a novel self-distillation technique; the ability to recognize short-term actions with fine-grained motion differences; superior performance in long-term video understanding compared to traditional feature-based models; and robust compatibility with multiple modalities, enhancing multi-modal video analysis. VideoMamba establishes a new standard for comprehensive and efficient video understanding."
    }
  ],
  "uncurate": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load."
    }
  ],
  "dataset": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load."
    },
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "This paper introdices an automated pipeline for building high-quality, diverse image datasets. Also proposes minor mofications to the archetecture and loss of a DINO ViT. A large ViT model with 1 billion parameters was trained and distilled into smaller models."
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "This paper introduces PatchDropout, a simple training technique for Vision Transformers ViT that drops random image patches, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/research/2017/08/23/A-downsampled-variant-of-imagenet-as-an-alternative-to-the-cifar-datasets.html",
      "date": "August 23, 2017",
      "summary": "This paper proposes downsampled versions of ImageNet: ImageNet64x64, ImageNet32x32, and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics."
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Data augmentation improves generalization, and robustness against image corruptions. However, the use of complex augmentation pipelines typically involves tuning of an enormous number of hyper parameters. RandAugment addresses this by reducing the search space. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets."
    },
    {
      "title": "SVHN: Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View House Numbers."
    },
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets transformers as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "This study introduces the Vision Transformer ViT archetecture. The paper shows that a pure transformer applied directly to sequences of image patches can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "CLIP: Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns from raw text descriptions of images, moving beyond training on fixed object categories. By pre-training on 400 million (image, text) pairs using caption-image matching, the method achieves state-of-the-art image representations from scratch. This allows for zero-shot transfer to various downstream tasks using natural language, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse datasets, showing competitive results against fully supervised baselines without task-specific training, and matching the accuracy of ResNet-50 on ImageNet zero-shot without using its training examples."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. A ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training. Synthetic images generated by FDSL circumvent issues associated with real images, offering a promising avenue for pre-training general models. Investigations suggest that simple object contours can match fractal-based dataset performance and increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model consists of two parallel transformer encoders (text prompt and image) and a shared mask decoder. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. "
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "This paper investigates the geometric and statistical properties of representations across layers in large transformers used for self-supervised learning on various data types. The authors observe common evolution patterns, with data manifolds expanding initially, contracting at intermediate layers, and the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. They find that semantic information peaks after initial expansion, a trend consistent across models and datasets. The study proposes an unsupervised method to identify layers richest in semantic content, suggesting that those at a relative ID minimum are optimal for downstream tasks."
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey provides a comprehensive review of transformer-based visual segmentation, a crucial field for various applications. The authors cover the evolution from convolutional methods to vision transformers, offering a unified framework to simplify understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas such as 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. The authors re-evaluate these methods on established datasets, outline current challenges, and suggest future research directions."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets for formula-driven supervised learning (FDSL). The authors identify the optimal FDSL parameters and maximize synthetic image variety, which is crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. The authors demonstrate FDSL's potential for continuous improvement and its ability to avoid issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "This paper introduces a self-supervised vision model pretraining framework inspired by energy-based models (EBMs), leveraging symmetric mappings in deep networks without auxiliary components. The framework models energy estimation and data restoration through the network's forward and backward passes, assigning low energy to unlabeled dataset samples and using gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting various pretext tasks. Experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "This paper explores the interactions between training data amount, model regularization or data augmentation (AugReg), model size, and compute budget for Vision Transformers (ViT) in various vision tasks. The authors find that using more compute and AugReg can achieve the same performance as training with significantly more data. They demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "This research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. The authors validate these approaches using the ImageNet-1k dataset and confirm their findings with the ImageNet-v2 test set, evaluating transfer performance across six additional datasets."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "This paper introduces a novel approach to Radiance Field methods for novel-view synthesis that addresses the challenges of high visual quality, costly training, and real-time rendering of unbounded, complete scenes at 1080p resolution. The authors propose three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. The method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    },
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper introduces the AVA dataset, which features 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. The authors highlight the challenges in action recognition and introduce a novel localization approach that surpasses existing benchmarks but shows modest performance on AVA (15.6% mAP), indicating the need for advanced video understanding methods."
    },
    {
      "title": "Curriculum Learning by Dynamic Instance Hardness",
      "url": "/research/2020/12/06/Curriculum-Learning-by-Dynamic-Instance-Hardness.html",
      "date": "December 6, 2020",
      "summary": "This paper improves curriculum learning via introduces Dynamic Instance Hardness (DIH), a method that measures a sample's learning difficulty over time. DIH provides a stable indicator of learning progress by tracking the exponential moving average of a sample's hardness. DIHCL enhances learning efficiency and model accuracy without extra computational costs by leveraging data from the training process itself. Tested on 11 datasets, DIHCL surpasses traditional training methods and recent curriculum learning techniques in both efficiency and effectiveness."
    },
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "This paper introduces DiffBIR, a framework that utilizes pretrained text-to-image diffusion models for blind image restoration. The two-stage pipeline involves pretraining a restoration module on various degradations to enhance real-world applicability, followed by the use of latent diffusion models for realistic restoration. The authors introduce a novel injective modulation sub-network, LAControlNet, for fine-tuning and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, a controllable module allows users to adjust quality and fidelity during the denoising process. Extensive tests demonstrate DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets."
    },
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "This paper introduces FineAction, a large-scale, fine-grained video dataset designed to address the limitations of current temporal action localization (TAL) benchmarks, which rely on coarse action classes and lead to model overfitting and ambiguous annotations. FineAction contains 103K instances across 106 action categories in 17K videos, offering a diverse and densely annotated dataset with co-occurring actions that pose new challenges for TAL. The authors evaluate popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and propose a baseline method achieving a 13.17% mAP. FineAction aims to advance TAL research and is accessible online."
    },
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper introduces the concept of grokking, a sudden onset of perfect generalisation long after overfitting, on small, algorithmically generated datasets. The study also finds that generalization on smaller datasets requires more optimization. The authors suggest that such datasets are ideal for investigating the puzzling phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of Vision Transformers (ViTs) without relying on large-scale pre-training datasets like ImageNet and JFT or requiring modifications to the architecture or loss functions. The authors show that this approach improves ViT performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, while maintaining ViT's attention to relevant regions and robustness, despite ViT's inherent lack of inductive biases and typical dependence on large-scale pre-training."
    },
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "This paper introduces a novel approach called mixed-order self-paced curriculum learning (Mo-SCL) to address the challenges faced by self-paced curriculum learning (SCL) in medical image analysis tasks, such as universal lesion detection. These challenges include inaccurate difficulty estimation and the under-utilization of hard samples. Mo-SCL combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches. Through theoretical analysis and experiments on the DeepLesion dataset, the authors demonstrate that Mo-SCL enhances lesion detection accuracy in state-of-the-art methods without requiring additional network modifications."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "This paper introduces Online Normalization, a novel method for normalizing neural network hidden activations that offers a batch-independent alternative with comparable accuracy to Batch Normalization. Online Normalization addresses Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. The method is applicable to recurrent, fully connected networks, and those with high activation memory requirements. The authors demonstrate its effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    },
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "This paper introduces QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU while maintaining 16-bit task performance. QLORA utilizes 4-bit quantized language models and Low Rank Adapters (LoRA) and incorporates several memory-saving innovations without compromising performance. The authors' best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. The authors applied QLORA to finetune over 1,000 models and analyzed performance across various datasets, model types, and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Their findings suggest that GPT-4 evaluations are a viable substitute for human assessments and question the reliability of current chatbot benchmarks. The authors make their models and 4-bit training code public."
    },
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "This paper introduces the Kinetics Human Action Video dataset. The authors note that the limited number of videos in existing datasets like UCF-101 and HMDB-51 hinders the ability to effectively evaluate architectures due to similar performance across these small benchmarks. Kinetics contains 400 classes with over 400 video clips per class from challenging YouTube videos. The paper analyzes the impact of this larger dataset on the performance of existing architectures as well as the benefits of pre-training models on Kinetics. The authors introduce the Two-Stream Inflated 3D ConvNet (I3D), an extension of 2D ConvNets to 3D for improved video feature extraction. When pre-trained on Kinetics, this I3D model sets new benchmarks for action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    },
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces an iterative self-training framework for sim-to-real 6D object pose estimation to enable cost-effective robotic bin-picking. The authors create a photo-realistic simulator to synthesize virtual data for training an initial pose estimation network (teacher model). This teacher predicts poses on unlabeled real data, and an adaptive selection scheme filters reliable predictions to generate pseudo-labels for training a student model on real data. Iteratively refining the teacher with the student improves pseudo-label quality. Evaluated on public benchmarks and a new dataset, their method shows 11.49% and 22.62% ADD(-S) improvements and a 19.54% increase in bin-picking success, demonstrating the effectiveness of this iterative sim-to-real approach."
    },
    {
      "title": "SinLU: Sinu-Sigmoidal Linear Uni",
      "url": "/research/2021/12/02/SinLU-Sinu-Sigmoidal-Linear-Uni.html",
      "date": "December 2, 2021",
      "summary": "This paper introduces a non-linear activation function called the Sinu-sigmoidal Linear Unit (SinLU), formulated as SinLU(x) = (x + a sin bx) · σ(x). It incorporates a sine wave for added functionality over traditional linear units, with two trainable parameters controlling the sinusoidal participation. The authors compare SinLU's performance to ReLU, GELU, and SiLU across various domains, models, and standard datasets. The results demonstrate SinLU's robustness and superior performance due to the incorporation of the trainable sine wave parameters, facilitating easy training and fast convergence."
    },
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This paper investigates using high-quality rendering software and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle. This synthetic dataset is used to augment limited real training data for image classification and semantic segmentation tasks. While models trained solely on synthetic images showed low accuracy on real validation data, including even small amounts of real data significantly improved performance. Augmenting real training data with synthetic images outperformed using only real images. Furthermore, pretraining models on the synthetic dataset before transfer learning significantly reduced training costs by allowing most of the training to be completed upfront using the synthetic data."
    },
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "This paper introduces tools for training monocular depth estimation models with multiple datasets despite incompatible annotations. Their approach uses a robust training objective, multi-objective learning for data integration, and pretraining encoders on auxiliary tasks. Tested across five datasets, including 3D films as a novel data source, their methods achieve superior zero-shot cross-dataset generalization, outperforming existing benchmarks through principled multi-dataset training."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "The paper introduces a system that trains deep neural networks for object detection using only synthetic images generated with domain randomization. This involves randomizing non-realistic simulator parameters like lighting and object textures, allowing the network to identify essential object features. Their findings show that networks can achieve impressive performance using just synthetic data, and further improve with fine-tuning on real data. This suggests the potential of using cost-effective synthetic data for training, avoiding the need to acquire vast amounts of real-world data or create detailed synthetic environments. They validate their method on car bounding box detection using the KITTI dataset."
    },
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "The authors introduce VideoMAE, a data-efficient self-supervised video pre-training (SSVP) approach that utilizes video masked autoencoders with a novel high-ratio video tube masking technique inspired by ImageMAE. Key findings include the effectiveness of high masking ratios (90-95%) due to video's temporal redundancy, strong performance on small datasets (~3k-4k videos) without extra data highlighting the importance of high-level structure learning, and data quality being more crucial than quantity with domain shift being significant. Notably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone without extra data."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    },
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "The paper focuses on evaluating the performance of recent advancements in image transformers, such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, on the Tiny ImageNet dataset for transfer learning tasks. While these models are typically trained on large datasets like ImageNet-21k and then fine-tuned on ImageNet-1k, their assessments often overlook the Tiny ImageNet benchmark. The study updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. This underscores the importance of evaluating these models on the Tiny ImageNet dataset for transfer learning performance."
    },
    {
      "title": "Deep Residual Learning for Image Recognition",
      "url": "/research/2015/12/10/Deep-Residual-Learning-for-Image-Recognition.html",
      "date": "December 10, 2015",
      "summary": "We introduce a residual learning framework to train deeper neural networks effectively. By reformulating layers to learn residual functions relative to inputs, our approach facilitates optimization and achieves higher accuracy with increased depth. Evaluations on ImageNet demonstrate the effectiveness of residual networks up to 152 layers deep, outperforming previous architectures. Our method achieves 3.57% error on the ImageNet test set, winning 1st place in the ILSVRC 2015 classification task. Additionally, our deep representations lead to a 28% improvement on the COCO object detection dataset. In the ILSVRC & COCO 2015 competitions, our approach secured 1st place in ImageNet detection, localization, COCO detection, and segmentation tasks."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Searching for Activation Functions",
      "url": "/research/2017/10/27/Searching-for-Activation-Functions.html",
      "date": "October 27, 2017",
      "summary": "This study explores the impact of activation functions on deep network training and performance. While Rectified Linear Unit (ReLU) is widely used, alternatives have not consistently outperformed it. We propose using automatic search techniques to discover new activation functions. Through exhaustive and reinforcement learning-based searches, we identify novel functions. Empirical evaluation shows that our best discovered function, Swish (f(x) = x · sigmoid(βx)), performs better than ReLU on deeper models across challenging datasets. Replacing ReLUs with Swish units improves classification accuracy on ImageNet, for example, by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. Swish's simplicity and similarity to ReLU facilitate its adoption in neural networks."
    },
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    },
    {
      "title": "Automated Learning Rate Scheduler for Large-batch",
      "url": "/research/2021/07/13/Automated-Learning-Rate-Scheduler-for-Large-batch.html",
      "date": "July 13, 2021",
      "summary": "Large-batch training is crucial for deep learning with large datasets and models, but requires specific learning rate (LR) schedules for optimal performance, especially under limited training epochs. This work introduces an automated LR scheduling algorithm for large-batch neural network training within a fixed epoch budget, consisting of adaptive warmup and predefined decay phases. The LR is dynamically adjusted based on training loss monitored through Gaussian process smoothing, facilitating low computational overhead. When integrated with adaptive stochastic optimizers like AdamP and LAMB, this scheduler eliminates the need for extensive hyperparameter tuning and achieves competitive or superior results on various image classification tasks across different batch sizes and architectures."
    },
    {
      "title": "Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks",
      "url": "/research/2020/06/11/Batch-Normalization-Provably-Avoids-Rank-Collapse-for-Randomly-Initialised-Deep-Networks.html",
      "date": "June 11, 2020",
      "summary": "We investigate the challenge of training randomly initialized deep neural networks due to spectral instabilities in products of random matrices. Batch normalization emerges as an effective solution to prevent rank collapse in both linear and ReLU networks. Leveraging tools from Markov chain theory, we establish a lower rank bound for deep linear networks. Empirical findings show that this rank robustness extends to ReLU networks. Our experiments on real-world datasets underscore the significance of rank stability in training modern deep neural architectures."
    },
    {
      "title": "Learning Rate Perturbation: A Generic Plugin of Learning Rate schedule towards Flatter Local Minima",
      "url": "/research/2022/08/25/Learning-Rate-Perturbation-A-Generic-Plugin-of-Learning-Rate-schedule-towards-Flatter-Local-Minima.html",
      "date": "August 25, 2022",
      "summary": "Learning rate is crucial in neural network training, yet existing schedules lack theoretical backing, often leading to suboptimal choices made through trial and error. To address this, we propose LEAP, a plugin enhancing various learning rate schedules by introducing perturbations. This simple yet effective strategy favors flat minima, ensuring better generalization. Extensive experiments demonstrate LEAP's ability to improve performance across diverse datasets and learning rate schedules, including constant ones."
    },
    {
      "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
      "url": "/research/2021/07/24/The-Many-Faces-of-Robustness-A-Critical-Analysis-of-Out-of-Distribution-Generalization.html",
      "date": "July 24, 2021",
      "summary": "We present four novel real-world distribution shift datasets encompassing changes in image style, blurriness, location, camera settings, and more. Evaluating existing methods for enhancing out-of-distribution robustness, we discover that employing larger models and artificial data augmentations can enhance robustness against real-world distribution shifts, contradicting prior claims. Our findings demonstrate that improvements in artificial robustness benchmarks can indeed transfer to real-world distribution shifts, contrary to prior assumptions. Additionally, we introduce a novel data augmentation technique that surpasses models pretrained with significantly more labeled data, emphasizing its efficacy in addressing real-world distribution shifts. While some methods consistently mitigate texture and local image statistics shifts, they fail to address other shifts like geographic changes. Our results underscore the necessity for future research to examine multiple distribution shifts concurrently, as no single method consistently improves robustness across all evaluated scenarios."
    },
    {
      "title": "Deep learning via Hessian-free optimization",
      "url": "/research/2010/06/21/Deep-learning-via-Hessian-free-optimization.html",
      "date": "June 21, 2010",
      "summary": "We present a novel 2nd-order optimization method inspired by the Hessian-free approach, applied to deep auto-encoders. Achieving superior results without pre-training compared to Hinton & Salakhutdinov (2006), our method is practical, user-friendly, scalable to large datasets, and versatile across various model classes. Additionally, we address the challenge of pathological curvature in deep learning, highlighting how our method effectively mitigates this issue."
    },
    {
      "title": "ImageNet-D: A new challenging robustness dataset inspired by domain adaptation",
      "url": "/research/2023/05/05/ImageNet-D-A-new-challenging-robustness-dataset-inspired-by-domain-adaptation.html",
      "date": "May 5, 2023",
      "summary": "We introduce ImageNet-D, a novel dataset designed to evaluate the robustness of ImageNet-trained models across various domains. With six distinct domains including Real, Painting, Clipart, Sketch, Infograph, and Quick-draw, ImageNet-D challenges even state-of-the-art models, revealing interpretable errors. For instance, our leading EfficientNet-L2 model exhibits a significant performance decrease, dropping from 11.6% on clean ImageNet to 29.2% on the Real domain."
    },
    {
      "title": "Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks",
      "url": "/research/2017/10/23/Cosine-Normalization-Using-Cosine-Similarity-Instead-of-Dot-Product-in-Neural-Networks.html",
      "date": "October 23, 2017",
      "summary": "In traditional multi-layer neural networks, the dot product between the output and weight vectors of preceding layers serves as input to the activation function, resulting in unbounded outputs and increased variance. This variance can lead to poor generalization and hinder training by exacerbating internal covariate shift. To address this, we propose cosine normalization, which replaces the dot product with cosine similarity or centered cosine similarity (Pearson Correlation Coefficient). We evaluate cosine normalization against batch, weight, and layer normalization in fully-connected and convolutional neural networks across various datasets. Our experiments demonstrate that cosine normalization outperforms other normalization techniques."
    },
    {
      "title": "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization",
      "url": "/research/2023/07/04/Gradient-Norm-Aware-Minimization-Seeks-First-Order-Flatness-and-Improves-Generalization.html",
      "date": "July 4, 2023",
      "summary": "Recent advancements have highlighted the effectiveness of flat minima in enhancing generalization, particularly through Sharpness-Aware Minimization (SAM). However, existing definitions of flatness, such as zeroth-order flatness, have limitations in discerning between minima with low and high generalization errors. To address this, we propose first-order flatness, which considers maximal gradient norm within a perturbation radius. We introduce Gradient norm Aware Minimization (GAM) as a novel training approach to achieve uniformly small curvature across all directions. Experimental results demonstrate GAM's ability to enhance generalization compared to standard optimizers like SGD and AdamW across various datasets and networks. Moreover, GAM facilitates SAM in identifying flatter minima, leading to improved generalization."
    },
    {
      "title": "Penalizing Gradient Norm for Efficiently Improving Generalization in Deep Learning",
      "url": "/research/2022/06/26/Penalizing-Gradient-Norm-for-Efficiently-Improving-Generalization-in-Deep-Learning.html",
      "date": "June 26, 2022",
      "summary": "This paper proposes a method to enhance the generalization of deep neural networks (DNNs) by penalizing the gradient norm of the loss function during optimization. By constraining the gradient norm, the optimizers tend to find flat minima, improving generalization. We efficiently implement this method using first-order approximation within the gradient descent framework. Experimental results demonstrate improved generalization across various models and datasets. Additionally, we show that a recent method, sharpness-aware minimization, is a special case of our approach, with our method achieving new state-of-the-art performance on tested tasks."
    },
    {
      "title": "Do Vision Transformers See Like Convolutional Neural Networks?",
      "url": "/research/2022/03/03/Do-Vision-Transformers-See-Like-Convolutional-Neural-Networks.html",
      "date": "March 3, 2022",
      "summary": "Convolutional Neural Networks (CNNs) have been prominent in processing visual data, but recent studies show Vision Transformers (ViTs) can match or outperform CNNs in image classification. This work examines how ViTs solve classification tasks, discovering significant differences from CNNs, including uniformity in ViT representations across layers due to self-attention and residual connections that enhance feature propagation. ViTs also maintain spatial information effectively, influenced by classification methods. The study further explores how dataset scale affects ViT features and their transferability, linking these findings to new architectures like MLP-Mixer."
    },
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    },
    {
      "title": "ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Network",
      "url": "/research/2021/06/29/ASAM-Adaptive-Sharpness-Aware-Minimization-for-Scale-Invariant-Learning-of-Deep-Neural-Network.html",
      "date": "June 29, 2021",
      "summary": "Recent advances in learning algorithms have highlighted the importance of the sharpness of the loss surface as a key indicator of the generalization gap, achieving state-of-the-art results. However, traditional sharpness measures, defined within a fixed region, suffer from sensitivity to parameter scaling, weakening their predictive value for the generalization gap. This paper introduces a scale-invariant measure, adaptive sharpness, along with a new generalization bound. We present a new learning method, adaptive sharpness-aware minimization (ASAM), which leverages this bound. Our experiments across various benchmark datasets demonstrate that ASAM significantly enhances model generalization."
    },
    {
      "title": "Chronos: Learning the Language of Time Series",
      "url": "/research/2024/03/12/Chronos-Learning-the-Language-of-Time-Series.html",
      "date": "March 12, 2024",
      "summary": "Chronos is a framework that enhances pretrained probabilistic time series models by tokenizing time series data for training with transformer-based architectures, notably the T5 family. Pretrained on a mix of public and synthetic datasets created via Gaussian processes, Chronos models outshine others in 42 benchmark datasets by showing superior performance on familiar datasets and competitive or better zero-shot capabilities on new datasets. This illustrates Chronos's ability to generalize across different domains, offering a simpler approach to forecasting tasks."
    },
    {
      "title": "Fine-tuning with Very Large Dropout",
      "url": "/research/2024/03/01/Fine-tuning-with-Very-Large-Dropout.html",
      "date": "March 1, 2024",
      "summary": "This study challenges the notion that machine learning practices assume training and testing data have the same distribution. It explores the effectiveness of high dropout rates, as opposed to ensemble techniques, in developing rich data representations suitable for multiple distribution scenarios. These representations surpass those achieved by traditional in-distribution performance regularization and the implicit sparsity induced by common stochastic gradient methods. While training deep networks from scratch with high dropout rates is impractical, fine-tuning pre-trained models under these conditions is feasible and yields better out-of-distribution performance than ensembles and model averaging techniques like model soups. This finding is significant due to the growing relevance of fine-tuning with large pre-trained models, offering insights into the nature of rich representations and the linear characteristics of fine-tuning large networks with small datasets."
    },
    {
      "title": "From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning",
      "url": "/research/2023/03/16/From-MNIST-to-ImageNet-and-Back:-Benchmarking-Continual-Curriculum-Learning.html",
      "date": "March 16, 2023",
      "summary": "Continual learning (CL) is a promising trend in machine learning, aiming to develop robust models and strategies for dynamic environments by incorporating new knowledge while retaining past knowledge. CL research is fragmented, with various protocols, tasks, datasets, and metrics, often not reflecting real-world complexity and tailored to specific strategies. This work addresses this gap by introducing two novel CL benchmarks with heterogeneous tasks from six image datasets, varying in complexity and quality. These benchmarks fairly evaluate state-of-the-art CL strategies in scenarios closer to real-world conditions. The benchmarks present tasks in both increasing and decreasing complexity, assessing models' ability to exploit task structure. The work emphasizes a rigorous, reproducible evaluation protocol for measuring generalization and memory retention in models. Experimental results show that popular CL strategies perform poorly on these benchmarks, exhibit high forgetting, and struggle with curriculum task ordering. These findings underscore the need for rigorous comparisons and the development of new CL strategies for complex scenarios."
    },
    {
      "title": "Neural Network Diffusion",
      "url": "/research/2024/02/20/Neural-Network-Diffusion.html",
      "date": "February 20, 2024",
      "summary": "Diffusion models, known for their success in image and video generation, can also generate high-performing neural network parameters, as demonstrated in this study. By employing a simple combination of an autoencoder and a standard latent diffusion model, our method involves extracting latent representations of network parameters, which are then synthesized from random noise by the diffusion model. These new representations, processed through the autoencoder's decoder, serve as fresh network parameters. Tested across various architectures and datasets, this diffusion approach consistently produces models with comparable or superior performance to traditionally trained networks at minimal extra cost. Importantly, the generated models show distinct performance differences from the trained ones, suggesting further exploration into the versatile applications of diffusion models."
    },
    {
      "title": "SGDR: Stochastic Gradient Descent With Warm Restarts",
      "url": "/research/2017/05/02/SGDR-Stochastic-Gradient-Descent-With-Warm-Restarts.html",
      "date": "May 2, 2017",
      "summary": "This paper introduces a warm restart technique for stochastic gradient descent aimed at enhancing anytime performance in deep neural network training. It showcases empirical performance improvements on CIFAR-10 and CIFAR-100 datasets, achieving state-of-the-art results with 3.14% and 16.21% error rates, respectively. Additionally, the technique's benefits are demonstrated on an EEG dataset and a downsampled ImageNet dataset."
    },
    {
      "title": "Suppressing Pink Elephants with Direct Principle Feedback",
      "url": "/research/2024/02/13/Suppressing-Pink-Elephants-with-Direct-Principle-Feedback.html",
      "date": "February 13, 2024",
      "summary": "Existing methods for controlling language models focus on training desired behaviors, but often lack the flexibility needed for diverse applications. The authors address this with the Pink Elephant Problem, demonstrating the need for language models to adapt to different contexts by avoiding certain topics (the Pink Elephant) in favor of others (Grey Elephant). The authors introduce Direct Principle Feedback (DPF), an adaptation of Constitutional AI that improves control by directly applying critiques and revisions without ranking responses. Our study shows that a 13B LLaMA 2 model fine-tuned with DPF on a synthetic dataset outperforms existing models and matches GPT-4 in managing the Pink Elephant Problem."
    },
    {
      "title": "Towards a Unified Theoretical Understanding of Non-contrastive Learning via Rank Differential Mechanism",
      "url": "/research/2023/03/04/Towards-a-Unified-Theoretical-Understanding-of-Non-contrastive-Learning-via-Rank-Differential-Mechanism.html",
      "date": "March 4, 2023",
      "summary": "Recent non-contrastive learning methods like BYOL, SimSiam, SwAV, and DINO have shown that asymmetric architectural designs can achieve good self-supervised visual learning performance by aligning positive pairs alone, despite a lack of unified theoretical understanding on how these designs prevent feature collapse. This study introduces the Rank Differential Mechanism (RDM) as a unified theoretical framework for non-contrastive learning. RDM demonstrates that these asymmetric designs maintain a consistent rank difference in output features, enhancing effective dimensionality and preventing feature collapse. Unlike previous theories, RDM applies to designs both with and without a predictor, offering a comprehensive understanding of non-contrastive learning methods and guiding the development of new variants. Experiments confirm that these variants perform comparably, if not better than, existing methods on benchmark datasets."
    },
    {
      "title": "VideoMamba: State Space Model for Efficient Video Understanding",
      "url": "/research/2024/03/12/VideoMamba-State-Space-Model-for-Efficient-Video-Understanding.html",
      "date": "March 12, 2024",
      "summary": "This work introduces VideoMamba, an innovative approach that addresses local redundancy and global dependencies in video understanding by adapting the Mamba framework to video analysis. VideoMamba outperforms existing 3D convolutional neural networks and video transformers through its linear-complexity operator, which facilitates efficient long-term modeling for high-resolution, long-duration videos. Its effectiveness is demonstrated across four main areas: scalability in the visual domain without needing extensive dataset pretraining, thanks to a novel self-distillation technique; the ability to recognize short-term actions with fine-grained motion differences; superior performance in long-term video understanding compared to traditional feature-based models; and robust compatibility with multiple modalities, enhancing multi-modal video analysis. VideoMamba establishes a new standard for comprehensive and efficient video understanding."
    },
    {
      "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum",
      "url": "/research/2023/09/30/When-to-Learn-What:-Model-Adaptive-Data-Augmentation-Curriculum.html",
      "date": "September 30, 2023",
      "summary": "Data augmentation (DA) improves neural network generalization by enforcing invariances and symmetries to predefined transformations applied to input data. However, fixed augmentation policies affect samples differently at various training stages, and existing approaches cannot adapt policies to individual samples and the training model. This paper proposes Model-Adaptive Data Augmentation (MADAug) which trains an augmentation policy network to determine when to learn what. Unlike previous work, MADAug selects augmentation operators for each input image with a model-adaptive policy that varies between training stages, creating an optimized data augmentation curriculum. The policy is trained using a bi-level optimization scheme to minimize validation-set loss of a model trained with policy-produced augmentations. Extensive evaluations on multiple image classification tasks and network architectures show that MADAug outperforms or matches existing DA approaches, enhances fairness by improving all classes, particularly the difficult ones, and performs better when transferred to fine-grained datasets. Additionally, the auto-optimized policy in MADAug gradually increases perturbations, forming an easy-to-hard curriculum."
    }
  ],
  "load": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load."
    }
  ],
  "simclr": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a method for contrastive learning of visual representations. Authors find that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch size with more training steps improve representation quality. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    }
  ],
  "composition": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a method for contrastive learning of visual representations. Authors find that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch size with more training steps improve representation quality. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    }
  ],
  "nonlinear": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a method for contrastive learning of visual representations. Authors find that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch size with more training steps improve representation quality. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "This paper introduces Mask-Reconstruct Augmentation (MRA), an image augmentation technique that leverages self-supervised masked autoencoders to generate distorted inputs and address the issue of overfitting in deep neural networks. Inspired by the success of masked image modeling in self-supervised learning, MRA uses nonlinear transformations for regularization, as opposed to current hand-crafted linear techniques like scale, flip, and color jitter. Extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks."
    },
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "This paper explores Gated Linear Units (GLUs), which involve the component-wise product of two linear projections, with one undergoing a sigmoid function. The authors investigate GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers. They find that certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "url": "/research/2014/02/19/Exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks.html",
      "date": "February 19, 2014",
      "summary": "This study aims to enhance our comprehension of deep learning dynamics by analyzing deep linear neural networks. Despite their linear input-output mapping, these networks display nonlinear gradient descent dynamics, resulting in phenomena like prolonged plateaus and swift transitions to improved solutions. Through analytical exploration, we reveal that as network depth increases indefinitely, learning speed can remain finite under specific initial conditions. Unsupervised pretraining, under certain data conditions, can discover these favorable initial conditions, unlike random Gaussian initializations. We introduce a novel class of random orthogonal initial conditions that, akin to unsupervised pretraining, enable depth-independent learning. These conditions also ensure gradient propagation in deep nonlinear networks, particularly when operating at the edge of chaos."
    }
  ],
  "loss": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a method for contrastive learning of visual representations. Authors find that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch size with more training steps improve representation quality. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "An Investigation of how Label Smoothing Affects Generalization",
      "url": "/research/2020/10/23/AN-INVESTIGATION-OF-HOW-LABEL-SMOOTHING-AFFECTS-GENERALIZATION.html",
      "date": "October 23, 2020",
      "summary": "Label smoothing has been empirically shown to reduce overfitting and improve generalization. However, its mathematical underpinnings remain unclear. This paper explains label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. The authors identify a method for calculating a label smoothing value that minimizes generalization loss."
    },
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/research/2020/09/10/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning.html",
      "date": "September 10, 2020",
      "summary": "This paper introduces BYOL, a self-supervised learning method that does not need negative pairs. BYOL uses a teacher student pair of networks, where the teacher is the EMA of the student. The teacher and student are given two differently augmented versionos the input, and the loss is the cross entropy difference between the outputs."
    },
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a self-supervised method that adds a contrastive loss to the Masked Autoencoder. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning."
    },
    {
      "title": "ADAMW: Decoupled Weight Decay Regularization",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "This paper shows that L2 regularization and weight decay are equivalent for standard SGD but not for adaptive methods like ADAM. To account of this, these authors separate weight decay from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate."
    },
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "This paper introdices an automated pipeline for building high-quality, diverse image datasets. Also proposes minor mofications to the archetecture and loss of a DINO ViT. A large ViT model with 1 billion parameters was trained and distilled into smaller models."
    },
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "The Batch Normalization paper argued the method's effectiveness came from reducing internal covariate shift. In this paper the authors argue that actually the main benifit of Batch Norm is loss landscape smoothing."
    },
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Most deep learning systems are designed to be trained once, or possibly pretrained and fintuned. These systems perform quite poorly in continual learning setups where training is ongoing. This is primarily due to two problems: catastrophic forgetting and loss of plasticity. This paper addresses the second. Various architectures and techniques were tested, with L2-regularization and shrink and perturm improving plasticity a little. This paper then introduces Continual Backpropagation, which reinitializes dead units, and seems to maintain plasticity indefinitely "
    },
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. Taking this to the extreme, this study trains a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss even when trained on pure noise."
    },
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrives incrementally, it's common to build a sequence of models that incorporate progressively more data. It has been shown that simply continuing training of a model on new data often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "This paper introduces a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. This decouples batch size from the loss, and allows the authors to increase the batch size and also improves performance even with smaller batches. Pushing the batch size to one million showed diminishing returns, with 32k being a good middle ground."
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores self-supervision through automatic colorization as an alternative to ImageNet pretraining. Self-supervised training achieved state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. This paper highlights the significance of loss formulation, training specifics, and network architecture when pretraining through colorization. It also revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and feature adaptability upon fine-tuning. The findings suggest that colorization offers a better supervisory signal comparable to various types of ImageNet pretraining."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "This paper proposes Hard Patches Mining (HPM), a novel framework for masked image modeling (MIM) pre-training that goes beyond simply solving given problems. HPM aims for the model to generate more challenging tasks for itself, using reconstruction loss as a metric for task difficulty. It incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "This paper introduces a Fourier neural network (FNN) that aligns with Fourier decomposition by utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design allows for easy integration with more complex networks for data processing tasks. The authors demonstrate the FNN's efficacy on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits of the FNN include solution validity beyond the training scope, model interpretability, and ease of use."
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of Vision Transformers (ViTs) without relying on large-scale pre-training datasets like ImageNet and JFT or requiring modifications to the architecture or loss functions. The authors show that this approach improves ViT performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, while maintaining ViT's attention to relevant regions and robustness, despite ViT's inherent lack of inductive biases and typical dependence on large-scale pre-training."
    },
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "This paper introduces a novel approach called mixed-order self-paced curriculum learning (Mo-SCL) to address the challenges faced by self-paced curriculum learning (SCL) in medical image analysis tasks, such as universal lesion detection. These challenges include inaccurate difficulty estimation and the under-utilization of hard samples. Mo-SCL combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches. Through theoretical analysis and experiments on the DeepLesion dataset, the authors demonstrate that Mo-SCL enhances lesion detection accuracy in state-of-the-art methods without requiring additional network modifications."
    },
    {
      "title": "Stable Diffusion 3: Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "This paper focuses on improving diffusion models for high-dimensional data like images. The authors enhance noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Their large-scale study shows superior performance over established diffusion methods in high-resolution text-to-image synthesis. A new transformer-based architecture is introduced for better text comprehension, typography, and human preference in text-to-image generation. The findings demonstrate predictable scaling trends, with lower validation loss tied to improved synthesis. The largest models outperform state-of-the-art approaches."
    },
    {
      "title": "Taming the Waves: Sine as Activation Function in Deep Neural Networks",
      "url": "/research/2016/11/04/Taming-the-Waves-Sine-as-Activation-Function-in-Deep-Neural-Networks.html",
      "date": "November 4, 2016",
      "summary": "This paper explores the challenges in training deep neural networks that use sinusoidal activation functions. The authors explain that the difficulty arises from the emergence of numerous shallow local minima in the loss landscape. The study reveals that successful learning in typical classification tasks occurs when the network effectively ignores the periodic cycles of the sinusoidal functions. However, for certain non-trivial tasks, networks with sinusoidal activations can outperform those using traditional monotonic activation functions."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article provides an extensive review of vision transformer techniques applied specifically to human action recognition. These action transformers are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also examines optimizing spatio-temporal attention in transformers for longer sequences and different learning strategies like self-supervised and zero-shot learning with their respective loss functions. Additionally, it assesses progress in benchmark evaluation scores and discusses challenges and future directions in this research area."
    },
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    },
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "Automated Learning Rate Scheduler for Large-batch",
      "url": "/research/2021/07/13/Automated-Learning-Rate-Scheduler-for-Large-batch.html",
      "date": "July 13, 2021",
      "summary": "Large-batch training is crucial for deep learning with large datasets and models, but requires specific learning rate (LR) schedules for optimal performance, especially under limited training epochs. This work introduces an automated LR scheduling algorithm for large-batch neural network training within a fixed epoch budget, consisting of adaptive warmup and predefined decay phases. The LR is dynamically adjusted based on training loss monitored through Gaussian process smoothing, facilitating low computational overhead. When integrated with adaptive stochastic optimizers like AdamP and LAMB, this scheduler eliminates the need for extensive hyperparameter tuning and achieves competitive or superior results on various image classification tasks across different batch sizes and architectures."
    },
    {
      "title": "On the Maximum Hessian Eigenvalue and Generalization",
      "url": "/research/2023/05/23/On-the-Maximum-Hessian-Eigenvalue-and-Generalization.html",
      "date": "May 23, 2023",
      "summary": "This study investigates the relationship between training interventions and the generalization of deep networks. While previous research suggests that flatter solutions generalize better than sharper ones, particularly measured by λmax, the largest eigenvalue of the Hessian of the loss, this paper challenges this notion. Through experiments, we demonstrate that larger learning rates reduce λmax for all batch sizes but do not consistently improve generalization. Additionally, scaling batch size and learning rate simultaneously can change λmax without affecting generalization. Sharpness-Aware Minimization (SAM) produces smaller λmax but does not consistently enhance generalization, especially with larger batch sizes. Excessive dropout probabilities can degrade generalization, despite promoting smaller λmax. Batch normalization, while not consistently reducing λmax, still improves generalization. These findings question λmax's role in explaining generalization in neural networks, highlighting the limits of its explanatory power."
    },
    {
      "title": "Penalizing Gradient Norm for Efficiently Improving Generalization in Deep Learning",
      "url": "/research/2022/06/26/Penalizing-Gradient-Norm-for-Efficiently-Improving-Generalization-in-Deep-Learning.html",
      "date": "June 26, 2022",
      "summary": "This paper proposes a method to enhance the generalization of deep neural networks (DNNs) by penalizing the gradient norm of the loss function during optimization. By constraining the gradient norm, the optimizers tend to find flat minima, improving generalization. We efficiently implement this method using first-order approximation within the gradient descent framework. Experimental results demonstrate improved generalization across various models and datasets. Additionally, we show that a recent method, sharpness-aware minimization, is a special case of our approach, with our method achieving new state-of-the-art performance on tested tasks."
    },
    {
      "title": "SAM as an Optimal Relaxation of Bayes",
      "url": "/research/2023/12/10/SAM-as-an-Optimal-Relaxation-of-Bayes.html",
      "date": "December 10, 2023",
      "summary": "We introduce SAM, a method enhancing generalization in deep learning, as a relaxation of the Bayes objective. SAM replaces the expected negative-loss with an optimal convex lower bound derived using the Fenchel biconjugate. This connection enables an Adam-like extension of SAM, offering automatic uncertainty estimates and potential accuracy improvements. Bridging adversarial and Bayesian methods, our work paves the way for robustness enhancement."
    },
    {
      "title": "Sharpness-Aware Training for Free",
      "url": "/research/2023/03/02/Sharpness-Aware-Training-for-Free.html",
      "date": "March 2, 2023",
      "summary": "Modern deep neural networks (DNNs) excel in performance but often suffer from over-parameterization, leading to increased generalization error without tailored training strategies. Sharpness-Aware Minimization (SAM) has proven effective in reducing generalization error by minimizing sharpness in the loss landscape. However, SAM incurs a significant computational overhead. This paper introduces Sharpness-Aware Training for Free (SAF), which mitigates sharpness at nearly zero additional computational cost. SAF achieves this by preventing sudden drops in loss within sharp local minima during weight updates. A novel trajectory loss, based on KL-divergence between current and past DNN outputs, replaces SAM's sharpness measure, guiding convergence towards flat minima for enhanced generalization. Empirical results demonstrate SAF's effectiveness on ImageNet with comparable computational efficiency to the base optimizer."
    },
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    },
    {
      "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
      "url": "/research/2022/03/19/Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training.html",
      "date": "March 19, 2022",
      "summary": "Sharpness-Aware Minimization (SAM) enhances generalization by optimizing a neighborhood-based perturbed loss, but it doesn't always favor flat minima due to both sharp and flat minima having low perturbed losses. We introduce a new measure, the surrogate gap, reflecting the dominant Hessian eigenvalue at small neighborhood radii, which is simple to compute and can be minimized during training. We propose the Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), an advancement over SAM with minimal additional computational cost. GSAM employs a dual-step approach: first, minimizing the perturbed loss similar to SAM, and second, reducing the surrogate gap without affecting the perturbed loss to target regions with low loss and sharpness, thus achieving superior generalization. GSAM is theoretically robust, showing better convergence and empirical generalization improvements, notably a +3.2% gain over SAM and +5.4% over AdamW in ImageNet accuracy for ViT-B/32."
    },
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    },
    {
      "title": "A Phase Transition between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention",
      "url": "/research/2024/02/06/A-Phase-Transition-between-Positional-and-Semantic-Learning-in-a-Solvable-Model-of-Dot-Product-Attention.html",
      "date": "February 6, 2024",
      "summary": "We explore the learning process of a dot-product attention layer, which learns both positional and semantic attention matrices, enabling tokens to attend based on position or meaning. Through experiments on an algorithmic task, we demonstrate that this architecture can use either mechanism for solving the task. Theoretically, we examine a non-linear self-attention layer with special query and key matrices, offering a closed-form solution for its non-convex loss landscape in high-dimensional data, which reveals a phase transition from positional to semantic mechanisms as sample complexity increases. We also show that the dot-product attention layer surpasses a linear positional baseline through the semantic mechanism with adequate data."
    },
    {
      "title": "ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Network",
      "url": "/research/2021/06/29/ASAM-Adaptive-Sharpness-Aware-Minimization-for-Scale-Invariant-Learning-of-Deep-Neural-Network.html",
      "date": "June 29, 2021",
      "summary": "Recent advances in learning algorithms have highlighted the importance of the sharpness of the loss surface as a key indicator of the generalization gap, achieving state-of-the-art results. However, traditional sharpness measures, defined within a fixed region, suffer from sensitivity to parameter scaling, weakening their predictive value for the generalization gap. This paper introduces a scale-invariant measure, adaptive sharpness, along with a new generalization bound. We present a new learning method, adaptive sharpness-aware minimization (ASAM), which leverages this bound. Our experiments across various benchmark datasets demonstrate that ASAM significantly enhances model generalization."
    },
    {
      "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
      "url": "/research/2024/02/28/CL-MAE:-Curriculum-Learned-Masked-Autoencoders.html",
      "date": "February 28, 2024",
      "summary": "This paper proposes a curriculum learning approach that updates the masking strategy of Masked Auto Encoders (MAE) to progressively increase the complexity of the self-supervised reconstruction task. To achieve this, a novel learnable masking module is introduced, capable of generating masks of varying complexities, and integrated into masked autoencoders (MAE). This module is jointly trained with the MAE, adjusting its behavior from partner (optimizing the same reconstruction loss) to adversary (optimizing the opposite loss), with a smooth transition regulated by a factor multiplied with the reconstruction loss. This training procedure creates an easy-to-hard curriculum. The Curriculum-Learned Masked Autoencoder (CL-MAE) is trained on ImageNet and demonstrates superior representation learning compared to MAE. Empirical results on five downstream tasks confirm that curriculum learning can successfully self-supervise masked autoencoders."
    },
    {
      "title": "The loss landscape of deep linear neural networks: a second-order analysis",
      "url": "/research/2022/03/11/The-loss-landscape-of-deep-linear-neural-networks-a-second-order-analysis.html",
      "date": "March 11, 2022",
      "summary": "This study investigates the optimization landscape of deep linear neural networks with square loss, focusing on the role of non-strict saddle points in algorithm dynamics, which has been previously understudied. the authors conduct a comprehensive second-order analysis, identifying global minimizers, strict, and non-strict saddle points among all critical points, alongside their critical values. Our findings, based on conditions related to the ranks of partial matrix products, contribute insights into global convergence and implicit regularization observed in optimizing linear neural networks. Additionally, the authors offer an explicit parameterization of global minimizers and identify extensive sets of strict and non-strict saddle points."
    },
    {
      "title": "Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes",
      "url": "/research/2017/11/28/Towards-Understanding-Generalization-of-Deep-Learning-Perspective-of-Loss-Landscapes.html",
      "date": "November 28, 2017",
      "summary": "This study examines why deep learning models, despite having more parameters than training samples, generalize well. It highlights the importance of the loss function's landscape, showing that areas leading to better generalization (good minima) are more prevalent than those leading to poor outcomes. This predominance facilitates the convergence of optimization methods to these beneficial minima. The research provides theoretical backing by analyzing 2-layer neural networks, noting that solutions with better generalization have a smaller Hessian matrix norm. For deeper networks, extensive numerical data supports these conclusions."
    },
    {
      "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum",
      "url": "/research/2023/09/30/When-to-Learn-What:-Model-Adaptive-Data-Augmentation-Curriculum.html",
      "date": "September 30, 2023",
      "summary": "Data augmentation (DA) improves neural network generalization by enforcing invariances and symmetries to predefined transformations applied to input data. However, fixed augmentation policies affect samples differently at various training stages, and existing approaches cannot adapt policies to individual samples and the training model. This paper proposes Model-Adaptive Data Augmentation (MADAug) which trains an augmentation policy network to determine when to learn what. Unlike previous work, MADAug selects augmentation operators for each input image with a model-adaptive policy that varies between training stages, creating an optimized data augmentation curriculum. The policy is trained using a bi-level optimization scheme to minimize validation-set loss of a model trained with policy-produced augmentations. Extensive evaluations on multiple image classification tasks and network architectures show that MADAug outperforms or matches existing DA approaches, enhances fairness by improving all classes, particularly the difficult ones, and performs better when transferred to fine-grained datasets. Additionally, the auto-optimized policy in MADAug gradually increases perturbations, forming an easy-to-hard curriculum."
    }
  ],
  "batch": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a method for contrastive learning of visual representations. Authors find that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch size with more training steps improve representation quality. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "The Batch Normalization paper argued the method's effectiveness came from reducing internal covariate shift. In this paper the authors argue that actually the main benifit of Batch Norm is loss landscape smoothing."
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "This paper introduces Batch Normalization, an online normalization technique which calculates mean and std across the batch dimension. Batch Norm improves training efficiency, allowing for higher learning rates, and decreasing hyper-parameter sensitivity, and sometimes removing the need for Dropout regularization. This paper suggests Batch Norm improves internal covariant shift, however later works have questioned this."
    },
    {
      "title": "Dropout Reduces Underfitting",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. Authors then introduce two dropout schedules: early dropout which prevents underfitting, and late dropout which prevents overfitting."
    },
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes."
    },
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/research/2017/11/06/Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization.html",
      "date": "November 6, 2017",
      "summary": "This paper itroduces Instance Normalization, an online normalization technique that calculates the mean and std across individual data instances. This is shown to work well in situations where there is very high vatiation within batches, such as in style transfer."
    },
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activitivations can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases."
    },
    {
      "title": "Rethinking Batch in BatchNorm",
      "url": "/research/2021/05/17/Rethinking-Batch-in-BatchNorm.html",
      "date": "May 17, 2021",
      "summary": "BatchNorm, essential in convolutional neural networks, behaves uniquely due to its batch-based operation, leading to performance issues in visual recognition tasks. This paper identifies these issues and suggests reevaluating the concept of batch in BatchNorm for better performance, aiming to guide researchers in its effective use."
    },
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "This paper introduces a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. This decouples batch size from the loss, and allows the authors to increase the batch size and also improves performance even with smaller batches. Pushing the batch size to one million showed diminishing returns, with 32k being a good middle ground."
    },
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "This paper introduces a novel approach called mixed-order self-paced curriculum learning (Mo-SCL) to address the challenges faced by self-paced curriculum learning (SCL) in medical image analysis tasks, such as universal lesion detection. These challenges include inaccurate difficulty estimation and the under-utilization of hard samples. Mo-SCL combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches. Through theoretical analysis and experiments on the DeepLesion dataset, the authors demonstrate that Mo-SCL enhances lesion detection accuracy in state-of-the-art methods without requiring additional network modifications."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "This paper introduces Online Normalization, a novel method for normalizing neural network hidden activations that offers a batch-independent alternative with comparable accuracy to Batch Normalization. Online Normalization addresses Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. The method is applicable to recurrent, fully connected networks, and those with high activation memory requirements. The authors demonstrate its effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    },
    {
      "title": "Automated Learning Rate Scheduler for Large-batch",
      "url": "/research/2021/07/13/Automated-Learning-Rate-Scheduler-for-Large-batch.html",
      "date": "July 13, 2021",
      "summary": "Large-batch training is crucial for deep learning with large datasets and models, but requires specific learning rate (LR) schedules for optimal performance, especially under limited training epochs. This work introduces an automated LR scheduling algorithm for large-batch neural network training within a fixed epoch budget, consisting of adaptive warmup and predefined decay phases. The LR is dynamically adjusted based on training loss monitored through Gaussian process smoothing, facilitating low computational overhead. When integrated with adaptive stochastic optimizers like AdamP and LAMB, this scheduler eliminates the need for extensive hyperparameter tuning and achieves competitive or superior results on various image classification tasks across different batch sizes and architectures."
    },
    {
      "title": "Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks",
      "url": "/research/2020/06/11/Batch-Normalization-Provably-Avoids-Rank-Collapse-for-Randomly-Initialised-Deep-Networks.html",
      "date": "June 11, 2020",
      "summary": "We investigate the challenge of training randomly initialized deep neural networks due to spectral instabilities in products of random matrices. Batch normalization emerges as an effective solution to prevent rank collapse in both linear and ReLU networks. Leveraging tools from Markov chain theory, we establish a lower rank bound for deep linear networks. Empirical findings show that this rank robustness extends to ReLU networks. Our experiments on real-world datasets underscore the significance of rank stability in training modern deep neural architectures."
    },
    {
      "title": "Don't Decay the Learning Rate, Increase the Batch Size",
      "url": "/research/2017/11/01/Don't-Decay-the-Learning-Rate,-Increase-the-Batch-Size.html",
      "date": "November 1, 2017",
      "summary": "This study demonstrates that increasing the batch size during training achieves similar learning outcomes as the common practice of decaying the learning rate, applicable to stochastic gradient descent and its variants, including with momentum and Adam optimization. This approach not only matches test accuracies within the same number of epochs but also enhances parallelism and reduces training time due to fewer parameter updates. Efficiency can be further improved by adjusting the learning rate and batch size proportionally, and although increasing the momentum coefficient and scaling the batch size accordingly may slightly lower test accuracy, it enables the use of large batch training without needing to tune hyper-parameters. Using these methods, ResNet-50 was trained on ImageNet to a 76.1% validation accuracy in less than 30 minutes."
    },
    {
      "title": "Large Batch Training of Convolutional Networks",
      "url": "/research/2017/09/13/Large-Batch-Training-of-Convolutional-Networks.html",
      "date": "September 13, 2017",
      "summary": "To expedite the training of large convolutional networks, computational units are added and trained with data-parallel synchronous Stochastic Gradient Descent (SGD) across units, increasing batch size with node count. However, larger batch sizes can reduce model accuracy. The existing method of large batch training—linear learning rate scaling with warm-up—is not universally effective and may cause training divergence. To address these challenges, we introduce a novel training algorithm, Layer-wise Adaptive Rate Scaling (LARS), enabling us to train Alexnet with a batch size of 8K and Resnet-50 with a batch size of 32K, without compromising accuracy."
    },
    {
      "title": "On the Maximum Hessian Eigenvalue and Generalization",
      "url": "/research/2023/05/23/On-the-Maximum-Hessian-Eigenvalue-and-Generalization.html",
      "date": "May 23, 2023",
      "summary": "This study investigates the relationship between training interventions and the generalization of deep networks. While previous research suggests that flatter solutions generalize better than sharper ones, particularly measured by λmax, the largest eigenvalue of the Hessian of the loss, this paper challenges this notion. Through experiments, we demonstrate that larger learning rates reduce λmax for all batch sizes but do not consistently improve generalization. Additionally, scaling batch size and learning rate simultaneously can change λmax without affecting generalization. Sharpness-Aware Minimization (SAM) produces smaller λmax but does not consistently enhance generalization, especially with larger batch sizes. Excessive dropout probabilities can degrade generalization, despite promoting smaller λmax. Batch normalization, while not consistently reducing λmax, still improves generalization. These findings question λmax's role in explaining generalization in neural networks, highlighting the limits of its explanatory power."
    },
    {
      "title": "Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks",
      "url": "/research/2017/10/23/Cosine-Normalization-Using-Cosine-Similarity-Instead-of-Dot-Product-in-Neural-Networks.html",
      "date": "October 23, 2017",
      "summary": "In traditional multi-layer neural networks, the dot product between the output and weight vectors of preceding layers serves as input to the activation function, resulting in unbounded outputs and increased variance. This variance can lead to poor generalization and hinder training by exacerbating internal covariate shift. To address this, we propose cosine normalization, which replaces the dot product with cosine similarity or centered cosine similarity (Pearson Correlation Coefficient). We evaluate cosine normalization against batch, weight, and layer normalization in fully-connected and convolutional neural networks across various datasets. Our experiments demonstrate that cosine normalization outperforms other normalization techniques."
    },
    {
      "title": "Symbolic Discovery of Optimization Algorithms",
      "url": "/research/2023/05/08/Symbolic-Discovery-of-Optimization-Algorithms.html",
      "date": "May 8, 2023",
      "summary": "We propose a method for algorithm discovery via program search, focusing on optimizing deep neural network training. Our approach, Lion (EvoLved Sign Momentum), is memory-efficient and achieves comparable or superior performance to widely-used optimizers such as Adam and Adafactor across various tasks. Specifically, Lion enhances accuracy on tasks like image classification and vision-language contrastive learning, while reducing training compute. Notably, Lion exhibits improved performance with larger batch sizes and requires smaller learning rates compared to Adam. Despite its effectiveness, Lion has limitations, which we analyze, and provide insights into its deployment and performance. Our implementation is publicly available and has been successfully utilized in Google's search ads CTR model."
    },
    {
      "title": "AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks",
      "url": "/research/2023/03/01/AdaSAM-Boosting-Sharpness-Aware-Minimization-with-Adaptive-Learning-Rate-and-Momentum-for-Training-Deep-Neural-Networks.html",
      "date": "March 1, 2023",
      "summary": "The Sharpness Aware Minimization (SAM) optimizer, known for enhancing the generalization of deep neural networks by introducing extra perturbation steps, is further developed into AdaSAM by integrating adaptive learning rates and momentum acceleration. Although AdaSAM has been empirically applied to large-scale networks, a theoretical understanding of its performance, considering the complexity of its components, was lacking. This study presents a theoretical analysis of AdaSAM's convergence in stochastic non-convex settings, demonstrating a convergence rate of O(1/√bT) that scales linearly with mini-batch size. By introducing a delayed second-order momentum term, the study successfully decouples and analyzes the intertwined effects of stochastic gradients, adaptive learning rates, and perturbations. This is the first work to offer a detailed convergence rate for SAM with adaptive mechanisms. Experimental results on various NLP tasks indicate AdaSAM's superior performance over SGD, AMSGrad, and SAM optimizers."
    },
    {
      "title": "Exponentially Faster Language Modeling",
      "url": "/research/2023/11/21/Exponentially-Faster-Language-Modeling.html",
      "date": "November 21, 2023",
      "summary": "UltraFastBERT, a BERT variant, operates with just 0.3% of its neurons—selectively using 12 out of 4095 neurons per layer—for inference, matching the performance of similar models. It replaces conventional feedforward networks with fast feedforward networks (FFFs) to achieve this efficiency. Although fully efficient conditional neural execution isn't yet practical, we offer a high-level CPU code that achieves a 78x speedup and a PyTorch implementation with a 40x speedup over standard batched feedforward inference."
    },
    {
      "title": "How Does Sharpness-Aware Minimization Minimize Sharpness?",
      "url": "/research/2023/01/05/How-Does-Sharpness-Aware-Minimization-Minimize-Sharpness.html",
      "date": "January 5, 2023",
      "summary": "Sharpness-Aware Minimization (SAM) enhances deep neural networks' generalization across various settings. Although SAM aims to penalize model sharpness using a computationally efficient approach, its exact operational mechanism and the sharpness notion it regularizes remain unclear, partly due to differing sharpness concepts used in its theoretical framework and empirical validation. This study identifies the precise sharpness concept SAM regulates and explains its mechanism. It reveals that the combined effect of SAM's two-step approximations, despite being individually misleading, correctly improves generalization when using full-batch gradients. Moreover, we demonstrate that the stochastic SAM version indeed regularizes a third sharpness notion, aligning more closely with practical performance. This effectiveness is attributed to the gradient's alignment with the Hessian's top eigenvector under SAM."
    }
  ],
  "step": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a method for contrastive learning of visual representations. Authors find that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch size with more training steps improve representation quality. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "ADAMW: Decoupled Weight Decay Regularization",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "This paper shows that L2 regularization and weight decay are equivalent for standard SGD but not for adaptive methods like ADAM. To account of this, these authors separate weight decay from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate."
    },
    {
      "title": "Eigencurve: Optimal Learning Rate Schedule For Sgd on Quadratic Objectives With Skewed Hessian Spectrums",
      "url": "/research/2022/06/14/Eigencurve-Optimal-Learning-Rate-Schedule-For-Sgd-on-Quadratic-Objectives-With-Skewed-Hessian-Spectrums.html",
      "date": "June 14, 2022",
      "summary": "Learning rate schedulers are essential in deep neural network training, but there's a gap between practical usage and theoretical understanding. This paper introduces Eigencurve, the first set of learning rate schedules achieving minimax optimal convergence rates for SGD on quadratic objectives with skewed eigenvalue distributions of the Hessian matrix. This condition is common in practice. Experimental results on CIFAR-10 image classification tasks demonstrate Eigencurve's superiority over step decay, particularly with fewer epochs. The theory inspires practical schedulers approximating Eigencurve, resembling cosine decay for some problems while outperforming it in others."
    },
    {
      "title": "AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks",
      "url": "/research/2023/03/01/AdaSAM-Boosting-Sharpness-Aware-Minimization-with-Adaptive-Learning-Rate-and-Momentum-for-Training-Deep-Neural-Networks.html",
      "date": "March 1, 2023",
      "summary": "The Sharpness Aware Minimization (SAM) optimizer, known for enhancing the generalization of deep neural networks by introducing extra perturbation steps, is further developed into AdaSAM by integrating adaptive learning rates and momentum acceleration. Although AdaSAM has been empirically applied to large-scale networks, a theoretical understanding of its performance, considering the complexity of its components, was lacking. This study presents a theoretical analysis of AdaSAM's convergence in stochastic non-convex settings, demonstrating a convergence rate of O(1/√bT) that scales linearly with mini-batch size. By introducing a delayed second-order momentum term, the study successfully decouples and analyzes the intertwined effects of stochastic gradients, adaptive learning rates, and perturbations. This is the first work to offer a detailed convergence rate for SAM with adaptive mechanisms. Experimental results on various NLP tasks indicate AdaSAM's superior performance over SGD, AMSGrad, and SAM optimizers."
    },
    {
      "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
      "url": "/research/2022/03/19/Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training.html",
      "date": "March 19, 2022",
      "summary": "Sharpness-Aware Minimization (SAM) enhances generalization by optimizing a neighborhood-based perturbed loss, but it doesn't always favor flat minima due to both sharp and flat minima having low perturbed losses. We introduce a new measure, the surrogate gap, reflecting the dominant Hessian eigenvalue at small neighborhood radii, which is simple to compute and can be minimized during training. We propose the Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), an advancement over SAM with minimal additional computational cost. GSAM employs a dual-step approach: first, minimizing the perturbed loss similar to SAM, and second, reducing the surrogate gap without affecting the perturbed loss to target regions with low loss and sharpness, thus achieving superior generalization. GSAM is theoretically robust, showing better convergence and empirical generalization improvements, notably a +3.2% gain over SAM and +5.4% over AdamW in ImageNet accuracy for ViT-B/32."
    },
    {
      "title": "How Does Sharpness-Aware Minimization Minimize Sharpness?",
      "url": "/research/2023/01/05/How-Does-Sharpness-Aware-Minimization-Minimize-Sharpness.html",
      "date": "January 5, 2023",
      "summary": "Sharpness-Aware Minimization (SAM) enhances deep neural networks' generalization across various settings. Although SAM aims to penalize model sharpness using a computationally efficient approach, its exact operational mechanism and the sharpness notion it regularizes remain unclear, partly due to differing sharpness concepts used in its theoretical framework and empirical validation. This study identifies the precise sharpness concept SAM regulates and explains its mechanism. It reveals that the combined effect of SAM's two-step approximations, despite being individually misleading, correctly improves generalization when using full-batch gradients. Moreover, we demonstrate that the stochastic SAM version indeed regularizes a third sharpness notion, aligning more closely with practical performance. This effectiveness is attributed to the gradient's alignment with the Hessian's top eigenvector under SAM."
    },
    {
      "title": "MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts",
      "url": "/research/2024/02/26/MoE-Mamba-Efficient-Selective-State-Space-Models-with-Mixture-of-Experts.html",
      "date": "February 26, 2024",
      "summary": "State Space Models (SSMs) are emerging as strong competitors in sequential modeling, challenging Transformers. Integrating Mixture of Experts (MoE) has enhanced Transformer-based models, including cutting-edge open models. We suggest combining SSMs with MoE to further unlock their scaling potential. Our model, MoE-Mamba, based on the SSM model Mamba, exceeds the performance of both Mamba and traditional Transformer-MoE models. Notably, MoE-Mamba achieves Mamba's performance with 2.35 times fewer training steps, maintaining Mamba's inference advantages over Transformers."
    }
  ],
  "linear": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a method for contrastive learning of visual representations. Authors find that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch size with more training steps improve representation quality. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "This paper introduces the concept of delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a measure of local complexity, analyzing the density of linear regions in the DNN input space. The authors find that during training linear regions emerge near training samples with the nonlinearities being forced towards decision boundries."
    },
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights. This paper shows deep networks have significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. The authors propose a stochastic projection rule that sets a new benchmark CIFAR-10 without data augmentation."
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. This self-supervised method uses multiple crops of an image. Given a context crop the network is trained to predict the embeddings of several target crops. When applied to Vision Transformers, I-JEPA acheived strong performance in various tasks like image classification and depth prediction, outperforming Masked Auto Encoding (MAE) in linear probing when controlling for compute."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "This paper introduces the Mamba archetecture, a selective state space model (SSSM) with input dependance. Mamba has better speed and scalability compared to transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences. Authors also present a hardware efficient scanning mechanism that is used in Mamba."
    },
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/research/2024/01/24/MambaByte-Token-free-Selective-State-Space-Model.html",
      "date": "January 24, 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "This paper introduces the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a multi-directional Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions."
    },
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "This paper intriduces the Kaiming He initialisation strategy for RELU like activation functions. This the default init in pytorch, improving model stability and decreasing training time for RELU networks. This study also introduces a Parametric Rectified Linear Unit (PReLU) activation function that extends traditional units, offering improved model fitting with negligible additional computational cost."
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets."
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "This paper introduces Xavier Glorot init for Sigmoid like activation functions. The study investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial."
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "This study introduces a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. The method involves creating spaces for network design that parameterize multiple network populations. A low-dimensional, efficient design space called RegNet is developed, based on modeling network widths and depths through a quantized linear function. The analysis of the RegNet space challenges existing design practices, offering simpler and faster networks effective across various computational budgets. RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs under similar conditions."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "This paper introduces Mask-Reconstruct Augmentation (MRA), an image augmentation technique that leverages self-supervised masked autoencoders to generate distorted inputs and address the issue of overfitting in deep neural networks. Inspired by the success of masked image modeling in self-supervised learning, MRA uses nonlinear transformations for regularization, as opposed to current hand-crafted linear techniques like scale, flip, and color jitter. Extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks."
    },
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "This paper explores Gated Linear Units (GLUs), which involve the component-wise product of two linear projections, with one undergoing a sigmoid function. The authors investigate GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers. They find that certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    },
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "This paper introduces the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. The authors' empirical evaluation across computer vision, natural language processing, and speech tasks demonstrates that GELU offers performance improvements over ReLU and ELU activations."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "This paper demonstrates that artificial neural networks (ANNs) are equivalent to Gaussian processes at initialization in the infinite-width limit and introduces the Neural Tangent Kernel (NTK), which describes ANNs' behavior during training. The NTK stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. The authors prove the positive-definiteness of the limiting NTK under certain conditions and show that the network function follows a linear differential equation during training for least-squares regression. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    },
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions, SiLU and dSiLU. It challenges the need for experience replay and separate target networks in deep reinforcement learning. By using on-policy learning with eligibility traces and softmax action selection, it achieves state-of-the-art results in stochastic SZ-Tetris and small-board Tetris with TD(λ) learning and shallow dSiLU agents. It also outperforms DQN in Atari 2600 with a deep Sarsa(λ) agent using SiLU/dSiLU, suggesting an alternative to traditional DQN approaches."
    },
    {
      "title": "SinLU: Sinu-Sigmoidal Linear Uni",
      "url": "/research/2021/12/02/SinLU-Sinu-Sigmoidal-Linear-Uni.html",
      "date": "December 2, 2021",
      "summary": "This paper introduces a non-linear activation function called the Sinu-sigmoidal Linear Unit (SinLU), formulated as SinLU(x) = (x + a sin bx) · σ(x). It incorporates a sine wave for added functionality over traditional linear units, with two trainable parameters controlling the sinusoidal participation. The authors compare SinLU's performance to ReLU, GELU, and SiLU across various domains, models, and standard datasets. The results demonstrate SinLU's robustness and superior performance due to the incorporation of the trainable sine wave parameters, facilitating easy training and fast convergence."
    },
    {
      "title": "Is Cosine-Similarity of Embeddings Really About Similarity?",
      "url": "/research/2024/03/08/Is-Cosine-Similarity-of-Embeddings-Really-About-Similarity.html",
      "date": "March 8, 2024",
      "summary": "Cosine similarity, often used to gauge semantic similarity between high-dimensional objects by comparing low-dimensional feature embeddings, can yield variable results compared to unnormalized dot products. We investigate this phenomenon by analyzing embeddings from regularized linear models, revealing that cosine similarity can produce arbitrary and even meaningless similarities. This applies not only to linear models but also to deep models due to the implicit effects of various regularizations. Consequently, we advise against relying solely on cosine similarity and suggest exploring alternative approaches."
    },
    {
      "title": "Searching for Activation Functions",
      "url": "/research/2017/10/27/Searching-for-Activation-Functions.html",
      "date": "October 27, 2017",
      "summary": "This study explores the impact of activation functions on deep network training and performance. While Rectified Linear Unit (ReLU) is widely used, alternatives have not consistently outperformed it. We propose using automatic search techniques to discover new activation functions. Through exhaustive and reinforcement learning-based searches, we identify novel functions. Empirical evaluation shows that our best discovered function, Swish (f(x) = x · sigmoid(βx)), performs better than ReLU on deeper models across challenging datasets. Replacing ReLUs with Swish units improves classification accuracy on ImageNet, for example, by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. Swish's simplicity and similarity to ReLU facilitate its adoption in neural networks."
    },
    {
      "title": "Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks",
      "url": "/research/2020/06/11/Batch-Normalization-Provably-Avoids-Rank-Collapse-for-Randomly-Initialised-Deep-Networks.html",
      "date": "June 11, 2020",
      "summary": "We investigate the challenge of training randomly initialized deep neural networks due to spectral instabilities in products of random matrices. Batch normalization emerges as an effective solution to prevent rank collapse in both linear and ReLU networks. Leveraging tools from Markov chain theory, we establish a lower rank bound for deep linear networks. Empirical findings show that this rank robustness extends to ReLU networks. Our experiments on real-world datasets underscore the significance of rank stability in training modern deep neural architectures."
    },
    {
      "title": "Large Batch Training of Convolutional Networks",
      "url": "/research/2017/09/13/Large-Batch-Training-of-Convolutional-Networks.html",
      "date": "September 13, 2017",
      "summary": "To expedite the training of large convolutional networks, computational units are added and trained with data-parallel synchronous Stochastic Gradient Descent (SGD) across units, increasing batch size with node count. However, larger batch sizes can reduce model accuracy. The existing method of large batch training—linear learning rate scaling with warm-up—is not universally effective and may cause training divergence. To address these challenges, we introduce a novel training algorithm, Layer-wise Adaptive Rate Scaling (LARS), enabling us to train Alexnet with a batch size of 8K and Resnet-50 with a batch size of 32K, without compromising accuracy."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "url": "/research/2014/02/19/Exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks.html",
      "date": "February 19, 2014",
      "summary": "This study aims to enhance our comprehension of deep learning dynamics by analyzing deep linear neural networks. Despite their linear input-output mapping, these networks display nonlinear gradient descent dynamics, resulting in phenomena like prolonged plateaus and swift transitions to improved solutions. Through analytical exploration, we reveal that as network depth increases indefinitely, learning speed can remain finite under specific initial conditions. Unsupervised pretraining, under certain data conditions, can discover these favorable initial conditions, unlike random Gaussian initializations. We introduce a novel class of random orthogonal initial conditions that, akin to unsupervised pretraining, enable depth-independent learning. These conditions also ensure gradient propagation in deep nonlinear networks, particularly when operating at the edge of chaos."
    },
    {
      "title": "FNet: Mixing Tokens with Fourier Transforms",
      "url": "/research/2022/05/26/FNet-Mixing-Tokens-with-Fourier-Transforms.html",
      "date": "May 26, 2022",
      "summary": "We demonstrate that Transformer encoder architectures can be accelerated with minimal impact on accuracy by substituting self-attention sublayers with simple linear transformations. Remarkably, using a standard Fourier Transform instead of the self-attention sublayer in a Transformer encoder achieves 92-97% of BERT models' accuracy on the GLUE benchmark, while training 80% faster on GPUs and 70% faster on TPUs for standard 512 input lengths. Our FNet model significantly outperforms in speed at longer input lengths, matching the accuracy of the most accurate models on the Long Range Arena benchmark and surpassing the fastest models in speed across all sequence lengths on GPUs and shorter lengths on TPUs. Additionally, FNet is lightweight in memory use and exceptionally efficient in smaller sizes, outperforming Transformer models under the same speed and accuracy constraints."
    },
    {
      "title": "A Phase Transition between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention",
      "url": "/research/2024/02/06/A-Phase-Transition-between-Positional-and-Semantic-Learning-in-a-Solvable-Model-of-Dot-Product-Attention.html",
      "date": "February 6, 2024",
      "summary": "We explore the learning process of a dot-product attention layer, which learns both positional and semantic attention matrices, enabling tokens to attend based on position or meaning. Through experiments on an algorithmic task, we demonstrate that this architecture can use either mechanism for solving the task. Theoretically, we examine a non-linear self-attention layer with special query and key matrices, offering a closed-form solution for its non-convex loss landscape in high-dimensional data, which reveals a phase transition from positional to semantic mechanisms as sample complexity increases. We also show that the dot-product attention layer surpasses a linear positional baseline through the semantic mechanism with adequate data."
    },
    {
      "title": "Fine-tuning with Very Large Dropout",
      "url": "/research/2024/03/01/Fine-tuning-with-Very-Large-Dropout.html",
      "date": "March 1, 2024",
      "summary": "This study challenges the notion that machine learning practices assume training and testing data have the same distribution. It explores the effectiveness of high dropout rates, as opposed to ensemble techniques, in developing rich data representations suitable for multiple distribution scenarios. These representations surpass those achieved by traditional in-distribution performance regularization and the implicit sparsity induced by common stochastic gradient methods. While training deep networks from scratch with high dropout rates is impractical, fine-tuning pre-trained models under these conditions is feasible and yields better out-of-distribution performance than ensembles and model averaging techniques like model soups. This finding is significant due to the growing relevance of fine-tuning with large pre-trained models, offering insights into the nature of rich representations and the linear characteristics of fine-tuning large networks with small datasets."
    },
    {
      "title": "The loss landscape of deep linear neural networks: a second-order analysis",
      "url": "/research/2022/03/11/The-loss-landscape-of-deep-linear-neural-networks-a-second-order-analysis.html",
      "date": "March 11, 2022",
      "summary": "This study investigates the optimization landscape of deep linear neural networks with square loss, focusing on the role of non-strict saddle points in algorithm dynamics, which has been previously understudied. the authors conduct a comprehensive second-order analysis, identifying global minimizers, strict, and non-strict saddle points among all critical points, alongside their critical values. Our findings, based on conditions related to the ranks of partial matrix products, contribute insights into global convergence and implicit regularization observed in optimizing linear neural networks. Additionally, the authors offer an explicit parameterization of global minimizers and identify extensive sets of strict and non-strict saddle points."
    },
    {
      "title": "VideoMamba: State Space Model for Efficient Video Understanding",
      "url": "/research/2024/03/12/VideoMamba-State-Space-Model-for-Efficient-Video-Understanding.html",
      "date": "March 12, 2024",
      "summary": "This work introduces VideoMamba, an innovative approach that addresses local redundancy and global dependencies in video understanding by adapting the Mamba framework to video analysis. VideoMamba outperforms existing 3D convolutional neural networks and video transformers through its linear-complexity operator, which facilitates efficient long-term modeling for high-resolution, long-duration videos. Its effectiveness is demonstrated across four main areas: scalability in the visual domain without needing extensive dataset pretraining, thanks to a novel self-distillation technique; the ability to recognize short-term actions with fine-grained motion differences; superior performance in long-term video understanding compared to traditional feature-based models; and robust compatibility with multiple modalities, enhancing multi-modal video analysis. VideoMamba establishes a new standard for comprehensive and efficient video understanding."
    },
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "classification": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a method for contrastive learning of visual representations. Authors find that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch size with more training steps improve representation quality. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. This self-supervised method uses multiple crops of an image. Given a context crop the network is trained to predict the embeddings of several target crops. When applied to Vision Transformers, I-JEPA acheived strong performance in various tasks like image classification and depth prediction, outperforming Masked Auto Encoding (MAE) in linear probing when controlling for compute."
    },
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/research/2019/04/27/Benchmarking-Neural-Network-Robustness-to-Common-Corruptions-and-Surface-Variations.html",
      "date": "April 27, 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and IMAGENET-P, which assesses perturbation robustness."
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing is a regularization technique that reduces overfitting and enhances classification performance. Label smothing involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution. This significantly improves classification accuracy and model robustness to noisy labels."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. The authors find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. "
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results. The method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. A convolutional neural network (CNN) was trained on over a million color images, operating as a feed-forward pass at test time. The algorithm's effectiveness was validated through a colorization Turing test, where it deceived participants in 32% of trials. The paper also illustrates how colorization can serve as an effective pretraining task for self-supervised feature learning, achieving state-of-the-art results on several benchmarks."
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores self-supervision through automatic colorization as an alternative to ImageNet pretraining. Self-supervised training achieved state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. This paper highlights the significance of loss formulation, training specifics, and network architecture when pretraining through colorization. It also revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and feature adaptability upon fine-tuning. The findings suggest that colorization offers a better supervisory signal comparable to various types of ImageNet pretraining."
    },
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/research/2022/10/10/Turbo-Training-with-Token-Dropout.html",
      "date": "October 10, 2022",
      "summary": "This paper introduces Turbo training, an efficient method for training Transformers on video-related tasks. The authors present a simple yet versatile training paradigm applicable to multiple video tasks and demonstrate its effectiveness across action classification, video-language representation learning, and long-video activity classification. Turbo training achieves competitive performance with up to 4× speed-up and reduced memory usage, enabling long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous resource-intensive methods.."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates how to achieve state-of-the-art certified adversarial robustness to ℓ2-norm bounded perturbations using only off-the-shelf pretrained models. The authors instantiate the denoised smoothing approach by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This method certifies 71% accuracy on ImageNet under adversarial perturbations constrained to an ℓ2 norm of ε = 0.5, improving upon the prior certified state-of-the-art by 14 percentage points and denoised smoothing by 30 percentage points, without requiring any fine-tuning or retraining of model parameters."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. This encoder serves as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The DMAE ViT-Base model achieves comparable or superior certified accuracy with fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on ImageNet. The model also shows high transferability to CIFAR-10, indicating its broad applicability."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "This research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. The authors validate these approaches using the ImageNet-1k dataset and confirm their findings with the ImageNet-v2 test set, evaluating transfer performance across six additional datasets."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "This paper introduces Mask-Reconstruct Augmentation (MRA), an image augmentation technique that leverages self-supervised masked autoencoders to generate distorted inputs and address the issue of overfitting in deep neural networks. Inspired by the success of masked image modeling in self-supervised learning, MRA uses nonlinear transformations for regularization, as opposed to current hand-crafted linear techniques like scale, flip, and color jitter. Extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks."
    },
    {
      "title": "DeiT III: Revenge of the ViT",
      "url": "/research/2023/01/18/DeiT-III-Revenge-of-the-ViT.html",
      "date": "January 18, 2023",
      "summary": "This paper explores the supervised training of Vision Transformers (ViTs) using a simplified training approach adapted from ResNet-50 that includes a novel data-augmentation method with just 3 augmentations. The study demonstrates that this method significantly improves ViTs' performance in image classification, transfer learning, and semantic segmentation over previous supervised training techniques. Moreover, it shows ViTs' performance can match newer architectures, providing a new benchmark for evaluating self-supervised methods on ViTs."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "This paper introduces Online Normalization, a novel method for normalizing neural network hidden activations that offers a batch-independent alternative with comparable accuracy to Batch Normalization. Online Normalization addresses Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. The method is applicable to recurrent, fully connected networks, and those with high activation memory requirements. The authors demonstrate its effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    },
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "This paper introduces the Kinetics Human Action Video dataset. The authors note that the limited number of videos in existing datasets like UCF-101 and HMDB-51 hinders the ability to effectively evaluate architectures due to similar performance across these small benchmarks. Kinetics contains 400 classes with over 400 video clips per class from challenging YouTube videos. The paper analyzes the impact of this larger dataset on the performance of existing architectures as well as the benefits of pre-training models on Kinetics. The authors introduce the Two-Stream Inflated 3D ConvNet (I3D), an extension of 2D ConvNets to 3D for improved video feature extraction. When pre-trained on Kinetics, this I3D model sets new benchmarks for action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    },
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "This paper introduces SlowFast networks, a new convolutional architecture for video recognition tasks. These networks have two pathways: a Slow pathway operating at low frame rates to capture spatial semantics, and a Fast pathway working at high frame rates with reduced channel capacity to efficiently capture motion details. The SlowFast models achieve top accuracy on major video benchmarks like Kinetics, Charades, and AVA, significantly improving performance for action classification and detection tasks."
    },
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This paper investigates using high-quality rendering software and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle. This synthetic dataset is used to augment limited real training data for image classification and semantic segmentation tasks. While models trained solely on synthetic images showed low accuracy on real validation data, including even small amounts of real data significantly improved performance. Augmenting real training data with synthetic images outperformed using only real images. Furthermore, pretraining models on the synthetic dataset before transfer learning significantly reduced training costs by allowing most of the training to be completed upfront using the synthetic data."
    },
    {
      "title": "Taming the Waves: Sine as Activation Function in Deep Neural Networks",
      "url": "/research/2016/11/04/Taming-the-Waves-Sine-as-Activation-Function-in-Deep-Neural-Networks.html",
      "date": "November 4, 2016",
      "summary": "This paper explores the challenges in training deep neural networks that use sinusoidal activation functions. The authors explain that the difficulty arises from the emergence of numerous shallow local minima in the loss landscape. The study reveals that successful learning in typical classification tasks occurs when the network effectively ignores the periodic cycles of the sinusoidal functions. However, for certain non-trivial tasks, networks with sinusoidal activations can outperform those using traditional monotonic activation functions."
    },
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "The paper introduces TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method that applies a single augmentation to each image. Despite its minimal complexity and cost, TrivialAugment outperforms existing methods across various image classification scenarios, as validated through extensive experiments against state-of-the-art methods and multiple ablation studies involving different augmentation spaces and methods. The work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Noting a stagnation in automatic augmentation research, the authors conclude by proposing best practices for future advancements in the field."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    },
    {
      "title": "Deep Residual Learning for Image Recognition",
      "url": "/research/2015/12/10/Deep-Residual-Learning-for-Image-Recognition.html",
      "date": "December 10, 2015",
      "summary": "We introduce a residual learning framework to train deeper neural networks effectively. By reformulating layers to learn residual functions relative to inputs, our approach facilitates optimization and achieves higher accuracy with increased depth. Evaluations on ImageNet demonstrate the effectiveness of residual networks up to 152 layers deep, outperforming previous architectures. Our method achieves 3.57% error on the ImageNet test set, winning 1st place in the ILSVRC 2015 classification task. Additionally, our deep representations lead to a 28% improvement on the COCO object detection dataset. In the ILSVRC & COCO 2015 competitions, our approach secured 1st place in ImageNet detection, localization, COCO detection, and segmentation tasks."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Searching for Activation Functions",
      "url": "/research/2017/10/27/Searching-for-Activation-Functions.html",
      "date": "October 27, 2017",
      "summary": "This study explores the impact of activation functions on deep network training and performance. While Rectified Linear Unit (ReLU) is widely used, alternatives have not consistently outperformed it. We propose using automatic search techniques to discover new activation functions. Through exhaustive and reinforcement learning-based searches, we identify novel functions. Empirical evaluation shows that our best discovered function, Swish (f(x) = x · sigmoid(βx)), performs better than ReLU on deeper models across challenging datasets. Replacing ReLUs with Swish units improves classification accuracy on ImageNet, for example, by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. Swish's simplicity and similarity to ReLU facilitate its adoption in neural networks."
    },
    {
      "title": "Understanding Deep Learning Requires Re- Thinking Generalization",
      "url": "/research/2017/02/26/Understanding-Deep-Learning-Requires-Re--Thinking-Generalization.html",
      "date": "February 26, 2017",
      "summary": "Extensive experiments reveal that traditional explanations for the strong generalization performance of large neural networks fall short. Despite conventional wisdom attributing success to model properties or regularization techniques, our findings show that state-of-the-art convolutional networks for image classification can easily overfit random labels. This phenomenon persists regardless of explicit regularization or substituting true images with random noise. Theoretical analysis suggests that shallow neural networks achieve perfect finite sample expressivity when parameters outnumber data points, as commonly observed. We interpret these results in contrast to traditional models."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "Automated Learning Rate Scheduler for Large-batch",
      "url": "/research/2021/07/13/Automated-Learning-Rate-Scheduler-for-Large-batch.html",
      "date": "July 13, 2021",
      "summary": "Large-batch training is crucial for deep learning with large datasets and models, but requires specific learning rate (LR) schedules for optimal performance, especially under limited training epochs. This work introduces an automated LR scheduling algorithm for large-batch neural network training within a fixed epoch budget, consisting of adaptive warmup and predefined decay phases. The LR is dynamically adjusted based on training loss monitored through Gaussian process smoothing, facilitating low computational overhead. When integrated with adaptive stochastic optimizers like AdamP and LAMB, this scheduler eliminates the need for extensive hyperparameter tuning and achieves competitive or superior results on various image classification tasks across different batch sizes and architectures."
    },
    {
      "title": "Eigencurve: Optimal Learning Rate Schedule For Sgd on Quadratic Objectives With Skewed Hessian Spectrums",
      "url": "/research/2022/06/14/Eigencurve-Optimal-Learning-Rate-Schedule-For-Sgd-on-Quadratic-Objectives-With-Skewed-Hessian-Spectrums.html",
      "date": "June 14, 2022",
      "summary": "Learning rate schedulers are essential in deep neural network training, but there's a gap between practical usage and theoretical understanding. This paper introduces Eigencurve, the first set of learning rate schedules achieving minimax optimal convergence rates for SGD on quadratic objectives with skewed eigenvalue distributions of the Hessian matrix. This condition is common in practice. Experimental results on CIFAR-10 image classification tasks demonstrate Eigencurve's superiority over step decay, particularly with fewer epochs. The theory inspires practical schedulers approximating Eigencurve, resembling cosine decay for some problems while outperforming it in others."
    },
    {
      "title": "Weight Agnostic Neural Networks",
      "url": "/research/2019/09/05/Weight-Agnostic-Neural-Networks.html",
      "date": "September 5, 2019",
      "summary": "This study investigates the importance of neural network architectures versus weight parameters for task performance. We introduce a method to find architectures capable of performing tasks without weight training. By assigning random weights, we show that minimal architectures can achieve notable performance on various tasks, including reinforcement learning and MNIST classification."
    },
    {
      "title": "Symbolic Discovery of Optimization Algorithms",
      "url": "/research/2023/05/08/Symbolic-Discovery-of-Optimization-Algorithms.html",
      "date": "May 8, 2023",
      "summary": "We propose a method for algorithm discovery via program search, focusing on optimizing deep neural network training. Our approach, Lion (EvoLved Sign Momentum), is memory-efficient and achieves comparable or superior performance to widely-used optimizers such as Adam and Adafactor across various tasks. Specifically, Lion enhances accuracy on tasks like image classification and vision-language contrastive learning, while reducing training compute. Notably, Lion exhibits improved performance with larger batch sizes and requires smaller learning rates compared to Adam. Despite its effectiveness, Lion has limitations, which we analyze, and provide insights into its deployment and performance. Our implementation is publicly available and has been successfully utilized in Google's search ads CTR model."
    },
    {
      "title": "Do Vision Transformers See Like Convolutional Neural Networks?",
      "url": "/research/2022/03/03/Do-Vision-Transformers-See-Like-Convolutional-Neural-Networks.html",
      "date": "March 3, 2022",
      "summary": "Convolutional Neural Networks (CNNs) have been prominent in processing visual data, but recent studies show Vision Transformers (ViTs) can match or outperform CNNs in image classification. This work examines how ViTs solve classification tasks, discovering significant differences from CNNs, including uniformity in ViT representations across layers due to self-attention and residual connections that enhance feature propagation. ViTs also maintain spatial information effectively, influenced by classification methods. The study further explores how dataset scale affects ViT features and their transferability, linking these findings to new architectures like MLP-Mixer."
    },
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    },
    {
      "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
      "url": "/research/2023/11/30/Dichotomy-of-Early-and-Late-Phase-Implicit-Biases-Can-Provably-Induce-Grokking.html",
      "date": "November 30, 2023",
      "summary": "Power et al. (2022) discovered that neural networks initially memorize arithmetic tasks, achieving perfect training accuracy but poor test accuracy, then suddenly achieve perfect test accuracy after extended training. This study explains this grokking phenomenon with the theory that early and late phase implicit biases cause this shift. Training homogeneous neural networks with large initialization and small weight decay on classification and regression tasks results in a prolonged period where the network acts like a kernel predictor, followed by a sudden shift to min-norm/max-margin predictors, significantly improving test accuracy."
    },
    {
      "title": "Non-negative Contrastive Learning",
      "url": "/research/2024/03/19/Non-negative-Contrastive-Learning.html",
      "date": "March 19, 2024",
      "summary": "Deep representations offer promising performance for downstream tasks but lack interpretability, posing a significant challenge. In this paper, we introduce Non-negative Contrastive Learning (NCL), a refinement of Non-negative Matrix Factorization (NMF) aimed at generating interpretable features. NCL enforces non-negativity constraints, akin to NMF, resulting in sparse and disentangled representations, unlike standard contrastive learning (CL). We establish theoretical guarantees on NCL's identifiability and downstream generalization. Empirically, NCL outperforms CL in feature disentanglement, selection, and downstream classification tasks. Moreover, NCL can be extended to other learning scenarios, benefiting supervised learning."
    },
    {
      "title": "V-Jepa: Latent Video Prediction For Visual Representation Learning",
      "url": "/research/2023/02/15/V-Jepa-Latent-Video-Prediction-For-Visual-Representation-Learning.html",
      "date": "February 15, 2023",
      "summary": "This paper introduces V-JEPA, a self-supervised video learning method that predicts masked spatio-temporal regions in latent space, effectively applying the masked-modelling principle of large language models to video. This approach generates visual features useful across various image and video tasks without needing model adjustment, achieving significant improvements on Kinetics-400 (82.1%) and Something-Something-v2 (71.2%) benchmarks, outperforming prior video models. V-JEPA also excels in motion understanding tasks, surpassing leading image models like DINOv2 and OpenCLIP, and achieves 77.9% on ImageNet classification with video training alone, setting a new standard for video models."
    },
    {
      "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum",
      "url": "/research/2023/09/30/When-to-Learn-What:-Model-Adaptive-Data-Augmentation-Curriculum.html",
      "date": "September 30, 2023",
      "summary": "Data augmentation (DA) improves neural network generalization by enforcing invariances and symmetries to predefined transformations applied to input data. However, fixed augmentation policies affect samples differently at various training stages, and existing approaches cannot adapt policies to individual samples and the training model. This paper proposes Model-Adaptive Data Augmentation (MADAug) which trains an augmentation policy network to determine when to learn what. Unlike previous work, MADAug selects augmentation operators for each input image with a model-adaptive policy that varies between training stages, creating an optimized data augmentation curriculum. The policy is trained using a bi-level optimization scheme to minimize validation-set loss of a model trained with policy-produced augmentations. Extensive evaluations on multiple image classification tasks and network architectures show that MADAug outperforms or matches existing DA approaches, enhances fairness by improving all classes, particularly the difficult ones, and performs better when transferred to fine-grained datasets. Additionally, the auto-optimized policy in MADAug gradually increases perturbations, forming an easy-to-hard curriculum."
    }
  ],
  "resnet": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a method for contrastive learning of visual representations. Authors find that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch size with more training steps improve representation quality. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "CLIP: Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns from raw text descriptions of images, moving beyond training on fixed object categories. By pre-training on 400 million (image, text) pairs using caption-image matching, the method achieves state-of-the-art image representations from scratch. This allows for zero-shot transfer to various downstream tasks using natural language, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse datasets, showing competitive results against fully supervised baselines without task-specific training, and matching the accuracy of ResNet-50 on ImageNet zero-shot without using its training examples."
    },
    {
      "title": "DeiT III: Revenge of the ViT",
      "url": "/research/2023/01/18/DeiT-III-Revenge-of-the-ViT.html",
      "date": "January 18, 2023",
      "summary": "This paper explores the supervised training of Vision Transformers (ViTs) using a simplified training approach adapted from ResNet-50 that includes a novel data-augmentation method with just 3 augmentations. The study demonstrates that this method significantly improves ViTs' performance in image classification, transfer learning, and semantic segmentation over previous supervised training techniques. Moreover, it shows ViTs' performance can match newer architectures, providing a new benchmark for evaluating self-supervised methods on ViTs."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "This study challenges the traditional understanding of how Convolutional Neural Networks (CNNs) recognize objects, revealing that they are more biased towards recognizing textures rather than learning complex shapes. The authors found that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, training a ResNet-50 on a stylized version of ImageNet, designed to emphasize shape, aligned the network's performance more closely with human behavior. This shape-based training matched human performance in controlled experiments and enhanced object detection and robustness against image distortions, highlighting the benefits of shape-based representations in visual recognition systems."
    },
    {
      "title": "Searching for Activation Functions",
      "url": "/research/2017/10/27/Searching-for-Activation-Functions.html",
      "date": "October 27, 2017",
      "summary": "This study explores the impact of activation functions on deep network training and performance. While Rectified Linear Unit (ReLU) is widely used, alternatives have not consistently outperformed it. We propose using automatic search techniques to discover new activation functions. Through exhaustive and reinforcement learning-based searches, we identify novel functions. Empirical evaluation shows that our best discovered function, Swish (f(x) = x · sigmoid(βx)), performs better than ReLU on deeper models across challenging datasets. Replacing ReLUs with Swish units improves classification accuracy on ImageNet, for example, by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. Swish's simplicity and similarity to ReLU facilitate its adoption in neural networks."
    },
    {
      "title": "Don't Decay the Learning Rate, Increase the Batch Size",
      "url": "/research/2017/11/01/Don't-Decay-the-Learning-Rate,-Increase-the-Batch-Size.html",
      "date": "November 1, 2017",
      "summary": "This study demonstrates that increasing the batch size during training achieves similar learning outcomes as the common practice of decaying the learning rate, applicable to stochastic gradient descent and its variants, including with momentum and Adam optimization. This approach not only matches test accuracies within the same number of epochs but also enhances parallelism and reduces training time due to fewer parameter updates. Efficiency can be further improved by adjusting the learning rate and batch size proportionally, and although increasing the momentum coefficient and scaling the batch size accordingly may slightly lower test accuracy, it enables the use of large batch training without needing to tune hyper-parameters. Using these methods, ResNet-50 was trained on ImageNet to a 76.1% validation accuracy in less than 30 minutes."
    },
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    }
  ],
  "label": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a method for contrastive learning of visual representations. Authors find that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch size with more training steps improve representation quality. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "An Investigation of how Label Smoothing Affects Generalization",
      "url": "/research/2020/10/23/AN-INVESTIGATION-OF-HOW-LABEL-SMOOTHING-AFFECTS-GENERALIZATION.html",
      "date": "October 23, 2020",
      "summary": "Label smoothing has been empirically shown to reduce overfitting and improve generalization. However, its mathematical underpinnings remain unclear. This paper explains label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. The authors identify a method for calculating a label smoothing value that minimizes generalization loss."
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing is a regularization technique that reduces overfitting and enhances classification performance. Label smothing involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution. This significantly improves classification accuracy and model robustness to noisy labels."
    },
    {
      "title": "SVHN: Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View House Numbers."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This study suggest that label smoothing not only boosts generalization but also enhances model calibration. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. Label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/research/2020/08/03/Are-Labels-Necessary-for-Neural-Architecture-Search.html",
      "date": "August 3, 2020",
      "summary": "The paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning the necessity of human-annotated labels for discovering effective neural architectures. Two experimental setups sample-based and search-based were investigated. The sample-based approach evaluated 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. The search-based experiments employed DARTS, a recognized NAS algorithm, with various unsupervised objectives. The architectures identified without labels performed competitively compared to those found with labels. The findings suggested that image statistics alone could be sufficient for identifying efficient neural architectures, potentially eliminating the need for human-annotated labels."
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores self-supervision through automatic colorization as an alternative to ImageNet pretraining. Self-supervised training achieved state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. This paper highlights the significance of loss formulation, training specifics, and network architecture when pretraining through colorization. It also revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and feature adaptability upon fine-tuning. The findings suggest that colorization offers a better supervisory signal comparable to various types of ImageNet pretraining."
    },
    {
      "title": "CLIP: Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns from raw text descriptions of images, moving beyond training on fixed object categories. By pre-training on 400 million (image, text) pairs using caption-image matching, the method achieves state-of-the-art image representations from scratch. This allows for zero-shot transfer to various downstream tasks using natural language, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse datasets, showing competitive results against fully supervised baselines without task-specific training, and matching the accuracy of ResNet-50 on ImageNet zero-shot without using its training examples."
    },
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "This paper introdices LOST, a method for unsupervised object localization in images. LOST utilizes activation features from a self-supervised pre-trained vision transformer. Unlike other methods, LOST works on individual images without relying on external object proposals or image collection exploration. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, demonstrating its effectiveness in unsupervised object discovery."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. A ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training. Synthetic images generated by FDSL circumvent issues associated with real images, offering a promising avenue for pre-training general models. Investigations suggest that simple object contours can match fractal-based dataset performance and increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets for formula-driven supervised learning (FDSL). The authors identify the optimal FDSL parameters and maximize synthetic image variety, which is crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. The authors demonstrate FDSL's potential for continuous improvement and its ability to avoid issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper introduces the AVA dataset, which features 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. The authors highlight the challenges in action recognition and introduce a novel localization approach that surpasses existing benchmarks but shows modest performance on AVA (15.6% mAP), indicating the need for advanced video understanding methods."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "This paper introduces FixMatch, a streamlined semi-supervised learning (SSL) algorithm. FixMatch generates pseudo-labels using a model's predictions on weakly-augmented unlabeled images, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness, and the code is publicly available."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    },
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    },
    {
      "title": "Understanding Deep Learning Requires Re- Thinking Generalization",
      "url": "/research/2017/02/26/Understanding-Deep-Learning-Requires-Re--Thinking-Generalization.html",
      "date": "February 26, 2017",
      "summary": "Extensive experiments reveal that traditional explanations for the strong generalization performance of large neural networks fall short. Despite conventional wisdom attributing success to model properties or regularization techniques, our findings show that state-of-the-art convolutional networks for image classification can easily overfit random labels. This phenomenon persists regardless of explicit regularization or substituting true images with random noise. Theoretical analysis suggests that shallow neural networks achieve perfect finite sample expressivity when parameters outnumber data points, as commonly observed. We interpret these results in contrast to traditional models."
    },
    {
      "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
      "url": "/research/2021/07/24/The-Many-Faces-of-Robustness-A-Critical-Analysis-of-Out-of-Distribution-Generalization.html",
      "date": "July 24, 2021",
      "summary": "We present four novel real-world distribution shift datasets encompassing changes in image style, blurriness, location, camera settings, and more. Evaluating existing methods for enhancing out-of-distribution robustness, we discover that employing larger models and artificial data augmentations can enhance robustness against real-world distribution shifts, contradicting prior claims. Our findings demonstrate that improvements in artificial robustness benchmarks can indeed transfer to real-world distribution shifts, contrary to prior assumptions. Additionally, we introduce a novel data augmentation technique that surpasses models pretrained with significantly more labeled data, emphasizing its efficacy in addressing real-world distribution shifts. While some methods consistently mitigate texture and local image statistics shifts, they fail to address other shifts like geographic changes. Our results underscore the necessity for future research to examine multiple distribution shifts concurrently, as no single method consistently improves robustness across all evaluated scenarios."
    },
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    }
  ],
  "outdo": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a method for contrastive learning of visual representations. Authors find that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch size with more training steps improve representation quality. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    }
  ],
  "alexnet": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a method for contrastive learning of visual representations. Authors find that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch size with more training steps improve representation quality. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Large Batch Training of Convolutional Networks",
      "url": "/research/2017/09/13/Large-Batch-Training-of-Convolutional-Networks.html",
      "date": "September 13, 2017",
      "summary": "To expedite the training of large convolutional networks, computational units are added and trained with data-parallel synchronous Stochastic Gradient Descent (SGD) across units, increasing batch size with node count. However, larger batch sizes can reduce model accuracy. The existing method of large batch training—linear learning rate scaling with warm-up—is not universally effective and may cause training divergence. To address these challenges, we introduce a novel training algorithm, Layer-wise Adaptive Rate Scaling (LARS), enabling us to train Alexnet with a batch size of 8K and Resnet-50 with a batch size of 32K, without compromising accuracy."
    }
  ],
  "time": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a method for contrastive learning of visual representations. Authors find that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch size with more training steps improve representation quality. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights. This paper shows deep networks have significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. The authors propose a stochastic projection rule that sets a new benchmark CIFAR-10 without data augmentation."
    },
    {
      "title": "Fast Feedforward Networks",
      "url": "/research/2023/09/18/Fast-Feedforward-Networks.html",
      "date": "September 18, 2023",
      "summary": "This paper introduces the fast feedforward (FFF) architecture, a log-time alternative to feedforward networks. FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "This paper introduces the Mamba archetecture, a selective state space model (SSSM) with input dependance. Mamba has better speed and scalability compared to transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences. Authors also present a hardware efficient scanning mechanism that is used in Mamba."
    },
    {
      "title": "Dropout as Data Augmentation",
      "url": "/research/2016/01/08/DROPOUT-AS-DATA-AUGMENTATION.html",
      "date": "January 8, 2016",
      "summary": "Dropout is typically interpreted as turning the test time network into an ensemble of the thinner training networks. This paper argues that dropout can also be interpreted as a kind of data augmentation in image space. Authors present an approach to project the dropout noise within a network back into the input space, visualising the augmented versions of the training data, and show that training a deterministic network on the augmented samples yields similar results. Authors then propose a new dropout noise scheme and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. Taking this to the extreme, this study trains a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss even when trained on pure noise."
    },
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "This paper intriduces the Kaiming He initialisation strategy for RELU like activation functions. This the default init in pytorch, improving model stability and decreasing training time for RELU networks. This study also introduces a Parametric Rectified Linear Unit (PReLU) activation function that extends traditional units, offering improved model fitting with negligible additional computational cost."
    },
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/research/2014/01/01/Dropout-A-Simple-Way-to-Prevent-Neural-Networks-from-Overfitting.html",
      "date": "January 1, 2014",
      "summary": "This paper introduces Dropout, a technique that addresses overfitting by randomly omitting units during training, leading to an exponential number of thinned networks. This effectively turns the test time network into an ensemble."
    },
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activitivations can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases."
    },
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrives incrementally, it's common to build a sequence of models that incorporate progressively more data. It has been shown that simply continuing training of a model on new data often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "This paper introduces Xavier Glorot init for Sigmoid like activation functions. The study investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results. The method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. A convolutional neural network (CNN) was trained on over a million color images, operating as a feed-forward pass at test time. The algorithm's effectiveness was validated through a colorization Turing test, where it deceived participants in 32% of trials. The paper also illustrates how colorization can serve as an effective pretraining task for self-supervised feature learning, achieving state-of-the-art results on several benchmarks."
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "This study introduces a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. The method involves creating spaces for network design that parameterize multiple network populations. A low-dimensional, efficient design space called RegNet is developed, based on modeling network widths and depths through a quantized linear function. The analysis of the RegNet space challenges existing design practices, offering simpler and faster networks effective across various computational budgets. RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs under similar conditions."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "This paper introduces Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperformed CLIP models on various downstream tasks. The speedup from this method enables experimentation with larger model sizes, more data, or longer training periods."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces TinyViT, a series of compact and efficient vision transformers (ViTs) designed for devices with limited resources. The authors employ a fast distillation framework to transfer knowledge from large pretrained models to smaller ones during pretraining, using sparsified logits from teacher models to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. With increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters, and demonstrates strong transferability across various downstream tasks."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "This paper introduces a novel approach to Radiance Field methods for novel-view synthesis that addresses the challenges of high visual quality, costly training, and real-time rendering of unbounded, complete scenes at 1080p resolution. The authors propose three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. The method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    },
    {
      "title": "Curriculum Learning by Dynamic Instance Hardness",
      "url": "/research/2020/12/06/Curriculum-Learning-by-Dynamic-Instance-Hardness.html",
      "date": "December 6, 2020",
      "summary": "This paper improves curriculum learning via introduces Dynamic Instance Hardness (DIH), a method that measures a sample's learning difficulty over time. DIH provides a stable indicator of learning progress by tracking the exponential moving average of a sample's hardness. DIHCL enhances learning efficiency and model accuracy without extra computational costs by leveraging data from the training process itself. Tested on 11 datasets, DIHCL surpasses traditional training methods and recent curriculum learning techniques in both efficiency and effectiveness."
    },
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "This paper investigates the importance of the warm-up stage in training transformers, and the impact of layer normalization placement. Using mean field theory, the authors demonstrate that the original Post-LN Transformer's design results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    },
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    },
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    },
    {
      "title": "Don't Decay the Learning Rate, Increase the Batch Size",
      "url": "/research/2017/11/01/Don't-Decay-the-Learning-Rate,-Increase-the-Batch-Size.html",
      "date": "November 1, 2017",
      "summary": "This study demonstrates that increasing the batch size during training achieves similar learning outcomes as the common practice of decaying the learning rate, applicable to stochastic gradient descent and its variants, including with momentum and Adam optimization. This approach not only matches test accuracies within the same number of epochs but also enhances parallelism and reduces training time due to fewer parameter updates. Efficiency can be further improved by adjusting the learning rate and batch size proportionally, and although increasing the momentum coefficient and scaling the batch size accordingly may slightly lower test accuracy, it enables the use of large batch training without needing to tune hyper-parameters. Using these methods, ResNet-50 was trained on ImageNet to a 76.1% validation accuracy in less than 30 minutes."
    },
    {
      "title": "Root Mean Square Layer Normalization",
      "url": "/research/2019/10/16/Root-Mean-Square-Layer-Normalization.html",
      "date": "October 16, 2019",
      "summary": "Layer normalization (LayerNorm) enhances deep neural network stability and convergence by re-centering and re-scaling inputs and weight matrices. However, its computational overhead slows networks, particularly RNNs. We introduce RMSNorm, which replaces re-centering with root mean square (RMS) regularization. RMSNorm maintains re-scaling invariance and adapts learning rates implicitly, while being computationally simpler than LayerNorm. We also propose partial RMSNorm (pRMSNorm), estimating RMS from a subset of inputs. Empirical results across various tasks and architectures demonstrate that RMSNorm achieves comparable performance to LayerNorm while reducing running time by 7%∼64%."
    },
    {
      "title": "On Layer Normalization in the Transformer Architecture",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architecture.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is widely used in natural language processing. Training it typically requires a carefully designed learning rate warm-up stage, crucial for performance but slowing optimization and requiring more hyperparameter tuning. We provide theoretical insights into the necessity of the warm-up stage, demonstrating that the location of layer normalization affects gradient behavior. For Post-LN Transformers, with layer normalization between residual blocks, large gradients near the output layer at initialization make training unstable without warm-up. Conversely, for Pre-LN Transformers, with layer normalization inside residual blocks, gradients are well-behaved, suggesting the removal of the warm-up stage. Experimental results show that Pre-LN Transformers without warm-up achieve comparable performance with less training time and hyperparameter tuning."
    },
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    },
    {
      "title": "Atomically Accurate De Novo Design of Single-domain Antibodies",
      "url": "/research/2024/03/18/Atomically-Accurate-De-Novo-Design-of-Single-domain-Antibodies.html",
      "date": "March 18, 2024",
      "summary": "Despite the critical role antibodies play in medicine, current methods for designing new antibodies targeting specific epitopes are time-consuming. Here, we showcase the effectiveness of a refined RFdiffusion network in designing novel antibody variable heavy chains (VHH’s) to target user-specified epitopes. Through experiments, we validate the binding capability of these designed VHH's to four disease-relevant epitopes. Furthermore, the cryo-EM structure analysis reveals that a designed VHH bound to influenza hemagglutinin closely matches the design model, affirming the accuracy of our approach."
    },
    {
      "title": "Chronos: Learning the Language of Time Series",
      "url": "/research/2024/03/12/Chronos-Learning-the-Language-of-Time-Series.html",
      "date": "March 12, 2024",
      "summary": "Chronos is a framework that enhances pretrained probabilistic time series models by tokenizing time series data for training with transformer-based architectures, notably the T5 family. Pretrained on a mix of public and synthetic datasets created via Gaussian processes, Chronos models outshine others in 42 benchmark datasets by showing superior performance on familiar datasets and competitive or better zero-shot capabilities on new datasets. This illustrates Chronos's ability to generalize across different domains, offering a simpler approach to forecasting tasks."
    },
    {
      "title": "MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts",
      "url": "/research/2024/02/26/MoE-Mamba-Efficient-Selective-State-Space-Models-with-Mixture-of-Experts.html",
      "date": "February 26, 2024",
      "summary": "State Space Models (SSMs) are emerging as strong competitors in sequential modeling, challenging Transformers. Integrating Mixture of Experts (MoE) has enhanced Transformer-based models, including cutting-edge open models. We suggest combining SSMs with MoE to further unlock their scaling potential. Our model, MoE-Mamba, based on the SSM model Mamba, exceeds the performance of both Mamba and traditional Transformer-MoE models. Notably, MoE-Mamba achieves Mamba's performance with 2.35 times fewer training steps, maintaining Mamba's inference advantages over Transformers."
    },
    {
      "title": "Self-Paced Absolute Learning Progress as a Regularized Approach to Curriculum Learning",
      "url": "/research/2023/06/09/Self-Paced-Absolute-Learning-Progress-as-a-Regularized-Approach-to-Curriculum-Learning.html",
      "date": "June 9, 2023",
      "summary": "Reinforcement Learning's usability is limited by its high computation times. Curriculum Reinforcement Learning accelerates learning by ordering tasks from simple to hard. Curricula based on Absolute Learning Progress (ALP) have shown success in various environments but waste computation on redundant tasks. This issue is addressed by introducing Self-Paced Absolute Learning Progress (SPALP), a regularization method inspired by Self-Paced Learning. Evaluated in three environments, SPALP achieves performance comparable to ALP in all cases and reaches it faster in two. Further improvements in SPALP's efficiency and performance are also discussed."
    },
    {
      "title": "The Platonic Representation Hypothesis",
      "url": "/research/2024/05/13/The-Platonic-Representation-Hypothesis.html",
      "date": "May 13, 2024",
      "summary": "This paper argues that AI model representations, especially in deep networks, are converging. It surveys examples in the literature showing how neural network representations align over time and across domains. It demonstrates convergence across data modalities, noting that as vision and language models grow, they measure distances between data points similarly. The authors hypothesize that this convergence leads to a shared statistical model of reality, termed the platonic representation and discuss possible selective pressures toward it. The paper also addresses the implications, limitations, and counterexamples to this trend."
    }
  ],
  "adam": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "ADAM is an optimization algorithm that imrpoves stochastic gradient decent with first and second order momentum terms. ADAM is easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "ADAMW: Decoupled Weight Decay Regularization",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "This paper shows that L2 regularization and weight decay are equivalent for standard SGD but not for adaptive methods like ADAM. To account of this, these authors separate weight decay from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "Don't Decay the Learning Rate, Increase the Batch Size",
      "url": "/research/2017/11/01/Don't-Decay-the-Learning-Rate,-Increase-the-Batch-Size.html",
      "date": "November 1, 2017",
      "summary": "This study demonstrates that increasing the batch size during training achieves similar learning outcomes as the common practice of decaying the learning rate, applicable to stochastic gradient descent and its variants, including with momentum and Adam optimization. This approach not only matches test accuracies within the same number of epochs but also enhances parallelism and reduces training time due to fewer parameter updates. Efficiency can be further improved by adjusting the learning rate and batch size proportionally, and although increasing the momentum coefficient and scaling the batch size accordingly may slightly lower test accuracy, it enables the use of large batch training without needing to tune hyper-parameters. Using these methods, ResNet-50 was trained on ImageNet to a 76.1% validation accuracy in less than 30 minutes."
    },
    {
      "title": "Symbolic Discovery of Optimization Algorithms",
      "url": "/research/2023/05/08/Symbolic-Discovery-of-Optimization-Algorithms.html",
      "date": "May 8, 2023",
      "summary": "We propose a method for algorithm discovery via program search, focusing on optimizing deep neural network training. Our approach, Lion (EvoLved Sign Momentum), is memory-efficient and achieves comparable or superior performance to widely-used optimizers such as Adam and Adafactor across various tasks. Specifically, Lion enhances accuracy on tasks like image classification and vision-language contrastive learning, while reducing training compute. Notably, Lion exhibits improved performance with larger batch sizes and requires smaller learning rates compared to Adam. Despite its effectiveness, Lion has limitations, which we analyze, and provide insights into its deployment and performance. Our implementation is publicly available and has been successfully utilized in Google's search ads CTR model."
    }
  ],
  "optimization": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "ADAM is an optimization algorithm that imrpoves stochastic gradient decent with first and second order momentum terms. ADAM is easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "ADAMW: Decoupled Weight Decay Regularization",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "This paper shows that L2 regularization and weight decay are equivalent for standard SGD but not for adaptive methods like ADAM. To account of this, these authors separate weight decay from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate."
    },
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "The Batch Normalization paper argued the method's effectiveness came from reducing internal covariate shift. In this paper the authors argue that actually the main benifit of Batch Norm is loss landscape smoothing."
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "This paper introduces DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. Authors also propose a dropout rate scheduler to optimize training."
    },
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets transformers as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "This paper introduces a self-supervised vision model pretraining framework inspired by energy-based models (EBMs), leveraging symmetric mappings in deep networks without auxiliary components. The framework models energy estimation and data restoration through the network's forward and backward passes, assigning low energy to unlabeled dataset samples and using gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting various pretext tasks. Experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "This paper introduces a novel approach to Radiance Field methods for novel-view synthesis that addresses the challenges of high visual quality, costly training, and real-time rendering of unbounded, complete scenes at 1080p resolution. The authors propose three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. The method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    },
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper introduces the concept of grokking, a sudden onset of perfect generalisation long after overfitting, on small, algorithmically generated datasets. The study also finds that generalization on smaller datasets requires more optimization. The authors suggest that such datasets are ideal for investigating the puzzling phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    },
    {
      "title": "Self-Paced Curriculum Learning",
      "url": "/research/2015/01/01/Self-Paced-Curriculum-Learning.html",
      "date": "January 1, 2015",
      "summary": "This paper introduces self-paced curriculum learning (SPCL), a unified framework combining curriculum learning (CL) and self-paced learning (SPL). SPCL leverages prior knowledge and ongoing learning progress through an optimization problem. It mimics collaborative instructor-student learning, exhibiting empirical advantages in two tasks."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article provides an extensive review of vision transformer techniques applied specifically to human action recognition. These action transformers are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also examines optimizing spatio-temporal attention in transformers for longer sequences and different learning strategies like self-supervised and zero-shot learning with their respective loss functions. Additionally, it assesses progress in benchmark evaluation scores and discusses challenges and future directions in this research area."
    },
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    },
    {
      "title": "Deep Residual Learning for Image Recognition",
      "url": "/research/2015/12/10/Deep-Residual-Learning-for-Image-Recognition.html",
      "date": "December 10, 2015",
      "summary": "We introduce a residual learning framework to train deeper neural networks effectively. By reformulating layers to learn residual functions relative to inputs, our approach facilitates optimization and achieves higher accuracy with increased depth. Evaluations on ImageNet demonstrate the effectiveness of residual networks up to 152 layers deep, outperforming previous architectures. Our method achieves 3.57% error on the ImageNet test set, winning 1st place in the ILSVRC 2015 classification task. Additionally, our deep representations lead to a 28% improvement on the COCO object detection dataset. In the ILSVRC & COCO 2015 competitions, our approach secured 1st place in ImageNet detection, localization, COCO detection, and segmentation tasks."
    },
    {
      "title": "A comprehensive review of Binary Neural Network",
      "url": "/research/2023/05/11/A-comprehensive-review-of-Binary-Neural-Network.html",
      "date": "May 11, 2023",
      "summary": "Deep learning (DL) is widely used in intelligent systems and real-life applications, but its deployment on computationally limited and energy-constrained devices requires efficient technologies like Binary Neural Networks (BNN). BNNs save significant storage, computation cost, and energy, making them ideal for small devices, despite the trade-offs in memory and performance. This article offers a comprehensive review of BNN developments, focusing on 1-bit activations and weights. It covers the evolution of BNN, from early models to advanced algorithms and design aspects, and addresses BNN optimization, deployment, computing architectures, and diverse applications. It also outlines potential research directions in BNN technology."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "Don't Decay the Learning Rate, Increase the Batch Size",
      "url": "/research/2017/11/01/Don't-Decay-the-Learning-Rate,-Increase-the-Batch-Size.html",
      "date": "November 1, 2017",
      "summary": "This study demonstrates that increasing the batch size during training achieves similar learning outcomes as the common practice of decaying the learning rate, applicable to stochastic gradient descent and its variants, including with momentum and Adam optimization. This approach not only matches test accuracies within the same number of epochs but also enhances parallelism and reduces training time due to fewer parameter updates. Efficiency can be further improved by adjusting the learning rate and batch size proportionally, and although increasing the momentum coefficient and scaling the batch size accordingly may slightly lower test accuracy, it enables the use of large batch training without needing to tune hyper-parameters. Using these methods, ResNet-50 was trained on ImageNet to a 76.1% validation accuracy in less than 30 minutes."
    },
    {
      "title": "Optimal learning rate schedules in high-dimensional non-convex optimization problems",
      "url": "/research/2022/02/09/Optimal-learning-rate-schedules-in-high-dimensional-non-convex-optimization-problems.html",
      "date": "February 9, 2022",
      "summary": "This paper explores the role of learning rate schedules in high-dimensional and non-convex optimization problems, focusing on Langevin optimization with a decaying learning rate. Analyzing models with Gaussian random functions on N-dimensional spheres, the study reveals that to accelerate optimization without getting trapped in saddles, a decay rate β < 1 is optimal, contrary to convex settings where β = 1 is preferred. Introducing a signal recovery component, the dynamics involve an exploration phase navigating through rough landscape parts and a convergence phase entering convex basins. It's found optimal to maintain a large learning rate during exploration to swiftly exit non-convex regions, then transition to β = 1 for rapid convergence to the solution. These findings are validated in a neural network regression task."
    },
    {
      "title": "Deep learning via Hessian-free optimization",
      "url": "/research/2010/06/21/Deep-learning-via-Hessian-free-optimization.html",
      "date": "June 21, 2010",
      "summary": "We present a novel 2nd-order optimization method inspired by the Hessian-free approach, applied to deep auto-encoders. Achieving superior results without pre-training compared to Hinton & Salakhutdinov (2006), our method is practical, user-friendly, scalable to large datasets, and versatile across various model classes. Additionally, we address the challenge of pathological curvature in deep learning, highlighting how our method effectively mitigates this issue."
    },
    {
      "title": "On Layer Normalization in the Transformer Architecture",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architecture.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is widely used in natural language processing. Training it typically requires a carefully designed learning rate warm-up stage, crucial for performance but slowing optimization and requiring more hyperparameter tuning. We provide theoretical insights into the necessity of the warm-up stage, demonstrating that the location of layer normalization affects gradient behavior. For Post-LN Transformers, with layer normalization between residual blocks, large gradients near the output layer at initialization make training unstable without warm-up. Conversely, for Pre-LN Transformers, with layer normalization inside residual blocks, gradients are well-behaved, suggesting the removal of the warm-up stage. Experimental results show that Pre-LN Transformers without warm-up achieve comparable performance with less training time and hyperparameter tuning."
    },
    {
      "title": "Model Generalization: A Sharpness Aware Optimization Perspective",
      "url": "/research/2022/08/14/Model-Generalization-A-Sharpness-Aware-Optimization-Perspective.html",
      "date": "August 14, 2022",
      "summary": "This study investigates the effectiveness of Sharpness-Aware Minimization (SAM) and adaptive Sharpness-Aware Minimization (ASAM) in enhancing model generalization. Through three experiments, we assess their impact from a sharpness-aware perspective. Results demonstrate that optimization techniques based on sharpness awareness can bolster model generalization. Furthermore, ASAM exhibits potential for enhancing generalization performance on un-normalized data, though additional research is required for validation."
    },
    {
      "title": "Penalizing Gradient Norm for Efficiently Improving Generalization in Deep Learning",
      "url": "/research/2022/06/26/Penalizing-Gradient-Norm-for-Efficiently-Improving-Generalization-in-Deep-Learning.html",
      "date": "June 26, 2022",
      "summary": "This paper proposes a method to enhance the generalization of deep neural networks (DNNs) by penalizing the gradient norm of the loss function during optimization. By constraining the gradient norm, the optimizers tend to find flat minima, improving generalization. We efficiently implement this method using first-order approximation within the gradient descent framework. Experimental results demonstrate improved generalization across various models and datasets. Additionally, we show that a recent method, sharpness-aware minimization, is a special case of our approach, with our method achieving new state-of-the-art performance on tested tasks."
    },
    {
      "title": "Symbolic Discovery of Optimization Algorithms",
      "url": "/research/2023/05/08/Symbolic-Discovery-of-Optimization-Algorithms.html",
      "date": "May 8, 2023",
      "summary": "We propose a method for algorithm discovery via program search, focusing on optimizing deep neural network training. Our approach, Lion (EvoLved Sign Momentum), is memory-efficient and achieves comparable or superior performance to widely-used optimizers such as Adam and Adafactor across various tasks. Specifically, Lion enhances accuracy on tasks like image classification and vision-language contrastive learning, while reducing training compute. Notably, Lion exhibits improved performance with larger batch sizes and requires smaller learning rates compared to Adam. Despite its effectiveness, Lion has limitations, which we analyze, and provide insights into its deployment and performance. Our implementation is publicly available and has been successfully utilized in Google's search ads CTR model."
    },
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    },
    {
      "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
      "url": "/research/2022/03/19/Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training.html",
      "date": "March 19, 2022",
      "summary": "Sharpness-Aware Minimization (SAM) enhances generalization by optimizing a neighborhood-based perturbed loss, but it doesn't always favor flat minima due to both sharp and flat minima having low perturbed losses. We introduce a new measure, the surrogate gap, reflecting the dominant Hessian eigenvalue at small neighborhood radii, which is simple to compute and can be minimized during training. We propose the Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), an advancement over SAM with minimal additional computational cost. GSAM employs a dual-step approach: first, minimizing the perturbed loss similar to SAM, and second, reducing the surrogate gap without affecting the perturbed loss to target regions with low loss and sharpness, thus achieving superior generalization. GSAM is theoretically robust, showing better convergence and empirical generalization improvements, notably a +3.2% gain over SAM and +5.4% over AdamW in ImageNet accuracy for ViT-B/32."
    },
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    },
    {
      "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
      "url": "/research/2024/02/28/CL-MAE:-Curriculum-Learned-Masked-Autoencoders.html",
      "date": "February 28, 2024",
      "summary": "This paper proposes a curriculum learning approach that updates the masking strategy of Masked Auto Encoders (MAE) to progressively increase the complexity of the self-supervised reconstruction task. To achieve this, a novel learnable masking module is introduced, capable of generating masks of varying complexities, and integrated into masked autoencoders (MAE). This module is jointly trained with the MAE, adjusting its behavior from partner (optimizing the same reconstruction loss) to adversary (optimizing the opposite loss), with a smooth transition regulated by a factor multiplied with the reconstruction loss. This training procedure creates an easy-to-hard curriculum. The Curriculum-Learned Masked Autoencoder (CL-MAE) is trained on ImageNet and demonstrates superior representation learning compared to MAE. Empirical results on five downstream tasks confirm that curriculum learning can successfully self-supervise masked autoencoders."
    },
    {
      "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
      "url": "/research/2024/02/27/The-Era-of-1-bit-LLMs-All-Large-Language-Models-are-in-1.58-Bits.html",
      "date": "February 27, 2024",
      "summary": "Recent research introduces BitNet b1.58, a ternary (values of -1, 0, 1) 1-bit Large Language Model (LLM) that matches traditional full-precision LLMs in performance and perplexity but is significantly more efficient in terms of latency, memory, throughput, and energy. This work establishes a new scaling law and training approach for future high-performance, cost-effective LLMs, while also facilitating the design of specialized hardware optimized for 1-bit LLMs."
    },
    {
      "title": "The loss landscape of deep linear neural networks: a second-order analysis",
      "url": "/research/2022/03/11/The-loss-landscape-of-deep-linear-neural-networks-a-second-order-analysis.html",
      "date": "March 11, 2022",
      "summary": "This study investigates the optimization landscape of deep linear neural networks with square loss, focusing on the role of non-strict saddle points in algorithm dynamics, which has been previously understudied. the authors conduct a comprehensive second-order analysis, identifying global minimizers, strict, and non-strict saddle points among all critical points, alongside their critical values. Our findings, based on conditions related to the ranks of partial matrix products, contribute insights into global convergence and implicit regularization observed in optimizing linear neural networks. Additionally, the authors offer an explicit parameterization of global minimizers and identify extensive sets of strict and non-strict saddle points."
    },
    {
      "title": "Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes",
      "url": "/research/2017/11/28/Towards-Understanding-Generalization-of-Deep-Learning-Perspective-of-Loss-Landscapes.html",
      "date": "November 28, 2017",
      "summary": "This study examines why deep learning models, despite having more parameters than training samples, generalize well. It highlights the importance of the loss function's landscape, showing that areas leading to better generalization (good minima) are more prevalent than those leading to poor outcomes. This predominance facilitates the convergence of optimization methods to these beneficial minima. The research provides theoretical backing by analyzing 2-layer neural networks, noting that solutions with better generalization have a smaller Hessian matrix norm. For deeper networks, extensive numerical data supports these conclusions."
    },
    {
      "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum",
      "url": "/research/2023/09/30/When-to-Learn-What:-Model-Adaptive-Data-Augmentation-Curriculum.html",
      "date": "September 30, 2023",
      "summary": "Data augmentation (DA) improves neural network generalization by enforcing invariances and symmetries to predefined transformations applied to input data. However, fixed augmentation policies affect samples differently at various training stages, and existing approaches cannot adapt policies to individual samples and the training model. This paper proposes Model-Adaptive Data Augmentation (MADAug) which trains an augmentation policy network to determine when to learn what. Unlike previous work, MADAug selects augmentation operators for each input image with a model-adaptive policy that varies between training stages, creating an optimized data augmentation curriculum. The policy is trained using a bi-level optimization scheme to minimize validation-set loss of a model trained with policy-produced augmentations. Extensive evaluations on multiple image classification tasks and network architectures show that MADAug outperforms or matches existing DA approaches, enhances fairness by improving all classes, particularly the difficult ones, and performs better when transferred to fine-grained datasets. Additionally, the auto-optimized policy in MADAug gradually increases perturbations, forming an easy-to-hard curriculum."
    }
  ],
  "imrpove": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "ADAM is an optimization algorithm that imrpoves stochastic gradient decent with first and second order momentum terms. ADAM is easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    }
  ],
  "gradient": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "ADAM is an optimization algorithm that imrpoves stochastic gradient decent with first and second order momentum terms. ADAM is easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "Dropout Reduces Underfitting",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. Authors then introduce two dropout schedules: early dropout which prevents underfitting, and late dropout which prevents overfitting."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "This paper investigates the importance of the warm-up stage in training transformers, and the impact of layer normalization placement. Using mean field theory, the authors demonstrate that the original Post-LN Transformer's design results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "This paper introduces Online Normalization, a novel method for normalizing neural network hidden activations that offers a batch-independent alternative with comparable accuracy to Batch Normalization. Online Normalization addresses Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. The method is applicable to recurrent, fully connected networks, and those with high activation memory requirements. The authors demonstrate its effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    },
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    },
    {
      "title": "Don't Decay the Learning Rate, Increase the Batch Size",
      "url": "/research/2017/11/01/Don't-Decay-the-Learning-Rate,-Increase-the-Batch-Size.html",
      "date": "November 1, 2017",
      "summary": "This study demonstrates that increasing the batch size during training achieves similar learning outcomes as the common practice of decaying the learning rate, applicable to stochastic gradient descent and its variants, including with momentum and Adam optimization. This approach not only matches test accuracies within the same number of epochs but also enhances parallelism and reduces training time due to fewer parameter updates. Efficiency can be further improved by adjusting the learning rate and batch size proportionally, and although increasing the momentum coefficient and scaling the batch size accordingly may slightly lower test accuracy, it enables the use of large batch training without needing to tune hyper-parameters. Using these methods, ResNet-50 was trained on ImageNet to a 76.1% validation accuracy in less than 30 minutes."
    },
    {
      "title": "Large Batch Training of Convolutional Networks",
      "url": "/research/2017/09/13/Large-Batch-Training-of-Convolutional-Networks.html",
      "date": "September 13, 2017",
      "summary": "To expedite the training of large convolutional networks, computational units are added and trained with data-parallel synchronous Stochastic Gradient Descent (SGD) across units, increasing batch size with node count. However, larger batch sizes can reduce model accuracy. The existing method of large batch training—linear learning rate scaling with warm-up—is not universally effective and may cause training divergence. To address these challenges, we introduce a novel training algorithm, Layer-wise Adaptive Rate Scaling (LARS), enabling us to train Alexnet with a batch size of 8K and Resnet-50 with a batch size of 32K, without compromising accuracy."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "url": "/research/2014/02/19/Exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks.html",
      "date": "February 19, 2014",
      "summary": "This study aims to enhance our comprehension of deep learning dynamics by analyzing deep linear neural networks. Despite their linear input-output mapping, these networks display nonlinear gradient descent dynamics, resulting in phenomena like prolonged plateaus and swift transitions to improved solutions. Through analytical exploration, we reveal that as network depth increases indefinitely, learning speed can remain finite under specific initial conditions. Unsupervised pretraining, under certain data conditions, can discover these favorable initial conditions, unlike random Gaussian initializations. We introduce a novel class of random orthogonal initial conditions that, akin to unsupervised pretraining, enable depth-independent learning. These conditions also ensure gradient propagation in deep nonlinear networks, particularly when operating at the edge of chaos."
    },
    {
      "title": "On Layer Normalization in the Transformer Architecture",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architecture.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is widely used in natural language processing. Training it typically requires a carefully designed learning rate warm-up stage, crucial for performance but slowing optimization and requiring more hyperparameter tuning. We provide theoretical insights into the necessity of the warm-up stage, demonstrating that the location of layer normalization affects gradient behavior. For Post-LN Transformers, with layer normalization between residual blocks, large gradients near the output layer at initialization make training unstable without warm-up. Conversely, for Pre-LN Transformers, with layer normalization inside residual blocks, gradients are well-behaved, suggesting the removal of the warm-up stage. Experimental results show that Pre-LN Transformers without warm-up achieve comparable performance with less training time and hyperparameter tuning."
    },
    {
      "title": "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization",
      "url": "/research/2023/07/04/Gradient-Norm-Aware-Minimization-Seeks-First-Order-Flatness-and-Improves-Generalization.html",
      "date": "July 4, 2023",
      "summary": "Recent advancements have highlighted the effectiveness of flat minima in enhancing generalization, particularly through Sharpness-Aware Minimization (SAM). However, existing definitions of flatness, such as zeroth-order flatness, have limitations in discerning between minima with low and high generalization errors. To address this, we propose first-order flatness, which considers maximal gradient norm within a perturbation radius. We introduce Gradient norm Aware Minimization (GAM) as a novel training approach to achieve uniformly small curvature across all directions. Experimental results demonstrate GAM's ability to enhance generalization compared to standard optimizers like SGD and AdamW across various datasets and networks. Moreover, GAM facilitates SAM in identifying flatter minima, leading to improved generalization."
    },
    {
      "title": "Penalizing Gradient Norm for Efficiently Improving Generalization in Deep Learning",
      "url": "/research/2022/06/26/Penalizing-Gradient-Norm-for-Efficiently-Improving-Generalization-in-Deep-Learning.html",
      "date": "June 26, 2022",
      "summary": "This paper proposes a method to enhance the generalization of deep neural networks (DNNs) by penalizing the gradient norm of the loss function during optimization. By constraining the gradient norm, the optimizers tend to find flat minima, improving generalization. We efficiently implement this method using first-order approximation within the gradient descent framework. Experimental results demonstrate improved generalization across various models and datasets. Additionally, we show that a recent method, sharpness-aware minimization, is a special case of our approach, with our method achieving new state-of-the-art performance on tested tasks."
    },
    {
      "title": "AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks",
      "url": "/research/2023/03/01/AdaSAM-Boosting-Sharpness-Aware-Minimization-with-Adaptive-Learning-Rate-and-Momentum-for-Training-Deep-Neural-Networks.html",
      "date": "March 1, 2023",
      "summary": "The Sharpness Aware Minimization (SAM) optimizer, known for enhancing the generalization of deep neural networks by introducing extra perturbation steps, is further developed into AdaSAM by integrating adaptive learning rates and momentum acceleration. Although AdaSAM has been empirically applied to large-scale networks, a theoretical understanding of its performance, considering the complexity of its components, was lacking. This study presents a theoretical analysis of AdaSAM's convergence in stochastic non-convex settings, demonstrating a convergence rate of O(1/√bT) that scales linearly with mini-batch size. By introducing a delayed second-order momentum term, the study successfully decouples and analyzes the intertwined effects of stochastic gradients, adaptive learning rates, and perturbations. This is the first work to offer a detailed convergence rate for SAM with adaptive mechanisms. Experimental results on various NLP tasks indicate AdaSAM's superior performance over SGD, AMSGrad, and SAM optimizers."
    },
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    },
    {
      "title": "Fine-tuning with Very Large Dropout",
      "url": "/research/2024/03/01/Fine-tuning-with-Very-Large-Dropout.html",
      "date": "March 1, 2024",
      "summary": "This study challenges the notion that machine learning practices assume training and testing data have the same distribution. It explores the effectiveness of high dropout rates, as opposed to ensemble techniques, in developing rich data representations suitable for multiple distribution scenarios. These representations surpass those achieved by traditional in-distribution performance regularization and the implicit sparsity induced by common stochastic gradient methods. While training deep networks from scratch with high dropout rates is impractical, fine-tuning pre-trained models under these conditions is feasible and yields better out-of-distribution performance than ensembles and model averaging techniques like model soups. This finding is significant due to the growing relevance of fine-tuning with large pre-trained models, offering insights into the nature of rich representations and the linear characteristics of fine-tuning large networks with small datasets."
    },
    {
      "title": "How Does Sharpness-Aware Minimization Minimize Sharpness?",
      "url": "/research/2023/01/05/How-Does-Sharpness-Aware-Minimization-Minimize-Sharpness.html",
      "date": "January 5, 2023",
      "summary": "Sharpness-Aware Minimization (SAM) enhances deep neural networks' generalization across various settings. Although SAM aims to penalize model sharpness using a computationally efficient approach, its exact operational mechanism and the sharpness notion it regularizes remain unclear, partly due to differing sharpness concepts used in its theoretical framework and empirical validation. This study identifies the precise sharpness concept SAM regulates and explains its mechanism. It reveals that the combined effect of SAM's two-step approximations, despite being individually misleading, correctly improves generalization when using full-batch gradients. Moreover, we demonstrate that the stochastic SAM version indeed regularizes a third sharpness notion, aligning more closely with practical performance. This effectiveness is attributed to the gradient's alignment with the Hessian's top eigenvector under SAM."
    },
    {
      "title": "SGDR: Stochastic Gradient Descent With Warm Restarts",
      "url": "/research/2017/05/02/SGDR-Stochastic-Gradient-Descent-With-Warm-Restarts.html",
      "date": "May 2, 2017",
      "summary": "This paper introduces a warm restart technique for stochastic gradient descent aimed at enhancing anytime performance in deep neural network training. It showcases empirical performance improvements on CIFAR-10 and CIFAR-100 datasets, achieving state-of-the-art results with 3.14% and 16.21% error rates, respectively. Additionally, the technique's benefits are demonstrated on an EEG dataset and a downsampled ImageNet dataset."
    }
  ],
  "order": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "ADAM is an optimization algorithm that imrpoves stochastic gradient decent with first and second order momentum terms. ADAM is easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, enhances vision tasks using Vision transformers by masking and reconstructing parts of an image. The compatibility of MIM with CNNs and its operational principle are unclear. this study reveals that MIM improves generalized feature extraction through middle-order patch interactions and introduces Architecture-Agnostic Masked Image Modeling (A2MIM), compatible with both Transformers and CNNs. Extensive testing demonstrates A2MIM's ability to enhance representation learning and transferability to various tasks without specialized modifications."
    },
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "This paper introduces an algorithm that identifies simple and highly generalizable neural networks by searching for extensive flat minima regions in the error function, where the error rate is relatively stable. Such flat minima are associated with lower overfitting risks based on minimum description length (MDL) principles. Despite requiring second-order derivative calculations, the algorithm has a complexity level comparable to backpropagation. When tested on feedforward and recurrent networks, as well as stock market prediction tasks, this algorithm outperformed traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization",
      "url": "/research/2023/07/04/Gradient-Norm-Aware-Minimization-Seeks-First-Order-Flatness-and-Improves-Generalization.html",
      "date": "July 4, 2023",
      "summary": "Recent advancements have highlighted the effectiveness of flat minima in enhancing generalization, particularly through Sharpness-Aware Minimization (SAM). However, existing definitions of flatness, such as zeroth-order flatness, have limitations in discerning between minima with low and high generalization errors. To address this, we propose first-order flatness, which considers maximal gradient norm within a perturbation radius. We introduce Gradient norm Aware Minimization (GAM) as a novel training approach to achieve uniformly small curvature across all directions. Experimental results demonstrate GAM's ability to enhance generalization compared to standard optimizers like SGD and AdamW across various datasets and networks. Moreover, GAM facilitates SAM in identifying flatter minima, leading to improved generalization."
    },
    {
      "title": "Penalizing Gradient Norm for Efficiently Improving Generalization in Deep Learning",
      "url": "/research/2022/06/26/Penalizing-Gradient-Norm-for-Efficiently-Improving-Generalization-in-Deep-Learning.html",
      "date": "June 26, 2022",
      "summary": "This paper proposes a method to enhance the generalization of deep neural networks (DNNs) by penalizing the gradient norm of the loss function during optimization. By constraining the gradient norm, the optimizers tend to find flat minima, improving generalization. We efficiently implement this method using first-order approximation within the gradient descent framework. Experimental results demonstrate improved generalization across various models and datasets. Additionally, we show that a recent method, sharpness-aware minimization, is a special case of our approach, with our method achieving new state-of-the-art performance on tested tasks."
    },
    {
      "title": "AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks",
      "url": "/research/2023/03/01/AdaSAM-Boosting-Sharpness-Aware-Minimization-with-Adaptive-Learning-Rate-and-Momentum-for-Training-Deep-Neural-Networks.html",
      "date": "March 1, 2023",
      "summary": "The Sharpness Aware Minimization (SAM) optimizer, known for enhancing the generalization of deep neural networks by introducing extra perturbation steps, is further developed into AdaSAM by integrating adaptive learning rates and momentum acceleration. Although AdaSAM has been empirically applied to large-scale networks, a theoretical understanding of its performance, considering the complexity of its components, was lacking. This study presents a theoretical analysis of AdaSAM's convergence in stochastic non-convex settings, demonstrating a convergence rate of O(1/√bT) that scales linearly with mini-batch size. By introducing a delayed second-order momentum term, the study successfully decouples and analyzes the intertwined effects of stochastic gradients, adaptive learning rates, and perturbations. This is the first work to offer a detailed convergence rate for SAM with adaptive mechanisms. Experimental results on various NLP tasks indicate AdaSAM's superior performance over SGD, AMSGrad, and SAM optimizers."
    },
    {
      "title": "From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning",
      "url": "/research/2023/03/16/From-MNIST-to-ImageNet-and-Back:-Benchmarking-Continual-Curriculum-Learning.html",
      "date": "March 16, 2023",
      "summary": "Continual learning (CL) is a promising trend in machine learning, aiming to develop robust models and strategies for dynamic environments by incorporating new knowledge while retaining past knowledge. CL research is fragmented, with various protocols, tasks, datasets, and metrics, often not reflecting real-world complexity and tailored to specific strategies. This work addresses this gap by introducing two novel CL benchmarks with heterogeneous tasks from six image datasets, varying in complexity and quality. These benchmarks fairly evaluate state-of-the-art CL strategies in scenarios closer to real-world conditions. The benchmarks present tasks in both increasing and decreasing complexity, assessing models' ability to exploit task structure. The work emphasizes a rigorous, reproducible evaluation protocol for measuring generalization and memory retention in models. Experimental results show that popular CL strategies perform poorly on these benchmarks, exhibit high forgetting, and struggle with curriculum task ordering. These findings underscore the need for rigorous comparisons and the development of new CL strategies for complex scenarios."
    },
    {
      "title": "Self-Paced Absolute Learning Progress as a Regularized Approach to Curriculum Learning",
      "url": "/research/2023/06/09/Self-Paced-Absolute-Learning-Progress-as-a-Regularized-Approach-to-Curriculum-Learning.html",
      "date": "June 9, 2023",
      "summary": "Reinforcement Learning's usability is limited by its high computation times. Curriculum Reinforcement Learning accelerates learning by ordering tasks from simple to hard. Curricula based on Absolute Learning Progress (ALP) have shown success in various environments but waste computation on redundant tasks. This issue is addressed by introducing Self-Paced Absolute Learning Progress (SPALP), a regularization method inspired by Self-Paced Learning. Evaluated in three environments, SPALP achieves performance comparable to ALP in all cases and reaches it faster in two. Further improvements in SPALP's efficiency and performance are also discussed."
    },
    {
      "title": "The loss landscape of deep linear neural networks: a second-order analysis",
      "url": "/research/2022/03/11/The-loss-landscape-of-deep-linear-neural-networks-a-second-order-analysis.html",
      "date": "March 11, 2022",
      "summary": "This study investigates the optimization landscape of deep linear neural networks with square loss, focusing on the role of non-strict saddle points in algorithm dynamics, which has been previously understudied. the authors conduct a comprehensive second-order analysis, identifying global minimizers, strict, and non-strict saddle points among all critical points, alongside their critical values. Our findings, based on conditions related to the ranks of partial matrix products, contribute insights into global convergence and implicit regularization observed in optimizing linear neural networks. Additionally, the authors offer an explicit parameterization of global minimizers and identify extensive sets of strict and non-strict saddle points."
    }
  ],
  "momentum": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "ADAM is an optimization algorithm that imrpoves stochastic gradient decent with first and second order momentum terms. ADAM is easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a self-supervised method that adds a contrastive loss to the Masked Autoencoder. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "This paper shows self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs or convnets. Also proposes a modification to BYOL self-supervised learning with a momentum encoder, multi-crop training, and feature sharping for the teacher network."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "Don't Decay the Learning Rate, Increase the Batch Size",
      "url": "/research/2017/11/01/Don't-Decay-the-Learning-Rate,-Increase-the-Batch-Size.html",
      "date": "November 1, 2017",
      "summary": "This study demonstrates that increasing the batch size during training achieves similar learning outcomes as the common practice of decaying the learning rate, applicable to stochastic gradient descent and its variants, including with momentum and Adam optimization. This approach not only matches test accuracies within the same number of epochs but also enhances parallelism and reduces training time due to fewer parameter updates. Efficiency can be further improved by adjusting the learning rate and batch size proportionally, and although increasing the momentum coefficient and scaling the batch size accordingly may slightly lower test accuracy, it enables the use of large batch training without needing to tune hyper-parameters. Using these methods, ResNet-50 was trained on ImageNet to a 76.1% validation accuracy in less than 30 minutes."
    },
    {
      "title": "Symbolic Discovery of Optimization Algorithms",
      "url": "/research/2023/05/08/Symbolic-Discovery-of-Optimization-Algorithms.html",
      "date": "May 8, 2023",
      "summary": "We propose a method for algorithm discovery via program search, focusing on optimizing deep neural network training. Our approach, Lion (EvoLved Sign Momentum), is memory-efficient and achieves comparable or superior performance to widely-used optimizers such as Adam and Adafactor across various tasks. Specifically, Lion enhances accuracy on tasks like image classification and vision-language contrastive learning, while reducing training compute. Notably, Lion exhibits improved performance with larger batch sizes and requires smaller learning rates compared to Adam. Despite its effectiveness, Lion has limitations, which we analyze, and provide insights into its deployment and performance. Our implementation is publicly available and has been successfully utilized in Google's search ads CTR model."
    },
    {
      "title": "AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks",
      "url": "/research/2023/03/01/AdaSAM-Boosting-Sharpness-Aware-Minimization-with-Adaptive-Learning-Rate-and-Momentum-for-Training-Deep-Neural-Networks.html",
      "date": "March 1, 2023",
      "summary": "The Sharpness Aware Minimization (SAM) optimizer, known for enhancing the generalization of deep neural networks by introducing extra perturbation steps, is further developed into AdaSAM by integrating adaptive learning rates and momentum acceleration. Although AdaSAM has been empirically applied to large-scale networks, a theoretical understanding of its performance, considering the complexity of its components, was lacking. This study presents a theoretical analysis of AdaSAM's convergence in stochastic non-convex settings, demonstrating a convergence rate of O(1/√bT) that scales linearly with mini-batch size. By introducing a delayed second-order momentum term, the study successfully decouples and analyzes the intertwined effects of stochastic gradients, adaptive learning rates, and perturbations. This is the first work to offer a detailed convergence rate for SAM with adaptive mechanisms. Experimental results on various NLP tasks indicate AdaSAM's superior performance over SGD, AMSGrad, and SAM optimizers."
    }
  ],
  "term": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "ADAM is an optimization algorithm that imrpoves stochastic gradient decent with first and second order momentum terms. ADAM is easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "This paper explores Gated Linear Units (GLUs), which involve the component-wise product of two linear projections, with one undergoing a sigmoid function. The authors investigate GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers. They find that certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    },
    {
      "title": "Language Modeling with Gated Convolutional Networks",
      "url": "/research/2017/09/08/Language-Modeling-with-Gated-Convolutional-Networks.html",
      "date": "September 8, 2017",
      "summary": "This paper introduces a novel language modeling approach using stacked convolutions that enable efficient parallel processing, in contrast to the traditionally used recurrent neural networks known for handling unbounded context. The proposed method features a simplified gating mechanism that outperforms previous models and demonstrates superior performance on the WikiText-103 benchmark, showing its capability to manage long-term dependencies. It also delivers competitive results on the Google Billion Words benchmark and significantly reduces sentence scoring latency compared to recurrent models. This marks the first instance of a non-recurrent model achieving comparable success to strong recurrent models on major language tasks."
    },
    {
      "title": "AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks",
      "url": "/research/2023/03/01/AdaSAM-Boosting-Sharpness-Aware-Minimization-with-Adaptive-Learning-Rate-and-Momentum-for-Training-Deep-Neural-Networks.html",
      "date": "March 1, 2023",
      "summary": "The Sharpness Aware Minimization (SAM) optimizer, known for enhancing the generalization of deep neural networks by introducing extra perturbation steps, is further developed into AdaSAM by integrating adaptive learning rates and momentum acceleration. Although AdaSAM has been empirically applied to large-scale networks, a theoretical understanding of its performance, considering the complexity of its components, was lacking. This study presents a theoretical analysis of AdaSAM's convergence in stochastic non-convex settings, demonstrating a convergence rate of O(1/√bT) that scales linearly with mini-batch size. By introducing a delayed second-order momentum term, the study successfully decouples and analyzes the intertwined effects of stochastic gradients, adaptive learning rates, and perturbations. This is the first work to offer a detailed convergence rate for SAM with adaptive mechanisms. Experimental results on various NLP tasks indicate AdaSAM's superior performance over SGD, AMSGrad, and SAM optimizers."
    },
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    },
    {
      "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
      "url": "/research/2024/02/27/The-Era-of-1-bit-LLMs-All-Large-Language-Models-are-in-1.58-Bits.html",
      "date": "February 27, 2024",
      "summary": "Recent research introduces BitNet b1.58, a ternary (values of -1, 0, 1) 1-bit Large Language Model (LLM) that matches traditional full-precision LLMs in performance and perplexity but is significantly more efficient in terms of latency, memory, throughput, and energy. This work establishes a new scaling law and training approach for future high-performance, cost-effective LLMs, while also facilitating the design of specialized hardware optimized for 1-bit LLMs."
    },
    {
      "title": "VideoMamba: State Space Model for Efficient Video Understanding",
      "url": "/research/2024/03/12/VideoMamba-State-Space-Model-for-Efficient-Video-Understanding.html",
      "date": "March 12, 2024",
      "summary": "This work introduces VideoMamba, an innovative approach that addresses local redundancy and global dependencies in video understanding by adapting the Mamba framework to video analysis. VideoMamba outperforms existing 3D convolutional neural networks and video transformers through its linear-complexity operator, which facilitates efficient long-term modeling for high-resolution, long-duration videos. Its effectiveness is demonstrated across four main areas: scalability in the visual domain without needing extensive dataset pretraining, thanks to a novel self-distillation technique; the ability to recognize short-term actions with fine-grained motion differences; superior performance in long-term video understanding compared to traditional feature-based models; and robust compatibility with multiple modalities, enhancing multi-modal video analysis. VideoMamba establishes a new standard for comprehensive and efficient video understanding."
    },
    {
      "title": "The Platonic Representation Hypothesis",
      "url": "/research/2024/05/13/The-Platonic-Representation-Hypothesis.html",
      "date": "May 13, 2024",
      "summary": "This paper argues that AI model representations, especially in deep networks, are converging. It surveys examples in the literature showing how neural network representations align over time and across domains. It demonstrates convergence across data modalities, noting that as vision and language models grow, they measure distances between data points similarly. The authors hypothesize that this convergence leads to a shared statistical model of reality, termed the platonic representation and discuss possible selective pressures toward it. The paper also addresses the implications, limitations, and counterexamples to this trend."
    },
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "memory": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "ADAM is an optimization algorithm that imrpoves stochastic gradient decent with first and second order momentum terms. ADAM is easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "This paper introduces PatchDropout, a simple training technique for Vision Transformers ViT that drops random image patches, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a Vision archetecture that uses bidirectional Mamba blocks, challenging the necessity of self-attention in vision. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient."
    },
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "This paper introduces Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperformed CLIP models on various downstream tasks. The speedup from this method enables experimentation with larger model sizes, more data, or longer training periods."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces TinyViT, a series of compact and efficient vision transformers (ViTs) designed for devices with limited resources. The authors employ a fast distillation framework to transfer knowledge from large pretrained models to smaller ones during pretraining, using sparsified logits from teacher models to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. With increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters, and demonstrates strong transferability across various downstream tasks."
    },
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/research/2022/10/10/Turbo-Training-with-Token-Dropout.html",
      "date": "October 10, 2022",
      "summary": "This paper introduces Turbo training, an efficient method for training Transformers on video-related tasks. The authors present a simple yet versatile training paradigm applicable to multiple video tasks and demonstrate its effectiveness across action classification, video-language representation learning, and long-video activity classification. Turbo training achieves competitive performance with up to 4× speed-up and reduced memory usage, enabling long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous resource-intensive methods.."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "This research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. The authors validate these approaches using the ImageNet-1k dataset and confirm their findings with the ImageNet-v2 test set, evaluating transfer performance across six additional datasets."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "This paper introduces Online Normalization, a novel method for normalizing neural network hidden activations that offers a batch-independent alternative with comparable accuracy to Batch Normalization. Online Normalization addresses Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. The method is applicable to recurrent, fully connected networks, and those with high activation memory requirements. The authors demonstrate its effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    },
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "This paper introduces QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU while maintaining 16-bit task performance. QLORA utilizes 4-bit quantized language models and Low Rank Adapters (LoRA) and incorporates several memory-saving innovations without compromising performance. The authors' best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. The authors applied QLORA to finetune over 1,000 models and analyzed performance across various datasets, model types, and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Their findings suggest that GPT-4 evaluations are a viable substitute for human assessments and question the reliability of current chatbot benchmarks. The authors make their models and 4-bit training code public."
    },
    {
      "title": "A comprehensive review of Binary Neural Network",
      "url": "/research/2023/05/11/A-comprehensive-review-of-Binary-Neural-Network.html",
      "date": "May 11, 2023",
      "summary": "Deep learning (DL) is widely used in intelligent systems and real-life applications, but its deployment on computationally limited and energy-constrained devices requires efficient technologies like Binary Neural Networks (BNN). BNNs save significant storage, computation cost, and energy, making them ideal for small devices, despite the trade-offs in memory and performance. This article offers a comprehensive review of BNN developments, focusing on 1-bit activations and weights. It covers the evolution of BNN, from early models to advanced algorithms and design aspects, and addresses BNN optimization, deployment, computing architectures, and diverse applications. It also outlines potential research directions in BNN technology."
    },
    {
      "title": "FNet: Mixing Tokens with Fourier Transforms",
      "url": "/research/2022/05/26/FNet-Mixing-Tokens-with-Fourier-Transforms.html",
      "date": "May 26, 2022",
      "summary": "We demonstrate that Transformer encoder architectures can be accelerated with minimal impact on accuracy by substituting self-attention sublayers with simple linear transformations. Remarkably, using a standard Fourier Transform instead of the self-attention sublayer in a Transformer encoder achieves 92-97% of BERT models' accuracy on the GLUE benchmark, while training 80% faster on GPUs and 70% faster on TPUs for standard 512 input lengths. Our FNet model significantly outperforms in speed at longer input lengths, matching the accuracy of the most accurate models on the Long Range Arena benchmark and surpassing the fastest models in speed across all sequence lengths on GPUs and shorter lengths on TPUs. Additionally, FNet is lightweight in memory use and exceptionally efficient in smaller sizes, outperforming Transformer models under the same speed and accuracy constraints."
    },
    {
      "title": "From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning",
      "url": "/research/2023/03/16/From-MNIST-to-ImageNet-and-Back:-Benchmarking-Continual-Curriculum-Learning.html",
      "date": "March 16, 2023",
      "summary": "Continual learning (CL) is a promising trend in machine learning, aiming to develop robust models and strategies for dynamic environments by incorporating new knowledge while retaining past knowledge. CL research is fragmented, with various protocols, tasks, datasets, and metrics, often not reflecting real-world complexity and tailored to specific strategies. This work addresses this gap by introducing two novel CL benchmarks with heterogeneous tasks from six image datasets, varying in complexity and quality. These benchmarks fairly evaluate state-of-the-art CL strategies in scenarios closer to real-world conditions. The benchmarks present tasks in both increasing and decreasing complexity, assessing models' ability to exploit task structure. The work emphasizes a rigorous, reproducible evaluation protocol for measuring generalization and memory retention in models. Experimental results show that popular CL strategies perform poorly on these benchmarks, exhibit high forgetting, and struggle with curriculum task ordering. These findings underscore the need for rigorous comparisons and the development of new CL strategies for complex scenarios."
    },
    {
      "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
      "url": "/research/2024/02/27/The-Era-of-1-bit-LLMs-All-Large-Language-Models-are-in-1.58-Bits.html",
      "date": "February 27, 2024",
      "summary": "Recent research introduces BitNet b1.58, a ternary (values of -1, 0, 1) 1-bit Large Language Model (LLM) that matches traditional full-precision LLMs in performance and perplexity but is significantly more efficient in terms of latency, memory, throughput, and energy. This work establishes a new scaling law and training approach for future high-performance, cost-effective LLMs, while also facilitating the design of specialized hardware optimized for 1-bit LLMs."
    },
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "adapt": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "ADAM is an optimization algorithm that imrpoves stochastic gradient decent with first and second order momentum terms. ADAM is easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activitivations can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "This research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. The authors validate these approaches using the ImageNet-1k dataset and confirm their findings with the ImageNet-v2 test set, evaluating transfer performance across six additional datasets."
    },
    {
      "title": "DeiT III: Revenge of the ViT",
      "url": "/research/2023/01/18/DeiT-III-Revenge-of-the-ViT.html",
      "date": "January 18, 2023",
      "summary": "This paper explores the supervised training of Vision Transformers (ViTs) using a simplified training approach adapted from ResNet-50 that includes a novel data-augmentation method with just 3 augmentations. The study demonstrates that this method significantly improves ViTs' performance in image classification, transfer learning, and semantic segmentation over previous supervised training techniques. Moreover, it shows ViTs' performance can match newer architectures, providing a new benchmark for evaluating self-supervised methods on ViTs."
    },
    {
      "title": "ImageNet-D: A new challenging robustness dataset inspired by domain adaptation",
      "url": "/research/2023/05/05/ImageNet-D-A-new-challenging-robustness-dataset-inspired-by-domain-adaptation.html",
      "date": "May 5, 2023",
      "summary": "We introduce ImageNet-D, a novel dataset designed to evaluate the robustness of ImageNet-trained models across various domains. With six distinct domains including Real, Painting, Clipart, Sketch, Infograph, and Quick-draw, ImageNet-D challenges even state-of-the-art models, revealing interpretable errors. For instance, our leading EfficientNet-L2 model exhibits a significant performance decrease, dropping from 11.6% on clean ImageNet to 29.2% on the Real domain."
    },
    {
      "title": "Suppressing Pink Elephants with Direct Principle Feedback",
      "url": "/research/2024/02/13/Suppressing-Pink-Elephants-with-Direct-Principle-Feedback.html",
      "date": "February 13, 2024",
      "summary": "Existing methods for controlling language models focus on training desired behaviors, but often lack the flexibility needed for diverse applications. The authors address this with the Pink Elephant Problem, demonstrating the need for language models to adapt to different contexts by avoiding certain topics (the Pink Elephant) in favor of others (Grey Elephant). The authors introduce Direct Principle Feedback (DPF), an adaptation of Constitutional AI that improves control by directly applying critiques and revisions without ranking responses. Our study shows that a 13B LLaMA 2 model fine-tuned with DPF on a synthetic dataset outperforms existing models and matches GPT-4 in managing the Pink Elephant Problem."
    },
    {
      "title": "VideoMamba: State Space Model for Efficient Video Understanding",
      "url": "/research/2024/03/12/VideoMamba-State-Space-Model-for-Efficient-Video-Understanding.html",
      "date": "March 12, 2024",
      "summary": "This work introduces VideoMamba, an innovative approach that addresses local redundancy and global dependencies in video understanding by adapting the Mamba framework to video analysis. VideoMamba outperforms existing 3D convolutional neural networks and video transformers through its linear-complexity operator, which facilitates efficient long-term modeling for high-resolution, long-duration videos. Its effectiveness is demonstrated across four main areas: scalability in the visual domain without needing extensive dataset pretraining, thanks to a novel self-distillation technique; the ability to recognize short-term actions with fine-grained motion differences; superior performance in long-term video understanding compared to traditional feature-based models; and robust compatibility with multiple modalities, enhancing multi-modal video analysis. VideoMamba establishes a new standard for comprehensive and efficient video understanding."
    },
    {
      "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum",
      "url": "/research/2023/09/30/When-to-Learn-What:-Model-Adaptive-Data-Augmentation-Curriculum.html",
      "date": "September 30, 2023",
      "summary": "Data augmentation (DA) improves neural network generalization by enforcing invariances and symmetries to predefined transformations applied to input data. However, fixed augmentation policies affect samples differently at various training stages, and existing approaches cannot adapt policies to individual samples and the training model. This paper proposes Model-Adaptive Data Augmentation (MADAug) which trains an augmentation policy network to determine when to learn what. Unlike previous work, MADAug selects augmentation operators for each input image with a model-adaptive policy that varies between training stages, creating an optimized data augmentation curriculum. The policy is trained using a bi-level optimization scheme to minimize validation-set loss of a model trained with policy-produced augmentations. Extensive evaluations on multiple image classification tasks and network architectures show that MADAug outperforms or matches existing DA approaches, enhances fairness by improving all classes, particularly the difficult ones, and performs better when transferred to fine-grained datasets. Additionally, the auto-optimized policy in MADAug gradually increases perturbations, forming an easy-to-hard curriculum."
    }
  ],
  "stationary": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "ADAM is an optimization algorithm that imrpoves stochastic gradient decent with first and second order momentum terms. ADAM is easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    }
  ],
  "hyper": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "ADAM is an optimization algorithm that imrpoves stochastic gradient decent with first and second order momentum terms. ADAM is easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "This paper introduces Batch Normalization, an online normalization technique which calculates mean and std across the batch dimension. Batch Norm improves training efficiency, allowing for higher learning rates, and decreasing hyper-parameter sensitivity, and sometimes removing the need for Dropout regularization. This paper suggests Batch Norm improves internal covariant shift, however later works have questioned this."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Data augmentation improves generalization, and robustness against image corruptions. However, the use of complex augmentation pipelines typically involves tuning of an enormous number of hyper parameters. RandAugment addresses this by reducing the search space. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets."
    },
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "This paper investigates the importance of the warm-up stage in training transformers, and the impact of layer normalization placement. Using mean field theory, the authors demonstrate that the original Post-LN Transformer's design results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    },
    {
      "title": "Don't Decay the Learning Rate, Increase the Batch Size",
      "url": "/research/2017/11/01/Don't-Decay-the-Learning-Rate,-Increase-the-Batch-Size.html",
      "date": "November 1, 2017",
      "summary": "This study demonstrates that increasing the batch size during training achieves similar learning outcomes as the common practice of decaying the learning rate, applicable to stochastic gradient descent and its variants, including with momentum and Adam optimization. This approach not only matches test accuracies within the same number of epochs but also enhances parallelism and reduces training time due to fewer parameter updates. Efficiency can be further improved by adjusting the learning rate and batch size proportionally, and although increasing the momentum coefficient and scaling the batch size accordingly may slightly lower test accuracy, it enables the use of large batch training without needing to tune hyper-parameters. Using these methods, ResNet-50 was trained on ImageNet to a 76.1% validation accuracy in less than 30 minutes."
    }
  ],
  "parameter": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "ADAM is an optimization algorithm that imrpoves stochastic gradient decent with first and second order momentum terms. ADAM is easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "This paper introdices an automated pipeline for building high-quality, diverse image datasets. Also proposes minor mofications to the archetecture and loss of a DINO ViT. A large ViT model with 1 billion parameters was trained and distilled into smaller models."
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "This paper introduces PatchDropout, a simple training technique for Vision Transformers ViT that drops random image patches, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "This paper introduces Batch Normalization, an online normalization technique which calculates mean and std across the batch dimension. Batch Norm improves training efficiency, allowing for higher learning rates, and decreasing hyper-parameter sensitivity, and sometimes removing the need for Dropout regularization. This paper suggests Batch Norm improves internal covariant shift, however later works have questioned this."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Data augmentation improves generalization, and robustness against image corruptions. However, the use of complex augmentation pipelines typically involves tuning of an enormous number of hyper parameters. RandAugment addresses this by reducing the search space. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets."
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of Vision Transformer (ViT) variants against common corruptions. The benchmarking reveals that overlapping patch embedding and convolutional feed-forward networks (FFNs) significantly enhance ViT robustness. The study also scrutinizes the effectiveness of CNN-based data augmentation strategies when applied to ViTs, finding that adversarial noise training is effective, while fourier-domain augmentation falls short. A new conditional method for generating dynamic augmentation parameters based on input images is proposed, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces TinyViT, a series of compact and efficient vision transformers (ViTs) designed for devices with limited resources. The authors employ a fast distillation framework to transfer knowledge from large pretrained models to smaller ones during pretraining, using sparsified logits from teacher models to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. With increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters, and demonstrates strong transferability across various downstream tasks."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets for formula-driven supervised learning (FDSL). The authors identify the optimal FDSL parameters and maximize synthetic image variety, which is crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. The authors demonstrate FDSL's potential for continuous improvement and its ability to avoid issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates how to achieve state-of-the-art certified adversarial robustness to ℓ2-norm bounded perturbations using only off-the-shelf pretrained models. The authors instantiate the denoised smoothing approach by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This method certifies 71% accuracy on ImageNet under adversarial perturbations constrained to an ℓ2 norm of ε = 0.5, improving upon the prior certified state-of-the-art by 14 percentage points and denoised smoothing by 30 percentage points, without requiring any fine-tuning or retraining of model parameters."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. This encoder serves as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The DMAE ViT-Base model achieves comparable or superior certified accuracy with fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on ImageNet. The model also shows high transferability to CIFAR-10, indicating its broad applicability."
    },
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "This paper investigates the importance of the warm-up stage in training transformers, and the impact of layer normalization placement. Using mean field theory, the authors demonstrate that the original Post-LN Transformer's design results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    },
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "This paper introduces QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU while maintaining 16-bit task performance. QLORA utilizes 4-bit quantized language models and Low Rank Adapters (LoRA) and incorporates several memory-saving innovations without compromising performance. The authors' best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. The authors applied QLORA to finetune over 1,000 models and analyzed performance across various datasets, model types, and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Their findings suggest that GPT-4 evaluations are a viable substitute for human assessments and question the reliability of current chatbot benchmarks. The authors make their models and 4-bit training code public."
    },
    {
      "title": "SinLU: Sinu-Sigmoidal Linear Uni",
      "url": "/research/2021/12/02/SinLU-Sinu-Sigmoidal-Linear-Uni.html",
      "date": "December 2, 2021",
      "summary": "This paper introduces a non-linear activation function called the Sinu-sigmoidal Linear Unit (SinLU), formulated as SinLU(x) = (x + a sin bx) · σ(x). It incorporates a sine wave for added functionality over traditional linear units, with two trainable parameters controlling the sinusoidal participation. The authors compare SinLU's performance to ReLU, GELU, and SiLU across various domains, models, and standard datasets. The results demonstrate SinLU's robustness and superior performance due to the incorporation of the trainable sine wave parameters, facilitating easy training and fast convergence."
    },
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "The paper investigates the use of synthetic data generated by graphics simulators for pre-training computer vision models. The authors find that model performance on downstream tasks varies with different simulation parameters used to generate the synthetic data. They introduce Task2Sim, a model that maps the requirements of a downstream task to the optimal simulation parameters for generating synthetic pre-training data."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "The paper introduces a system that trains deep neural networks for object detection using only synthetic images generated with domain randomization. This involves randomizing non-realistic simulator parameters like lighting and object textures, allowing the network to identify essential object features. Their findings show that networks can achieve impressive performance using just synthetic data, and further improve with fine-tuning on real data. This suggests the potential of using cost-effective synthetic data for training, avoiding the need to acquire vast amounts of real-world data or create detailed synthetic environments. They validate their method on car bounding box detection using the KITTI dataset."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "The paper proposes three normalization-centric modifications to improve Transformer training: PRENORM which introduces pre-norm residual connections and smaller initializations, enabling warmup-free, validation-based training with large learning rates; SCALENORM which suggests L2 normalization with a single scale parameter for faster training and better performance; and FIXNORM which reaffirms the efficacy of normalizing word embeddings to a fixed length."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    },
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    },
    {
      "title": "Understanding Deep Learning Requires Re- Thinking Generalization",
      "url": "/research/2017/02/26/Understanding-Deep-Learning-Requires-Re--Thinking-Generalization.html",
      "date": "February 26, 2017",
      "summary": "Extensive experiments reveal that traditional explanations for the strong generalization performance of large neural networks fall short. Despite conventional wisdom attributing success to model properties or regularization techniques, our findings show that state-of-the-art convolutional networks for image classification can easily overfit random labels. This phenomenon persists regardless of explicit regularization or substituting true images with random noise. Theoretical analysis suggests that shallow neural networks achieve perfect finite sample expressivity when parameters outnumber data points, as commonly observed. We interpret these results in contrast to traditional models."
    },
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    },
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    },
    {
      "title": "Don't Decay the Learning Rate, Increase the Batch Size",
      "url": "/research/2017/11/01/Don't-Decay-the-Learning-Rate,-Increase-the-Batch-Size.html",
      "date": "November 1, 2017",
      "summary": "This study demonstrates that increasing the batch size during training achieves similar learning outcomes as the common practice of decaying the learning rate, applicable to stochastic gradient descent and its variants, including with momentum and Adam optimization. This approach not only matches test accuracies within the same number of epochs but also enhances parallelism and reduces training time due to fewer parameter updates. Efficiency can be further improved by adjusting the learning rate and batch size proportionally, and although increasing the momentum coefficient and scaling the batch size accordingly may slightly lower test accuracy, it enables the use of large batch training without needing to tune hyper-parameters. Using these methods, ResNet-50 was trained on ImageNet to a 76.1% validation accuracy in less than 30 minutes."
    },
    {
      "title": "How to decay your learning rate",
      "url": "/research/2021/05/23/How-to-decay-your-learning-rate.html",
      "date": "May 23, 2021",
      "summary": "Empirical findings suggest that typical fine-tuned learning rate schedules decay the learning rate following weight norm fluctuations. This led to the development of ABEL, an automatic scheduler that adjusts the learning rate based on weight norm changes. ABEL performs comparably to tuned schedules but demonstrates greater robustness to parameter variations. Extensive experiments across various domains reveal that when the weight norm remains stable, simplified schedules yield equivalent performance to complex ones, resembling a constant learning rate with decay towards the end of training."
    },
    {
      "title": "Weight Agnostic Neural Networks",
      "url": "/research/2019/09/05/Weight-Agnostic-Neural-Networks.html",
      "date": "September 5, 2019",
      "summary": "This study investigates the importance of neural network architectures versus weight parameters for task performance. We introduce a method to find architectures capable of performing tasks without weight training. By assigning random weights, we show that minimal architectures can achieve notable performance on various tasks, including reinforcement learning and MNIST classification."
    },
    {
      "title": "Scaling Vision Transformers to 22 Billion Parameters",
      "url": "/research/2023/02/10/Scaling-Vision-Transformers-to-22-Billion-Parameters.html",
      "date": "February 10, 2023",
      "summary": "We introduce a method for efficiently training a 22B-parameter Vision Transformer (ViT-22B) and conduct various experiments to assess its performance. Compared to existing models, ViT-22B shows improved scalability and benefits such as enhanced fairness-performance tradeoff, alignment with human visual perception, and increased robustness. Our findings suggest promising avenues for achieving large-scale language model-like capabilities in vision tasks."
    },
    {
      "title": "DeepNet: Scaling Transformers to 1,000 Layers",
      "url": "/research/2022/03/01/DeepNet-Scaling-Transformers-to-1,000-Layers.html",
      "date": "March 1, 2022",
      "summary": "This paper introduces DEEPNORM, a novel method to stabilize extremely deep Transformers through a new normalization function and a theoretically derived initialization for the residual connection. DEEPNORM offers a stable and efficient training regime, blending the benefits of Post-LN's performance with Pre-LN's stability, allowing for the scaling of Transformers up to 1,000 layers without difficulty. This represents a significant advancement over previous models, with a 200-layer, 3.2B parameter model outperforming a 48-layer, 12B parameter state-of-the-art model by 5 BLEU points on a multilingual benchmark with 7,482 translation directions, suggesting a promising direction for scaling."
    },
    {
      "title": "ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Network",
      "url": "/research/2021/06/29/ASAM-Adaptive-Sharpness-Aware-Minimization-for-Scale-Invariant-Learning-of-Deep-Neural-Network.html",
      "date": "June 29, 2021",
      "summary": "Recent advances in learning algorithms have highlighted the importance of the sharpness of the loss surface as a key indicator of the generalization gap, achieving state-of-the-art results. However, traditional sharpness measures, defined within a fixed region, suffer from sensitivity to parameter scaling, weakening their predictive value for the generalization gap. This paper introduces a scale-invariant measure, adaptive sharpness, along with a new generalization bound. We present a new learning method, adaptive sharpness-aware minimization (ASAM), which leverages this bound. Our experiments across various benchmark datasets demonstrate that ASAM significantly enhances model generalization."
    },
    {
      "title": "Neural Network Diffusion",
      "url": "/research/2024/02/20/Neural-Network-Diffusion.html",
      "date": "February 20, 2024",
      "summary": "Diffusion models, known for their success in image and video generation, can also generate high-performing neural network parameters, as demonstrated in this study. By employing a simple combination of an autoencoder and a standard latent diffusion model, our method involves extracting latent representations of network parameters, which are then synthesized from random noise by the diffusion model. These new representations, processed through the autoencoder's decoder, serve as fresh network parameters. Tested across various architectures and datasets, this diffusion approach consistently produces models with comparable or superior performance to traditionally trained networks at minimal extra cost. Importantly, the generated models show distinct performance differences from the trained ones, suggesting further exploration into the versatile applications of diffusion models."
    },
    {
      "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
      "url": "/research/2024/03/13/Simple-and-Scalable-Strategies-to-Continually-Pre-train-Large-Language-Models.html",
      "date": "March 13, 2024",
      "summary": "Large language models (LLMs) often require re-training on new data, consuming extensive computational resources. We present an efficient method that combines learning rate re-warming, re-decaying, and data replay, effectively maintaining performance without full re-training. This approach works well across different data distributions, including minor shifts (English→English) and significant shifts (English→German), tested up to 405M and 10B parameter models. Our findings suggest that continual learning strategies can update LLMs with minimal computational cost, rivaling traditional re-training methods. Additionally, we propose alternatives to the cosine learning rate schedule to reduce forgetting, offering more flexibility in learning without a fixed token budget."
    },
    {
      "title": "Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes",
      "url": "/research/2017/11/28/Towards-Understanding-Generalization-of-Deep-Learning-Perspective-of-Loss-Landscapes.html",
      "date": "November 28, 2017",
      "summary": "This study examines why deep learning models, despite having more parameters than training samples, generalize well. It highlights the importance of the loss function's landscape, showing that areas leading to better generalization (good minima) are more prevalent than those leading to poor outcomes. This predominance facilitates the convergence of optimization methods to these beneficial minima. The research provides theoretical backing by analyzing 2-layer neural networks, noting that solutions with better generalization have a smaller Hessian matrix norm. For deeper networks, extensive numerical data supports these conclusions."
    },
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "adamax": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "ADAM is an optimization algorithm that imrpoves stochastic gradient decent with first and second order momentum terms. ADAM is easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    }
  ],
  "infinity": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "ADAM is an optimization algorithm that imrpoves stochastic gradient decent with first and second order momentum terms. ADAM is easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    }
  ],
  "normalization": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "ADAM is an optimization algorithm that imrpoves stochastic gradient decent with first and second order momentum terms. ADAM is easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "The Batch Normalization paper argued the method's effectiveness came from reducing internal covariate shift. In this paper the authors argue that actually the main benifit of Batch Norm is loss landscape smoothing."
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "This paper introduces Batch Normalization, an online normalization technique which calculates mean and std across the batch dimension. Batch Norm improves training efficiency, allowing for higher learning rates, and decreasing hyper-parameter sensitivity, and sometimes removing the need for Dropout regularization. This paper suggests Batch Norm improves internal covariant shift, however later works have questioned this."
    },
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes."
    },
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/research/2017/11/06/Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization.html",
      "date": "November 6, 2017",
      "summary": "This paper itroduces Instance Normalization, an online normalization technique that calculates the mean and std across individual data instances. This is shown to work well in situations where there is very high vatiation within batches, such as in style transfer."
    },
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activitivations can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases."
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets."
    },
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "This paper introduces a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. This decouples batch size from the loss, and allows the authors to increase the batch size and also improves performance even with smaller batches. Pushing the batch size to one million showed diminishing returns, with 32k being a good middle ground."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This paper is effectively a follow-up to DinoV2. The study addresses the issue of artifacts in feature maps of Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. The authors introduce a straightforward strategy of adding extra tokens to the input sequence of the Vision Transformer, which eliminates these artifacts for both supervised and self-supervised models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks, facilitates object discovery with larger models, and results in smoother feature and attention maps for downstream visual processing tasks."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates how to achieve state-of-the-art certified adversarial robustness to ℓ2-norm bounded perturbations using only off-the-shelf pretrained models. The authors instantiate the denoised smoothing approach by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This method certifies 71% accuracy on ImageNet under adversarial perturbations constrained to an ℓ2 norm of ε = 0.5, improving upon the prior certified state-of-the-art by 14 percentage points and denoised smoothing by 30 percentage points, without requiring any fine-tuning or retraining of model parameters."
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel normalization technique for self-supervised transformer training that normalizes the [CLS] token and regular tokens separately. The authors find that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks compared to previous models like masked autoencoders (MAE) that use a single normalization layer for all tokens."
    },
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "This paper investigates the importance of the warm-up stage in training transformers, and the impact of layer normalization placement. Using mean field theory, the authors demonstrate that the original Post-LN Transformer's design results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "This paper introduces Online Normalization, a novel method for normalizing neural network hidden activations that offers a batch-independent alternative with comparable accuracy to Batch Normalization. Online Normalization addresses Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. The method is applicable to recurrent, fully connected networks, and those with high activation memory requirements. The authors demonstrate its effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "The paper proposes three normalization-centric modifications to improve Transformer training: PRENORM which introduces pre-norm residual connections and smaller initializations, enabling warmup-free, validation-based training with large learning rates; SCALENORM which suggests L2 normalization with a single scale parameter for faster training and better performance; and FIXNORM which reaffirms the efficacy of normalizing word embeddings to a fixed length."
    },
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    },
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    },
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    },
    {
      "title": "Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks",
      "url": "/research/2020/06/11/Batch-Normalization-Provably-Avoids-Rank-Collapse-for-Randomly-Initialised-Deep-Networks.html",
      "date": "June 11, 2020",
      "summary": "We investigate the challenge of training randomly initialized deep neural networks due to spectral instabilities in products of random matrices. Batch normalization emerges as an effective solution to prevent rank collapse in both linear and ReLU networks. Leveraging tools from Markov chain theory, we establish a lower rank bound for deep linear networks. Empirical findings show that this rank robustness extends to ReLU networks. Our experiments on real-world datasets underscore the significance of rank stability in training modern deep neural architectures."
    },
    {
      "title": "How to decay your learning rate",
      "url": "/research/2021/05/23/How-to-decay-your-learning-rate.html",
      "date": "May 23, 2021",
      "summary": "Empirical findings suggest that typical fine-tuned learning rate schedules decay the learning rate following weight norm fluctuations. This led to the development of ABEL, an automatic scheduler that adjusts the learning rate based on weight norm changes. ABEL performs comparably to tuned schedules but demonstrates greater robustness to parameter variations. Extensive experiments across various domains reveal that when the weight norm remains stable, simplified schedules yield equivalent performance to complex ones, resembling a constant learning rate with decay towards the end of training."
    },
    {
      "title": "On the Maximum Hessian Eigenvalue and Generalization",
      "url": "/research/2023/05/23/On-the-Maximum-Hessian-Eigenvalue-and-Generalization.html",
      "date": "May 23, 2023",
      "summary": "This study investigates the relationship between training interventions and the generalization of deep networks. While previous research suggests that flatter solutions generalize better than sharper ones, particularly measured by λmax, the largest eigenvalue of the Hessian of the loss, this paper challenges this notion. Through experiments, we demonstrate that larger learning rates reduce λmax for all batch sizes but do not consistently improve generalization. Additionally, scaling batch size and learning rate simultaneously can change λmax without affecting generalization. Sharpness-Aware Minimization (SAM) produces smaller λmax but does not consistently enhance generalization, especially with larger batch sizes. Excessive dropout probabilities can degrade generalization, despite promoting smaller λmax. Batch normalization, while not consistently reducing λmax, still improves generalization. These findings question λmax's role in explaining generalization in neural networks, highlighting the limits of its explanatory power."
    },
    {
      "title": "Root Mean Square Layer Normalization",
      "url": "/research/2019/10/16/Root-Mean-Square-Layer-Normalization.html",
      "date": "October 16, 2019",
      "summary": "Layer normalization (LayerNorm) enhances deep neural network stability and convergence by re-centering and re-scaling inputs and weight matrices. However, its computational overhead slows networks, particularly RNNs. We introduce RMSNorm, which replaces re-centering with root mean square (RMS) regularization. RMSNorm maintains re-scaling invariance and adapts learning rates implicitly, while being computationally simpler than LayerNorm. We also propose partial RMSNorm (pRMSNorm), estimating RMS from a subset of inputs. Empirical results across various tasks and architectures demonstrate that RMSNorm achieves comparable performance to LayerNorm while reducing running time by 7%∼64%."
    },
    {
      "title": "Normalized Attention Without Probability Cage",
      "url": "/research/2020/05/19/Normalized-Attention-Without-Probability-Cage.html",
      "date": "May 19, 2020",
      "summary": "Softmax-attention architectures, especially popularized by Transformers, have seen significant advancements in various tasks. However, the geometric implications of softmax-attention remain underexplored. In this study, we demonstrate limitations arising from constraining attention weights to the probability simplex and its impact on the convex hull of value vectors. We reveal sequence length-dependent biases in Transformers towards token isolation at initialization and compare them with max- and sum-pooling, which are strong but often overlooked baselines. To address these issues, we propose a novel approach of replacing softmax with normalization in self-attention, resulting in a robust and widely applicable architecture. Our findings are supported by empirical results from over 25,000 trained models, and all results and implementations are publicly available."
    },
    {
      "title": "On Layer Normalization in the Transformer Architecture",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architecture.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is widely used in natural language processing. Training it typically requires a carefully designed learning rate warm-up stage, crucial for performance but slowing optimization and requiring more hyperparameter tuning. We provide theoretical insights into the necessity of the warm-up stage, demonstrating that the location of layer normalization affects gradient behavior. For Post-LN Transformers, with layer normalization between residual blocks, large gradients near the output layer at initialization make training unstable without warm-up. Conversely, for Pre-LN Transformers, with layer normalization inside residual blocks, gradients are well-behaved, suggesting the removal of the warm-up stage. Experimental results show that Pre-LN Transformers without warm-up achieve comparable performance with less training time and hyperparameter tuning."
    },
    {
      "title": "Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks",
      "url": "/research/2017/10/23/Cosine-Normalization-Using-Cosine-Similarity-Instead-of-Dot-Product-in-Neural-Networks.html",
      "date": "October 23, 2017",
      "summary": "In traditional multi-layer neural networks, the dot product between the output and weight vectors of preceding layers serves as input to the activation function, resulting in unbounded outputs and increased variance. This variance can lead to poor generalization and hinder training by exacerbating internal covariate shift. To address this, we propose cosine normalization, which replaces the dot product with cosine similarity or centered cosine similarity (Pearson Correlation Coefficient). We evaluate cosine normalization against batch, weight, and layer normalization in fully-connected and convolutional neural networks across various datasets. Our experiments demonstrate that cosine normalization outperforms other normalization techniques."
    },
    {
      "title": "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization",
      "url": "/research/2023/07/04/Gradient-Norm-Aware-Minimization-Seeks-First-Order-Flatness-and-Improves-Generalization.html",
      "date": "July 4, 2023",
      "summary": "Recent advancements have highlighted the effectiveness of flat minima in enhancing generalization, particularly through Sharpness-Aware Minimization (SAM). However, existing definitions of flatness, such as zeroth-order flatness, have limitations in discerning between minima with low and high generalization errors. To address this, we propose first-order flatness, which considers maximal gradient norm within a perturbation radius. We introduce Gradient norm Aware Minimization (GAM) as a novel training approach to achieve uniformly small curvature across all directions. Experimental results demonstrate GAM's ability to enhance generalization compared to standard optimizers like SGD and AdamW across various datasets and networks. Moreover, GAM facilitates SAM in identifying flatter minima, leading to improved generalization."
    },
    {
      "title": "Penalizing Gradient Norm for Efficiently Improving Generalization in Deep Learning",
      "url": "/research/2022/06/26/Penalizing-Gradient-Norm-for-Efficiently-Improving-Generalization-in-Deep-Learning.html",
      "date": "June 26, 2022",
      "summary": "This paper proposes a method to enhance the generalization of deep neural networks (DNNs) by penalizing the gradient norm of the loss function during optimization. By constraining the gradient norm, the optimizers tend to find flat minima, improving generalization. We efficiently implement this method using first-order approximation within the gradient descent framework. Experimental results demonstrate improved generalization across various models and datasets. Additionally, we show that a recent method, sharpness-aware minimization, is a special case of our approach, with our method achieving new state-of-the-art performance on tested tasks."
    },
    {
      "title": "DeepNet: Scaling Transformers to 1,000 Layers",
      "url": "/research/2022/03/01/DeepNet-Scaling-Transformers-to-1,000-Layers.html",
      "date": "March 1, 2022",
      "summary": "This paper introduces DEEPNORM, a novel method to stabilize extremely deep Transformers through a new normalization function and a theoretically derived initialization for the residual connection. DEEPNORM offers a stable and efficient training regime, blending the benefits of Post-LN's performance with Pre-LN's stability, allowing for the scaling of Transformers up to 1,000 layers without difficulty. This represents a significant advancement over previous models, with a 200-layer, 3.2B parameter model outperforming a 48-layer, 12B parameter state-of-the-art model by 5 BLEU points on a multilingual benchmark with 7,482 translation directions, suggesting a promising direction for scaling."
    },
    {
      "title": "Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes",
      "url": "/research/2017/11/28/Towards-Understanding-Generalization-of-Deep-Learning-Perspective-of-Loss-Landscapes.html",
      "date": "November 28, 2017",
      "summary": "This study examines why deep learning models, despite having more parameters than training samples, generalize well. It highlights the importance of the loss function's landscape, showing that areas leading to better generalization (good minima) are more prevalent than those leading to poor outcomes. This predominance facilitates the convergence of optimization methods to these beneficial minima. The research provides theoretical backing by analyzing 2-layer neural networks, noting that solutions with better generalization have a smaller Hessian matrix norm. For deeper networks, extensive numerical data supports these conclusions."
    },
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "smooth": [
    {
      "title": "An Investigation of how Label Smoothing Affects Generalization",
      "url": "/research/2020/10/23/AN-INVESTIGATION-OF-HOW-LABEL-SMOOTHING-AFFECTS-GENERALIZATION.html",
      "date": "October 23, 2020",
      "summary": "Label smoothing has been empirically shown to reduce overfitting and improve generalization. However, its mathematical underpinnings remain unclear. This paper explains label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. The authors identify a method for calculating a label smoothing value that minimizes generalization loss."
    },
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "The Batch Normalization paper argued the method's effectiveness came from reducing internal covariate shift. In this paper the authors argue that actually the main benifit of Batch Norm is loss landscape smoothing."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This study suggest that label smoothing not only boosts generalization but also enhances model calibration. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. Label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates how to achieve state-of-the-art certified adversarial robustness to ℓ2-norm bounded perturbations using only off-the-shelf pretrained models. The authors instantiate the denoised smoothing approach by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This method certifies 71% accuracy on ImageNet under adversarial perturbations constrained to an ℓ2 norm of ε = 0.5, improving upon the prior certified state-of-the-art by 14 percentage points and denoised smoothing by 30 percentage points, without requiring any fine-tuning or retraining of model parameters."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "Automated Learning Rate Scheduler for Large-batch",
      "url": "/research/2021/07/13/Automated-Learning-Rate-Scheduler-for-Large-batch.html",
      "date": "July 13, 2021",
      "summary": "Large-batch training is crucial for deep learning with large datasets and models, but requires specific learning rate (LR) schedules for optimal performance, especially under limited training epochs. This work introduces an automated LR scheduling algorithm for large-batch neural network training within a fixed epoch budget, consisting of adaptive warmup and predefined decay phases. The LR is dynamically adjusted based on training loss monitored through Gaussian process smoothing, facilitating low computational overhead. When integrated with adaptive stochastic optimizers like AdamP and LAMB, this scheduler eliminates the need for extensive hyperparameter tuning and achieves competitive or superior results on various image classification tasks across different batch sizes and architectures."
    },
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    }
  ],
  "generalization": [
    {
      "title": "An Investigation of how Label Smoothing Affects Generalization",
      "url": "/research/2020/10/23/AN-INVESTIGATION-OF-HOW-LABEL-SMOOTHING-AFFECTS-GENERALIZATION.html",
      "date": "October 23, 2020",
      "summary": "Label smoothing has been empirically shown to reduce overfitting and improve generalization. However, its mathematical underpinnings remain unclear. This paper explains label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. The authors identify a method for calculating a label smoothing value that minimizes generalization loss."
    },
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/research/2021/05/03/Robust-and-Generalizable-Visual-Representation-Learning-via-Random-Convolutions.html",
      "date": "May 3, 2021",
      "summary": "This study introduces random convolutions for data augmentation. Random convolutions approximately preserve shape while distorting local textures. This method significantly enhances performance on domain generalization and robustness / corruption benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods."
    },
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/research/2023/06/30/Impact-of-Noise-on-Calibration-and-Generalisation-of-Neural-Networks.html",
      "date": "June 30, 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. Activation noise is shown to significantly improve generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    },
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrives incrementally, it's common to build a sequence of models that incorporate progressively more data. It has been shown that simply continuing training of a model on new data often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, this paper introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, the authors enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. This simple method surpasses the current state-of-the-art in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Data augmentation improves generalization, and robustness against image corruptions. However, the use of complex augmentation pipelines typically involves tuning of an enormous number of hyper parameters. RandAugment addresses this by reducing the search space. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This study suggest that label smoothing not only boosts generalization but also enhances model calibration. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. Label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, enhances vision tasks using Vision transformers by masking and reconstructing parts of an image. The compatibility of MIM with CNNs and its operational principle are unclear. this study reveals that MIM improves generalized feature extraction through middle-order patch interactions and introduces Architecture-Agnostic Masked Image Modeling (A2MIM), compatible with both Transformers and CNNs. Extensive testing demonstrates A2MIM's ability to enhance representation learning and transferability to various tasks without specialized modifications."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper introduces the concept of grokking, a sudden onset of perfect generalisation long after overfitting, on small, algorithmically generated datasets. The study also finds that generalization on smaller datasets requires more optimization. The authors suggest that such datasets are ideal for investigating the puzzling phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "This paper demonstrates that artificial neural networks (ANNs) are equivalent to Gaussian processes at initialization in the infinite-width limit and introduces the Neural Tangent Kernel (NTK), which describes ANNs' behavior during training. The NTK stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. The authors prove the positive-definiteness of the limiting NTK under certain conditions and show that the network function follows a linear differential equation during training for least-squares regression. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    },
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "This paper introduces tools for training monocular depth estimation models with multiple datasets despite incompatible annotations. Their approach uses a robust training objective, multi-objective learning for data integration, and pretraining encoders on auxiliary tasks. Tested across five datasets, including 3D films as a novel data source, their methods achieve superior zero-shot cross-dataset generalization, outperforming existing benchmarks through principled multi-dataset training."
    },
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    },
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Understanding Deep Learning Requires Re- Thinking Generalization",
      "url": "/research/2017/02/26/Understanding-Deep-Learning-Requires-Re--Thinking-Generalization.html",
      "date": "February 26, 2017",
      "summary": "Extensive experiments reveal that traditional explanations for the strong generalization performance of large neural networks fall short. Despite conventional wisdom attributing success to model properties or regularization techniques, our findings show that state-of-the-art convolutional networks for image classification can easily overfit random labels. This phenomenon persists regardless of explicit regularization or substituting true images with random noise. Theoretical analysis suggests that shallow neural networks achieve perfect finite sample expressivity when parameters outnumber data points, as commonly observed. We interpret these results in contrast to traditional models."
    },
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    },
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    },
    {
      "title": "Learning Rate Perturbation: A Generic Plugin of Learning Rate schedule towards Flatter Local Minima",
      "url": "/research/2022/08/25/Learning-Rate-Perturbation-A-Generic-Plugin-of-Learning-Rate-schedule-towards-Flatter-Local-Minima.html",
      "date": "August 25, 2022",
      "summary": "Learning rate is crucial in neural network training, yet existing schedules lack theoretical backing, often leading to suboptimal choices made through trial and error. To address this, we propose LEAP, a plugin enhancing various learning rate schedules by introducing perturbations. This simple yet effective strategy favors flat minima, ensuring better generalization. Extensive experiments demonstrate LEAP's ability to improve performance across diverse datasets and learning rate schedules, including constant ones."
    },
    {
      "title": "On the Maximum Hessian Eigenvalue and Generalization",
      "url": "/research/2023/05/23/On-the-Maximum-Hessian-Eigenvalue-and-Generalization.html",
      "date": "May 23, 2023",
      "summary": "This study investigates the relationship between training interventions and the generalization of deep networks. While previous research suggests that flatter solutions generalize better than sharper ones, particularly measured by λmax, the largest eigenvalue of the Hessian of the loss, this paper challenges this notion. Through experiments, we demonstrate that larger learning rates reduce λmax for all batch sizes but do not consistently improve generalization. Additionally, scaling batch size and learning rate simultaneously can change λmax without affecting generalization. Sharpness-Aware Minimization (SAM) produces smaller λmax but does not consistently enhance generalization, especially with larger batch sizes. Excessive dropout probabilities can degrade generalization, despite promoting smaller λmax. Batch normalization, while not consistently reducing λmax, still improves generalization. These findings question λmax's role in explaining generalization in neural networks, highlighting the limits of its explanatory power."
    },
    {
      "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
      "url": "/research/2021/07/24/The-Many-Faces-of-Robustness-A-Critical-Analysis-of-Out-of-Distribution-Generalization.html",
      "date": "July 24, 2021",
      "summary": "We present four novel real-world distribution shift datasets encompassing changes in image style, blurriness, location, camera settings, and more. Evaluating existing methods for enhancing out-of-distribution robustness, we discover that employing larger models and artificial data augmentations can enhance robustness against real-world distribution shifts, contradicting prior claims. Our findings demonstrate that improvements in artificial robustness benchmarks can indeed transfer to real-world distribution shifts, contrary to prior assumptions. Additionally, we introduce a novel data augmentation technique that surpasses models pretrained with significantly more labeled data, emphasizing its efficacy in addressing real-world distribution shifts. While some methods consistently mitigate texture and local image statistics shifts, they fail to address other shifts like geographic changes. Our results underscore the necessity for future research to examine multiple distribution shifts concurrently, as no single method consistently improves robustness across all evaluated scenarios."
    },
    {
      "title": "Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks",
      "url": "/research/2017/10/23/Cosine-Normalization-Using-Cosine-Similarity-Instead-of-Dot-Product-in-Neural-Networks.html",
      "date": "October 23, 2017",
      "summary": "In traditional multi-layer neural networks, the dot product between the output and weight vectors of preceding layers serves as input to the activation function, resulting in unbounded outputs and increased variance. This variance can lead to poor generalization and hinder training by exacerbating internal covariate shift. To address this, we propose cosine normalization, which replaces the dot product with cosine similarity or centered cosine similarity (Pearson Correlation Coefficient). We evaluate cosine normalization against batch, weight, and layer normalization in fully-connected and convolutional neural networks across various datasets. Our experiments demonstrate that cosine normalization outperforms other normalization techniques."
    },
    {
      "title": "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization",
      "url": "/research/2023/07/04/Gradient-Norm-Aware-Minimization-Seeks-First-Order-Flatness-and-Improves-Generalization.html",
      "date": "July 4, 2023",
      "summary": "Recent advancements have highlighted the effectiveness of flat minima in enhancing generalization, particularly through Sharpness-Aware Minimization (SAM). However, existing definitions of flatness, such as zeroth-order flatness, have limitations in discerning between minima with low and high generalization errors. To address this, we propose first-order flatness, which considers maximal gradient norm within a perturbation radius. We introduce Gradient norm Aware Minimization (GAM) as a novel training approach to achieve uniformly small curvature across all directions. Experimental results demonstrate GAM's ability to enhance generalization compared to standard optimizers like SGD and AdamW across various datasets and networks. Moreover, GAM facilitates SAM in identifying flatter minima, leading to improved generalization."
    },
    {
      "title": "Model Generalization: A Sharpness Aware Optimization Perspective",
      "url": "/research/2022/08/14/Model-Generalization-A-Sharpness-Aware-Optimization-Perspective.html",
      "date": "August 14, 2022",
      "summary": "This study investigates the effectiveness of Sharpness-Aware Minimization (SAM) and adaptive Sharpness-Aware Minimization (ASAM) in enhancing model generalization. Through three experiments, we assess their impact from a sharpness-aware perspective. Results demonstrate that optimization techniques based on sharpness awareness can bolster model generalization. Furthermore, ASAM exhibits potential for enhancing generalization performance on un-normalized data, though additional research is required for validation."
    },
    {
      "title": "Penalizing Gradient Norm for Efficiently Improving Generalization in Deep Learning",
      "url": "/research/2022/06/26/Penalizing-Gradient-Norm-for-Efficiently-Improving-Generalization-in-Deep-Learning.html",
      "date": "June 26, 2022",
      "summary": "This paper proposes a method to enhance the generalization of deep neural networks (DNNs) by penalizing the gradient norm of the loss function during optimization. By constraining the gradient norm, the optimizers tend to find flat minima, improving generalization. We efficiently implement this method using first-order approximation within the gradient descent framework. Experimental results demonstrate improved generalization across various models and datasets. Additionally, we show that a recent method, sharpness-aware minimization, is a special case of our approach, with our method achieving new state-of-the-art performance on tested tasks."
    },
    {
      "title": "SAM as an Optimal Relaxation of Bayes",
      "url": "/research/2023/12/10/SAM-as-an-Optimal-Relaxation-of-Bayes.html",
      "date": "December 10, 2023",
      "summary": "We introduce SAM, a method enhancing generalization in deep learning, as a relaxation of the Bayes objective. SAM replaces the expected negative-loss with an optimal convex lower bound derived using the Fenchel biconjugate. This connection enables an Adam-like extension of SAM, offering automatic uncertainty estimates and potential accuracy improvements. Bridging adversarial and Bayesian methods, our work paves the way for robustness enhancement."
    },
    {
      "title": "Sharpness-Aware Training for Free",
      "url": "/research/2023/03/02/Sharpness-Aware-Training-for-Free.html",
      "date": "March 2, 2023",
      "summary": "Modern deep neural networks (DNNs) excel in performance but often suffer from over-parameterization, leading to increased generalization error without tailored training strategies. Sharpness-Aware Minimization (SAM) has proven effective in reducing generalization error by minimizing sharpness in the loss landscape. However, SAM incurs a significant computational overhead. This paper introduces Sharpness-Aware Training for Free (SAF), which mitigates sharpness at nearly zero additional computational cost. SAF achieves this by preventing sudden drops in loss within sharp local minima during weight updates. A novel trajectory loss, based on KL-divergence between current and past DNN outputs, replaces SAM's sharpness measure, guiding convergence towards flat minima for enhanced generalization. Empirical results demonstrate SAF's effectiveness on ImageNet with comparable computational efficiency to the base optimizer."
    },
    {
      "title": "AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks",
      "url": "/research/2023/03/01/AdaSAM-Boosting-Sharpness-Aware-Minimization-with-Adaptive-Learning-Rate-and-Momentum-for-Training-Deep-Neural-Networks.html",
      "date": "March 1, 2023",
      "summary": "The Sharpness Aware Minimization (SAM) optimizer, known for enhancing the generalization of deep neural networks by introducing extra perturbation steps, is further developed into AdaSAM by integrating adaptive learning rates and momentum acceleration. Although AdaSAM has been empirically applied to large-scale networks, a theoretical understanding of its performance, considering the complexity of its components, was lacking. This study presents a theoretical analysis of AdaSAM's convergence in stochastic non-convex settings, demonstrating a convergence rate of O(1/√bT) that scales linearly with mini-batch size. By introducing a delayed second-order momentum term, the study successfully decouples and analyzes the intertwined effects of stochastic gradients, adaptive learning rates, and perturbations. This is the first work to offer a detailed convergence rate for SAM with adaptive mechanisms. Experimental results on various NLP tasks indicate AdaSAM's superior performance over SGD, AMSGrad, and SAM optimizers."
    },
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    },
    {
      "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
      "url": "/research/2022/03/19/Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training.html",
      "date": "March 19, 2022",
      "summary": "Sharpness-Aware Minimization (SAM) enhances generalization by optimizing a neighborhood-based perturbed loss, but it doesn't always favor flat minima due to both sharp and flat minima having low perturbed losses. We introduce a new measure, the surrogate gap, reflecting the dominant Hessian eigenvalue at small neighborhood radii, which is simple to compute and can be minimized during training. We propose the Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), an advancement over SAM with minimal additional computational cost. GSAM employs a dual-step approach: first, minimizing the perturbed loss similar to SAM, and second, reducing the surrogate gap without affecting the perturbed loss to target regions with low loss and sharpness, thus achieving superior generalization. GSAM is theoretically robust, showing better convergence and empirical generalization improvements, notably a +3.2% gain over SAM and +5.4% over AdamW in ImageNet accuracy for ViT-B/32."
    },
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    },
    {
      "title": "ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Network",
      "url": "/research/2021/06/29/ASAM-Adaptive-Sharpness-Aware-Minimization-for-Scale-Invariant-Learning-of-Deep-Neural-Network.html",
      "date": "June 29, 2021",
      "summary": "Recent advances in learning algorithms have highlighted the importance of the sharpness of the loss surface as a key indicator of the generalization gap, achieving state-of-the-art results. However, traditional sharpness measures, defined within a fixed region, suffer from sensitivity to parameter scaling, weakening their predictive value for the generalization gap. This paper introduces a scale-invariant measure, adaptive sharpness, along with a new generalization bound. We present a new learning method, adaptive sharpness-aware minimization (ASAM), which leverages this bound. Our experiments across various benchmark datasets demonstrate that ASAM significantly enhances model generalization."
    },
    {
      "title": "Chronos: Learning the Language of Time Series",
      "url": "/research/2024/03/12/Chronos-Learning-the-Language-of-Time-Series.html",
      "date": "March 12, 2024",
      "summary": "Chronos is a framework that enhances pretrained probabilistic time series models by tokenizing time series data for training with transformer-based architectures, notably the T5 family. Pretrained on a mix of public and synthetic datasets created via Gaussian processes, Chronos models outshine others in 42 benchmark datasets by showing superior performance on familiar datasets and competitive or better zero-shot capabilities on new datasets. This illustrates Chronos's ability to generalize across different domains, offering a simpler approach to forecasting tasks."
    },
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    },
    {
      "title": "From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning",
      "url": "/research/2023/03/16/From-MNIST-to-ImageNet-and-Back:-Benchmarking-Continual-Curriculum-Learning.html",
      "date": "March 16, 2023",
      "summary": "Continual learning (CL) is a promising trend in machine learning, aiming to develop robust models and strategies for dynamic environments by incorporating new knowledge while retaining past knowledge. CL research is fragmented, with various protocols, tasks, datasets, and metrics, often not reflecting real-world complexity and tailored to specific strategies. This work addresses this gap by introducing two novel CL benchmarks with heterogeneous tasks from six image datasets, varying in complexity and quality. These benchmarks fairly evaluate state-of-the-art CL strategies in scenarios closer to real-world conditions. The benchmarks present tasks in both increasing and decreasing complexity, assessing models' ability to exploit task structure. The work emphasizes a rigorous, reproducible evaluation protocol for measuring generalization and memory retention in models. Experimental results show that popular CL strategies perform poorly on these benchmarks, exhibit high forgetting, and struggle with curriculum task ordering. These findings underscore the need for rigorous comparisons and the development of new CL strategies for complex scenarios."
    },
    {
      "title": "HYPO: Hyperspherical Out-of-Distribution Generalization",
      "url": "/research/2024/03/19/HYPO-Hyperspherical-Out-of-Distribution-Generalization.html",
      "date": "March 19, 2024",
      "summary": "We propose HYPO, a novel framework for out-of-distribution (OOD) generalization in machine learning, which learns domain-invariant features in a hyperspherical space. Our method focuses on aligning features of the same class across different domains close to their class prototypes and separating different class prototypes maximally. We offer theoretical justifications for its improvement on OOD generalization and show through experiments on OOD benchmarks that HYPO outperforms existing baselines with superior performance."
    },
    {
      "title": "How Does Sharpness-Aware Minimization Minimize Sharpness?",
      "url": "/research/2023/01/05/How-Does-Sharpness-Aware-Minimization-Minimize-Sharpness.html",
      "date": "January 5, 2023",
      "summary": "Sharpness-Aware Minimization (SAM) enhances deep neural networks' generalization across various settings. Although SAM aims to penalize model sharpness using a computationally efficient approach, its exact operational mechanism and the sharpness notion it regularizes remain unclear, partly due to differing sharpness concepts used in its theoretical framework and empirical validation. This study identifies the precise sharpness concept SAM regulates and explains its mechanism. It reveals that the combined effect of SAM's two-step approximations, despite being individually misleading, correctly improves generalization when using full-batch gradients. Moreover, we demonstrate that the stochastic SAM version indeed regularizes a third sharpness notion, aligning more closely with practical performance. This effectiveness is attributed to the gradient's alignment with the Hessian's top eigenvector under SAM."
    },
    {
      "title": "Non-negative Contrastive Learning",
      "url": "/research/2024/03/19/Non-negative-Contrastive-Learning.html",
      "date": "March 19, 2024",
      "summary": "Deep representations offer promising performance for downstream tasks but lack interpretability, posing a significant challenge. In this paper, we introduce Non-negative Contrastive Learning (NCL), a refinement of Non-negative Matrix Factorization (NMF) aimed at generating interpretable features. NCL enforces non-negativity constraints, akin to NMF, resulting in sparse and disentangled representations, unlike standard contrastive learning (CL). We establish theoretical guarantees on NCL's identifiability and downstream generalization. Empirically, NCL outperforms CL in feature disentanglement, selection, and downstream classification tasks. Moreover, NCL can be extended to other learning scenarios, benefiting supervised learning."
    },
    {
      "title": "Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes",
      "url": "/research/2017/11/28/Towards-Understanding-Generalization-of-Deep-Learning-Perspective-of-Loss-Landscapes.html",
      "date": "November 28, 2017",
      "summary": "This study examines why deep learning models, despite having more parameters than training samples, generalize well. It highlights the importance of the loss function's landscape, showing that areas leading to better generalization (good minima) are more prevalent than those leading to poor outcomes. This predominance facilitates the convergence of optimization methods to these beneficial minima. The research provides theoretical backing by analyzing 2-layer neural networks, noting that solutions with better generalization have a smaller Hessian matrix norm. For deeper networks, extensive numerical data supports these conclusions."
    },
    {
      "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum",
      "url": "/research/2023/09/30/When-to-Learn-What:-Model-Adaptive-Data-Augmentation-Curriculum.html",
      "date": "September 30, 2023",
      "summary": "Data augmentation (DA) improves neural network generalization by enforcing invariances and symmetries to predefined transformations applied to input data. However, fixed augmentation policies affect samples differently at various training stages, and existing approaches cannot adapt policies to individual samples and the training model. This paper proposes Model-Adaptive Data Augmentation (MADAug) which trains an augmentation policy network to determine when to learn what. Unlike previous work, MADAug selects augmentation operators for each input image with a model-adaptive policy that varies between training stages, creating an optimized data augmentation curriculum. The policy is trained using a bi-level optimization scheme to minimize validation-set loss of a model trained with policy-produced augmentations. Extensive evaluations on multiple image classification tasks and network architectures show that MADAug outperforms or matches existing DA approaches, enhances fairness by improving all classes, particularly the difficult ones, and performs better when transferred to fine-grained datasets. Additionally, the auto-optimized policy in MADAug gradually increases perturbations, forming an easy-to-hard curriculum."
    }
  ],
  "underpinning": [
    {
      "title": "An Investigation of how Label Smoothing Affects Generalization",
      "url": "/research/2020/10/23/AN-INVESTIGATION-OF-HOW-LABEL-SMOOTHING-AFFECTS-GENERALIZATION.html",
      "date": "October 23, 2020",
      "summary": "Label smoothing has been empirically shown to reduce overfitting and improve generalization. However, its mathematical underpinnings remain unclear. This paper explains label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. The authors identify a method for calculating a label smoothing value that minimizes generalization loss."
    }
  ],
  "control": [
    {
      "title": "An Investigation of how Label Smoothing Affects Generalization",
      "url": "/research/2020/10/23/AN-INVESTIGATION-OF-HOW-LABEL-SMOOTHING-AFFECTS-GENERALIZATION.html",
      "date": "October 23, 2020",
      "summary": "Label smoothing has been empirically shown to reduce overfitting and improve generalization. However, its mathematical underpinnings remain unclear. This paper explains label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. The authors identify a method for calculating a label smoothing value that minimizes generalization loss."
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. This self-supervised method uses multiple crops of an image. Given a context crop the network is trained to predict the embeddings of several target crops. When applied to Vision Transformers, I-JEPA acheived strong performance in various tasks like image classification and depth prediction, outperforming Masked Auto Encoding (MAE) in linear probing when controlling for compute."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "This paper introduces a novel approach to Radiance Field methods for novel-view synthesis that addresses the challenges of high visual quality, costly training, and real-time rendering of unbounded, complete scenes at 1080p resolution. The authors propose three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. The method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    },
    {
      "title": "SinLU: Sinu-Sigmoidal Linear Uni",
      "url": "/research/2021/12/02/SinLU-Sinu-Sigmoidal-Linear-Uni.html",
      "date": "December 2, 2021",
      "summary": "This paper introduces a non-linear activation function called the Sinu-sigmoidal Linear Unit (SinLU), formulated as SinLU(x) = (x + a sin bx) · σ(x). It incorporates a sine wave for added functionality over traditional linear units, with two trainable parameters controlling the sinusoidal participation. The authors compare SinLU's performance to ReLU, GELU, and SiLU across various domains, models, and standard datasets. The results demonstrate SinLU's robustness and superior performance due to the incorporation of the trainable sine wave parameters, facilitating easy training and fast convergence."
    },
    {
      "title": "Learning and Leveraging World Models in Visual Representation Learning",
      "url": "/research/2024/03/01/Learning-and-Leveraging-World-Models-in-Visual-Representation-Learning.html",
      "date": "March 1, 2024",
      "summary": "The Joint-Embedding Predictive Architecture (JEPA) is a self-supervised method previously used for predicting missing input parts. This study expands JEPA to predict a wider range of corruptions by introducing Image World Models (IWM), which learn the effects of global photometric transformations in latent space. Key to effective IWM learning are conditioning, prediction difficulty, and model capacity. The study demonstrates that IWM's predictive world model, when fine-tuned, can tackle diverse tasks and either matches or outperforms existing self-supervised techniques. Additionally, it enables control over the abstraction level of learned representations, achieving either invariant or equivariant representations, similar to contrastive methods or masked image modeling, respectively."
    },
    {
      "title": "Suppressing Pink Elephants with Direct Principle Feedback",
      "url": "/research/2024/02/13/Suppressing-Pink-Elephants-with-Direct-Principle-Feedback.html",
      "date": "February 13, 2024",
      "summary": "Existing methods for controlling language models focus on training desired behaviors, but often lack the flexibility needed for diverse applications. The authors address this with the Pink Elephant Problem, demonstrating the need for language models to adapt to different contexts by avoiding certain topics (the Pink Elephant) in favor of others (Grey Elephant). The authors introduce Direct Principle Feedback (DPF), an adaptation of Constitutional AI that improves control by directly applying critiques and revisions without ranking responses. Our study shows that a 13B LLaMA 2 model fine-tuned with DPF on a synthetic dataset outperforms existing models and matches GPT-4 in managing the Pink Elephant Problem."
    }
  ],
  "calculate": [
    {
      "title": "An Investigation of how Label Smoothing Affects Generalization",
      "url": "/research/2020/10/23/AN-INVESTIGATION-OF-HOW-LABEL-SMOOTHING-AFFECTS-GENERALIZATION.html",
      "date": "October 23, 2020",
      "summary": "Label smoothing has been empirically shown to reduce overfitting and improve generalization. However, its mathematical underpinnings remain unclear. This paper explains label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. The authors identify a method for calculating a label smoothing value that minimizes generalization loss."
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "This paper introduces Batch Normalization, an online normalization technique which calculates mean and std across the batch dimension. Batch Norm improves training efficiency, allowing for higher learning rates, and decreasing hyper-parameter sensitivity, and sometimes removing the need for Dropout regularization. This paper suggests Batch Norm improves internal covariant shift, however later works have questioned this."
    },
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/research/2017/11/06/Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization.html",
      "date": "November 6, 2017",
      "summary": "This paper itroduces Instance Normalization, an online normalization technique that calculates the mean and std across individual data instances. This is shown to work well in situations where there is very high vatiation within batches, such as in style transfer."
    }
  ],
  "value": [
    {
      "title": "An Investigation of how Label Smoothing Affects Generalization",
      "url": "/research/2020/10/23/AN-INVESTIGATION-OF-HOW-LABEL-SMOOTHING-AFFECTS-GENERALIZATION.html",
      "date": "October 23, 2020",
      "summary": "Label smoothing has been empirically shown to reduce overfitting and improve generalization. However, its mathematical underpinnings remain unclear. This paper explains label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. The authors identify a method for calculating a label smoothing value that minimizes generalization loss."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces sinusoidal representation networks (SIRENs), which utilize periodic activation functions like Sin to effectively capture complex natural signals and their derivatives, addressing the limitations of neural networks parameterized for continuous, differentiable signal representations. The authors' analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. SIRENs are also applied in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations. The authors extend SIRENs' use with hypernetworks to learn priors for SIREN functions."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/2020/06/17/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "June 17, 2020",
      "summary": "Implicit neural representations, parameterized by continuous, differentiable neural networks, offer numerous advantages over conventional representations. However, existing architectures struggle with fine detail and fail to capture spatial and temporal derivatives, crucial for many physical signals described by partial differential equations. We introduce sinusoidal representation networks (SIRENs) utilizing periodic activation functions, ideal for representing complex signals and their derivatives. Analyzing SIREN activation statistics informs a principled initialization scheme. We demonstrate SIRENs' efficacy in representing various signals and solving boundary value problems, including Eikonal, Poisson, Helmholtz, and wave equations. Additionally, we integrate SIRENs with hypernetworks to learn priors over the SIREN function space. Visit the project website for detailed demonstrations."
    },
    {
      "title": "Normalized Attention Without Probability Cage",
      "url": "/research/2020/05/19/Normalized-Attention-Without-Probability-Cage.html",
      "date": "May 19, 2020",
      "summary": "Softmax-attention architectures, especially popularized by Transformers, have seen significant advancements in various tasks. However, the geometric implications of softmax-attention remain underexplored. In this study, we demonstrate limitations arising from constraining attention weights to the probability simplex and its impact on the convex hull of value vectors. We reveal sequence length-dependent biases in Transformers towards token isolation at initialization and compare them with max- and sum-pooling, which are strong but often overlooked baselines. To address these issues, we propose a novel approach of replacing softmax with normalization in self-attention, resulting in a robust and widely applicable architecture. Our findings are supported by empirical results from over 25,000 trained models, and all results and implementations are publicly available."
    },
    {
      "title": "ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Network",
      "url": "/research/2021/06/29/ASAM-Adaptive-Sharpness-Aware-Minimization-for-Scale-Invariant-Learning-of-Deep-Neural-Network.html",
      "date": "June 29, 2021",
      "summary": "Recent advances in learning algorithms have highlighted the importance of the sharpness of the loss surface as a key indicator of the generalization gap, achieving state-of-the-art results. However, traditional sharpness measures, defined within a fixed region, suffer from sensitivity to parameter scaling, weakening their predictive value for the generalization gap. This paper introduces a scale-invariant measure, adaptive sharpness, along with a new generalization bound. We present a new learning method, adaptive sharpness-aware minimization (ASAM), which leverages this bound. Our experiments across various benchmark datasets demonstrate that ASAM significantly enhances model generalization."
    },
    {
      "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
      "url": "/research/2024/02/27/The-Era-of-1-bit-LLMs-All-Large-Language-Models-are-in-1.58-Bits.html",
      "date": "February 27, 2024",
      "summary": "Recent research introduces BitNet b1.58, a ternary (values of -1, 0, 1) 1-bit Large Language Model (LLM) that matches traditional full-precision LLMs in performance and perplexity but is significantly more efficient in terms of latency, memory, throughput, and energy. This work establishes a new scaling law and training approach for future high-performance, cost-effective LLMs, while also facilitating the design of specialized hardware optimized for 1-bit LLMs."
    },
    {
      "title": "The loss landscape of deep linear neural networks: a second-order analysis",
      "url": "/research/2022/03/11/The-loss-landscape-of-deep-linear-neural-networks-a-second-order-analysis.html",
      "date": "March 11, 2022",
      "summary": "This study investigates the optimization landscape of deep linear neural networks with square loss, focusing on the role of non-strict saddle points in algorithm dynamics, which has been previously understudied. the authors conduct a comprehensive second-order analysis, identifying global minimizers, strict, and non-strict saddle points among all critical points, alongside their critical values. Our findings, based on conditions related to the ranks of partial matrix products, contribute insights into global convergence and implicit regularization observed in optimizing linear neural networks. Additionally, the authors offer an explicit parameterization of global minimizers and identify extensive sets of strict and non-strict saddle points."
    }
  ],
  "bootstrap": [
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/research/2020/09/10/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning.html",
      "date": "September 10, 2020",
      "summary": "This paper introduces BYOL, a self-supervised learning method that does not need negative pairs. BYOL uses a teacher student pair of networks, where the teacher is the EMA of the student. The teacher and student are given two differently augmented versionos the input, and the loss is the cross entropy difference between the outputs."
    }
  ],
  "byol": [
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/research/2020/09/10/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning.html",
      "date": "September 10, 2020",
      "summary": "This paper introduces BYOL, a self-supervised learning method that does not need negative pairs. BYOL uses a teacher student pair of networks, where the teacher is the EMA of the student. The teacher and student are given two differently augmented versionos the input, and the loss is the cross entropy difference between the outputs."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "This paper shows self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs or convnets. Also proposes a modification to BYOL self-supervised learning with a momentum encoder, multi-crop training, and feature sharping for the teacher network."
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Towards a Unified Theoretical Understanding of Non-contrastive Learning via Rank Differential Mechanism",
      "url": "/research/2023/03/04/Towards-a-Unified-Theoretical-Understanding-of-Non-contrastive-Learning-via-Rank-Differential-Mechanism.html",
      "date": "March 4, 2023",
      "summary": "Recent non-contrastive learning methods like BYOL, SimSiam, SwAV, and DINO have shown that asymmetric architectural designs can achieve good self-supervised visual learning performance by aligning positive pairs alone, despite a lack of unified theoretical understanding on how these designs prevent feature collapse. This study introduces the Rank Differential Mechanism (RDM) as a unified theoretical framework for non-contrastive learning. RDM demonstrates that these asymmetric designs maintain a consistent rank difference in output features, enhancing effective dimensionality and preventing feature collapse. Unlike previous theories, RDM applies to designs both with and without a predictor, offering a comprehensive understanding of non-contrastive learning methods and guiding the development of new variants. Experiments confirm that these variants perform comparably, if not better than, existing methods on benchmark datasets."
    }
  ],
  "pair": [
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/research/2020/09/10/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning.html",
      "date": "September 10, 2020",
      "summary": "This paper introduces BYOL, a self-supervised learning method that does not need negative pairs. BYOL uses a teacher student pair of networks, where the teacher is the EMA of the student. The teacher and student are given two differently augmented versionos the input, and the loss is the cross entropy difference between the outputs."
    },
    {
      "title": "CLIP: Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns from raw text descriptions of images, moving beyond training on fixed object categories. By pre-training on 400 million (image, text) pairs using caption-image matching, the method achieves state-of-the-art image representations from scratch. This allows for zero-shot transfer to various downstream tasks using natural language, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse datasets, showing competitive results against fully supervised baselines without task-specific training, and matching the accuracy of ResNet-50 on ImageNet zero-shot without using its training examples."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "This paper introduces Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperformed CLIP models on various downstream tasks. The speedup from this method enables experimentation with larger model sizes, more data, or longer training periods."
    },
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "This paper introduces a curriculum learning framework for image-caption pretraining, inspired by children's language learning from cognitive science, to address the challenges of aligning multiple concepts from captions to objects in images. The method starts with simple image-caption pairs and gradually increases complexity by adding more concepts, leveraging knowledge from each phase for subsequent learning. This approach outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios."
    },
    {
      "title": "Towards a Unified Theoretical Understanding of Non-contrastive Learning via Rank Differential Mechanism",
      "url": "/research/2023/03/04/Towards-a-Unified-Theoretical-Understanding-of-Non-contrastive-Learning-via-Rank-Differential-Mechanism.html",
      "date": "March 4, 2023",
      "summary": "Recent non-contrastive learning methods like BYOL, SimSiam, SwAV, and DINO have shown that asymmetric architectural designs can achieve good self-supervised visual learning performance by aligning positive pairs alone, despite a lack of unified theoretical understanding on how these designs prevent feature collapse. This study introduces the Rank Differential Mechanism (RDM) as a unified theoretical framework for non-contrastive learning. RDM demonstrates that these asymmetric designs maintain a consistent rank difference in output features, enhancing effective dimensionality and preventing feature collapse. Unlike previous theories, RDM applies to designs both with and without a predictor, offering a comprehensive understanding of non-contrastive learning methods and guiding the development of new variants. Experiments confirm that these variants perform comparably, if not better than, existing methods on benchmark datasets."
    }
  ],
  "teacher": [
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/research/2020/09/10/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning.html",
      "date": "September 10, 2020",
      "summary": "This paper introduces BYOL, a self-supervised learning method that does not need negative pairs. BYOL uses a teacher student pair of networks, where the teacher is the EMA of the student. The teacher and student are given two differently augmented versionos the input, and the loss is the cross entropy difference between the outputs."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "This paper shows self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs or convnets. Also proposes a modification to BYOL self-supervised learning with a momentum encoder, multi-crop training, and feature sharping for the teacher network."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This study suggest that label smoothing not only boosts generalization but also enhances model calibration. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. Label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces TinyViT, a series of compact and efficient vision transformers (ViTs) designed for devices with limited resources. The authors employ a fast distillation framework to transfer knowledge from large pretrained models to smaller ones during pretraining, using sparsified logits from teacher models to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. With increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters, and demonstrates strong transferability across various downstream tasks."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "Training data-efficient image transformers & distillation through attention",
      "url": "/research/2021/01/15/Training-data-efficient-image-transformers-&-distillation-through-attention.html",
      "date": "January 15, 2021",
      "summary": "This paper introduces DeiT, an efficient method for training vision transformers. The authors present a unique teacher-student strategy for transformers, leveraging a distillation token for efficient learning from a convolutional network teacher. This method achieves performance on par with convolutional networks, with up to 85.2% accuracy on ImageNet, and demonstrates effective transferability to other tasks."
    },
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces an iterative self-training framework for sim-to-real 6D object pose estimation to enable cost-effective robotic bin-picking. The authors create a photo-realistic simulator to synthesize virtual data for training an initial pose estimation network (teacher model). This teacher predicts poses on unlabeled real data, and an adaptive selection scheme filters reliable predictions to generate pseudo-labels for training a student model on real data. Iteratively refining the teacher with the student improves pseudo-label quality. Evaluated on public benchmarks and a new dataset, their method shows 11.49% and 22.62% ADD(-S) improvements and a 19.54% increase in bin-picking success, demonstrating the effectiveness of this iterative sim-to-real approach."
    }
  ],
  "student": [
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/research/2020/09/10/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning.html",
      "date": "September 10, 2020",
      "summary": "This paper introduces BYOL, a self-supervised learning method that does not need negative pairs. BYOL uses a teacher student pair of networks, where the teacher is the EMA of the student. The teacher and student are given two differently augmented versionos the input, and the loss is the cross entropy difference between the outputs."
    },
    {
      "title": "Training data-efficient image transformers & distillation through attention",
      "url": "/research/2021/01/15/Training-data-efficient-image-transformers-&-distillation-through-attention.html",
      "date": "January 15, 2021",
      "summary": "This paper introduces DeiT, an efficient method for training vision transformers. The authors present a unique teacher-student strategy for transformers, leveraging a distillation token for efficient learning from a convolutional network teacher. This method achieves performance on par with convolutional networks, with up to 85.2% accuracy on ImageNet, and demonstrates effective transferability to other tasks."
    },
    {
      "title": "Self-Paced Curriculum Learning",
      "url": "/research/2015/01/01/Self-Paced-Curriculum-Learning.html",
      "date": "January 1, 2015",
      "summary": "This paper introduces self-paced curriculum learning (SPCL), a unified framework combining curriculum learning (CL) and self-paced learning (SPL). SPCL leverages prior knowledge and ongoing learning progress through an optimization problem. It mimics collaborative instructor-student learning, exhibiting empirical advantages in two tasks."
    },
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces an iterative self-training framework for sim-to-real 6D object pose estimation to enable cost-effective robotic bin-picking. The authors create a photo-realistic simulator to synthesize virtual data for training an initial pose estimation network (teacher model). This teacher predicts poses on unlabeled real data, and an adaptive selection scheme filters reliable predictions to generate pseudo-labels for training a student model on real data. Iteratively refining the teacher with the student improves pseudo-label quality. Evaluated on public benchmarks and a new dataset, their method shows 11.49% and 22.62% ADD(-S) improvements and a 19.54% increase in bin-picking success, demonstrating the effectiveness of this iterative sim-to-real approach."
    }
  ],
  "ema": [
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/research/2020/09/10/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning.html",
      "date": "September 10, 2020",
      "summary": "This paper introduces BYOL, a self-supervised learning method that does not need negative pairs. BYOL uses a teacher student pair of networks, where the teacher is the EMA of the student. The teacher and student are given two differently augmented versionos the input, and the loss is the cross entropy difference between the outputs."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    }
  ],
  "give": [
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/research/2020/09/10/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning.html",
      "date": "September 10, 2020",
      "summary": "This paper introduces BYOL, a self-supervised learning method that does not need negative pairs. BYOL uses a teacher student pair of networks, where the teacher is the EMA of the student. The teacher and student are given two differently augmented versionos the input, and the loss is the cross entropy difference between the outputs."
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. This self-supervised method uses multiple crops of an image. Given a context crop the network is trained to predict the embeddings of several target crops. When applied to Vision Transformers, I-JEPA acheived strong performance in various tasks like image classification and depth prediction, outperforming Masked Auto Encoding (MAE) in linear probing when controlling for compute."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) uses synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. Given this perspective, a one-instance fractal database (OFDB) is developed where only a single image per category is present. Despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M comparable or superior results are obtaine to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning."
    },
    {
      "title": "Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding",
      "url": "/research/2021/11/16/Improved-Robustness-of-Vision-Transformer-via-PreLayerNorm-in-Patch-Embedding.html",
      "date": "November 16, 2021",
      "summary": "Recent advancements in vision transformers (ViTs) have shown superior performance across diverse visual tasks, surpassing convolutional neural networks (CNNs). Given ViT's distinct architecture, understanding its behavior and reliability is imperative. This paper investigates ViT's robustness by comparing it with CNNs under various image corruptions relevant to real-world vision tasks. While ViT generally exhibits comparable or improved robustness over CNNs, it consistently underperforms in contrast enhancement tasks. Analysis suggests that positional embedding in ViT's patch embedding may malfunction with color scale changes. We propose PreLayerNorm, a modified patch embedding structure, to address this issue and ensure scale-invariant behavior in ViT. ViT with PreLayerNorm demonstrates enhanced robustness across various corruptions, particularly in contrast-varying environments."
    }
  ],
  "versiono": [
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/research/2020/09/10/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning.html",
      "date": "September 10, 2020",
      "summary": "This paper introduces BYOL, a self-supervised learning method that does not need negative pairs. BYOL uses a teacher student pair of networks, where the teacher is the EMA of the student. The teacher and student are given two differently augmented versionos the input, and the loss is the cross entropy difference between the outputs."
    }
  ],
  "cross": [
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/research/2020/09/10/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning.html",
      "date": "September 10, 2020",
      "summary": "This paper introduces BYOL, a self-supervised learning method that does not need negative pairs. BYOL uses a teacher student pair of networks, where the teacher is the EMA of the student. The teacher and student are given two differently augmented versionos the input, and the loss is the cross entropy difference between the outputs."
    },
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/research/2023/03/08/Extreme-Masking-for-Learning-Instance-and-Distributed-Visual-Representations.html",
      "date": "March 8, 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance."
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "This paper introduces the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a multi-directional Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions."
    },
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "This paper introduces tools for training monocular depth estimation models with multiple datasets despite incompatible annotations. Their approach uses a robust training objective, multi-objective learning for data integration, and pretraining encoders on auxiliary tasks. Tested across five datasets, including 3D films as a novel data source, their methods achieve superior zero-shot cross-dataset generalization, outperforming existing benchmarks through principled multi-dataset training."
    },
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    }
  ],
  "entropy": [
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/research/2020/09/10/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning.html",
      "date": "September 10, 2020",
      "summary": "This paper introduces BYOL, a self-supervised learning method that does not need negative pairs. BYOL uses a teacher student pair of networks, where the teacher is the EMA of the student. The teacher and student are given two differently augmented versionos the input, and the loss is the cross entropy difference between the outputs."
    }
  ],
  "difference": [
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/research/2020/09/10/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning.html",
      "date": "September 10, 2020",
      "summary": "This paper introduces BYOL, a self-supervised learning method that does not need negative pairs. BYOL uses a teacher student pair of networks, where the teacher is the EMA of the student. The teacher and student are given two differently augmented versionos the input, and the loss is the cross entropy difference between the outputs."
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, the authors demonstrate the close relationship between these two families. This analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized."
    },
    {
      "title": "Do Vision Transformers See Like Convolutional Neural Networks?",
      "url": "/research/2022/03/03/Do-Vision-Transformers-See-Like-Convolutional-Neural-Networks.html",
      "date": "March 3, 2022",
      "summary": "Convolutional Neural Networks (CNNs) have been prominent in processing visual data, but recent studies show Vision Transformers (ViTs) can match or outperform CNNs in image classification. This work examines how ViTs solve classification tasks, discovering significant differences from CNNs, including uniformity in ViT representations across layers due to self-attention and residual connections that enhance feature propagation. ViTs also maintain spatial information effectively, influenced by classification methods. The study further explores how dataset scale affects ViT features and their transferability, linking these findings to new architectures like MLP-Mixer."
    },
    {
      "title": "Neural Network Diffusion",
      "url": "/research/2024/02/20/Neural-Network-Diffusion.html",
      "date": "February 20, 2024",
      "summary": "Diffusion models, known for their success in image and video generation, can also generate high-performing neural network parameters, as demonstrated in this study. By employing a simple combination of an autoencoder and a standard latent diffusion model, our method involves extracting latent representations of network parameters, which are then synthesized from random noise by the diffusion model. These new representations, processed through the autoencoder's decoder, serve as fresh network parameters. Tested across various architectures and datasets, this diffusion approach consistently produces models with comparable or superior performance to traditionally trained networks at minimal extra cost. Importantly, the generated models show distinct performance differences from the trained ones, suggesting further exploration into the versatile applications of diffusion models."
    },
    {
      "title": "Towards a Unified Theoretical Understanding of Non-contrastive Learning via Rank Differential Mechanism",
      "url": "/research/2023/03/04/Towards-a-Unified-Theoretical-Understanding-of-Non-contrastive-Learning-via-Rank-Differential-Mechanism.html",
      "date": "March 4, 2023",
      "summary": "Recent non-contrastive learning methods like BYOL, SimSiam, SwAV, and DINO have shown that asymmetric architectural designs can achieve good self-supervised visual learning performance by aligning positive pairs alone, despite a lack of unified theoretical understanding on how these designs prevent feature collapse. This study introduces the Rank Differential Mechanism (RDM) as a unified theoretical framework for non-contrastive learning. RDM demonstrates that these asymmetric designs maintain a consistent rank difference in output features, enhancing effective dimensionality and preventing feature collapse. Unlike previous theories, RDM applies to designs both with and without a predictor, offering a comprehensive understanding of non-contrastive learning methods and guiding the development of new variants. Experiments confirm that these variants perform comparably, if not better than, existing methods on benchmark datasets."
    },
    {
      "title": "VideoMamba: State Space Model for Efficient Video Understanding",
      "url": "/research/2024/03/12/VideoMamba-State-Space-Model-for-Efficient-Video-Understanding.html",
      "date": "March 12, 2024",
      "summary": "This work introduces VideoMamba, an innovative approach that addresses local redundancy and global dependencies in video understanding by adapting the Mamba framework to video analysis. VideoMamba outperforms existing 3D convolutional neural networks and video transformers through its linear-complexity operator, which facilitates efficient long-term modeling for high-resolution, long-duration videos. Its effectiveness is demonstrated across four main areas: scalability in the visual domain without needing extensive dataset pretraining, thanks to a novel self-distillation technique; the ability to recognize short-term actions with fine-grained motion differences; superior performance in long-term video understanding compared to traditional feature-based models; and robust compatibility with multiple modalities, enhancing multi-modal video analysis. VideoMamba establishes a new standard for comprehensive and efficient video understanding."
    }
  ],
  "cmae": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a self-supervised method that adds a contrastive loss to the Masked Autoencoder. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning."
    }
  ],
  "add": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a self-supervised method that adds a contrastive loss to the Masked Autoencoder. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning."
    },
    {
      "title": "Dropout as Data Augmentation",
      "url": "/research/2016/01/08/DROPOUT-AS-DATA-AUGMENTATION.html",
      "date": "January 8, 2016",
      "summary": "Dropout is typically interpreted as turning the test time network into an ensemble of the thinner training networks. This paper argues that dropout can also be interpreted as a kind of data augmentation in image space. Authors present an approach to project the dropout noise within a network back into the input space, visualising the augmented versions of the training data, and show that training a deterministic network on the augmented samples yields similar results. Authors then propose a new dropout noise scheme and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This paper is effectively a follow-up to DinoV2. The study addresses the issue of artifacts in feature maps of Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. The authors introduce a straightforward strategy of adding extra tokens to the input sequence of the Vision Transformer, which eliminates these artifacts for both supervised and self-supervised models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks, facilitates object discovery with larger models, and results in smoother feature and attention maps for downstream visual processing tasks."
    },
    {
      "title": "SinLU: Sinu-Sigmoidal Linear Uni",
      "url": "/research/2021/12/02/SinLU-Sinu-Sigmoidal-Linear-Uni.html",
      "date": "December 2, 2021",
      "summary": "This paper introduces a non-linear activation function called the Sinu-sigmoidal Linear Unit (SinLU), formulated as SinLU(x) = (x + a sin bx) · σ(x). It incorporates a sine wave for added functionality over traditional linear units, with two trainable parameters controlling the sinusoidal participation. The authors compare SinLU's performance to ReLU, GELU, and SiLU across various domains, models, and standard datasets. The results demonstrate SinLU's robustness and superior performance due to the incorporation of the trainable sine wave parameters, facilitating easy training and fast convergence."
    },
    {
      "title": "Large Batch Training of Convolutional Networks",
      "url": "/research/2017/09/13/Large-Batch-Training-of-Convolutional-Networks.html",
      "date": "September 13, 2017",
      "summary": "To expedite the training of large convolutional networks, computational units are added and trained with data-parallel synchronous Stochastic Gradient Descent (SGD) across units, increasing batch size with node count. However, larger batch sizes can reduce model accuracy. The existing method of large batch training—linear learning rate scaling with warm-up—is not universally effective and may cause training divergence. To address these challenges, we introduce a novel training algorithm, Layer-wise Adaptive Rate Scaling (LARS), enabling us to train Alexnet with a batch size of 8K and Resnet-50 with a batch size of 32K, without compromising accuracy."
    }
  ],
  "branch": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a self-supervised method that adds a contrastive loss to the Masked Autoencoder. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning."
    }
  ],
  "encoder": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a self-supervised method that adds a contrastive loss to the Masked Autoencoder. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "This paper shows self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs or convnets. Also proposes a modification to BYOL self-supervised learning with a momentum encoder, multi-crop training, and feature sharping for the teacher network."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. MAE involves masking random patches of an input image and reconstructing the missing pixels using an asymmetric encoder-decoder architecture and a lightweight decoder. Masking a significant portion of the input image creates a challenging yet informative self-supervisory task. These innovations allow for efficient training of large models, tripling training speed and enhancing accuracy. A vanilla ViT-Huge model reaches top accuracy (87.8%) on ImageNet-1K data among similar methods. The model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating promising scaling potential."
    },
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model consists of two parallel transformer encoders (text prompt and image) and a shared mask decoder. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. "
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper investigates how self-supervised pretraining methods learn part-aware representations. The authors describe contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting that these methods predispose encoders to recognize object parts. Through empirical comparison, they find that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. This encoder serves as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The DMAE ViT-Base model achieves comparable or superior certified accuracy with fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on ImageNet. The model also shows high transferability to CIFAR-10, indicating its broad applicability."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "This paper introduces a self-supervised vision model pretraining framework inspired by energy-based models (EBMs), leveraging symmetric mappings in deep networks without auxiliary components. The framework models energy estimation and data restoration through the network's forward and backward passes, assigning low energy to unlabeled dataset samples and using gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting various pretext tasks. Experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "This paper introduces a curriculum learning framework for image-caption pretraining, inspired by children's language learning from cognitive science, to address the challenges of aligning multiple concepts from captions to objects in images. The method starts with simple image-caption pairs and gradually increases complexity by adding more concepts, leveraging knowledge from each phase for subsequent learning. This approach outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios."
    },
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "This paper introduces tools for training monocular depth estimation models with multiple datasets despite incompatible annotations. Their approach uses a robust training objective, multi-objective learning for data integration, and pretraining encoders on auxiliary tasks. Tested across five datasets, including 3D films as a novel data source, their methods achieve superior zero-shot cross-dataset generalization, outperforming existing benchmarks through principled multi-dataset training."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    },
    {
      "title": "Deep learning via Hessian-free optimization",
      "url": "/research/2010/06/21/Deep-learning-via-Hessian-free-optimization.html",
      "date": "June 21, 2010",
      "summary": "We present a novel 2nd-order optimization method inspired by the Hessian-free approach, applied to deep auto-encoders. Achieving superior results without pre-training compared to Hinton & Salakhutdinov (2006), our method is practical, user-friendly, scalable to large datasets, and versatile across various model classes. Additionally, we address the challenge of pathological curvature in deep learning, highlighting how our method effectively mitigates this issue."
    },
    {
      "title": "FNet: Mixing Tokens with Fourier Transforms",
      "url": "/research/2022/05/26/FNet-Mixing-Tokens-with-Fourier-Transforms.html",
      "date": "May 26, 2022",
      "summary": "We demonstrate that Transformer encoder architectures can be accelerated with minimal impact on accuracy by substituting self-attention sublayers with simple linear transformations. Remarkably, using a standard Fourier Transform instead of the self-attention sublayer in a Transformer encoder achieves 92-97% of BERT models' accuracy on the GLUE benchmark, while training 80% faster on GPUs and 70% faster on TPUs for standard 512 input lengths. Our FNet model significantly outperforms in speed at longer input lengths, matching the accuracy of the most accurate models on the Long Range Arena benchmark and surpassing the fastest models in speed across all sequence lengths on GPUs and shorter lengths on TPUs. Additionally, FNet is lightweight in memory use and exceptionally efficient in smaller sizes, outperforming Transformer models under the same speed and accuracy constraints."
    },
    {
      "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
      "url": "/research/2024/02/28/CL-MAE:-Curriculum-Learned-Masked-Autoencoders.html",
      "date": "February 28, 2024",
      "summary": "This paper proposes a curriculum learning approach that updates the masking strategy of Masked Auto Encoders (MAE) to progressively increase the complexity of the self-supervised reconstruction task. To achieve this, a novel learnable masking module is introduced, capable of generating masks of varying complexities, and integrated into masked autoencoders (MAE). This module is jointly trained with the MAE, adjusting its behavior from partner (optimizing the same reconstruction loss) to adversary (optimizing the opposite loss), with a smooth transition regulated by a factor multiplied with the reconstruction loss. This training procedure creates an easy-to-hard curriculum. The Curriculum-Learned Masked Autoencoder (CL-MAE) is trained on ImageNet and demonstrates superior representation learning compared to MAE. Empirical results on five downstream tasks confirm that curriculum learning can successfully self-supervise masked autoencoders."
    }
  ],
  "decoder": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a self-supervised method that adds a contrastive loss to the Masked Autoencoder. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. MAE involves masking random patches of an input image and reconstructing the missing pixels using an asymmetric encoder-decoder architecture and a lightweight decoder. Masking a significant portion of the input image creates a challenging yet informative self-supervisory task. These innovations allow for efficient training of large models, tripling training speed and enhancing accuracy. A vanilla ViT-Huge model reaches top accuracy (87.8%) on ImageNet-1K data among similar methods. The model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating promising scaling potential."
    },
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model consists of two parallel transformer encoders (text prompt and image) and a shared mask decoder. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. "
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "This paper introduces a self-supervised vision model pretraining framework inspired by energy-based models (EBMs), leveraging symmetric mappings in deep networks without auxiliary components. The framework models energy estimation and data restoration through the network's forward and backward passes, assigning low energy to unlabeled dataset samples and using gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting various pretext tasks. Experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    },
    {
      "title": "Neural Network Diffusion",
      "url": "/research/2024/02/20/Neural-Network-Diffusion.html",
      "date": "February 20, 2024",
      "summary": "Diffusion models, known for their success in image and video generation, can also generate high-performing neural network parameters, as demonstrated in this study. By employing a simple combination of an autoencoder and a standard latent diffusion model, our method involves extracting latent representations of network parameters, which are then synthesized from random noise by the diffusion model. These new representations, processed through the autoencoder's decoder, serve as fresh network parameters. Tested across various architectures and datasets, this diffusion approach consistently produces models with comparable or superior performance to traditionally trained networks at minimal extra cost. Importantly, the generated models show distinct performance differences from the trained ones, suggesting further exploration into the versatile applications of diffusion models."
    }
  ],
  "discriminability": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a self-supervised method that adds a contrastive loss to the Masked Autoencoder. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning."
    }
  ],
  "adamw": [
    {
      "title": "ADAMW: Decoupled Weight Decay Regularization",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "This paper shows that L2 regularization and weight decay are equivalent for standard SGD but not for adaptive methods like ADAM. To account of this, these authors separate weight decay from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization",
      "url": "/research/2023/07/04/Gradient-Norm-Aware-Minimization-Seeks-First-Order-Flatness-and-Improves-Generalization.html",
      "date": "July 4, 2023",
      "summary": "Recent advancements have highlighted the effectiveness of flat minima in enhancing generalization, particularly through Sharpness-Aware Minimization (SAM). However, existing definitions of flatness, such as zeroth-order flatness, have limitations in discerning between minima with low and high generalization errors. To address this, we propose first-order flatness, which considers maximal gradient norm within a perturbation radius. We introduce Gradient norm Aware Minimization (GAM) as a novel training approach to achieve uniformly small curvature across all directions. Experimental results demonstrate GAM's ability to enhance generalization compared to standard optimizers like SGD and AdamW across various datasets and networks. Moreover, GAM facilitates SAM in identifying flatter minima, leading to improved generalization."
    },
    {
      "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
      "url": "/research/2022/03/19/Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training.html",
      "date": "March 19, 2022",
      "summary": "Sharpness-Aware Minimization (SAM) enhances generalization by optimizing a neighborhood-based perturbed loss, but it doesn't always favor flat minima due to both sharp and flat minima having low perturbed losses. We introduce a new measure, the surrogate gap, reflecting the dominant Hessian eigenvalue at small neighborhood radii, which is simple to compute and can be minimized during training. We propose the Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), an advancement over SAM with minimal additional computational cost. GSAM employs a dual-step approach: first, minimizing the perturbed loss similar to SAM, and second, reducing the surrogate gap without affecting the perturbed loss to target regions with low loss and sharpness, thus achieving superior generalization. GSAM is theoretically robust, showing better convergence and empirical generalization improvements, notably a +3.2% gain over SAM and +5.4% over AdamW in ImageNet accuracy for ViT-B/32."
    }
  ],
  "decouple": [
    {
      "title": "ADAMW: Decoupled Weight Decay Regularization",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "This paper shows that L2 regularization and weight decay are equivalent for standard SGD but not for adaptive methods like ADAM. To account of this, these authors separate weight decay from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate."
    },
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "This paper introduces a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. This decouples batch size from the loss, and allows the authors to increase the batch size and also improves performance even with smaller batches. Pushing the batch size to one million showed diminishing returns, with 32k being a good middle ground."
    },
    {
      "title": "AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks",
      "url": "/research/2023/03/01/AdaSAM-Boosting-Sharpness-Aware-Minimization-with-Adaptive-Learning-Rate-and-Momentum-for-Training-Deep-Neural-Networks.html",
      "date": "March 1, 2023",
      "summary": "The Sharpness Aware Minimization (SAM) optimizer, known for enhancing the generalization of deep neural networks by introducing extra perturbation steps, is further developed into AdaSAM by integrating adaptive learning rates and momentum acceleration. Although AdaSAM has been empirically applied to large-scale networks, a theoretical understanding of its performance, considering the complexity of its components, was lacking. This study presents a theoretical analysis of AdaSAM's convergence in stochastic non-convex settings, demonstrating a convergence rate of O(1/√bT) that scales linearly with mini-batch size. By introducing a delayed second-order momentum term, the study successfully decouples and analyzes the intertwined effects of stochastic gradients, adaptive learning rates, and perturbations. This is the first work to offer a detailed convergence rate for SAM with adaptive mechanisms. Experimental results on various NLP tasks indicate AdaSAM's superior performance over SGD, AMSGrad, and SAM optimizers."
    }
  ],
  "weight": [
    {
      "title": "ADAMW: Decoupled Weight Decay Regularization",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "This paper shows that L2 regularization and weight decay are equivalent for standard SGD but not for adaptive methods like ADAM. To account of this, these authors separate weight decay from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate."
    },
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights. This paper shows deep networks have significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. The authors propose a stochastic projection rule that sets a new benchmark CIFAR-10 without data augmentation."
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing is a regularization technique that reduces overfitting and enhances classification performance. Label smothing involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution. This significantly improves classification accuracy and model robustness to noisy labels."
    },
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/research/2019/07/26/DropAttention-A-Regularization-Method-for-Fully-Connected-Self-Attention-Networks.html",
      "date": "July 26, 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. DropAttention uses a mask to zero out elements in the attention matrix. Experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    },
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a method for dropout in Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "This research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. The authors validate these approaches using the ImageNet-1k dataset and confirm their findings with the ImageNet-v2 test set, evaluating transfer performance across six additional datasets."
    },
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "This paper introduces the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. The authors' empirical evaluation across computer vision, natural language processing, and speech tasks demonstrates that GELU offers performance improvements over ReLU and ELU activations."
    },
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "This paper introduces an algorithm that identifies simple and highly generalizable neural networks by searching for extensive flat minima regions in the error function, where the error rate is relatively stable. Such flat minima are associated with lower overfitting risks based on minimum description length (MDL) principles. Despite requiring second-order derivative calculations, the algorithm has a complexity level comparable to backpropagation. When tested on feedforward and recurrent networks, as well as stock market prediction tasks, this algorithm outperformed traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    },
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    },
    {
      "title": "A comprehensive review of Binary Neural Network",
      "url": "/research/2023/05/11/A-comprehensive-review-of-Binary-Neural-Network.html",
      "date": "May 11, 2023",
      "summary": "Deep learning (DL) is widely used in intelligent systems and real-life applications, but its deployment on computationally limited and energy-constrained devices requires efficient technologies like Binary Neural Networks (BNN). BNNs save significant storage, computation cost, and energy, making them ideal for small devices, despite the trade-offs in memory and performance. This article offers a comprehensive review of BNN developments, focusing on 1-bit activations and weights. It covers the evolution of BNN, from early models to advanced algorithms and design aspects, and addresses BNN optimization, deployment, computing architectures, and diverse applications. It also outlines potential research directions in BNN technology."
    },
    {
      "title": "How to decay your learning rate",
      "url": "/research/2021/05/23/How-to-decay-your-learning-rate.html",
      "date": "May 23, 2021",
      "summary": "Empirical findings suggest that typical fine-tuned learning rate schedules decay the learning rate following weight norm fluctuations. This led to the development of ABEL, an automatic scheduler that adjusts the learning rate based on weight norm changes. ABEL performs comparably to tuned schedules but demonstrates greater robustness to parameter variations. Extensive experiments across various domains reveal that when the weight norm remains stable, simplified schedules yield equivalent performance to complex ones, resembling a constant learning rate with decay towards the end of training."
    },
    {
      "title": "Root Mean Square Layer Normalization",
      "url": "/research/2019/10/16/Root-Mean-Square-Layer-Normalization.html",
      "date": "October 16, 2019",
      "summary": "Layer normalization (LayerNorm) enhances deep neural network stability and convergence by re-centering and re-scaling inputs and weight matrices. However, its computational overhead slows networks, particularly RNNs. We introduce RMSNorm, which replaces re-centering with root mean square (RMS) regularization. RMSNorm maintains re-scaling invariance and adapts learning rates implicitly, while being computationally simpler than LayerNorm. We also propose partial RMSNorm (pRMSNorm), estimating RMS from a subset of inputs. Empirical results across various tasks and architectures demonstrate that RMSNorm achieves comparable performance to LayerNorm while reducing running time by 7%∼64%."
    },
    {
      "title": "Weight Agnostic Neural Networks",
      "url": "/research/2019/09/05/Weight-Agnostic-Neural-Networks.html",
      "date": "September 5, 2019",
      "summary": "This study investigates the importance of neural network architectures versus weight parameters for task performance. We introduce a method to find architectures capable of performing tasks without weight training. By assigning random weights, we show that minimal architectures can achieve notable performance on various tasks, including reinforcement learning and MNIST classification."
    },
    {
      "title": "Normalized Attention Without Probability Cage",
      "url": "/research/2020/05/19/Normalized-Attention-Without-Probability-Cage.html",
      "date": "May 19, 2020",
      "summary": "Softmax-attention architectures, especially popularized by Transformers, have seen significant advancements in various tasks. However, the geometric implications of softmax-attention remain underexplored. In this study, we demonstrate limitations arising from constraining attention weights to the probability simplex and its impact on the convex hull of value vectors. We reveal sequence length-dependent biases in Transformers towards token isolation at initialization and compare them with max- and sum-pooling, which are strong but often overlooked baselines. To address these issues, we propose a novel approach of replacing softmax with normalization in self-attention, resulting in a robust and widely applicable architecture. Our findings are supported by empirical results from over 25,000 trained models, and all results and implementations are publicly available."
    },
    {
      "title": "Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks",
      "url": "/research/2017/10/23/Cosine-Normalization-Using-Cosine-Similarity-Instead-of-Dot-Product-in-Neural-Networks.html",
      "date": "October 23, 2017",
      "summary": "In traditional multi-layer neural networks, the dot product between the output and weight vectors of preceding layers serves as input to the activation function, resulting in unbounded outputs and increased variance. This variance can lead to poor generalization and hinder training by exacerbating internal covariate shift. To address this, we propose cosine normalization, which replaces the dot product with cosine similarity or centered cosine similarity (Pearson Correlation Coefficient). We evaluate cosine normalization against batch, weight, and layer normalization in fully-connected and convolutional neural networks across various datasets. Our experiments demonstrate that cosine normalization outperforms other normalization techniques."
    },
    {
      "title": "Sharpness-Aware Training for Free",
      "url": "/research/2023/03/02/Sharpness-Aware-Training-for-Free.html",
      "date": "March 2, 2023",
      "summary": "Modern deep neural networks (DNNs) excel in performance but often suffer from over-parameterization, leading to increased generalization error without tailored training strategies. Sharpness-Aware Minimization (SAM) has proven effective in reducing generalization error by minimizing sharpness in the loss landscape. However, SAM incurs a significant computational overhead. This paper introduces Sharpness-Aware Training for Free (SAF), which mitigates sharpness at nearly zero additional computational cost. SAF achieves this by preventing sudden drops in loss within sharp local minima during weight updates. A novel trajectory loss, based on KL-divergence between current and past DNN outputs, replaces SAM's sharpness measure, guiding convergence towards flat minima for enhanced generalization. Empirical results demonstrate SAF's effectiveness on ImageNet with comparable computational efficiency to the base optimizer."
    },
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    },
    {
      "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
      "url": "/research/2023/11/30/Dichotomy-of-Early-and-Late-Phase-Implicit-Biases-Can-Provably-Induce-Grokking.html",
      "date": "November 30, 2023",
      "summary": "Power et al. (2022) discovered that neural networks initially memorize arithmetic tasks, achieving perfect training accuracy but poor test accuracy, then suddenly achieve perfect test accuracy after extended training. This study explains this grokking phenomenon with the theory that early and late phase implicit biases cause this shift. Training homogeneous neural networks with large initialization and small weight decay on classification and regression tasks results in a prolonged period where the network acts like a kernel predictor, followed by a sudden shift to min-norm/max-margin predictors, significantly improving test accuracy."
    },
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "decay": [
    {
      "title": "ADAMW: Decoupled Weight Decay Regularization",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "This paper shows that L2 regularization and weight decay are equivalent for standard SGD but not for adaptive methods like ADAM. To account of this, these authors separate weight decay from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate."
    },
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "This paper introduces an algorithm that identifies simple and highly generalizable neural networks by searching for extensive flat minima regions in the error function, where the error rate is relatively stable. Such flat minima are associated with lower overfitting risks based on minimum description length (MDL) principles. Despite requiring second-order derivative calculations, the algorithm has a complexity level comparable to backpropagation. When tested on feedforward and recurrent networks, as well as stock market prediction tasks, this algorithm outperformed traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    },
    {
      "title": "Automated Learning Rate Scheduler for Large-batch",
      "url": "/research/2021/07/13/Automated-Learning-Rate-Scheduler-for-Large-batch.html",
      "date": "July 13, 2021",
      "summary": "Large-batch training is crucial for deep learning with large datasets and models, but requires specific learning rate (LR) schedules for optimal performance, especially under limited training epochs. This work introduces an automated LR scheduling algorithm for large-batch neural network training within a fixed epoch budget, consisting of adaptive warmup and predefined decay phases. The LR is dynamically adjusted based on training loss monitored through Gaussian process smoothing, facilitating low computational overhead. When integrated with adaptive stochastic optimizers like AdamP and LAMB, this scheduler eliminates the need for extensive hyperparameter tuning and achieves competitive or superior results on various image classification tasks across different batch sizes and architectures."
    },
    {
      "title": "Don't Decay the Learning Rate, Increase the Batch Size",
      "url": "/research/2017/11/01/Don't-Decay-the-Learning-Rate,-Increase-the-Batch-Size.html",
      "date": "November 1, 2017",
      "summary": "This study demonstrates that increasing the batch size during training achieves similar learning outcomes as the common practice of decaying the learning rate, applicable to stochastic gradient descent and its variants, including with momentum and Adam optimization. This approach not only matches test accuracies within the same number of epochs but also enhances parallelism and reduces training time due to fewer parameter updates. Efficiency can be further improved by adjusting the learning rate and batch size proportionally, and although increasing the momentum coefficient and scaling the batch size accordingly may slightly lower test accuracy, it enables the use of large batch training without needing to tune hyper-parameters. Using these methods, ResNet-50 was trained on ImageNet to a 76.1% validation accuracy in less than 30 minutes."
    },
    {
      "title": "Eigencurve: Optimal Learning Rate Schedule For Sgd on Quadratic Objectives With Skewed Hessian Spectrums",
      "url": "/research/2022/06/14/Eigencurve-Optimal-Learning-Rate-Schedule-For-Sgd-on-Quadratic-Objectives-With-Skewed-Hessian-Spectrums.html",
      "date": "June 14, 2022",
      "summary": "Learning rate schedulers are essential in deep neural network training, but there's a gap between practical usage and theoretical understanding. This paper introduces Eigencurve, the first set of learning rate schedules achieving minimax optimal convergence rates for SGD on quadratic objectives with skewed eigenvalue distributions of the Hessian matrix. This condition is common in practice. Experimental results on CIFAR-10 image classification tasks demonstrate Eigencurve's superiority over step decay, particularly with fewer epochs. The theory inspires practical schedulers approximating Eigencurve, resembling cosine decay for some problems while outperforming it in others."
    },
    {
      "title": "How to decay your learning rate",
      "url": "/research/2021/05/23/How-to-decay-your-learning-rate.html",
      "date": "May 23, 2021",
      "summary": "Empirical findings suggest that typical fine-tuned learning rate schedules decay the learning rate following weight norm fluctuations. This led to the development of ABEL, an automatic scheduler that adjusts the learning rate based on weight norm changes. ABEL performs comparably to tuned schedules but demonstrates greater robustness to parameter variations. Extensive experiments across various domains reveal that when the weight norm remains stable, simplified schedules yield equivalent performance to complex ones, resembling a constant learning rate with decay towards the end of training."
    },
    {
      "title": "Optimal learning rate schedules in high-dimensional non-convex optimization problems",
      "url": "/research/2022/02/09/Optimal-learning-rate-schedules-in-high-dimensional-non-convex-optimization-problems.html",
      "date": "February 9, 2022",
      "summary": "This paper explores the role of learning rate schedules in high-dimensional and non-convex optimization problems, focusing on Langevin optimization with a decaying learning rate. Analyzing models with Gaussian random functions on N-dimensional spheres, the study reveals that to accelerate optimization without getting trapped in saddles, a decay rate β < 1 is optimal, contrary to convex settings where β = 1 is preferred. Introducing a signal recovery component, the dynamics involve an exploration phase navigating through rough landscape parts and a convergence phase entering convex basins. It's found optimal to maintain a large learning rate during exploration to swiftly exit non-convex regions, then transition to β = 1 for rapid convergence to the solution. These findings are validated in a neural network regression task."
    },
    {
      "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
      "url": "/research/2023/11/30/Dichotomy-of-Early-and-Late-Phase-Implicit-Biases-Can-Provably-Induce-Grokking.html",
      "date": "November 30, 2023",
      "summary": "Power et al. (2022) discovered that neural networks initially memorize arithmetic tasks, achieving perfect training accuracy but poor test accuracy, then suddenly achieve perfect test accuracy after extended training. This study explains this grokking phenomenon with the theory that early and late phase implicit biases cause this shift. Training homogeneous neural networks with large initialization and small weight decay on classification and regression tasks results in a prolonged period where the network acts like a kernel predictor, followed by a sudden shift to min-norm/max-margin predictors, significantly improving test accuracy."
    }
  ],
  "regularization": [
    {
      "title": "ADAMW: Decoupled Weight Decay Regularization",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "This paper shows that L2 regularization and weight decay are equivalent for standard SGD but not for adaptive methods like ADAM. To account of this, these authors separate weight decay from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate."
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "This paper introduces Batch Normalization, an online normalization technique which calculates mean and std across the batch dimension. Batch Norm improves training efficiency, allowing for higher learning rates, and decreasing hyper-parameter sensitivity, and sometimes removing the need for Dropout regularization. This paper suggests Batch Norm improves internal covariant shift, however later works have questioned this."
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing is a regularization technique that reduces overfitting and enhances classification performance. Label smothing involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution. This significantly improves classification accuracy and model robustness to noisy labels."
    },
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/research/2019/07/26/DropAttention-A-Regularization-Method-for-Fully-Connected-Self-Attention-Networks.html",
      "date": "July 26, 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. DropAttention uses a mask to zero out elements in the attention matrix. Experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    },
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a method for dropout in Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers."
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "This paper introduces DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. Authors also propose a dropout rate scheduler to optimize training."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "This paper explores the interactions between training data amount, model regularization or data augmentation (AugReg), model size, and compute budget for Vision Transformers (ViT) in various vision tasks. The authors find that using more compute and AugReg can achieve the same performance as training with significantly more data. They demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "This paper introduces Mask-Reconstruct Augmentation (MRA), an image augmentation technique that leverages self-supervised masked autoencoders to generate distorted inputs and address the issue of overfitting in deep neural networks. Inspired by the success of masked image modeling in self-supervised learning, MRA uses nonlinear transformations for regularization, as opposed to current hand-crafted linear techniques like scale, flip, and color jitter. Extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks."
    },
    {
      "title": "Better plain ViT baselines for ImageNet-1k",
      "url": "/research/2022/05/02/Better-plain-ViT-baselines-for-ImageNet-1k.html",
      "date": "May 2, 2022",
      "summary": "This paper challenges the common belief that the Vision Transformer (ViT) model requires sophisticated regularization techniques to excel on ImageNet-1k scale data. The authors present minor modifications to the original ViT vanilla training setting that significantly improve the performance of plain ViT models. They find that standard data augmentation is sufficient, with 90 epochs of training surpassing 76% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reaching 80% in less than one day."
    },
    {
      "title": "Is Cosine-Similarity of Embeddings Really About Similarity?",
      "url": "/research/2024/03/08/Is-Cosine-Similarity-of-Embeddings-Really-About-Similarity.html",
      "date": "March 8, 2024",
      "summary": "Cosine similarity, often used to gauge semantic similarity between high-dimensional objects by comparing low-dimensional feature embeddings, can yield variable results compared to unnormalized dot products. We investigate this phenomenon by analyzing embeddings from regularized linear models, revealing that cosine similarity can produce arbitrary and even meaningless similarities. This applies not only to linear models but also to deep models due to the implicit effects of various regularizations. Consequently, we advise against relying solely on cosine similarity and suggest exploring alternative approaches."
    },
    {
      "title": "Understanding Deep Learning Requires Re- Thinking Generalization",
      "url": "/research/2017/02/26/Understanding-Deep-Learning-Requires-Re--Thinking-Generalization.html",
      "date": "February 26, 2017",
      "summary": "Extensive experiments reveal that traditional explanations for the strong generalization performance of large neural networks fall short. Despite conventional wisdom attributing success to model properties or regularization techniques, our findings show that state-of-the-art convolutional networks for image classification can easily overfit random labels. This phenomenon persists regardless of explicit regularization or substituting true images with random noise. Theoretical analysis suggests that shallow neural networks achieve perfect finite sample expressivity when parameters outnumber data points, as commonly observed. We interpret these results in contrast to traditional models."
    },
    {
      "title": "Root Mean Square Layer Normalization",
      "url": "/research/2019/10/16/Root-Mean-Square-Layer-Normalization.html",
      "date": "October 16, 2019",
      "summary": "Layer normalization (LayerNorm) enhances deep neural network stability and convergence by re-centering and re-scaling inputs and weight matrices. However, its computational overhead slows networks, particularly RNNs. We introduce RMSNorm, which replaces re-centering with root mean square (RMS) regularization. RMSNorm maintains re-scaling invariance and adapts learning rates implicitly, while being computationally simpler than LayerNorm. We also propose partial RMSNorm (pRMSNorm), estimating RMS from a subset of inputs. Empirical results across various tasks and architectures demonstrate that RMSNorm achieves comparable performance to LayerNorm while reducing running time by 7%∼64%."
    },
    {
      "title": "Fine-tuning with Very Large Dropout",
      "url": "/research/2024/03/01/Fine-tuning-with-Very-Large-Dropout.html",
      "date": "March 1, 2024",
      "summary": "This study challenges the notion that machine learning practices assume training and testing data have the same distribution. It explores the effectiveness of high dropout rates, as opposed to ensemble techniques, in developing rich data representations suitable for multiple distribution scenarios. These representations surpass those achieved by traditional in-distribution performance regularization and the implicit sparsity induced by common stochastic gradient methods. While training deep networks from scratch with high dropout rates is impractical, fine-tuning pre-trained models under these conditions is feasible and yields better out-of-distribution performance than ensembles and model averaging techniques like model soups. This finding is significant due to the growing relevance of fine-tuning with large pre-trained models, offering insights into the nature of rich representations and the linear characteristics of fine-tuning large networks with small datasets."
    },
    {
      "title": "How Does Sharpness-Aware Minimization Minimize Sharpness?",
      "url": "/research/2023/01/05/How-Does-Sharpness-Aware-Minimization-Minimize-Sharpness.html",
      "date": "January 5, 2023",
      "summary": "Sharpness-Aware Minimization (SAM) enhances deep neural networks' generalization across various settings. Although SAM aims to penalize model sharpness using a computationally efficient approach, its exact operational mechanism and the sharpness notion it regularizes remain unclear, partly due to differing sharpness concepts used in its theoretical framework and empirical validation. This study identifies the precise sharpness concept SAM regulates and explains its mechanism. It reveals that the combined effect of SAM's two-step approximations, despite being individually misleading, correctly improves generalization when using full-batch gradients. Moreover, we demonstrate that the stochastic SAM version indeed regularizes a third sharpness notion, aligning more closely with practical performance. This effectiveness is attributed to the gradient's alignment with the Hessian's top eigenvector under SAM."
    },
    {
      "title": "Self-Paced Absolute Learning Progress as a Regularized Approach to Curriculum Learning",
      "url": "/research/2023/06/09/Self-Paced-Absolute-Learning-Progress-as-a-Regularized-Approach-to-Curriculum-Learning.html",
      "date": "June 9, 2023",
      "summary": "Reinforcement Learning's usability is limited by its high computation times. Curriculum Reinforcement Learning accelerates learning by ordering tasks from simple to hard. Curricula based on Absolute Learning Progress (ALP) have shown success in various environments but waste computation on redundant tasks. This issue is addressed by introducing Self-Paced Absolute Learning Progress (SPALP), a regularization method inspired by Self-Paced Learning. Evaluated in three environments, SPALP achieves performance comparable to ALP in all cases and reaches it faster in two. Further improvements in SPALP's efficiency and performance are also discussed."
    },
    {
      "title": "The loss landscape of deep linear neural networks: a second-order analysis",
      "url": "/research/2022/03/11/The-loss-landscape-of-deep-linear-neural-networks-a-second-order-analysis.html",
      "date": "March 11, 2022",
      "summary": "This study investigates the optimization landscape of deep linear neural networks with square loss, focusing on the role of non-strict saddle points in algorithm dynamics, which has been previously understudied. the authors conduct a comprehensive second-order analysis, identifying global minimizers, strict, and non-strict saddle points among all critical points, alongside their critical values. Our findings, based on conditions related to the ranks of partial matrix products, contribute insights into global convergence and implicit regularization observed in optimizing linear neural networks. Additionally, the authors offer an explicit parameterization of global minimizers and identify extensive sets of strict and non-strict saddle points."
    }
  ],
  "l2": [
    {
      "title": "ADAMW: Decoupled Weight Decay Regularization",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "This paper shows that L2 regularization and weight decay are equivalent for standard SGD but not for adaptive methods like ADAM. To account of this, these authors separate weight decay from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "The paper proposes three normalization-centric modifications to improve Transformer training: PRENORM which introduces pre-norm residual connections and smaller initializations, enabling warmup-free, validation-based training with large learning rates; SCALENORM which suggests L2 normalization with a single scale parameter for faster training and better performance; and FIXNORM which reaffirms the efficacy of normalizing word embeddings to a fixed length."
    },
    {
      "title": "ImageNet-D: A new challenging robustness dataset inspired by domain adaptation",
      "url": "/research/2023/05/05/ImageNet-D-A-new-challenging-robustness-dataset-inspired-by-domain-adaptation.html",
      "date": "May 5, 2023",
      "summary": "We introduce ImageNet-D, a novel dataset designed to evaluate the robustness of ImageNet-trained models across various domains. With six distinct domains including Real, Painting, Clipart, Sketch, Infograph, and Quick-draw, ImageNet-D challenges even state-of-the-art models, revealing interpretable errors. For instance, our leading EfficientNet-L2 model exhibits a significant performance decrease, dropping from 11.6% on clean ImageNet to 29.2% on the Real domain."
    }
  ],
  "sgd": [
    {
      "title": "ADAMW: Decoupled Weight Decay Regularization",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "This paper shows that L2 regularization and weight decay are equivalent for standard SGD but not for adaptive methods like ADAM. To account of this, these authors separate weight decay from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate."
    },
    {
      "title": "Dropout Reduces Underfitting",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. Authors then introduce two dropout schedules: early dropout which prevents underfitting, and late dropout which prevents overfitting."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "Eigencurve: Optimal Learning Rate Schedule For Sgd on Quadratic Objectives With Skewed Hessian Spectrums",
      "url": "/research/2022/06/14/Eigencurve-Optimal-Learning-Rate-Schedule-For-Sgd-on-Quadratic-Objectives-With-Skewed-Hessian-Spectrums.html",
      "date": "June 14, 2022",
      "summary": "Learning rate schedulers are essential in deep neural network training, but there's a gap between practical usage and theoretical understanding. This paper introduces Eigencurve, the first set of learning rate schedules achieving minimax optimal convergence rates for SGD on quadratic objectives with skewed eigenvalue distributions of the Hessian matrix. This condition is common in practice. Experimental results on CIFAR-10 image classification tasks demonstrate Eigencurve's superiority over step decay, particularly with fewer epochs. The theory inspires practical schedulers approximating Eigencurve, resembling cosine decay for some problems while outperforming it in others."
    },
    {
      "title": "Large Batch Training of Convolutional Networks",
      "url": "/research/2017/09/13/Large-Batch-Training-of-Convolutional-Networks.html",
      "date": "September 13, 2017",
      "summary": "To expedite the training of large convolutional networks, computational units are added and trained with data-parallel synchronous Stochastic Gradient Descent (SGD) across units, increasing batch size with node count. However, larger batch sizes can reduce model accuracy. The existing method of large batch training—linear learning rate scaling with warm-up—is not universally effective and may cause training divergence. To address these challenges, we introduce a novel training algorithm, Layer-wise Adaptive Rate Scaling (LARS), enabling us to train Alexnet with a batch size of 8K and Resnet-50 with a batch size of 32K, without compromising accuracy."
    },
    {
      "title": "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization",
      "url": "/research/2023/07/04/Gradient-Norm-Aware-Minimization-Seeks-First-Order-Flatness-and-Improves-Generalization.html",
      "date": "July 4, 2023",
      "summary": "Recent advancements have highlighted the effectiveness of flat minima in enhancing generalization, particularly through Sharpness-Aware Minimization (SAM). However, existing definitions of flatness, such as zeroth-order flatness, have limitations in discerning between minima with low and high generalization errors. To address this, we propose first-order flatness, which considers maximal gradient norm within a perturbation radius. We introduce Gradient norm Aware Minimization (GAM) as a novel training approach to achieve uniformly small curvature across all directions. Experimental results demonstrate GAM's ability to enhance generalization compared to standard optimizers like SGD and AdamW across various datasets and networks. Moreover, GAM facilitates SAM in identifying flatter minima, leading to improved generalization."
    },
    {
      "title": "AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks",
      "url": "/research/2023/03/01/AdaSAM-Boosting-Sharpness-Aware-Minimization-with-Adaptive-Learning-Rate-and-Momentum-for-Training-Deep-Neural-Networks.html",
      "date": "March 1, 2023",
      "summary": "The Sharpness Aware Minimization (SAM) optimizer, known for enhancing the generalization of deep neural networks by introducing extra perturbation steps, is further developed into AdaSAM by integrating adaptive learning rates and momentum acceleration. Although AdaSAM has been empirically applied to large-scale networks, a theoretical understanding of its performance, considering the complexity of its components, was lacking. This study presents a theoretical analysis of AdaSAM's convergence in stochastic non-convex settings, demonstrating a convergence rate of O(1/√bT) that scales linearly with mini-batch size. By introducing a delayed second-order momentum term, the study successfully decouples and analyzes the intertwined effects of stochastic gradients, adaptive learning rates, and perturbations. This is the first work to offer a detailed convergence rate for SAM with adaptive mechanisms. Experimental results on various NLP tasks indicate AdaSAM's superior performance over SGD, AMSGrad, and SAM optimizers."
    },
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    }
  ],
  "account": [
    {
      "title": "ADAMW: Decoupled Weight Decay Regularization",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "This paper shows that L2 regularization and weight decay are equivalent for standard SGD but not for adaptive methods like ADAM. To account of this, these authors separate weight decay from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate."
    }
  ],
  "separate": [
    {
      "title": "ADAMW: Decoupled Weight Decay Regularization",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "This paper shows that L2 regularization and weight decay are equivalent for standard SGD but not for adaptive methods like ADAM. To account of this, these authors separate weight decay from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate."
    }
  ],
  "factor": [
    {
      "title": "ADAMW: Decoupled Weight Decay Regularization",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "This paper shows that L2 regularization and weight decay are equivalent for standard SGD but not for adaptive methods like ADAM. To account of this, these authors separate weight decay from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "This paper introduces FixMatch, a streamlined semi-supervised learning (SSL) algorithm. FixMatch generates pseudo-labels using a model's predictions on weakly-augmented unlabeled images, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness, and the code is publicly available."
    },
    {
      "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
      "url": "/research/2024/02/28/CL-MAE:-Curriculum-Learned-Masked-Autoencoders.html",
      "date": "February 28, 2024",
      "summary": "This paper proposes a curriculum learning approach that updates the masking strategy of Masked Auto Encoders (MAE) to progressively increase the complexity of the self-supervised reconstruction task. To achieve this, a novel learnable masking module is introduced, capable of generating masks of varying complexities, and integrated into masked autoencoders (MAE). This module is jointly trained with the MAE, adjusting its behavior from partner (optimizing the same reconstruction loss) to adversary (optimizing the opposite loss), with a smooth transition regulated by a factor multiplied with the reconstruction loss. This training procedure creates an easy-to-hard curriculum. The Curriculum-Learned Masked Autoencoder (CL-MAE) is trained on ImageNet and demonstrates superior representation learning compared to MAE. Empirical results on five downstream tasks confirm that curriculum learning can successfully self-supervise masked autoencoders."
    }
  ],
  "dinov2": [
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "This paper introdices an automated pipeline for building high-quality, diverse image datasets. Also proposes minor mofications to the archetecture and loss of a DINO ViT. A large ViT model with 1 billion parameters was trained and distilled into smaller models."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This paper is effectively a follow-up to DinoV2. The study addresses the issue of artifacts in feature maps of Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. The authors introduce a straightforward strategy of adding extra tokens to the input sequence of the Vision Transformer, which eliminates these artifacts for both supervised and self-supervised models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks, facilitates object discovery with larger models, and results in smoother feature and attention maps for downstream visual processing tasks."
    },
    {
      "title": "V-Jepa: Latent Video Prediction For Visual Representation Learning",
      "url": "/research/2023/02/15/V-Jepa-Latent-Video-Prediction-For-Visual-Representation-Learning.html",
      "date": "February 15, 2023",
      "summary": "This paper introduces V-JEPA, a self-supervised video learning method that predicts masked spatio-temporal regions in latent space, effectively applying the masked-modelling principle of large language models to video. This approach generates visual features useful across various image and video tasks without needing model adjustment, achieving significant improvements on Kinetics-400 (82.1%) and Something-Something-v2 (71.2%) benchmarks, outperforming prior video models. V-JEPA also excels in motion understanding tasks, surpassing leading image models like DINOv2 and OpenCLIP, and achieves 77.9% on ImageNet classification with video training alone, setting a new standard for video models."
    }
  ],
  "introdice": [
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "This paper introdices an automated pipeline for building high-quality, diverse image datasets. Also proposes minor mofications to the archetecture and loss of a DINO ViT. A large ViT model with 1 billion parameters was trained and distilled into smaller models."
    },
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "This paper introdices LOST, a method for unsupervised object localization in images. LOST utilizes activation features from a self-supervised pre-trained vision transformer. Unlike other methods, LOST works on individual images without relying on external object proposals or image collection exploration. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, demonstrating its effectiveness in unsupervised object discovery."
    }
  ],
  "automate": [
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "This paper introdices an automated pipeline for building high-quality, diverse image datasets. Also proposes minor mofications to the archetecture and loss of a DINO ViT. A large ViT model with 1 billion parameters was trained and distilled into smaller models."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Data augmentation improves generalization, and robustness against image corruptions. However, the use of complex augmentation pipelines typically involves tuning of an enormous number of hyper parameters. RandAugment addresses this by reducing the search space. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets."
    },
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "The study presents a novel approach for detection identification of Holstein-Friesian cattle. This technique offers a completely hands-off solution for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training when new cattle are introduced. The system utilizes convolutional neural networks and deep metric learning, achieving high accuracy with a 93.8% success rate in identifying cattle unseen during training, using only half of the population."
    },
    {
      "title": "Automated Learning Rate Scheduler for Large-batch",
      "url": "/research/2021/07/13/Automated-Learning-Rate-Scheduler-for-Large-batch.html",
      "date": "July 13, 2021",
      "summary": "Large-batch training is crucial for deep learning with large datasets and models, but requires specific learning rate (LR) schedules for optimal performance, especially under limited training epochs. This work introduces an automated LR scheduling algorithm for large-batch neural network training within a fixed epoch budget, consisting of adaptive warmup and predefined decay phases. The LR is dynamically adjusted based on training loss monitored through Gaussian process smoothing, facilitating low computational overhead. When integrated with adaptive stochastic optimizers like AdamP and LAMB, this scheduler eliminates the need for extensive hyperparameter tuning and achieves competitive or superior results on various image classification tasks across different batch sizes and architectures."
    }
  ],
  "pipeline": [
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "This paper introdices an automated pipeline for building high-quality, diverse image datasets. Also proposes minor mofications to the archetecture and loss of a DINO ViT. A large ViT model with 1 billion parameters was trained and distilled into smaller models."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Data augmentation improves generalization, and robustness against image corruptions. However, the use of complex augmentation pipelines typically involves tuning of an enormous number of hyper parameters. RandAugment addresses this by reducing the search space. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets."
    },
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "This paper introduces DiffBIR, a framework that utilizes pretrained text-to-image diffusion models for blind image restoration. The two-stage pipeline involves pretraining a restoration module on various degradations to enhance real-world applicability, followed by the use of latent diffusion models for realistic restoration. The authors introduce a novel injective modulation sub-network, LAControlNet, for fine-tuning and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, a controllable module allows users to adjust quality and fidelity during the denoising process. Extensive tests demonstrate DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets."
    }
  ],
  "mofication": [
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "This paper introdices an automated pipeline for building high-quality, diverse image datasets. Also proposes minor mofications to the archetecture and loss of a DINO ViT. A large ViT model with 1 billion parameters was trained and distilled into smaller models."
    }
  ],
  "archetecture": [
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "This paper introdices an automated pipeline for building high-quality, diverse image datasets. Also proposes minor mofications to the archetecture and loss of a DINO ViT. A large ViT model with 1 billion parameters was trained and distilled into smaller models."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "This paper introduces the Mamba archetecture, a selective state space model (SSSM) with input dependance. Mamba has better speed and scalability compared to transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences. Authors also present a hardware efficient scanning mechanism that is used in Mamba."
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a Vision archetecture that uses bidirectional Mamba blocks, challenging the necessity of self-attention in vision. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "This study introduces the Vision Transformer ViT archetecture. The paper shows that a pure transformer applied directly to sequences of image patches can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    }
  ],
  "dino": [
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "This paper introdices an automated pipeline for building high-quality, diverse image datasets. Also proposes minor mofications to the archetecture and loss of a DINO ViT. A large ViT model with 1 billion parameters was trained and distilled into smaller models."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "This paper shows self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs or convnets. Also proposes a modification to BYOL self-supervised learning with a momentum encoder, multi-crop training, and feature sharping for the teacher network."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    },
    {
      "title": "Towards a Unified Theoretical Understanding of Non-contrastive Learning via Rank Differential Mechanism",
      "url": "/research/2023/03/04/Towards-a-Unified-Theoretical-Understanding-of-Non-contrastive-Learning-via-Rank-Differential-Mechanism.html",
      "date": "March 4, 2023",
      "summary": "Recent non-contrastive learning methods like BYOL, SimSiam, SwAV, and DINO have shown that asymmetric architectural designs can achieve good self-supervised visual learning performance by aligning positive pairs alone, despite a lack of unified theoretical understanding on how these designs prevent feature collapse. This study introduces the Rank Differential Mechanism (RDM) as a unified theoretical framework for non-contrastive learning. RDM demonstrates that these asymmetric designs maintain a consistent rank difference in output features, enhancing effective dimensionality and preventing feature collapse. Unlike previous theories, RDM applies to designs both with and without a predictor, offering a comprehensive understanding of non-contrastive learning methods and guiding the development of new variants. Experiments confirm that these variants perform comparably, if not better than, existing methods on benchmark datasets."
    }
  ],
  "vit": [
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "This paper introdices an automated pipeline for building high-quality, diverse image datasets. Also proposes minor mofications to the archetecture and loss of a DINO ViT. A large ViT model with 1 billion parameters was trained and distilled into smaller models."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "This paper shows self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs or convnets. Also proposes a modification to BYOL self-supervised learning with a momentum encoder, multi-crop training, and feature sharping for the teacher network."
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "This paper introduces PatchDropout, a simple training technique for Vision Transformers ViT that drops random image patches, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. The authors find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. "
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study looks at self-supervised pre-training methods on small scale or lightweight Vision Transformers (ViTs). Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods."
    },
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/research/2021/08/16/An-Empirical-Study-of-Training-Self-Supervised-Vision-Transformers.html",
      "date": "August 16, 2021",
      "summary": "This study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "This study introduces the Vision Transformer ViT archetecture. The paper shows that a pure transformer applied directly to sequences of image patches can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, enhances vision tasks using Vision transformers by masking and reconstructing parts of an image. The compatibility of MIM with CNNs and its operational principle are unclear. this study reveals that MIM improves generalized feature extraction through middle-order patch interactions and introduces Architecture-Agnostic Masked Image Modeling (A2MIM), compatible with both Transformers and CNNs. Extensive testing demonstrates A2MIM's ability to enhance representation learning and transferability to various tasks without specialized modifications."
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM) for self-supervised visual representation in vision transformers ViT. Five different learning objectives were proposed, which degraded the input image in various ways similar to masking. Design principles are proposed for  token-based pre-training of vision transformers. The most effective strategy combined preserving the original image style with spatial misalignment and masking. This approach outperformed traditional MIM on downstream recognition tasks without increasing computational demands."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of Vision Transformer (ViT) variants against common corruptions. The benchmarking reveals that overlapping patch embedding and convolutional feed-forward networks (FFNs) significantly enhance ViT robustness. The study also scrutinizes the effectiveness of CNN-based data augmentation strategies when applied to ViTs, finding that adversarial noise training is effective, while fourier-domain augmentation falls short. A new conditional method for generating dynamic augmentation parameters based on input images is proposed, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. In this paper Masked Image Residual Learning (MIRL), a self-supervised learning framework, was introduced to alleviate the degradation issue and enable effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Extensive testing shows that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. A ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training. Synthetic images generated by FDSL circumvent issues associated with real images, offering a promising avenue for pre-training general models. Investigations suggest that simple object contours can match fractal-based dataset performance and increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces TinyViT, a series of compact and efficient vision transformers (ViTs) designed for devices with limited resources. The authors employ a fast distillation framework to transfer knowledge from large pretrained models to smaller ones during pretraining, using sparsified logits from teacher models to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. With increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters, and demonstrates strong transferability across various downstream tasks."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This paper is effectively a follow-up to DinoV2. The study addresses the issue of artifacts in feature maps of Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. The authors introduce a straightforward strategy of adding extra tokens to the input sequence of the Vision Transformer, which eliminates these artifacts for both supervised and self-supervised models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks, facilitates object discovery with larger models, and results in smoother feature and attention maps for downstream visual processing tasks."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets for formula-driven supervised learning (FDSL). The authors identify the optimal FDSL parameters and maximize synthetic image variety, which is crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. The authors demonstrate FDSL's potential for continuous improvement and its ability to avoid issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. This encoder serves as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The DMAE ViT-Base model achieves comparable or superior certified accuracy with fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on ImageNet. The model also shows high transferability to CIFAR-10, indicating its broad applicability."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "This paper explores the interactions between training data amount, model regularization or data augmentation (AugReg), model size, and compute budget for Vision Transformers (ViT) in various vision tasks. The authors find that using more compute and AugReg can achieve the same performance as training with significantly more data. They demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    },
    {
      "title": "Better plain ViT baselines for ImageNet-1k",
      "url": "/research/2022/05/02/Better-plain-ViT-baselines-for-ImageNet-1k.html",
      "date": "May 2, 2022",
      "summary": "This paper challenges the common belief that the Vision Transformer (ViT) model requires sophisticated regularization techniques to excel on ImageNet-1k scale data. The authors present minor modifications to the original ViT vanilla training setting that significantly improve the performance of plain ViT models. They find that standard data augmentation is sufficient, with 90 epochs of training surpassing 76% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reaching 80% in less than one day."
    },
    {
      "title": "DeiT III: Revenge of the ViT",
      "url": "/research/2023/01/18/DeiT-III-Revenge-of-the-ViT.html",
      "date": "January 18, 2023",
      "summary": "This paper explores the supervised training of Vision Transformers (ViTs) using a simplified training approach adapted from ResNet-50 that includes a novel data-augmentation method with just 3 augmentations. The study demonstrates that this method significantly improves ViTs' performance in image classification, transfer learning, and semantic segmentation over previous supervised training techniques. Moreover, it shows ViTs' performance can match newer architectures, providing a new benchmark for evaluating self-supervised methods on ViTs."
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of Vision Transformers (ViTs) without relying on large-scale pre-training datasets like ImageNet and JFT or requiring modifications to the architecture or loss functions. The authors show that this approach improves ViT performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, while maintaining ViT's attention to relevant regions and robustness, despite ViT's inherent lack of inductive biases and typical dependence on large-scale pre-training."
    },
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "The authors introduce VideoMAE, a data-efficient self-supervised video pre-training (SSVP) approach that utilizes video masked autoencoders with a novel high-ratio video tube masking technique inspired by ImageMAE. Key findings include the effectiveness of high masking ratios (90-95%) due to video's temporal redundancy, strong performance on small datasets (~3k-4k videos) without extra data highlighting the importance of high-level structure learning, and data quality being more crucial than quantity with domain shift being significant. Notably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone without extra data."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    },
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "The paper focuses on evaluating the performance of recent advancements in image transformers, such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, on the Tiny ImageNet dataset for transfer learning tasks. While these models are typically trained on large datasets like ImageNet-21k and then fine-tuned on ImageNet-1k, their assessments often overlook the Tiny ImageNet benchmark. The study updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. This underscores the importance of evaluating these models on the Tiny ImageNet dataset for transfer learning performance."
    },
    {
      "title": "Are Convolutional Neural Networks or Transformers more like human vision?",
      "url": "/research/2021/07/01/Are-Convolutional-Neural-Networks-or-Transformers-more-like-human-vision.html",
      "date": "July 1, 2021",
      "summary": "This study investigates the performance of Convolutional Neural Networks (CNNs) and attention-based Vision Transformers (ViTs) in computer vision tasks. While CNNs excel in accuracy, ViTs offer a different approach with weaker inductive biases. By analyzing error patterns, we find that ViTs exhibit consistency with human errors, suggesting potential for more human-like vision models and insights into human object recognition."
    },
    {
      "title": "Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding",
      "url": "/research/2021/11/16/Improved-Robustness-of-Vision-Transformer-via-PreLayerNorm-in-Patch-Embedding.html",
      "date": "November 16, 2021",
      "summary": "Recent advancements in vision transformers (ViTs) have shown superior performance across diverse visual tasks, surpassing convolutional neural networks (CNNs). Given ViT's distinct architecture, understanding its behavior and reliability is imperative. This paper investigates ViT's robustness by comparing it with CNNs under various image corruptions relevant to real-world vision tasks. While ViT generally exhibits comparable or improved robustness over CNNs, it consistently underperforms in contrast enhancement tasks. Analysis suggests that positional embedding in ViT's patch embedding may malfunction with color scale changes. We propose PreLayerNorm, a modified patch embedding structure, to address this issue and ensure scale-invariant behavior in ViT. ViT with PreLayerNorm demonstrates enhanced robustness across various corruptions, particularly in contrast-varying environments."
    },
    {
      "title": "Scaling Vision Transformers to 22 Billion Parameters",
      "url": "/research/2023/02/10/Scaling-Vision-Transformers-to-22-Billion-Parameters.html",
      "date": "February 10, 2023",
      "summary": "We introduce a method for efficiently training a 22B-parameter Vision Transformer (ViT-22B) and conduct various experiments to assess its performance. Compared to existing models, ViT-22B shows improved scalability and benefits such as enhanced fairness-performance tradeoff, alignment with human visual perception, and increased robustness. Our findings suggest promising avenues for achieving large-scale language model-like capabilities in vision tasks."
    },
    {
      "title": "Do Vision Transformers See Like Convolutional Neural Networks?",
      "url": "/research/2022/03/03/Do-Vision-Transformers-See-Like-Convolutional-Neural-Networks.html",
      "date": "March 3, 2022",
      "summary": "Convolutional Neural Networks (CNNs) have been prominent in processing visual data, but recent studies show Vision Transformers (ViTs) can match or outperform CNNs in image classification. This work examines how ViTs solve classification tasks, discovering significant differences from CNNs, including uniformity in ViT representations across layers due to self-attention and residual connections that enhance feature propagation. ViTs also maintain spatial information effectively, influenced by classification methods. The study further explores how dataset scale affects ViT features and their transferability, linking these findings to new architectures like MLP-Mixer."
    },
    {
      "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
      "url": "/research/2022/03/19/Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training.html",
      "date": "March 19, 2022",
      "summary": "Sharpness-Aware Minimization (SAM) enhances generalization by optimizing a neighborhood-based perturbed loss, but it doesn't always favor flat minima due to both sharp and flat minima having low perturbed losses. We introduce a new measure, the surrogate gap, reflecting the dominant Hessian eigenvalue at small neighborhood radii, which is simple to compute and can be minimized during training. We propose the Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), an advancement over SAM with minimal additional computational cost. GSAM employs a dual-step approach: first, minimizing the perturbed loss similar to SAM, and second, reducing the surrogate gap without affecting the perturbed loss to target regions with low loss and sharpness, thus achieving superior generalization. GSAM is theoretically robust, showing better convergence and empirical generalization improvements, notably a +3.2% gain over SAM and +5.4% over AdamW in ImageNet accuracy for ViT-B/32."
    },
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    }
  ],
  "distillation": [
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "This paper introdices an automated pipeline for building high-quality, diverse image datasets. Also proposes minor mofications to the archetecture and loss of a DINO ViT. A large ViT model with 1 billion parameters was trained and distilled into smaller models."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This study suggest that label smoothing not only boosts generalization but also enhances model calibration. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. Label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study looks at self-supervised pre-training methods on small scale or lightweight Vision Transformers (ViTs). Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces TinyViT, a series of compact and efficient vision transformers (ViTs) designed for devices with limited resources. The authors employ a fast distillation framework to transfer knowledge from large pretrained models to smaller ones during pretraining, using sparsified logits from teacher models to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. With increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters, and demonstrates strong transferability across various downstream tasks."
    },
    {
      "title": "Training data-efficient image transformers & distillation through attention",
      "url": "/research/2021/01/15/Training-data-efficient-image-transformers-&-distillation-through-attention.html",
      "date": "January 15, 2021",
      "summary": "This paper introduces DeiT, an efficient method for training vision transformers. The authors present a unique teacher-student strategy for transformers, leveraging a distillation token for efficient learning from a convolutional network teacher. This method achieves performance on par with convolutional networks, with up to 85.2% accuracy on ImageNet, and demonstrates effective transferability to other tasks."
    },
    {
      "title": "VideoMamba: State Space Model for Efficient Video Understanding",
      "url": "/research/2024/03/12/VideoMamba-State-Space-Model-for-Efficient-Video-Understanding.html",
      "date": "March 12, 2024",
      "summary": "This work introduces VideoMamba, an innovative approach that addresses local redundancy and global dependencies in video understanding by adapting the Mamba framework to video analysis. VideoMamba outperforms existing 3D convolutional neural networks and video transformers through its linear-complexity operator, which facilitates efficient long-term modeling for high-resolution, long-duration videos. Its effectiveness is demonstrated across four main areas: scalability in the visual domain without needing extensive dataset pretraining, thanks to a novel self-distillation technique; the ability to recognize short-term actions with fine-grained motion differences; superior performance in long-term video understanding compared to traditional feature-based models; and robust compatibility with multiple modalities, enhancing multi-modal video analysis. VideoMamba establishes a new standard for comprehensive and efficient video understanding."
    }
  ],
  "grok": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "This paper introduces the concept of delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a measure of local complexity, analyzing the density of linear regions in the DNN input space. The authors find that during training linear regions emerge near training samples with the nonlinearities being forced towards decision boundries."
    },
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper introduces the concept of grokking, a sudden onset of perfect generalisation long after overfitting, on small, algorithmically generated datasets. The study also finds that generalization on smaller datasets requires more optimization. The authors suggest that such datasets are ideal for investigating the puzzling phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    },
    {
      "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
      "url": "/research/2023/11/30/Dichotomy-of-Early-and-Late-Phase-Implicit-Biases-Can-Provably-Induce-Grokking.html",
      "date": "November 30, 2023",
      "summary": "Power et al. (2022) discovered that neural networks initially memorize arithmetic tasks, achieving perfect training accuracy but poor test accuracy, then suddenly achieve perfect test accuracy after extended training. This study explains this grokking phenomenon with the theory that early and late phase implicit biases cause this shift. Training homogeneous neural networks with large initialization and small weight decay on classification and regression tasks results in a prolonged period where the network acts like a kernel predictor, followed by a sudden shift to min-norm/max-margin predictors, significantly improving test accuracy."
    }
  ],
  "here": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "This paper introduces the concept of delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a measure of local complexity, analyzing the density of linear regions in the DNN input space. The authors find that during training linear regions emerge near training samples with the nonlinearities being forced towards decision boundries."
    },
    {
      "title": "Atomically Accurate De Novo Design of Single-domain Antibodies",
      "url": "/research/2024/03/18/Atomically-Accurate-De-Novo-Design-of-Single-domain-Antibodies.html",
      "date": "March 18, 2024",
      "summary": "Despite the critical role antibodies play in medicine, current methods for designing new antibodies targeting specific epitopes are time-consuming. Here, we showcase the effectiveness of a refined RFdiffusion network in designing novel antibody variable heavy chains (VHH’s) to target user-specified epitopes. Through experiments, we validate the binding capability of these designed VHH's to four disease-relevant epitopes. Furthermore, the cryo-EM structure analysis reveals that a designed VHH bound to influenza hemagglutinin closely matches the design model, affirming the accuracy of our approach."
    }
  ],
  "become": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "This paper introduces the concept of delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a measure of local complexity, analyzing the density of linear regions in the DNN input space. The authors find that during training linear regions emerge near training samples with the nonlinearities being forced towards decision boundries."
    }
  ],
  "emergence": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "This paper introduces the concept of delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a measure of local complexity, analyzing the density of linear regions in the DNN input space. The authors find that during training linear regions emerge near training samples with the nonlinearities being forced towards decision boundries."
    },
    {
      "title": "Taming the Waves: Sine as Activation Function in Deep Neural Networks",
      "url": "/research/2016/11/04/Taming-the-Waves-Sine-as-Activation-Function-in-Deep-Neural-Networks.html",
      "date": "November 4, 2016",
      "summary": "This paper explores the challenges in training deep neural networks that use sinusoidal activation functions. The authors explain that the difficulty arises from the emergence of numerous shallow local minima in the loss landscape. The study reveals that successful learning in typical classification tasks occurs when the network effectively ignores the periodic cycles of the sinusoidal functions. However, for certain non-trivial tasks, networks with sinusoidal activations can outperform those using traditional monotonic activation functions."
    }
  ],
  "complexity": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "This paper introduces the concept of delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a measure of local complexity, analyzing the density of linear regions in the DNN input space. The authors find that during training linear regions emerge near training samples with the nonlinearities being forced towards decision boundries."
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "This paper introduces the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a multi-directional Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions."
    },
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "This paper introduces a curriculum learning framework for image-caption pretraining, inspired by children's language learning from cognitive science, to address the challenges of aligning multiple concepts from captions to objects in images. The method starts with simple image-caption pairs and gradually increases complexity by adding more concepts, leveraging knowledge from each phase for subsequent learning. This approach outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios."
    },
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "This paper introduces an algorithm that identifies simple and highly generalizable neural networks by searching for extensive flat minima regions in the error function, where the error rate is relatively stable. Such flat minima are associated with lower overfitting risks based on minimum description length (MDL) principles. Despite requiring second-order derivative calculations, the algorithm has a complexity level comparable to backpropagation. When tested on feedforward and recurrent networks, as well as stock market prediction tasks, this algorithm outperformed traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    },
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "The paper introduces TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method that applies a single augmentation to each image. Despite its minimal complexity and cost, TrivialAugment outperforms existing methods across various image classification scenarios, as validated through extensive experiments against state-of-the-art methods and multiple ablation studies involving different augmentation spaces and methods. The work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Noting a stagnation in automatic augmentation research, the authors conclude by proposing best practices for future advancements in the field."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    },
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    },
    {
      "title": "AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks",
      "url": "/research/2023/03/01/AdaSAM-Boosting-Sharpness-Aware-Minimization-with-Adaptive-Learning-Rate-and-Momentum-for-Training-Deep-Neural-Networks.html",
      "date": "March 1, 2023",
      "summary": "The Sharpness Aware Minimization (SAM) optimizer, known for enhancing the generalization of deep neural networks by introducing extra perturbation steps, is further developed into AdaSAM by integrating adaptive learning rates and momentum acceleration. Although AdaSAM has been empirically applied to large-scale networks, a theoretical understanding of its performance, considering the complexity of its components, was lacking. This study presents a theoretical analysis of AdaSAM's convergence in stochastic non-convex settings, demonstrating a convergence rate of O(1/√bT) that scales linearly with mini-batch size. By introducing a delayed second-order momentum term, the study successfully decouples and analyzes the intertwined effects of stochastic gradients, adaptive learning rates, and perturbations. This is the first work to offer a detailed convergence rate for SAM with adaptive mechanisms. Experimental results on various NLP tasks indicate AdaSAM's superior performance over SGD, AMSGrad, and SAM optimizers."
    },
    {
      "title": "A Phase Transition between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention",
      "url": "/research/2024/02/06/A-Phase-Transition-between-Positional-and-Semantic-Learning-in-a-Solvable-Model-of-Dot-Product-Attention.html",
      "date": "February 6, 2024",
      "summary": "We explore the learning process of a dot-product attention layer, which learns both positional and semantic attention matrices, enabling tokens to attend based on position or meaning. Through experiments on an algorithmic task, we demonstrate that this architecture can use either mechanism for solving the task. Theoretically, we examine a non-linear self-attention layer with special query and key matrices, offering a closed-form solution for its non-convex loss landscape in high-dimensional data, which reveals a phase transition from positional to semantic mechanisms as sample complexity increases. We also show that the dot-product attention layer surpasses a linear positional baseline through the semantic mechanism with adequate data."
    },
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    },
    {
      "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
      "url": "/research/2024/02/28/CL-MAE:-Curriculum-Learned-Masked-Autoencoders.html",
      "date": "February 28, 2024",
      "summary": "This paper proposes a curriculum learning approach that updates the masking strategy of Masked Auto Encoders (MAE) to progressively increase the complexity of the self-supervised reconstruction task. To achieve this, a novel learnable masking module is introduced, capable of generating masks of varying complexities, and integrated into masked autoencoders (MAE). This module is jointly trained with the MAE, adjusting its behavior from partner (optimizing the same reconstruction loss) to adversary (optimizing the opposite loss), with a smooth transition regulated by a factor multiplied with the reconstruction loss. This training procedure creates an easy-to-hard curriculum. The Curriculum-Learned Masked Autoencoder (CL-MAE) is trained on ImageNet and demonstrates superior representation learning compared to MAE. Empirical results on five downstream tasks confirm that curriculum learning can successfully self-supervise masked autoencoders."
    },
    {
      "title": "From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning",
      "url": "/research/2023/03/16/From-MNIST-to-ImageNet-and-Back:-Benchmarking-Continual-Curriculum-Learning.html",
      "date": "March 16, 2023",
      "summary": "Continual learning (CL) is a promising trend in machine learning, aiming to develop robust models and strategies for dynamic environments by incorporating new knowledge while retaining past knowledge. CL research is fragmented, with various protocols, tasks, datasets, and metrics, often not reflecting real-world complexity and tailored to specific strategies. This work addresses this gap by introducing two novel CL benchmarks with heterogeneous tasks from six image datasets, varying in complexity and quality. These benchmarks fairly evaluate state-of-the-art CL strategies in scenarios closer to real-world conditions. The benchmarks present tasks in both increasing and decreasing complexity, assessing models' ability to exploit task structure. The work emphasizes a rigorous, reproducible evaluation protocol for measuring generalization and memory retention in models. Experimental results show that popular CL strategies perform poorly on these benchmarks, exhibit high forgetting, and struggle with curriculum task ordering. These findings underscore the need for rigorous comparisons and the development of new CL strategies for complex scenarios."
    },
    {
      "title": "Neural Networks Learn Statistics of Increasing Complexity",
      "url": "/research/2024/02/13/Neural-Networks-Learn-Statistics-of-Increasing-Complexity.html",
      "date": "February 13, 2024",
      "summary": "The distributional simplicity bias (DSB) theory suggests neural networks first learn basic patterns in data before understanding more complex correlations. We provide new evidence supporting DSB, showing networks initially excel with data matching training set's simple statistics but this ability diminishes later. Extending DSB to discrete domains, we demonstrate an equivalence between n-gram frequencies and vector moments, also observing this bias in large language models (LLMs). Additionally, by adjusting low-level statistics of images to resemble another class, we reveal that networks in early training phases misclassify these edited images as if they belonged to the target class."
    },
    {
      "title": "Transformers with Learnable Activation Functions",
      "url": "/research/2023/02/14/Transformers-with-Learnable-Activation-Functions.html",
      "date": "February 14, 2023",
      "summary": "Activation functions significantly affect model performance by reducing data complexity, yet their selection in Transformer-based language models is often overlooked. This paper explores the impact of using rational activation functions (RAFs), which unlike fixed activation functions (FAFs), can learn optimal functions from data. Our experiments demonstrate that Transformer models with RAFs (RAFT) outperform those with FAFs (FAFT), achieving a 5.71 point higher score on the GLUE benchmark with only 100 training examples and a 2.05 point increase on SQuAD with full data. The varied shapes of learned RAFs across layers and tasks suggest a new method for analyzing and understanding large pre-trained language models."
    },
    {
      "title": "VideoMamba: State Space Model for Efficient Video Understanding",
      "url": "/research/2024/03/12/VideoMamba-State-Space-Model-for-Efficient-Video-Understanding.html",
      "date": "March 12, 2024",
      "summary": "This work introduces VideoMamba, an innovative approach that addresses local redundancy and global dependencies in video understanding by adapting the Mamba framework to video analysis. VideoMamba outperforms existing 3D convolutional neural networks and video transformers through its linear-complexity operator, which facilitates efficient long-term modeling for high-resolution, long-duration videos. Its effectiveness is demonstrated across four main areas: scalability in the visual domain without needing extensive dataset pretraining, thanks to a novel self-distillation technique; the ability to recognize short-term actions with fine-grained motion differences; superior performance in long-term video understanding compared to traditional feature-based models; and robust compatibility with multiple modalities, enhancing multi-modal video analysis. VideoMamba establishes a new standard for comprehensive and efficient video understanding."
    }
  ],
  "density": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "This paper introduces the concept of delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a measure of local complexity, analyzing the density of linear regions in the DNN input space. The authors find that during training linear regions emerge near training samples with the nonlinearities being forced towards decision boundries."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "This paper introduces a novel approach to Radiance Field methods for novel-view synthesis that addresses the challenges of high visual quality, costly training, and real-time rendering of unbounded, complete scenes at 1080p resolution. The authors propose three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. The method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "region": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "This paper introduces the concept of delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a measure of local complexity, analyzing the density of linear regions in the DNN input space. The authors find that during training linear regions emerge near training samples with the nonlinearities being forced towards decision boundries."
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of Vision Transformers (ViTs) without relying on large-scale pre-training datasets like ImageNet and JFT or requiring modifications to the architecture or loss functions. The authors show that this approach improves ViT performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, while maintaining ViT's attention to relevant regions and robustness, despite ViT's inherent lack of inductive biases and typical dependence on large-scale pre-training."
    },
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "This paper introduces an algorithm that identifies simple and highly generalizable neural networks by searching for extensive flat minima regions in the error function, where the error rate is relatively stable. Such flat minima are associated with lower overfitting risks based on minimum description length (MDL) principles. Despite requiring second-order derivative calculations, the algorithm has a complexity level comparable to backpropagation. When tested on feedforward and recurrent networks, as well as stock market prediction tasks, this algorithm outperformed traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    },
    {
      "title": "Optimal learning rate schedules in high-dimensional non-convex optimization problems",
      "url": "/research/2022/02/09/Optimal-learning-rate-schedules-in-high-dimensional-non-convex-optimization-problems.html",
      "date": "February 9, 2022",
      "summary": "This paper explores the role of learning rate schedules in high-dimensional and non-convex optimization problems, focusing on Langevin optimization with a decaying learning rate. Analyzing models with Gaussian random functions on N-dimensional spheres, the study reveals that to accelerate optimization without getting trapped in saddles, a decay rate β < 1 is optimal, contrary to convex settings where β = 1 is preferred. Introducing a signal recovery component, the dynamics involve an exploration phase navigating through rough landscape parts and a convergence phase entering convex basins. It's found optimal to maintain a large learning rate during exploration to swiftly exit non-convex regions, then transition to β = 1 for rapid convergence to the solution. These findings are validated in a neural network regression task."
    },
    {
      "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
      "url": "/research/2022/03/19/Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training.html",
      "date": "March 19, 2022",
      "summary": "Sharpness-Aware Minimization (SAM) enhances generalization by optimizing a neighborhood-based perturbed loss, but it doesn't always favor flat minima due to both sharp and flat minima having low perturbed losses. We introduce a new measure, the surrogate gap, reflecting the dominant Hessian eigenvalue at small neighborhood radii, which is simple to compute and can be minimized during training. We propose the Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), an advancement over SAM with minimal additional computational cost. GSAM employs a dual-step approach: first, minimizing the perturbed loss similar to SAM, and second, reducing the surrogate gap without affecting the perturbed loss to target regions with low loss and sharpness, thus achieving superior generalization. GSAM is theoretically robust, showing better convergence and empirical generalization improvements, notably a +3.2% gain over SAM and +5.4% over AdamW in ImageNet accuracy for ViT-B/32."
    },
    {
      "title": "ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Network",
      "url": "/research/2021/06/29/ASAM-Adaptive-Sharpness-Aware-Minimization-for-Scale-Invariant-Learning-of-Deep-Neural-Network.html",
      "date": "June 29, 2021",
      "summary": "Recent advances in learning algorithms have highlighted the importance of the sharpness of the loss surface as a key indicator of the generalization gap, achieving state-of-the-art results. However, traditional sharpness measures, defined within a fixed region, suffer from sensitivity to parameter scaling, weakening their predictive value for the generalization gap. This paper introduces a scale-invariant measure, adaptive sharpness, along with a new generalization bound. We present a new learning method, adaptive sharpness-aware minimization (ASAM), which leverages this bound. Our experiments across various benchmark datasets demonstrate that ASAM significantly enhances model generalization."
    },
    {
      "title": "V-Jepa: Latent Video Prediction For Visual Representation Learning",
      "url": "/research/2023/02/15/V-Jepa-Latent-Video-Prediction-For-Visual-Representation-Learning.html",
      "date": "February 15, 2023",
      "summary": "This paper introduces V-JEPA, a self-supervised video learning method that predicts masked spatio-temporal regions in latent space, effectively applying the masked-modelling principle of large language models to video. This approach generates visual features useful across various image and video tasks without needing model adjustment, achieving significant improvements on Kinetics-400 (82.1%) and Something-Something-v2 (71.2%) benchmarks, outperforming prior video models. V-JEPA also excels in motion understanding tasks, surpassing leading image models like DINOv2 and OpenCLIP, and achieves 77.9% on ImageNet classification with video training alone, setting a new standard for video models."
    }
  ],
  "space": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "This paper introduces the concept of delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a measure of local complexity, analyzing the density of linear regions in the DNN input space. The authors find that during training linear regions emerge near training samples with the nonlinearities being forced towards decision boundries."
    },
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/research/2022/08/05/Efficiently-Modeling-Long-Sequences-with-Structured-State-Spaces.html",
      "date": "August 5, 2022",
      "summary": "This paper introduces the Structured State Space sequence model (S4), a more efficient parameterization of State Space Models, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    },
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "State Space Models (SSM), despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. To improve SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "This paper introduces the Mamba archetecture, a selective state space model (SSSM) with input dependance. Mamba has better speed and scalability compared to transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences. Authors also present a hardware efficient scanning mechanism that is used in Mamba."
    },
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/research/2024/01/24/MambaByte-Token-free-Selective-State-Space-Model.html",
      "date": "January 24, 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "This paper introduces the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a multi-directional Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions."
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a Vision archetecture that uses bidirectional Mamba blocks, challenging the necessity of self-attention in vision. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient."
    },
    {
      "title": "Dropout as Data Augmentation",
      "url": "/research/2016/01/08/DROPOUT-AS-DATA-AUGMENTATION.html",
      "date": "January 8, 2016",
      "summary": "Dropout is typically interpreted as turning the test time network into an ensemble of the thinner training networks. This paper argues that dropout can also be interpreted as a kind of data augmentation in image space. Authors present an approach to project the dropout noise within a network back into the input space, visualising the augmented versions of the training data, and show that training a deterministic network on the augmented samples yields similar results. Authors then propose a new dropout noise scheme and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Data augmentation improves generalization, and robustness against image corruptions. However, the use of complex augmentation pipelines typically involves tuning of an enormous number of hyper parameters. RandAugment addresses this by reducing the search space. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets."
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "This study introduces a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. The method involves creating spaces for network design that parameterize multiple network populations. A low-dimensional, efficient design space called RegNet is developed, based on modeling network widths and depths through a quantized linear function. The analysis of the RegNet space challenges existing design practices, offering simpler and faster networks effective across various computational budgets. RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs under similar conditions."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets for formula-driven supervised learning (FDSL). The authors identify the optimal FDSL parameters and maximize synthetic image variety, which is crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. The authors demonstrate FDSL's potential for continuous improvement and its ability to avoid issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel normalization technique for self-supervised transformer training that normalizes the [CLS] token and regular tokens separately. The authors find that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks compared to previous models like masked autoencoders (MAE) that use a single normalization layer for all tokens."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "This paper introduces a novel approach to Radiance Field methods for novel-view synthesis that addresses the challenges of high visual quality, costly training, and real-time rendering of unbounded, complete scenes at 1080p resolution. The authors propose three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. The method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "This paper demonstrates that artificial neural networks (ANNs) are equivalent to Gaussian processes at initialization in the infinite-width limit and introduces the Neural Tangent Kernel (NTK), which describes ANNs' behavior during training. The NTK stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. The authors prove the positive-definiteness of the limiting NTK under certain conditions and show that the network function follows a linear differential equation during training for least-squares regression. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    },
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "The paper introduces TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method that applies a single augmentation to each image. Despite its minimal complexity and cost, TrivialAugment outperforms existing methods across various image classification scenarios, as validated through extensive experiments against state-of-the-art methods and multiple ablation studies involving different augmentation spaces and methods. The work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Noting a stagnation in automatic augmentation research, the authors conclude by proposing best practices for future advancements in the field."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/2020/06/17/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "June 17, 2020",
      "summary": "Implicit neural representations, parameterized by continuous, differentiable neural networks, offer numerous advantages over conventional representations. However, existing architectures struggle with fine detail and fail to capture spatial and temporal derivatives, crucial for many physical signals described by partial differential equations. We introduce sinusoidal representation networks (SIRENs) utilizing periodic activation functions, ideal for representing complex signals and their derivatives. Analyzing SIREN activation statistics informs a principled initialization scheme. We demonstrate SIRENs' efficacy in representing various signals and solving boundary value problems, including Eikonal, Poisson, Helmholtz, and wave equations. Additionally, we integrate SIRENs with hypernetworks to learn priors over the SIREN function space. Visit the project website for detailed demonstrations."
    },
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    },
    {
      "title": "HYPO: Hyperspherical Out-of-Distribution Generalization",
      "url": "/research/2024/03/19/HYPO-Hyperspherical-Out-of-Distribution-Generalization.html",
      "date": "March 19, 2024",
      "summary": "We propose HYPO, a novel framework for out-of-distribution (OOD) generalization in machine learning, which learns domain-invariant features in a hyperspherical space. Our method focuses on aligning features of the same class across different domains close to their class prototypes and separating different class prototypes maximally. We offer theoretical justifications for its improvement on OOD generalization and show through experiments on OOD benchmarks that HYPO outperforms existing baselines with superior performance."
    },
    {
      "title": "Learning and Leveraging World Models in Visual Representation Learning",
      "url": "/research/2024/03/01/Learning-and-Leveraging-World-Models-in-Visual-Representation-Learning.html",
      "date": "March 1, 2024",
      "summary": "The Joint-Embedding Predictive Architecture (JEPA) is a self-supervised method previously used for predicting missing input parts. This study expands JEPA to predict a wider range of corruptions by introducing Image World Models (IWM), which learn the effects of global photometric transformations in latent space. Key to effective IWM learning are conditioning, prediction difficulty, and model capacity. The study demonstrates that IWM's predictive world model, when fine-tuned, can tackle diverse tasks and either matches or outperforms existing self-supervised techniques. Additionally, it enables control over the abstraction level of learned representations, achieving either invariant or equivariant representations, similar to contrastive methods or masked image modeling, respectively."
    },
    {
      "title": "MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts",
      "url": "/research/2024/02/26/MoE-Mamba-Efficient-Selective-State-Space-Models-with-Mixture-of-Experts.html",
      "date": "February 26, 2024",
      "summary": "State Space Models (SSMs) are emerging as strong competitors in sequential modeling, challenging Transformers. Integrating Mixture of Experts (MoE) has enhanced Transformer-based models, including cutting-edge open models. We suggest combining SSMs with MoE to further unlock their scaling potential. Our model, MoE-Mamba, based on the SSM model Mamba, exceeds the performance of both Mamba and traditional Transformer-MoE models. Notably, MoE-Mamba achieves Mamba's performance with 2.35 times fewer training steps, maintaining Mamba's inference advantages over Transformers."
    },
    {
      "title": "V-Jepa: Latent Video Prediction For Visual Representation Learning",
      "url": "/research/2023/02/15/V-Jepa-Latent-Video-Prediction-For-Visual-Representation-Learning.html",
      "date": "February 15, 2023",
      "summary": "This paper introduces V-JEPA, a self-supervised video learning method that predicts masked spatio-temporal regions in latent space, effectively applying the masked-modelling principle of large language models to video. This approach generates visual features useful across various image and video tasks without needing model adjustment, achieving significant improvements on Kinetics-400 (82.1%) and Something-Something-v2 (71.2%) benchmarks, outperforming prior video models. V-JEPA also excels in motion understanding tasks, surpassing leading image models like DINOv2 and OpenCLIP, and achieves 77.9% on ImageNet classification with video training alone, setting a new standard for video models."
    },
    {
      "title": "VideoMamba: State Space Model for Efficient Video Understanding",
      "url": "/research/2024/03/12/VideoMamba-State-Space-Model-for-Efficient-Video-Understanding.html",
      "date": "March 12, 2024",
      "summary": "This work introduces VideoMamba, an innovative approach that addresses local redundancy and global dependencies in video understanding by adapting the Mamba framework to video analysis. VideoMamba outperforms existing 3D convolutional neural networks and video transformers through its linear-complexity operator, which facilitates efficient long-term modeling for high-resolution, long-duration videos. Its effectiveness is demonstrated across four main areas: scalability in the visual domain without needing extensive dataset pretraining, thanks to a novel self-distillation technique; the ability to recognize short-term actions with fine-grained motion differences; superior performance in long-term video understanding compared to traditional feature-based models; and robust compatibility with multiple modalities, enhancing multi-modal video analysis. VideoMamba establishes a new standard for comprehensive and efficient video understanding."
    },
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "emerge": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "This paper introduces the concept of delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a measure of local complexity, analyzing the density of linear regions in the DNN input space. The authors find that during training linear regions emerge near training samples with the nonlinearities being forced towards decision boundries."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "This paper shows self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs or convnets. Also proposes a modification to BYOL self-supervised learning with a momentum encoder, multi-crop training, and feature sharping for the teacher network."
    },
    {
      "title": "Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks",
      "url": "/research/2020/06/11/Batch-Normalization-Provably-Avoids-Rank-Collapse-for-Randomly-Initialised-Deep-Networks.html",
      "date": "June 11, 2020",
      "summary": "We investigate the challenge of training randomly initialized deep neural networks due to spectral instabilities in products of random matrices. Batch normalization emerges as an effective solution to prevent rank collapse in both linear and ReLU networks. Leveraging tools from Markov chain theory, we establish a lower rank bound for deep linear networks. Empirical findings show that this rank robustness extends to ReLU networks. Our experiments on real-world datasets underscore the significance of rank stability in training modern deep neural architectures."
    }
  ],
  "nonlinearity": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "This paper introduces the concept of delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a measure of local complexity, analyzing the density of linear regions in the DNN input space. The authors find that during training linear regions emerge near training samples with the nonlinearities being forced towards decision boundries."
    }
  ],
  "force": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "This paper introduces the concept of delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a measure of local complexity, analyzing the density of linear regions in the DNN input space. The authors find that during training linear regions emerge near training samples with the nonlinearities being forced towards decision boundries."
    }
  ],
  "decision": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "This paper introduces the concept of delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a measure of local complexity, analyzing the density of linear regions in the DNN input space. The authors find that during training linear regions emerge near training samples with the nonlinearities being forced towards decision boundries."
    }
  ],
  "boundry": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "This paper introduces the concept of delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a measure of local complexity, analyzing the density of linear regions in the DNN input space. The authors find that during training linear regions emerge near training samples with the nonlinearities being forced towards decision boundries."
    }
  ],
  "binarization": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights. This paper shows deep networks have significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. The authors propose a stochastic projection rule that sets a new benchmark CIFAR-10 without data augmentation."
    }
  ],
  "distortion": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights. This paper shows deep networks have significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. The authors propose a stochastic projection rule that sets a new benchmark CIFAR-10 without data augmentation."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "This study challenges the traditional understanding of how Convolutional Neural Networks (CNNs) recognize objects, revealing that they are more biased towards recognizing textures rather than learning complex shapes. The authors found that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, training a ResNet-50 on a stylized version of ImageNet, designed to emphasize shape, aligned the network's performance more closely with human behavior. This shape-based training matched human performance in controlled experiments and enhanced object detection and robustness against image distortions, highlighting the benefits of shape-based representations in visual recognition systems."
    }
  ],
  "quantize": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights. This paper shows deep networks have significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. The authors propose a stochastic projection rule that sets a new benchmark CIFAR-10 without data augmentation."
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "This study introduces a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. The method involves creating spaces for network design that parameterize multiple network populations. A low-dimensional, efficient design space called RegNet is developed, based on modeling network widths and depths through a quantized linear function. The analysis of the RegNet space challenges existing design practices, offering simpler and faster networks effective across various computational budgets. RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs under similar conditions."
    },
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "This paper introduces QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU while maintaining 16-bit task performance. QLORA utilizes 4-bit quantized language models and Low Rank Adapters (LoRA) and incorporates several memory-saving innovations without compromising performance. The authors' best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. The authors applied QLORA to finetune over 1,000 models and analyzed performance across various datasets, model types, and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Their findings suggest that GPT-4 evaluations are a viable substitute for human assessments and question the reliability of current chatbot benchmarks. The authors make their models and 4-bit training code public."
    }
  ],
  "resilience": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights. This paper shows deep networks have significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. The authors propose a stochastic projection rule that sets a new benchmark CIFAR-10 without data augmentation."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. The authors find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. "
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Incorporating diverse image styles into CNN training data reduces texture bias, enhances shape recognition, and improves resilience against common image corruptions. This is typically explained as decreasing model reliance on high frequency texture information. This paper challenges this explaniation through large scale testing with natural images, edge information, and stylization, finding no direct link between shape bias and robustness. The enhanced corruption robustness is instead attributed to style variation in data augmentation, with increased shape bias being an indirect effect."
    }
  ],
  "projection": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights. This paper shows deep networks have significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. The authors propose a stochastic projection rule that sets a new benchmark CIFAR-10 without data augmentation."
    },
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "This paper explores Gated Linear Units (GLUs), which involve the component-wise product of two linear projections, with one undergoing a sigmoid function. The authors investigate GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers. They find that certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    }
  ],
  "quantization": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights. This paper shows deep networks have significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. The authors propose a stochastic projection rule that sets a new benchmark CIFAR-10 without data augmentation."
    }
  ],
  "rule": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights. This paper shows deep networks have significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. The authors propose a stochastic projection rule that sets a new benchmark CIFAR-10 without data augmentation."
    }
  ],
  "cifar": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights. This paper shows deep networks have significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. The authors propose a stochastic projection rule that sets a new benchmark CIFAR-10 without data augmentation."
    },
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/research/2017/08/23/A-downsampled-variant-of-imagenet-as-an-alternative-to-the-cifar-datasets.html",
      "date": "August 23, 2017",
      "summary": "This paper proposes downsampled versions of ImageNet: ImageNet64x64, ImageNet32x32, and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "This study introduces the Vision Transformer ViT archetecture. The paper shows that a pure transformer applied directly to sequences of image patches can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. This encoder serves as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The DMAE ViT-Base model achieves comparable or superior certified accuracy with fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on ImageNet. The model also shows high transferability to CIFAR-10, indicating its broad applicability."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "This paper introduces FixMatch, a streamlined semi-supervised learning (SSL) algorithm. FixMatch generates pseudo-labels using a model's predictions on weakly-augmented unlabeled images, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness, and the code is publicly available."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "This paper introduces Online Normalization, a novel method for normalizing neural network hidden activations that offers a batch-independent alternative with comparable accuracy to Batch Normalization. Online Normalization addresses Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. The method is applicable to recurrent, fully connected networks, and those with high activation memory requirements. The authors demonstrate its effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Eigencurve: Optimal Learning Rate Schedule For Sgd on Quadratic Objectives With Skewed Hessian Spectrums",
      "url": "/research/2022/06/14/Eigencurve-Optimal-Learning-Rate-Schedule-For-Sgd-on-Quadratic-Objectives-With-Skewed-Hessian-Spectrums.html",
      "date": "June 14, 2022",
      "summary": "Learning rate schedulers are essential in deep neural network training, but there's a gap between practical usage and theoretical understanding. This paper introduces Eigencurve, the first set of learning rate schedules achieving minimax optimal convergence rates for SGD on quadratic objectives with skewed eigenvalue distributions of the Hessian matrix. This condition is common in practice. Experimental results on CIFAR-10 image classification tasks demonstrate Eigencurve's superiority over step decay, particularly with fewer epochs. The theory inspires practical schedulers approximating Eigencurve, resembling cosine decay for some problems while outperforming it in others."
    },
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    },
    {
      "title": "SGDR: Stochastic Gradient Descent With Warm Restarts",
      "url": "/research/2017/05/02/SGDR-Stochastic-Gradient-Descent-With-Warm-Restarts.html",
      "date": "May 2, 2017",
      "summary": "This paper introduces a warm restart technique for stochastic gradient descent aimed at enhancing anytime performance in deep neural network training. It showcases empirical performance improvements on CIFAR-10 and CIFAR-100 datasets, achieving state-of-the-art results with 3.14% and 16.21% error rates, respectively. Additionally, the technique's benefits are demonstrated on an EEG dataset and a downsampled ImageNet dataset."
    }
  ],
  "contain": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "This paper shows self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs or convnets. Also proposes a modification to BYOL self-supervised learning with a momentum encoder, multi-crop training, and feature sharping for the teacher network."
    },
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "This paper introduces FineAction, a large-scale, fine-grained video dataset designed to address the limitations of current temporal action localization (TAL) benchmarks, which rely on coarse action classes and lead to model overfitting and ambiguous annotations. FineAction contains 103K instances across 106 action categories in 17K videos, offering a diverse and densely annotated dataset with co-occurring actions that pose new challenges for TAL. The authors evaluate popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and propose a baseline method achieving a 13.17% mAP. FineAction aims to advance TAL research and is accessible online."
    },
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "This paper introduces the Kinetics Human Action Video dataset. The authors note that the limited number of videos in existing datasets like UCF-101 and HMDB-51 hinders the ability to effectively evaluate architectures due to similar performance across these small benchmarks. Kinetics contains 400 classes with over 400 video clips per class from challenging YouTube videos. The paper analyzes the impact of this larger dataset on the performance of existing architectures as well as the benefits of pre-training models on Kinetics. The authors introduce the Two-Stream Inflated 3D ConvNet (I3D), an extension of 2D ConvNets to 3D for improved video feature extraction. When pre-trained on Kinetics, this I3D model sets new benchmarks for action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    }
  ],
  "information": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "This paper shows self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs or convnets. Also proposes a modification to BYOL self-supervised learning with a momentum encoder, multi-crop training, and feature sharping for the teacher network."
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Incorporating diverse image styles into CNN training data reduces texture bias, enhances shape recognition, and improves resilience against common image corruptions. This is typically explained as decreasing model reliance on high frequency texture information. This paper challenges this explaniation through large scale testing with natural images, edge information, and stylization, finding no direct link between shape bias and robustness. The enhanced corruption robustness is instead attributed to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "This paper investigates the geometric and statistical properties of representations across layers in large transformers used for self-supervised learning on various data types. The authors observe common evolution patterns, with data manifolds expanding initially, contracting at intermediate layers, and the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. They find that semantic information peaks after initial expansion, a trend consistent across models and datasets. The study proposes an unsupervised method to identify layers richest in semantic content, suggesting that those at a relative ID minimum are optimal for downstream tasks."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Do Vision Transformers See Like Convolutional Neural Networks?",
      "url": "/research/2022/03/03/Do-Vision-Transformers-See-Like-Convolutional-Neural-Networks.html",
      "date": "March 3, 2022",
      "summary": "Convolutional Neural Networks (CNNs) have been prominent in processing visual data, but recent studies show Vision Transformers (ViTs) can match or outperform CNNs in image classification. This work examines how ViTs solve classification tasks, discovering significant differences from CNNs, including uniformity in ViT representations across layers due to self-attention and residual connections that enhance feature propagation. ViTs also maintain spatial information effectively, influenced by classification methods. The study further explores how dataset scale affects ViT features and their transferability, linking these findings to new architectures like MLP-Mixer."
    },
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    }
  ],
  "multicrop": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "This paper shows self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs or convnets. Also proposes a modification to BYOL self-supervised learning with a momentum encoder, multi-crop training, and feature sharping for the teacher network."
    }
  ],
  "sharp": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "This paper shows self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs or convnets. Also proposes a modification to BYOL self-supervised learning with a momentum encoder, multi-crop training, and feature sharping for the teacher network."
    }
  ],
  "sequence": [
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/research/2022/08/05/Efficiently-Modeling-Long-Sequences-with-Structured-State-Spaces.html",
      "date": "August 5, 2022",
      "summary": "This paper introduces the Structured State Space sequence model (S4), a more efficient parameterization of State Space Models, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    },
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "State Space Models (SSM), despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. To improve SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "This paper introduces the Mamba archetecture, a selective state space model (SSSM) with input dependance. Mamba has better speed and scalability compared to transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences. Authors also present a hardware efficient scanning mechanism that is used in Mamba."
    },
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/research/2024/01/24/MambaByte-Token-free-Selective-State-Space-Model.html",
      "date": "January 24, 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    },
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrives incrementally, it's common to build a sequence of models that incorporate progressively more data. It has been shown that simply continuing training of a model on new data often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "This study introduces the Vision Transformer ViT archetecture. The paper shows that a pure transformer applied directly to sequences of image patches can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This paper is effectively a follow-up to DinoV2. The study addresses the issue of artifacts in feature maps of Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. The authors introduce a straightforward strategy of adding extra tokens to the input sequence of the Vision Transformer, which eliminates these artifacts for both supervised and self-supervised models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks, facilitates object discovery with larger models, and results in smoother feature and attention maps for downstream visual processing tasks."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article provides an extensive review of vision transformer techniques applied specifically to human action recognition. These action transformers are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also examines optimizing spatio-temporal attention in transformers for longer sequences and different learning strategies like self-supervised and zero-shot learning with their respective loss functions. Additionally, it assesses progress in benchmark evaluation scores and discusses challenges and future directions in this research area."
    },
    {
      "title": "Normalized Attention Without Probability Cage",
      "url": "/research/2020/05/19/Normalized-Attention-Without-Probability-Cage.html",
      "date": "May 19, 2020",
      "summary": "Softmax-attention architectures, especially popularized by Transformers, have seen significant advancements in various tasks. However, the geometric implications of softmax-attention remain underexplored. In this study, we demonstrate limitations arising from constraining attention weights to the probability simplex and its impact on the convex hull of value vectors. We reveal sequence length-dependent biases in Transformers towards token isolation at initialization and compare them with max- and sum-pooling, which are strong but often overlooked baselines. To address these issues, we propose a novel approach of replacing softmax with normalization in self-attention, resulting in a robust and widely applicable architecture. Our findings are supported by empirical results from over 25,000 trained models, and all results and implementations are publicly available."
    },
    {
      "title": "FNet: Mixing Tokens with Fourier Transforms",
      "url": "/research/2022/05/26/FNet-Mixing-Tokens-with-Fourier-Transforms.html",
      "date": "May 26, 2022",
      "summary": "We demonstrate that Transformer encoder architectures can be accelerated with minimal impact on accuracy by substituting self-attention sublayers with simple linear transformations. Remarkably, using a standard Fourier Transform instead of the self-attention sublayer in a Transformer encoder achieves 92-97% of BERT models' accuracy on the GLUE benchmark, while training 80% faster on GPUs and 70% faster on TPUs for standard 512 input lengths. Our FNet model significantly outperforms in speed at longer input lengths, matching the accuracy of the most accurate models on the Long Range Arena benchmark and surpassing the fastest models in speed across all sequence lengths on GPUs and shorter lengths on TPUs. Additionally, FNet is lightweight in memory use and exceptionally efficient in smaller sizes, outperforming Transformer models under the same speed and accuracy constraints."
    }
  ],
  "structure": [
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/research/2022/08/05/Efficiently-Modeling-Long-Sequences-with-Structured-State-Spaces.html",
      "date": "August 5, 2022",
      "summary": "This paper introduces the Structured State Space sequence model (S4), a more efficient parameterization of State Space Models, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    },
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. Taking this to the extreme, this study trains a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss even when trained on pure noise."
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "This paper introduces DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. Authors also propose a dropout rate scheduler to optimize training."
    },
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "The authors introduce VideoMAE, a data-efficient self-supervised video pre-training (SSVP) approach that utilizes video masked autoencoders with a novel high-ratio video tube masking technique inspired by ImageMAE. Key findings include the effectiveness of high masking ratios (90-95%) due to video's temporal redundancy, strong performance on small datasets (~3k-4k videos) without extra data highlighting the importance of high-level structure learning, and data quality being more crucial than quantity with domain shift being significant. Notably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone without extra data."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    },
    {
      "title": "Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding",
      "url": "/research/2021/11/16/Improved-Robustness-of-Vision-Transformer-via-PreLayerNorm-in-Patch-Embedding.html",
      "date": "November 16, 2021",
      "summary": "Recent advancements in vision transformers (ViTs) have shown superior performance across diverse visual tasks, surpassing convolutional neural networks (CNNs). Given ViT's distinct architecture, understanding its behavior and reliability is imperative. This paper investigates ViT's robustness by comparing it with CNNs under various image corruptions relevant to real-world vision tasks. While ViT generally exhibits comparable or improved robustness over CNNs, it consistently underperforms in contrast enhancement tasks. Analysis suggests that positional embedding in ViT's patch embedding may malfunction with color scale changes. We propose PreLayerNorm, a modified patch embedding structure, to address this issue and ensure scale-invariant behavior in ViT. ViT with PreLayerNorm demonstrates enhanced robustness across various corruptions, particularly in contrast-varying environments."
    },
    {
      "title": "Atomically Accurate De Novo Design of Single-domain Antibodies",
      "url": "/research/2024/03/18/Atomically-Accurate-De-Novo-Design-of-Single-domain-Antibodies.html",
      "date": "March 18, 2024",
      "summary": "Despite the critical role antibodies play in medicine, current methods for designing new antibodies targeting specific epitopes are time-consuming. Here, we showcase the effectiveness of a refined RFdiffusion network in designing novel antibody variable heavy chains (VHH’s) to target user-specified epitopes. Through experiments, we validate the binding capability of these designed VHH's to four disease-relevant epitopes. Furthermore, the cryo-EM structure analysis reveals that a designed VHH bound to influenza hemagglutinin closely matches the design model, affirming the accuracy of our approach."
    },
    {
      "title": "From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning",
      "url": "/research/2023/03/16/From-MNIST-to-ImageNet-and-Back:-Benchmarking-Continual-Curriculum-Learning.html",
      "date": "March 16, 2023",
      "summary": "Continual learning (CL) is a promising trend in machine learning, aiming to develop robust models and strategies for dynamic environments by incorporating new knowledge while retaining past knowledge. CL research is fragmented, with various protocols, tasks, datasets, and metrics, often not reflecting real-world complexity and tailored to specific strategies. This work addresses this gap by introducing two novel CL benchmarks with heterogeneous tasks from six image datasets, varying in complexity and quality. These benchmarks fairly evaluate state-of-the-art CL strategies in scenarios closer to real-world conditions. The benchmarks present tasks in both increasing and decreasing complexity, assessing models' ability to exploit task structure. The work emphasizes a rigorous, reproducible evaluation protocol for measuring generalization and memory retention in models. Experimental results show that popular CL strategies perform poorly on these benchmarks, exhibit high forgetting, and struggle with curriculum task ordering. These findings underscore the need for rigorous comparisons and the development of new CL strategies for complex scenarios."
    },
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "s4": [
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/research/2022/08/05/Efficiently-Modeling-Long-Sequences-with-Structured-State-Spaces.html",
      "date": "August 5, 2022",
      "summary": "This paper introduces the Structured State Space sequence model (S4), a more efficient parameterization of State Space Models, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    }
  ],
  "parameterization": [
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/research/2022/08/05/Efficiently-Modeling-Long-Sequences-with-Structured-State-Spaces.html",
      "date": "August 5, 2022",
      "summary": "This paper introduces the Structured State Space sequence model (S4), a more efficient parameterization of State Space Models, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    },
    {
      "title": "Sharpness-Aware Training for Free",
      "url": "/research/2023/03/02/Sharpness-Aware-Training-for-Free.html",
      "date": "March 2, 2023",
      "summary": "Modern deep neural networks (DNNs) excel in performance but often suffer from over-parameterization, leading to increased generalization error without tailored training strategies. Sharpness-Aware Minimization (SAM) has proven effective in reducing generalization error by minimizing sharpness in the loss landscape. However, SAM incurs a significant computational overhead. This paper introduces Sharpness-Aware Training for Free (SAF), which mitigates sharpness at nearly zero additional computational cost. SAF achieves this by preventing sudden drops in loss within sharp local minima during weight updates. A novel trajectory loss, based on KL-divergence between current and past DNN outputs, replaces SAM's sharpness measure, guiding convergence towards flat minima for enhanced generalization. Empirical results demonstrate SAF's effectiveness on ImageNet with comparable computational efficiency to the base optimizer."
    },
    {
      "title": "The loss landscape of deep linear neural networks: a second-order analysis",
      "url": "/research/2022/03/11/The-loss-landscape-of-deep-linear-neural-networks-a-second-order-analysis.html",
      "date": "March 11, 2022",
      "summary": "This study investigates the optimization landscape of deep linear neural networks with square loss, focusing on the role of non-strict saddle points in algorithm dynamics, which has been previously understudied. the authors conduct a comprehensive second-order analysis, identifying global minimizers, strict, and non-strict saddle points among all critical points, alongside their critical values. Our findings, based on conditions related to the ranks of partial matrix products, contribute insights into global convergence and implicit regularization observed in optimizing linear neural networks. Additionally, the authors offer an explicit parameterization of global minimizers and identify extensive sets of strict and non-strict saddle points."
    }
  ],
  "instance": [
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/research/2023/03/08/Extreme-Masking-for-Learning-Instance-and-Distributed-Visual-Representations.html",
      "date": "March 8, 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance."
    },
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/research/2017/11/06/Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization.html",
      "date": "November 6, 2017",
      "summary": "This paper itroduces Instance Normalization, an online normalization technique that calculates the mean and std across individual data instances. This is shown to work well in situations where there is very high vatiation within batches, such as in style transfer."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) uses synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. Given this perspective, a one-instance fractal database (OFDB) is developed where only a single image per category is present. Despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M comparable or superior results are obtaine to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "Curriculum Learning by Dynamic Instance Hardness",
      "url": "/research/2020/12/06/Curriculum-Learning-by-Dynamic-Instance-Hardness.html",
      "date": "December 6, 2020",
      "summary": "This paper improves curriculum learning via introduces Dynamic Instance Hardness (DIH), a method that measures a sample's learning difficulty over time. DIH provides a stable indicator of learning progress by tracking the exponential moving average of a sample's hardness. DIHCL enhances learning efficiency and model accuracy without extra computational costs by leveraging data from the training process itself. Tested on 11 datasets, DIHCL surpasses traditional training methods and recent curriculum learning techniques in both efficiency and effectiveness."
    },
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "This paper introduces FineAction, a large-scale, fine-grained video dataset designed to address the limitations of current temporal action localization (TAL) benchmarks, which rely on coarse action classes and lead to model overfitting and ambiguous annotations. FineAction contains 103K instances across 106 action categories in 17K videos, offering a diverse and densely annotated dataset with co-occurring actions that pose new challenges for TAL. The authors evaluate popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and propose a baseline method achieving a 13.17% mAP. FineAction aims to advance TAL research and is accessible online."
    },
    {
      "title": "Language Modeling with Gated Convolutional Networks",
      "url": "/research/2017/09/08/Language-Modeling-with-Gated-Convolutional-Networks.html",
      "date": "September 8, 2017",
      "summary": "This paper introduces a novel language modeling approach using stacked convolutions that enable efficient parallel processing, in contrast to the traditionally used recurrent neural networks known for handling unbounded context. The proposed method features a simplified gating mechanism that outperforms previous models and demonstrates superior performance on the WikiText-103 benchmark, showing its capability to manage long-term dependencies. It also delivers competitive results on the Google Billion Words benchmark and significantly reduces sentence scoring latency compared to recurrent models. This marks the first instance of a non-recurrent model achieving comparable success to strong recurrent models on major language tasks."
    },
    {
      "title": "ImageNet-D: A new challenging robustness dataset inspired by domain adaptation",
      "url": "/research/2023/05/05/ImageNet-D-A-new-challenging-robustness-dataset-inspired-by-domain-adaptation.html",
      "date": "May 5, 2023",
      "summary": "We introduce ImageNet-D, a novel dataset designed to evaluate the robustness of ImageNet-trained models across various domains. With six distinct domains including Real, Painting, Clipart, Sketch, Infograph, and Quick-draw, ImageNet-D challenges even state-of-the-art models, revealing interpretable errors. For instance, our leading EfficientNet-L2 model exhibits a significant performance decrease, dropping from 11.6% on clean ImageNet to 29.2% on the Real domain."
    }
  ],
  "distribute": [
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/research/2023/03/08/Extreme-Masking-for-Learning-Instance-and-Distributed-Visual-Representations.html",
      "date": "March 8, 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance."
    },
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    }
  ],
  "extrema": [
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/research/2023/03/08/Extreme-Masking-for-Learning-Instance-and-Distributed-Visual-Representations.html",
      "date": "March 8, 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance."
    }
  ],
  "attention": [
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/research/2023/03/08/Extreme-Masking-for-Learning-Instance-and-Distributed-Visual-Representations.html",
      "date": "March 8, 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance."
    },
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "State Space Models (SSM), despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. To improve SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing."
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a Vision archetecture that uses bidirectional Mamba blocks, challenging the necessity of self-attention in vision. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient."
    },
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/research/2019/07/26/DropAttention-A-Regularization-Method-for-Fully-Connected-Self-Attention-Networks.html",
      "date": "July 26, 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. DropAttention uses a mask to zero out elements in the attention matrix. Experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    },
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a method for dropout in Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. The authors find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. "
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "This paper introduces DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. Authors also propose a dropout rate scheduler to optimize training."
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study looks at self-supervised pre-training methods on small scale or lightweight Vision Transformers (ViTs). Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This paper is effectively a follow-up to DinoV2. The study addresses the issue of artifacts in feature maps of Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. The authors introduce a straightforward strategy of adding extra tokens to the input sequence of the Vision Transformer, which eliminates these artifacts for both supervised and self-supervised models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks, facilitates object discovery with larger models, and results in smoother feature and attention maps for downstream visual processing tasks."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "This research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. The authors validate these approaches using the ImageNet-1k dataset and confirm their findings with the ImageNet-v2 test set, evaluating transfer performance across six additional datasets."
    },
    {
      "title": "Training data-efficient image transformers & distillation through attention",
      "url": "/research/2021/01/15/Training-data-efficient-image-transformers-&-distillation-through-attention.html",
      "date": "January 15, 2021",
      "summary": "This paper introduces DeiT, an efficient method for training vision transformers. The authors present a unique teacher-student strategy for transformers, leveraging a distillation token for efficient learning from a convolutional network teacher. This method achieves performance on par with convolutional networks, with up to 85.2% accuracy on ImageNet, and demonstrates effective transferability to other tasks."
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of Vision Transformers (ViTs) without relying on large-scale pre-training datasets like ImageNet and JFT or requiring modifications to the architecture or loss functions. The authors show that this approach improves ViT performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, while maintaining ViT's attention to relevant regions and robustness, despite ViT's inherent lack of inductive biases and typical dependence on large-scale pre-training."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "The paper proposes three normalization-centric modifications to improve Transformer training: PRENORM which introduces pre-norm residual connections and smaller initializations, enabling warmup-free, validation-based training with large learning rates; SCALENORM which suggests L2 normalization with a single scale parameter for faster training and better performance; and FIXNORM which reaffirms the efficacy of normalizing word embeddings to a fixed length."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article provides an extensive review of vision transformer techniques applied specifically to human action recognition. These action transformers are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also examines optimizing spatio-temporal attention in transformers for longer sequences and different learning strategies like self-supervised and zero-shot learning with their respective loss functions. Additionally, it assesses progress in benchmark evaluation scores and discusses challenges and future directions in this research area."
    },
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "The paper focuses on evaluating the performance of recent advancements in image transformers, such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, on the Tiny ImageNet dataset for transfer learning tasks. While these models are typically trained on large datasets like ImageNet-21k and then fine-tuned on ImageNet-1k, their assessments often overlook the Tiny ImageNet benchmark. The study updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. This underscores the importance of evaluating these models on the Tiny ImageNet dataset for transfer learning performance."
    },
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    },
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    },
    {
      "title": "Normalized Attention Without Probability Cage",
      "url": "/research/2020/05/19/Normalized-Attention-Without-Probability-Cage.html",
      "date": "May 19, 2020",
      "summary": "Softmax-attention architectures, especially popularized by Transformers, have seen significant advancements in various tasks. However, the geometric implications of softmax-attention remain underexplored. In this study, we demonstrate limitations arising from constraining attention weights to the probability simplex and its impact on the convex hull of value vectors. We reveal sequence length-dependent biases in Transformers towards token isolation at initialization and compare them with max- and sum-pooling, which are strong but often overlooked baselines. To address these issues, we propose a novel approach of replacing softmax with normalization in self-attention, resulting in a robust and widely applicable architecture. Our findings are supported by empirical results from over 25,000 trained models, and all results and implementations are publicly available."
    },
    {
      "title": "Do Vision Transformers See Like Convolutional Neural Networks?",
      "url": "/research/2022/03/03/Do-Vision-Transformers-See-Like-Convolutional-Neural-Networks.html",
      "date": "March 3, 2022",
      "summary": "Convolutional Neural Networks (CNNs) have been prominent in processing visual data, but recent studies show Vision Transformers (ViTs) can match or outperform CNNs in image classification. This work examines how ViTs solve classification tasks, discovering significant differences from CNNs, including uniformity in ViT representations across layers due to self-attention and residual connections that enhance feature propagation. ViTs also maintain spatial information effectively, influenced by classification methods. The study further explores how dataset scale affects ViT features and their transferability, linking these findings to new architectures like MLP-Mixer."
    },
    {
      "title": "FNet: Mixing Tokens with Fourier Transforms",
      "url": "/research/2022/05/26/FNet-Mixing-Tokens-with-Fourier-Transforms.html",
      "date": "May 26, 2022",
      "summary": "We demonstrate that Transformer encoder architectures can be accelerated with minimal impact on accuracy by substituting self-attention sublayers with simple linear transformations. Remarkably, using a standard Fourier Transform instead of the self-attention sublayer in a Transformer encoder achieves 92-97% of BERT models' accuracy on the GLUE benchmark, while training 80% faster on GPUs and 70% faster on TPUs for standard 512 input lengths. Our FNet model significantly outperforms in speed at longer input lengths, matching the accuracy of the most accurate models on the Long Range Arena benchmark and surpassing the fastest models in speed across all sequence lengths on GPUs and shorter lengths on TPUs. Additionally, FNet is lightweight in memory use and exceptionally efficient in smaller sizes, outperforming Transformer models under the same speed and accuracy constraints."
    },
    {
      "title": "A Phase Transition between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention",
      "url": "/research/2024/02/06/A-Phase-Transition-between-Positional-and-Semantic-Learning-in-a-Solvable-Model-of-Dot-Product-Attention.html",
      "date": "February 6, 2024",
      "summary": "We explore the learning process of a dot-product attention layer, which learns both positional and semantic attention matrices, enabling tokens to attend based on position or meaning. Through experiments on an algorithmic task, we demonstrate that this architecture can use either mechanism for solving the task. Theoretically, we examine a non-linear self-attention layer with special query and key matrices, offering a closed-form solution for its non-convex loss landscape in high-dimensional data, which reveals a phase transition from positional to semantic mechanisms as sample complexity increases. We also show that the dot-product attention layer surpasses a linear positional baseline through the semantic mechanism with adequate data."
    },
    {
      "title": "On the Relationship Between Self-Attention And Convolutional Layers",
      "url": "/research/2020/01/10/On-the-Relationship-Between-Self-Attention-And-Convolutional-Layers.html",
      "date": "January 10, 2020",
      "summary": "Recent studies have shown that attention mechanisms can rival or even outperform convolutional layers in vision tasks, challenging their dominance. Ramachandran et al. (2019) demonstrated that attention could entirely replace convolution, achieving top results. This work investigates whether attention layers function like convolutional layers, finding that multi-head self-attention layers, with enough heads, match the expressiveness of convolutional layers. Numerical experiments confirm that self-attention layers learn to focus on pixel-grid patterns akin to convolutional layers, supporting our findings."
    },
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "block": [
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/research/2023/03/08/Extreme-Masking-for-Learning-Instance-and-Distributed-Visual-Representations.html",
      "date": "March 8, 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance."
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a Vision archetecture that uses bidirectional Mamba blocks, challenging the necessity of self-attention in vision. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient."
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, this paper introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, the authors enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. This simple method surpasses the current state-of-the-art in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets transformers as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "This paper investigates the importance of the warm-up stage in training transformers, and the impact of layer normalization placement. Using mean field theory, the authors demonstrate that the original Post-LN Transformer's design results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "On Layer Normalization in the Transformer Architecture",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architecture.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is widely used in natural language processing. Training it typically requires a carefully designed learning rate warm-up stage, crucial for performance but slowing optimization and requiring more hyperparameter tuning. We provide theoretical insights into the necessity of the warm-up stage, demonstrating that the location of layer normalization affects gradient behavior. For Post-LN Transformers, with layer normalization between residual blocks, large gradients near the output layer at initialization make training unstable without warm-up. Conversely, for Pre-LN Transformers, with layer normalization inside residual blocks, gradients are well-behaved, suggesting the removal of the warm-up stage. Experimental results show that Pre-LN Transformers without warm-up achieve comparable performance with less training time and hyperparameter tuning."
    },
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "siamese": [
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/research/2023/03/08/Extreme-Masking-for-Learning-Instance-and-Distributed-Visual-Representations.html",
      "date": "March 8, 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. The Authors show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements."
    }
  ],
  "accelerates": [
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/research/2023/03/08/Extreme-Masking-for-Learning-Instance-and-Distributed-Visual-Representations.html",
      "date": "March 8, 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance."
    },
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    },
    {
      "title": "Self-Paced Absolute Learning Progress as a Regularized Approach to Curriculum Learning",
      "url": "/research/2023/06/09/Self-Paced-Absolute-Learning-Progress-as-a-Regularized-Approach-to-Curriculum-Learning.html",
      "date": "June 9, 2023",
      "summary": "Reinforcement Learning's usability is limited by its high computation times. Curriculum Reinforcement Learning accelerates learning by ordering tasks from simple to hard. Curricula based on Absolute Learning Progress (ALP) have shown success in various environments but waste computation on redundant tasks. This issue is addressed by introducing Self-Paced Absolute Learning Progress (SPALP), a regularization method inspired by Self-Paced Learning. Evaluated in three environments, SPALP achieves performance comparable to ALP in all cases and reaches it faster in two. Further improvements in SPALP's efficiency and performance are also discussed."
    }
  ],
  "feedforward": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/research/2023/09/18/Fast-Feedforward-Networks.html",
      "date": "September 18, 2023",
      "summary": "This paper introduces the fast feedforward (FFF) architecture, a log-time alternative to feedforward networks. FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution."
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "This paper introduces Xavier Glorot init for Sigmoid like activation functions. The study investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial."
    },
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "This paper introduces an algorithm that identifies simple and highly generalizable neural networks by searching for extensive flat minima regions in the error function, where the error rate is relatively stable. Such flat minima are associated with lower overfitting risks based on minimum description length (MDL) principles. Despite requiring second-order derivative calculations, the algorithm has a complexity level comparable to backpropagation. When tested on feedforward and recurrent networks, as well as stock market prediction tasks, this algorithm outperformed traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    },
    {
      "title": "Exponentially Faster Language Modeling",
      "url": "/research/2023/11/21/Exponentially-Faster-Language-Modeling.html",
      "date": "November 21, 2023",
      "summary": "UltraFastBERT, a BERT variant, operates with just 0.3% of its neurons—selectively using 12 out of 4095 neurons per layer—for inference, matching the performance of similar models. It replaces conventional feedforward networks with fast feedforward networks (FFFs) to achieve this efficiency. Although fully efficient conditional neural execution isn't yet practical, we offer a high-level CPU code that achieves a 78x speedup and a PyTorch implementation with a 40x speedup over standard batched feedforward inference."
    }
  ],
  "fff": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/research/2023/09/18/Fast-Feedforward-Networks.html",
      "date": "September 18, 2023",
      "summary": "This paper introduces the fast feedforward (FFF) architecture, a log-time alternative to feedforward networks. FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution."
    },
    {
      "title": "Exponentially Faster Language Modeling",
      "url": "/research/2023/11/21/Exponentially-Faster-Language-Modeling.html",
      "date": "November 21, 2023",
      "summary": "UltraFastBERT, a BERT variant, operates with just 0.3% of its neurons—selectively using 12 out of 4095 neurons per layer—for inference, matching the performance of similar models. It replaces conventional feedforward networks with fast feedforward networks (FFFs) to achieve this efficiency. Although fully efficient conditional neural execution isn't yet practical, we offer a high-level CPU code that achieves a 78x speedup and a PyTorch implementation with a 40x speedup over standard batched feedforward inference."
    }
  ],
  "log": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/research/2023/09/18/Fast-Feedforward-Networks.html",
      "date": "September 18, 2023",
      "summary": "This paper introduces the fast feedforward (FFF) architecture, a log-time alternative to feedforward networks. FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution."
    },
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    }
  ],
  "x": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/research/2023/09/18/Fast-Feedforward-Networks.html",
      "date": "September 18, 2023",
      "summary": "This paper introduces the fast feedforward (FFF) architecture, a log-time alternative to feedforward networks. FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution."
    },
    {
      "title": "SinLU: Sinu-Sigmoidal Linear Uni",
      "url": "/research/2021/12/02/SinLU-Sinu-Sigmoidal-Linear-Uni.html",
      "date": "December 2, 2021",
      "summary": "This paper introduces a non-linear activation function called the Sinu-sigmoidal Linear Unit (SinLU), formulated as SinLU(x) = (x + a sin bx) · σ(x). It incorporates a sine wave for added functionality over traditional linear units, with two trainable parameters controlling the sinusoidal participation. The authors compare SinLU's performance to ReLU, GELU, and SiLU across various domains, models, and standard datasets. The results demonstrate SinLU's robustness and superior performance due to the incorporation of the trainable sine wave parameters, facilitating easy training and fast convergence."
    },
    {
      "title": "Searching for Activation Functions",
      "url": "/research/2017/10/27/Searching-for-Activation-Functions.html",
      "date": "October 27, 2017",
      "summary": "This study explores the impact of activation functions on deep network training and performance. While Rectified Linear Unit (ReLU) is widely used, alternatives have not consistently outperformed it. We propose using automatic search techniques to discover new activation functions. Through exhaustive and reinforcement learning-based searches, we identify novel functions. Empirical evaluation shows that our best discovered function, Swish (f(x) = x · sigmoid(βx)), performs better than ReLU on deeper models across challenging datasets. Replacing ReLUs with Swish units improves classification accuracy on ImageNet, for example, by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. Swish's simplicity and similarity to ReLU facilitate its adoption in neural networks."
    },
    {
      "title": "Exponentially Faster Language Modeling",
      "url": "/research/2023/11/21/Exponentially-Faster-Language-Modeling.html",
      "date": "November 21, 2023",
      "summary": "UltraFastBERT, a BERT variant, operates with just 0.3% of its neurons—selectively using 12 out of 4095 neurons per layer—for inference, matching the performance of similar models. It replaces conventional feedforward networks with fast feedforward networks (FFFs) to achieve this efficiency. Although fully efficient conditional neural execution isn't yet practical, we offer a high-level CPU code that achieves a 78x speedup and a PyTorch implementation with a 40x speedup over standard batched feedforward inference."
    }
  ],
  "mixture": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/research/2023/09/18/Fast-Feedforward-Networks.html",
      "date": "September 18, 2023",
      "summary": "This paper introduces the fast feedforward (FFF) architecture, a log-time alternative to feedforward networks. FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution."
    },
    {
      "title": "MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts",
      "url": "/research/2024/02/26/MoE-Mamba-Efficient-Selective-State-Space-Models-with-Mixture-of-Experts.html",
      "date": "February 26, 2024",
      "summary": "State Space Models (SSMs) are emerging as strong competitors in sequential modeling, challenging Transformers. Integrating Mixture of Experts (MoE) has enhanced Transformer-based models, including cutting-edge open models. We suggest combining SSMs with MoE to further unlock their scaling potential. Our model, MoE-Mamba, based on the SSM model Mamba, exceeds the performance of both Mamba and traditional Transformer-MoE models. Notably, MoE-Mamba achieves Mamba's performance with 2.35 times fewer training steps, maintaining Mamba's inference advantages over Transformers."
    }
  ],
  "expert": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/research/2023/09/18/Fast-Feedforward-Networks.html",
      "date": "September 18, 2023",
      "summary": "This paper introduces the fast feedforward (FFF) architecture, a log-time alternative to feedforward networks. FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution."
    },
    {
      "title": "MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts",
      "url": "/research/2024/02/26/MoE-Mamba-Efficient-Selective-State-Space-Models-with-Mixture-of-Experts.html",
      "date": "February 26, 2024",
      "summary": "State Space Models (SSMs) are emerging as strong competitors in sequential modeling, challenging Transformers. Integrating Mixture of Experts (MoE) has enhanced Transformer-based models, including cutting-edge open models. We suggest combining SSMs with MoE to further unlock their scaling potential. Our model, MoE-Mamba, based on the SSM model Mamba, exceeds the performance of both Mamba and traditional Transformer-MoE models. Notably, MoE-Mamba achieves Mamba's performance with 2.35 times fewer training steps, maintaining Mamba's inference advantages over Transformers."
    }
  ],
  "thank": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/research/2023/09/18/Fast-Feedforward-Networks.html",
      "date": "September 18, 2023",
      "summary": "This paper introduces the fast feedforward (FFF) architecture, a log-time alternative to feedforward networks. FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution."
    },
    {
      "title": "VideoMamba: State Space Model for Efficient Video Understanding",
      "url": "/research/2024/03/12/VideoMamba-State-Space-Model-for-Efficient-Video-Understanding.html",
      "date": "March 12, 2024",
      "summary": "This work introduces VideoMamba, an innovative approach that addresses local redundancy and global dependencies in video understanding by adapting the Mamba framework to video analysis. VideoMamba outperforms existing 3D convolutional neural networks and video transformers through its linear-complexity operator, which facilitates efficient long-term modeling for high-resolution, long-duration videos. Its effectiveness is demonstrated across four main areas: scalability in the visual domain without needing extensive dataset pretraining, thanks to a novel self-distillation technique; the ability to recognize short-term actions with fine-grained motion differences; superior performance in long-term video understanding compared to traditional feature-based models; and robust compatibility with multiple modalities, enhancing multi-modal video analysis. VideoMamba establishes a new standard for comprehensive and efficient video understanding."
    }
  ],
  "argue": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "The Batch Normalization paper argued the method's effectiveness came from reducing internal covariate shift. In this paper the authors argue that actually the main benifit of Batch Norm is loss landscape smoothing."
    },
    {
      "title": "Dropout as Data Augmentation",
      "url": "/research/2016/01/08/DROPOUT-AS-DATA-AUGMENTATION.html",
      "date": "January 8, 2016",
      "summary": "Dropout is typically interpreted as turning the test time network into an ensemble of the thinner training networks. This paper argues that dropout can also be interpreted as a kind of data augmentation in image space. Authors present an approach to project the dropout noise within a network back into the input space, visualising the augmented versions of the training data, and show that training a deterministic network on the augmented samples yields similar results. Authors then propose a new dropout noise scheme and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/ON-INTERACTION-BETWEEN-AUGMENTATIONS-AND-CORRUPTIONS-IN-NATURAL-CORRUPTION-ROBUSTNESS.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, the correlation between data augmentations and test-time corruptions remains unclear. This paper introduces Minimal Sample Distance, and demonstrates a strong correlation between augmentation-corruption similarity and performance. Authors argue that training with augmentations that are perceptually similar to corruptions enhances test error."
    },
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets transformers as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet."
    },
    {
      "title": "The Platonic Representation Hypothesis",
      "url": "/research/2024/05/13/The-Platonic-Representation-Hypothesis.html",
      "date": "May 13, 2024",
      "summary": "This paper argues that AI model representations, especially in deep networks, are converging. It surveys examples in the literature showing how neural network representations align over time and across domains. It demonstrates convergence across data modalities, noting that as vision and language models grow, they measure distances between data points similarly. The authors hypothesize that this convergence leads to a shared statistical model of reality, termed the platonic representation and discuss possible selective pressures toward it. The paper also addresses the implications, limitations, and counterexamples to this trend."
    }
  ],
  "come": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "The Batch Normalization paper argued the method's effectiveness came from reducing internal covariate shift. In this paper the authors argue that actually the main benifit of Batch Norm is loss landscape smoothing."
    }
  ],
  "covariate": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "The Batch Normalization paper argued the method's effectiveness came from reducing internal covariate shift. In this paper the authors argue that actually the main benifit of Batch Norm is loss landscape smoothing."
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "This paper introduces Batch Normalization, an online normalization technique which calculates mean and std across the batch dimension. Batch Norm improves training efficiency, allowing for higher learning rates, and decreasing hyper-parameter sensitivity, and sometimes removing the need for Dropout regularization. This paper suggests Batch Norm improves internal covariant shift, however later works have questioned this."
    },
    {
      "title": "Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks",
      "url": "/research/2017/10/23/Cosine-Normalization-Using-Cosine-Similarity-Instead-of-Dot-Product-in-Neural-Networks.html",
      "date": "October 23, 2017",
      "summary": "In traditional multi-layer neural networks, the dot product between the output and weight vectors of preceding layers serves as input to the activation function, resulting in unbounded outputs and increased variance. This variance can lead to poor generalization and hinder training by exacerbating internal covariate shift. To address this, we propose cosine normalization, which replaces the dot product with cosine similarity or centered cosine similarity (Pearson Correlation Coefficient). We evaluate cosine normalization against batch, weight, and layer normalization in fully-connected and convolutional neural networks across various datasets. Our experiments demonstrate that cosine normalization outperforms other normalization techniques."
    }
  ],
  "benifit": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "The Batch Normalization paper argued the method's effectiveness came from reducing internal covariate shift. In this paper the authors argue that actually the main benifit of Batch Norm is loss landscape smoothing."
    }
  ],
  "landscape": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "The Batch Normalization paper argued the method's effectiveness came from reducing internal covariate shift. In this paper the authors argue that actually the main benifit of Batch Norm is loss landscape smoothing."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "Taming the Waves: Sine as Activation Function in Deep Neural Networks",
      "url": "/research/2016/11/04/Taming-the-Waves-Sine-as-Activation-Function-in-Deep-Neural-Networks.html",
      "date": "November 4, 2016",
      "summary": "This paper explores the challenges in training deep neural networks that use sinusoidal activation functions. The authors explain that the difficulty arises from the emergence of numerous shallow local minima in the loss landscape. The study reveals that successful learning in typical classification tasks occurs when the network effectively ignores the periodic cycles of the sinusoidal functions. However, for certain non-trivial tasks, networks with sinusoidal activations can outperform those using traditional monotonic activation functions."
    },
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    },
    {
      "title": "Optimal learning rate schedules in high-dimensional non-convex optimization problems",
      "url": "/research/2022/02/09/Optimal-learning-rate-schedules-in-high-dimensional-non-convex-optimization-problems.html",
      "date": "February 9, 2022",
      "summary": "This paper explores the role of learning rate schedules in high-dimensional and non-convex optimization problems, focusing on Langevin optimization with a decaying learning rate. Analyzing models with Gaussian random functions on N-dimensional spheres, the study reveals that to accelerate optimization without getting trapped in saddles, a decay rate β < 1 is optimal, contrary to convex settings where β = 1 is preferred. Introducing a signal recovery component, the dynamics involve an exploration phase navigating through rough landscape parts and a convergence phase entering convex basins. It's found optimal to maintain a large learning rate during exploration to swiftly exit non-convex regions, then transition to β = 1 for rapid convergence to the solution. These findings are validated in a neural network regression task."
    },
    {
      "title": "Sharpness-Aware Training for Free",
      "url": "/research/2023/03/02/Sharpness-Aware-Training-for-Free.html",
      "date": "March 2, 2023",
      "summary": "Modern deep neural networks (DNNs) excel in performance but often suffer from over-parameterization, leading to increased generalization error without tailored training strategies. Sharpness-Aware Minimization (SAM) has proven effective in reducing generalization error by minimizing sharpness in the loss landscape. However, SAM incurs a significant computational overhead. This paper introduces Sharpness-Aware Training for Free (SAF), which mitigates sharpness at nearly zero additional computational cost. SAF achieves this by preventing sudden drops in loss within sharp local minima during weight updates. A novel trajectory loss, based on KL-divergence between current and past DNN outputs, replaces SAM's sharpness measure, guiding convergence towards flat minima for enhanced generalization. Empirical results demonstrate SAF's effectiveness on ImageNet with comparable computational efficiency to the base optimizer."
    },
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    },
    {
      "title": "A Phase Transition between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention",
      "url": "/research/2024/02/06/A-Phase-Transition-between-Positional-and-Semantic-Learning-in-a-Solvable-Model-of-Dot-Product-Attention.html",
      "date": "February 6, 2024",
      "summary": "We explore the learning process of a dot-product attention layer, which learns both positional and semantic attention matrices, enabling tokens to attend based on position or meaning. Through experiments on an algorithmic task, we demonstrate that this architecture can use either mechanism for solving the task. Theoretically, we examine a non-linear self-attention layer with special query and key matrices, offering a closed-form solution for its non-convex loss landscape in high-dimensional data, which reveals a phase transition from positional to semantic mechanisms as sample complexity increases. We also show that the dot-product attention layer surpasses a linear positional baseline through the semantic mechanism with adequate data."
    },
    {
      "title": "The loss landscape of deep linear neural networks: a second-order analysis",
      "url": "/research/2022/03/11/The-loss-landscape-of-deep-linear-neural-networks-a-second-order-analysis.html",
      "date": "March 11, 2022",
      "summary": "This study investigates the optimization landscape of deep linear neural networks with square loss, focusing on the role of non-strict saddle points in algorithm dynamics, which has been previously understudied. the authors conduct a comprehensive second-order analysis, identifying global minimizers, strict, and non-strict saddle points among all critical points, alongside their critical values. Our findings, based on conditions related to the ranks of partial matrix products, contribute insights into global convergence and implicit regularization observed in optimizing linear neural networks. Additionally, the authors offer an explicit parameterization of global minimizers and identify extensive sets of strict and non-strict saddle points."
    },
    {
      "title": "Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes",
      "url": "/research/2017/11/28/Towards-Understanding-Generalization-of-Deep-Learning-Perspective-of-Loss-Landscapes.html",
      "date": "November 28, 2017",
      "summary": "This study examines why deep learning models, despite having more parameters than training samples, generalize well. It highlights the importance of the loss function's landscape, showing that areas leading to better generalization (good minima) are more prevalent than those leading to poor outcomes. This predominance facilitates the convergence of optimization methods to these beneficial minima. The research provides theoretical backing by analyzing 2-layer neural networks, noting that solutions with better generalization have a smaller Hessian matrix norm. For deeper networks, extensive numerical data supports these conclusions."
    }
  ],
  "hippo": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "State Space Models (SSM), despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. To improve SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing."
    }
  ],
  "modeling": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "State Space Models (SSM), despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. To improve SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "This paper introduces the Mamba archetecture, a selective state space model (SSSM) with input dependance. Mamba has better speed and scalability compared to transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences. Authors also present a hardware efficient scanning mechanism that is used in Mamba."
    },
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/research/2024/01/24/MambaByte-Token-free-Selective-State-Space-Model.html",
      "date": "January 24, 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. The Authors show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements."
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, enhances vision tasks using Vision transformers by masking and reconstructing parts of an image. The compatibility of MIM with CNNs and its operational principle are unclear. this study reveals that MIM improves generalized feature extraction through middle-order patch interactions and introduces Architecture-Agnostic Masked Image Modeling (A2MIM), compatible with both Transformers and CNNs. Extensive testing demonstrates A2MIM's ability to enhance representation learning and transferability to various tasks without specialized modifications."
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM) for self-supervised visual representation in vision transformers ViT. Five different learning objectives were proposed, which degraded the input image in various ways similar to masking. Design principles are proposed for  token-based pre-training of vision transformers. The most effective strategy combined preserving the original image style with spatial misalignment and masking. This approach outperformed traditional MIM on downstream recognition tasks without increasing computational demands."
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "This study introduces a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. The method involves creating spaces for network design that parameterize multiple network populations. A low-dimensional, efficient design space called RegNet is developed, based on modeling network widths and depths through a quantized linear function. The analysis of the RegNet space challenges existing design practices, offering simpler and faster networks effective across various computational budgets. RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs under similar conditions."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. In this paper Masked Image Residual Learning (MIRL), a self-supervised learning framework, was introduced to alleviate the degradation issue and enable effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Extensive testing shows that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper investigates how self-supervised pretraining methods learn part-aware representations. The authors describe contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting that these methods predispose encoders to recognize object parts. Through empirical comparison, they find that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "This paper proposes Hard Patches Mining (HPM), a novel framework for masked image modeling (MIM) pre-training that goes beyond simply solving given problems. HPM aims for the model to generate more challenging tasks for itself, using reconstruction loss as a metric for task difficulty. It incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "This paper introduces Mask-Reconstruct Augmentation (MRA), an image augmentation technique that leverages self-supervised masked autoencoders to generate distorted inputs and address the issue of overfitting in deep neural networks. Inspired by the success of masked image modeling in self-supervised learning, MRA uses nonlinear transformations for regularization, as opposed to current hand-crafted linear techniques like scale, flip, and color jitter. Extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "This paper introduces Online Normalization, a novel method for normalizing neural network hidden activations that offers a batch-independent alternative with comparable accuracy to Batch Normalization. Online Normalization addresses Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. The method is applicable to recurrent, fully connected networks, and those with high activation memory requirements. The authors demonstrate its effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    },
    {
      "title": "Language Modeling with Gated Convolutional Networks",
      "url": "/research/2017/09/08/Language-Modeling-with-Gated-Convolutional-Networks.html",
      "date": "September 8, 2017",
      "summary": "This paper introduces a novel language modeling approach using stacked convolutions that enable efficient parallel processing, in contrast to the traditionally used recurrent neural networks known for handling unbounded context. The proposed method features a simplified gating mechanism that outperforms previous models and demonstrates superior performance on the WikiText-103 benchmark, showing its capability to manage long-term dependencies. It also delivers competitive results on the Google Billion Words benchmark and significantly reduces sentence scoring latency compared to recurrent models. This marks the first instance of a non-recurrent model achieving comparable success to strong recurrent models on major language tasks."
    },
    {
      "title": "Exponentially Faster Language Modeling",
      "url": "/research/2023/11/21/Exponentially-Faster-Language-Modeling.html",
      "date": "November 21, 2023",
      "summary": "UltraFastBERT, a BERT variant, operates with just 0.3% of its neurons—selectively using 12 out of 4095 neurons per layer—for inference, matching the performance of similar models. It replaces conventional feedforward networks with fast feedforward networks (FFFs) to achieve this efficiency. Although fully efficient conditional neural execution isn't yet practical, we offer a high-level CPU code that achieves a 78x speedup and a PyTorch implementation with a 40x speedup over standard batched feedforward inference."
    },
    {
      "title": "Learning and Leveraging World Models in Visual Representation Learning",
      "url": "/research/2024/03/01/Learning-and-Leveraging-World-Models-in-Visual-Representation-Learning.html",
      "date": "March 1, 2024",
      "summary": "The Joint-Embedding Predictive Architecture (JEPA) is a self-supervised method previously used for predicting missing input parts. This study expands JEPA to predict a wider range of corruptions by introducing Image World Models (IWM), which learn the effects of global photometric transformations in latent space. Key to effective IWM learning are conditioning, prediction difficulty, and model capacity. The study demonstrates that IWM's predictive world model, when fine-tuned, can tackle diverse tasks and either matches or outperforms existing self-supervised techniques. Additionally, it enables control over the abstraction level of learned representations, achieving either invariant or equivariant representations, similar to contrastive methods or masked image modeling, respectively."
    }
  ],
  "ssm": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "State Space Models (SSM), despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. To improve SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing."
    },
    {
      "title": "MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts",
      "url": "/research/2024/02/26/MoE-Mamba-Efficient-Selective-State-Space-Models-with-Mixture-of-Experts.html",
      "date": "February 26, 2024",
      "summary": "State Space Models (SSMs) are emerging as strong competitors in sequential modeling, challenging Transformers. Integrating Mixture of Experts (MoE) has enhanced Transformer-based models, including cutting-edge open models. We suggest combining SSMs with MoE to further unlock their scaling potential. Our model, MoE-Mamba, based on the SSM model Mamba, exceeds the performance of both Mamba and traditional Transformer-MoE models. Notably, MoE-Mamba achieves Mamba's performance with 2.35 times fewer training steps, maintaining Mamba's inference advantages over Transformers."
    }
  ],
  "length": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "State Space Models (SSM), despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. To improve SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing."
    },
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "This paper introduces an algorithm that identifies simple and highly generalizable neural networks by searching for extensive flat minima regions in the error function, where the error rate is relatively stable. Such flat minima are associated with lower overfitting risks based on minimum description length (MDL) principles. Despite requiring second-order derivative calculations, the algorithm has a complexity level comparable to backpropagation. When tested on feedforward and recurrent networks, as well as stock market prediction tasks, this algorithm outperformed traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "The paper proposes three normalization-centric modifications to improve Transformer training: PRENORM which introduces pre-norm residual connections and smaller initializations, enabling warmup-free, validation-based training with large learning rates; SCALENORM which suggests L2 normalization with a single scale parameter for faster training and better performance; and FIXNORM which reaffirms the efficacy of normalizing word embeddings to a fixed length."
    },
    {
      "title": "FNet: Mixing Tokens with Fourier Transforms",
      "url": "/research/2022/05/26/FNet-Mixing-Tokens-with-Fourier-Transforms.html",
      "date": "May 26, 2022",
      "summary": "We demonstrate that Transformer encoder architectures can be accelerated with minimal impact on accuracy by substituting self-attention sublayers with simple linear transformations. Remarkably, using a standard Fourier Transform instead of the self-attention sublayer in a Transformer encoder achieves 92-97% of BERT models' accuracy on the GLUE benchmark, while training 80% faster on GPUs and 70% faster on TPUs for standard 512 input lengths. Our FNet model significantly outperforms in speed at longer input lengths, matching the accuracy of the most accurate models on the Long Range Arena benchmark and surpassing the fastest models in speed across all sequence lengths on GPUs and shorter lengths on TPUs. Additionally, FNet is lightweight in memory use and exceptionally efficient in smaller sizes, outperforming Transformer models under the same speed and accuracy constraints."
    }
  ],
  "underperform": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "State Space Models (SSM), despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. To improve SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing."
    },
    {
      "title": "Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding",
      "url": "/research/2021/11/16/Improved-Robustness-of-Vision-Transformer-via-PreLayerNorm-in-Patch-Embedding.html",
      "date": "November 16, 2021",
      "summary": "Recent advancements in vision transformers (ViTs) have shown superior performance across diverse visual tasks, surpassing convolutional neural networks (CNNs). Given ViT's distinct architecture, understanding its behavior and reliability is imperative. This paper investigates ViT's robustness by comparing it with CNNs under various image corruptions relevant to real-world vision tasks. While ViT generally exhibits comparable or improved robustness over CNNs, it consistently underperforms in contrast enhancement tasks. Analysis suggests that positional embedding in ViT's patch embedding may malfunction with color scale changes. We propose PreLayerNorm, a modified patch embedding structure, to address this issue and ensure scale-invariant behavior in ViT. ViT with PreLayerNorm demonstrates enhanced robustness across various corruptions, particularly in contrast-varying environments."
    }
  ],
  "suffer": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "State Space Models (SSM), despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. To improve SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "Sharpness-Aware Training for Free",
      "url": "/research/2023/03/02/Sharpness-Aware-Training-for-Free.html",
      "date": "March 2, 2023",
      "summary": "Modern deep neural networks (DNNs) excel in performance but often suffer from over-parameterization, leading to increased generalization error without tailored training strategies. Sharpness-Aware Minimization (SAM) has proven effective in reducing generalization error by minimizing sharpness in the loss landscape. However, SAM incurs a significant computational overhead. This paper introduces Sharpness-Aware Training for Free (SAF), which mitigates sharpness at nearly zero additional computational cost. SAF achieves this by preventing sudden drops in loss within sharp local minima during weight updates. A novel trajectory loss, based on KL-divergence between current and past DNN outputs, replaces SAM's sharpness measure, guiding convergence towards flat minima for enhanced generalization. Empirical results demonstrate SAF's effectiveness on ImageNet with comparable computational efficiency to the base optimizer."
    },
    {
      "title": "ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Network",
      "url": "/research/2021/06/29/ASAM-Adaptive-Sharpness-Aware-Minimization-for-Scale-Invariant-Learning-of-Deep-Neural-Network.html",
      "date": "June 29, 2021",
      "summary": "Recent advances in learning algorithms have highlighted the importance of the sharpness of the loss surface as a key indicator of the generalization gap, achieving state-of-the-art results. However, traditional sharpness measures, defined within a fixed region, suffer from sensitivity to parameter scaling, weakening their predictive value for the generalization gap. This paper introduces a scale-invariant measure, adaptive sharpness, along with a new generalization bound. We present a new learning method, adaptive sharpness-aware minimization (ASAM), which leverages this bound. Our experiments across various benchmark datasets demonstrate that ASAM significantly enhances model generalization."
    }
  ],
  "hardware": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "State Space Models (SSM), despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. To improve SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "This paper introduces the Mamba archetecture, a selective state space model (SSSM) with input dependance. Mamba has better speed and scalability compared to transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences. Authors also present a hardware efficient scanning mechanism that is used in Mamba."
    },
    {
      "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
      "url": "/research/2024/02/27/The-Era-of-1-bit-LLMs-All-Large-Language-Models-are-in-1.58-Bits.html",
      "date": "February 27, 2024",
      "summary": "Recent research introduces BitNet b1.58, a ternary (values of -1, 0, 1) 1-bit Large Language Model (LLM) that matches traditional full-precision LLMs in performance and perplexity but is significantly more efficient in terms of latency, memory, throughput, and energy. This work establishes a new scaling law and training approach for future high-performance, cost-effective LLMs, while also facilitating the design of specialized hardware optimized for 1-bit LLMs."
    }
  ],
  "utilization": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "State Space Models (SSM), despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. To improve SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "This paper introduces a novel approach called mixed-order self-paced curriculum learning (Mo-SCL) to address the challenges faced by self-paced curriculum learning (SCL) in medical image analysis tasks, such as universal lesion detection. These challenges include inaccurate difficulty estimation and the under-utilization of hard samples. Mo-SCL combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches. Through theoretical analysis and experiments on the DeepLesion dataset, the authors demonstrate that Mo-SCL enhances lesion detection accuracy in state-of-the-art methods without requiring additional network modifications."
    }
  ],
  "layer": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "State Space Models (SSM), despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. To improve SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing."
    },
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/research/2019/07/26/DropAttention-A-Regularization-Method-for-Fully-Connected-Self-Attention-Networks.html",
      "date": "July 26, 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. DropAttention uses a mask to zero out elements in the attention matrix. Experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    },
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a method for dropout in Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers."
    },
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activitivations can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases."
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, this paper introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, the authors enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. This simple method surpasses the current state-of-the-art in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "This paper introduces Xavier Glorot init for Sigmoid like activation functions. The study investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial."
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This study suggest that label smoothing not only boosts generalization but also enhances model calibration. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. Label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study looks at self-supervised pre-training methods on small scale or lightweight Vision Transformers (ViTs). Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods."
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. In this paper Masked Image Residual Learning (MIRL), a self-supervised learning framework, was introduced to alleviate the degradation issue and enable effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Extensive testing shows that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "This paper investigates the geometric and statistical properties of representations across layers in large transformers used for self-supervised learning on various data types. The authors observe common evolution patterns, with data manifolds expanding initially, contracting at intermediate layers, and the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. They find that semantic information peaks after initial expansion, a trend consistent across models and datasets. The study proposes an unsupervised method to identify layers richest in semantic content, suggesting that those at a relative ID minimum are optimal for downstream tasks."
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel normalization technique for self-supervised transformer training that normalizes the [CLS] token and regular tokens separately. The authors find that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks compared to previous models like masked autoencoders (MAE) that use a single normalization layer for all tokens."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "This research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. The authors validate these approaches using the ImageNet-1k dataset and confirm their findings with the ImageNet-v2 test set, evaluating transfer performance across six additional datasets."
    },
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "This paper introduces a Fourier neural network (FNN) that aligns with Fourier decomposition by utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design allows for easy integration with more complex networks for data processing tasks. The authors demonstrate the FNN's efficacy on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits of the FNN include solution validity beyond the training scope, model interpretability, and ease of use."
    },
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "This paper investigates the importance of the warm-up stage in training transformers, and the impact of layer normalization placement. Using mean field theory, the authors demonstrate that the original Post-LN Transformer's design results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    },
    {
      "title": "Deep Residual Learning for Image Recognition",
      "url": "/research/2015/12/10/Deep-Residual-Learning-for-Image-Recognition.html",
      "date": "December 10, 2015",
      "summary": "We introduce a residual learning framework to train deeper neural networks effectively. By reformulating layers to learn residual functions relative to inputs, our approach facilitates optimization and achieves higher accuracy with increased depth. Evaluations on ImageNet demonstrate the effectiveness of residual networks up to 152 layers deep, outperforming previous architectures. Our method achieves 3.57% error on the ImageNet test set, winning 1st place in the ILSVRC 2015 classification task. Additionally, our deep representations lead to a 28% improvement on the COCO object detection dataset. In the ILSVRC & COCO 2015 competitions, our approach secured 1st place in ImageNet detection, localization, COCO detection, and segmentation tasks."
    },
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    },
    {
      "title": "Root Mean Square Layer Normalization",
      "url": "/research/2019/10/16/Root-Mean-Square-Layer-Normalization.html",
      "date": "October 16, 2019",
      "summary": "Layer normalization (LayerNorm) enhances deep neural network stability and convergence by re-centering and re-scaling inputs and weight matrices. However, its computational overhead slows networks, particularly RNNs. We introduce RMSNorm, which replaces re-centering with root mean square (RMS) regularization. RMSNorm maintains re-scaling invariance and adapts learning rates implicitly, while being computationally simpler than LayerNorm. We also propose partial RMSNorm (pRMSNorm), estimating RMS from a subset of inputs. Empirical results across various tasks and architectures demonstrate that RMSNorm achieves comparable performance to LayerNorm while reducing running time by 7%∼64%."
    },
    {
      "title": "On Layer Normalization in the Transformer Architecture",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architecture.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is widely used in natural language processing. Training it typically requires a carefully designed learning rate warm-up stage, crucial for performance but slowing optimization and requiring more hyperparameter tuning. We provide theoretical insights into the necessity of the warm-up stage, demonstrating that the location of layer normalization affects gradient behavior. For Post-LN Transformers, with layer normalization between residual blocks, large gradients near the output layer at initialization make training unstable without warm-up. Conversely, for Pre-LN Transformers, with layer normalization inside residual blocks, gradients are well-behaved, suggesting the removal of the warm-up stage. Experimental results show that Pre-LN Transformers without warm-up achieve comparable performance with less training time and hyperparameter tuning."
    },
    {
      "title": "Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks",
      "url": "/research/2017/10/23/Cosine-Normalization-Using-Cosine-Similarity-Instead-of-Dot-Product-in-Neural-Networks.html",
      "date": "October 23, 2017",
      "summary": "In traditional multi-layer neural networks, the dot product between the output and weight vectors of preceding layers serves as input to the activation function, resulting in unbounded outputs and increased variance. This variance can lead to poor generalization and hinder training by exacerbating internal covariate shift. To address this, we propose cosine normalization, which replaces the dot product with cosine similarity or centered cosine similarity (Pearson Correlation Coefficient). We evaluate cosine normalization against batch, weight, and layer normalization in fully-connected and convolutional neural networks across various datasets. Our experiments demonstrate that cosine normalization outperforms other normalization techniques."
    },
    {
      "title": "DeepNet: Scaling Transformers to 1,000 Layers",
      "url": "/research/2022/03/01/DeepNet-Scaling-Transformers-to-1,000-Layers.html",
      "date": "March 1, 2022",
      "summary": "This paper introduces DEEPNORM, a novel method to stabilize extremely deep Transformers through a new normalization function and a theoretically derived initialization for the residual connection. DEEPNORM offers a stable and efficient training regime, blending the benefits of Post-LN's performance with Pre-LN's stability, allowing for the scaling of Transformers up to 1,000 layers without difficulty. This represents a significant advancement over previous models, with a 200-layer, 3.2B parameter model outperforming a 48-layer, 12B parameter state-of-the-art model by 5 BLEU points on a multilingual benchmark with 7,482 translation directions, suggesting a promising direction for scaling."
    },
    {
      "title": "Do Vision Transformers See Like Convolutional Neural Networks?",
      "url": "/research/2022/03/03/Do-Vision-Transformers-See-Like-Convolutional-Neural-Networks.html",
      "date": "March 3, 2022",
      "summary": "Convolutional Neural Networks (CNNs) have been prominent in processing visual data, but recent studies show Vision Transformers (ViTs) can match or outperform CNNs in image classification. This work examines how ViTs solve classification tasks, discovering significant differences from CNNs, including uniformity in ViT representations across layers due to self-attention and residual connections that enhance feature propagation. ViTs also maintain spatial information effectively, influenced by classification methods. The study further explores how dataset scale affects ViT features and their transferability, linking these findings to new architectures like MLP-Mixer."
    },
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    },
    {
      "title": "A Phase Transition between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention",
      "url": "/research/2024/02/06/A-Phase-Transition-between-Positional-and-Semantic-Learning-in-a-Solvable-Model-of-Dot-Product-Attention.html",
      "date": "February 6, 2024",
      "summary": "We explore the learning process of a dot-product attention layer, which learns both positional and semantic attention matrices, enabling tokens to attend based on position or meaning. Through experiments on an algorithmic task, we demonstrate that this architecture can use either mechanism for solving the task. Theoretically, we examine a non-linear self-attention layer with special query and key matrices, offering a closed-form solution for its non-convex loss landscape in high-dimensional data, which reveals a phase transition from positional to semantic mechanisms as sample complexity increases. We also show that the dot-product attention layer surpasses a linear positional baseline through the semantic mechanism with adequate data."
    },
    {
      "title": "Exponentially Faster Language Modeling",
      "url": "/research/2023/11/21/Exponentially-Faster-Language-Modeling.html",
      "date": "November 21, 2023",
      "summary": "UltraFastBERT, a BERT variant, operates with just 0.3% of its neurons—selectively using 12 out of 4095 neurons per layer—for inference, matching the performance of similar models. It replaces conventional feedforward networks with fast feedforward networks (FFFs) to achieve this efficiency. Although fully efficient conditional neural execution isn't yet practical, we offer a high-level CPU code that achieves a 78x speedup and a PyTorch implementation with a 40x speedup over standard batched feedforward inference."
    },
    {
      "title": "On the Relationship Between Self-Attention And Convolutional Layers",
      "url": "/research/2020/01/10/On-the-Relationship-Between-Self-Attention-And-Convolutional-Layers.html",
      "date": "January 10, 2020",
      "summary": "Recent studies have shown that attention mechanisms can rival or even outperform convolutional layers in vision tasks, challenging their dominance. Ramachandran et al. (2019) demonstrated that attention could entirely replace convolution, achieving top results. This work investigates whether attention layers function like convolutional layers, finding that multi-head self-attention layers, with enough heads, match the expressiveness of convolutional layers. Numerical experiments confirm that self-attention layers learn to focus on pixel-grid patterns akin to convolutional layers, supporting our findings."
    },
    {
      "title": "Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes",
      "url": "/research/2017/11/28/Towards-Understanding-Generalization-of-Deep-Learning-Perspective-of-Loss-Landscapes.html",
      "date": "November 28, 2017",
      "summary": "This study examines why deep learning models, despite having more parameters than training samples, generalize well. It highlights the importance of the loss function's landscape, showing that areas leading to better generalization (good minima) are more prevalent than those leading to poor outcomes. This predominance facilitates the convergence of optimization methods to these beneficial minima. The research provides theoretical backing by analyzing 2-layer neural networks, noting that solutions with better generalization have a smaller Hessian matrix norm. For deeper networks, extensive numerical data supports these conclusions."
    },
    {
      "title": "Transformers with Learnable Activation Functions",
      "url": "/research/2023/02/14/Transformers-with-Learnable-Activation-Functions.html",
      "date": "February 14, 2023",
      "summary": "Activation functions significantly affect model performance by reducing data complexity, yet their selection in Transformer-based language models is often overlooked. This paper explores the impact of using rational activation functions (RAFs), which unlike fixed activation functions (FAFs), can learn optimal functions from data. Our experiments demonstrate that Transformer models with RAFs (RAFT) outperform those with FAFs (FAFT), achieving a 5.71 point higher score on the GLUE benchmark with only 100 training examples and a 2.05 point increase on SQuAD with full data. The varied shapes of learned RAFs across layers and tasks suggest a new method for analyzing and understanding large pre-trained language models."
    }
  ],
  "h3": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "State Space Models (SSM), despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. To improve SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing."
    }
  ],
  "recall": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "State Space Models (SSM), despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. To improve SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing."
    }
  ],
  "comparison": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "State Space Models (SSM), despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. To improve SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper investigates how self-supervised pretraining methods learn part-aware representations. The authors describe contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting that these methods predispose encoders to recognize object parts. Through empirical comparison, they find that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning",
      "url": "/research/2023/03/16/From-MNIST-to-ImageNet-and-Back:-Benchmarking-Continual-Curriculum-Learning.html",
      "date": "March 16, 2023",
      "summary": "Continual learning (CL) is a promising trend in machine learning, aiming to develop robust models and strategies for dynamic environments by incorporating new knowledge while retaining past knowledge. CL research is fragmented, with various protocols, tasks, datasets, and metrics, often not reflecting real-world complexity and tailored to specific strategies. This work addresses this gap by introducing two novel CL benchmarks with heterogeneous tasks from six image datasets, varying in complexity and quality. These benchmarks fairly evaluate state-of-the-art CL strategies in scenarios closer to real-world conditions. The benchmarks present tasks in both increasing and decreasing complexity, assessing models' ability to exploit task structure. The work emphasizes a rigorous, reproducible evaluation protocol for measuring generalization and memory retention in models. Experimental results show that popular CL strategies perform poorly on these benchmarks, exhibit high forgetting, and struggle with curriculum task ordering. These findings underscore the need for rigorous comparisons and the development of new CL strategies for complex scenarios."
    }
  ],
  "narrow": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "State Space Models (SSM), despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. To improve SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing."
    }
  ],
  "gap": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "State Space Models (SSM), despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. To improve SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing."
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, the authors demonstrate the close relationship between these two families. This analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "The paper introduces a system that trains deep neural networks for object detection using only synthetic images generated with domain randomization. This involves randomizing non-realistic simulator parameters like lighting and object textures, allowing the network to identify essential object features. Their findings show that networks can achieve impressive performance using just synthetic data, and further improve with fine-tuning on real data. This suggests the potential of using cost-effective synthetic data for training, avoiding the need to acquire vast amounts of real-world data or create detailed synthetic environments. They validate their method on car bounding box detection using the KITTI dataset."
    },
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    },
    {
      "title": "Eigencurve: Optimal Learning Rate Schedule For Sgd on Quadratic Objectives With Skewed Hessian Spectrums",
      "url": "/research/2022/06/14/Eigencurve-Optimal-Learning-Rate-Schedule-For-Sgd-on-Quadratic-Objectives-With-Skewed-Hessian-Spectrums.html",
      "date": "June 14, 2022",
      "summary": "Learning rate schedulers are essential in deep neural network training, but there's a gap between practical usage and theoretical understanding. This paper introduces Eigencurve, the first set of learning rate schedules achieving minimax optimal convergence rates for SGD on quadratic objectives with skewed eigenvalue distributions of the Hessian matrix. This condition is common in practice. Experimental results on CIFAR-10 image classification tasks demonstrate Eigencurve's superiority over step decay, particularly with fewer epochs. The theory inspires practical schedulers approximating Eigencurve, resembling cosine decay for some problems while outperforming it in others."
    },
    {
      "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
      "url": "/research/2022/03/19/Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training.html",
      "date": "March 19, 2022",
      "summary": "Sharpness-Aware Minimization (SAM) enhances generalization by optimizing a neighborhood-based perturbed loss, but it doesn't always favor flat minima due to both sharp and flat minima having low perturbed losses. We introduce a new measure, the surrogate gap, reflecting the dominant Hessian eigenvalue at small neighborhood radii, which is simple to compute and can be minimized during training. We propose the Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), an advancement over SAM with minimal additional computational cost. GSAM employs a dual-step approach: first, minimizing the perturbed loss similar to SAM, and second, reducing the surrogate gap without affecting the perturbed loss to target regions with low loss and sharpness, thus achieving superior generalization. GSAM is theoretically robust, showing better convergence and empirical generalization improvements, notably a +3.2% gain over SAM and +5.4% over AdamW in ImageNet accuracy for ViT-B/32."
    },
    {
      "title": "ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Network",
      "url": "/research/2021/06/29/ASAM-Adaptive-Sharpness-Aware-Minimization-for-Scale-Invariant-Learning-of-Deep-Neural-Network.html",
      "date": "June 29, 2021",
      "summary": "Recent advances in learning algorithms have highlighted the importance of the sharpness of the loss surface as a key indicator of the generalization gap, achieving state-of-the-art results. However, traditional sharpness measures, defined within a fixed region, suffer from sensitivity to parameter scaling, weakening their predictive value for the generalization gap. This paper introduces a scale-invariant measure, adaptive sharpness, along with a new generalization bound. We present a new learning method, adaptive sharpness-aware minimization (ASAM), which leverages this bound. Our experiments across various benchmark datasets demonstrate that ASAM significantly enhances model generalization."
    },
    {
      "title": "From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning",
      "url": "/research/2023/03/16/From-MNIST-to-ImageNet-and-Back:-Benchmarking-Continual-Curriculum-Learning.html",
      "date": "March 16, 2023",
      "summary": "Continual learning (CL) is a promising trend in machine learning, aiming to develop robust models and strategies for dynamic environments by incorporating new knowledge while retaining past knowledge. CL research is fragmented, with various protocols, tasks, datasets, and metrics, often not reflecting real-world complexity and tailored to specific strategies. This work addresses this gap by introducing two novel CL benchmarks with heterogeneous tasks from six image datasets, varying in complexity and quality. These benchmarks fairly evaluate state-of-the-art CL strategies in scenarios closer to real-world conditions. The benchmarks present tasks in both increasing and decreasing complexity, assessing models' ability to exploit task structure. The work emphasizes a rigorous, reproducible evaluation protocol for measuring generalization and memory retention in models. Experimental results show that popular CL strategies perform poorly on these benchmarks, exhibit high forgetting, and struggle with curriculum task ordering. These findings underscore the need for rigorous comparisons and the development of new CL strategies for complex scenarios."
    }
  ],
  "flashconv": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "State Space Models (SSM), despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. To improve SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing."
    }
  ],
  "speeds": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "State Space Models (SSM), despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. To improve SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "This paper introduces a novel approach to Radiance Field methods for novel-view synthesis that addresses the challenges of high visual quality, costly training, and real-time rendering of unbounded, complete scenes at 1080p resolution. The authors propose three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. The method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "ijepa": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. This self-supervised method uses multiple crops of an image. Given a context crop the network is trained to predict the embeddings of several target crops. When applied to Vision Transformers, I-JEPA acheived strong performance in various tasks like image classification and depth prediction, outperforming Masked Auto Encoding (MAE) in linear probing when controlling for compute."
    }
  ],
  "joint": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. This self-supervised method uses multiple crops of an image. Given a context crop the network is trained to predict the embeddings of several target crops. When applied to Vision Transformers, I-JEPA acheived strong performance in various tasks like image classification and depth prediction, outperforming Masked Auto Encoding (MAE) in linear probing when controlling for compute."
    },
    {
      "title": "Learning and Leveraging World Models in Visual Representation Learning",
      "url": "/research/2024/03/01/Learning-and-Leveraging-World-Models-in-Visual-Representation-Learning.html",
      "date": "March 1, 2024",
      "summary": "The Joint-Embedding Predictive Architecture (JEPA) is a self-supervised method previously used for predicting missing input parts. This study expands JEPA to predict a wider range of corruptions by introducing Image World Models (IWM), which learn the effects of global photometric transformations in latent space. Key to effective IWM learning are conditioning, prediction difficulty, and model capacity. The study demonstrates that IWM's predictive world model, when fine-tuned, can tackle diverse tasks and either matches or outperforms existing self-supervised techniques. Additionally, it enables control over the abstraction level of learned representations, achieving either invariant or equivariant representations, similar to contrastive methods or masked image modeling, respectively."
    }
  ],
  "embedding": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. This self-supervised method uses multiple crops of an image. Given a context crop the network is trained to predict the embeddings of several target crops. When applied to Vision Transformers, I-JEPA acheived strong performance in various tasks like image classification and depth prediction, outperforming Masked Auto Encoding (MAE) in linear probing when controlling for compute."
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel normalization technique for self-supervised transformer training that normalizes the [CLS] token and regular tokens separately. The authors find that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks compared to previous models like masked autoencoders (MAE) that use a single normalization layer for all tokens."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "The paper proposes three normalization-centric modifications to improve Transformer training: PRENORM which introduces pre-norm residual connections and smaller initializations, enabling warmup-free, validation-based training with large learning rates; SCALENORM which suggests L2 normalization with a single scale parameter for faster training and better performance; and FIXNORM which reaffirms the efficacy of normalizing word embeddings to a fixed length."
    },
    {
      "title": "Is Cosine-Similarity of Embeddings Really About Similarity?",
      "url": "/research/2024/03/08/Is-Cosine-Similarity-of-Embeddings-Really-About-Similarity.html",
      "date": "March 8, 2024",
      "summary": "Cosine similarity, often used to gauge semantic similarity between high-dimensional objects by comparing low-dimensional feature embeddings, can yield variable results compared to unnormalized dot products. We investigate this phenomenon by analyzing embeddings from regularized linear models, revealing that cosine similarity can produce arbitrary and even meaningless similarities. This applies not only to linear models but also to deep models due to the implicit effects of various regularizations. Consequently, we advise against relying solely on cosine similarity and suggest exploring alternative approaches."
    },
    {
      "title": "Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding",
      "url": "/research/2021/11/16/Improved-Robustness-of-Vision-Transformer-via-PreLayerNorm-in-Patch-Embedding.html",
      "date": "November 16, 2021",
      "summary": "Recent advancements in vision transformers (ViTs) have shown superior performance across diverse visual tasks, surpassing convolutional neural networks (CNNs). Given ViT's distinct architecture, understanding its behavior and reliability is imperative. This paper investigates ViT's robustness by comparing it with CNNs under various image corruptions relevant to real-world vision tasks. While ViT generally exhibits comparable or improved robustness over CNNs, it consistently underperforms in contrast enhancement tasks. Analysis suggests that positional embedding in ViT's patch embedding may malfunction with color scale changes. We propose PreLayerNorm, a modified patch embedding structure, to address this issue and ensure scale-invariant behavior in ViT. ViT with PreLayerNorm demonstrates enhanced robustness across various corruptions, particularly in contrast-varying environments."
    },
    {
      "title": "Learning and Leveraging World Models in Visual Representation Learning",
      "url": "/research/2024/03/01/Learning-and-Leveraging-World-Models-in-Visual-Representation-Learning.html",
      "date": "March 1, 2024",
      "summary": "The Joint-Embedding Predictive Architecture (JEPA) is a self-supervised method previously used for predicting missing input parts. This study expands JEPA to predict a wider range of corruptions by introducing Image World Models (IWM), which learn the effects of global photometric transformations in latent space. Key to effective IWM learning are conditioning, prediction difficulty, and model capacity. The study demonstrates that IWM's predictive world model, when fine-tuned, can tackle diverse tasks and either matches or outperforms existing self-supervised techniques. Additionally, it enables control over the abstraction level of learned representations, achieving either invariant or equivariant representations, similar to contrastive methods or masked image modeling, respectively."
    }
  ],
  "crop": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. This self-supervised method uses multiple crops of an image. Given a context crop the network is trained to predict the embeddings of several target crops. When applied to Vision Transformers, I-JEPA acheived strong performance in various tasks like image classification and depth prediction, outperforming Masked Auto Encoding (MAE) in linear probing when controlling for compute."
    }
  ],
  "acheived": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. This self-supervised method uses multiple crops of an image. Given a context crop the network is trained to predict the embeddings of several target crops. When applied to Vision Transformers, I-JEPA acheived strong performance in various tasks like image classification and depth prediction, outperforming Masked Auto Encoding (MAE) in linear probing when controlling for compute."
    }
  ],
  "outperforming": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. This self-supervised method uses multiple crops of an image. Given a context crop the network is trained to predict the embeddings of several target crops. When applied to Vision Transformers, I-JEPA acheived strong performance in various tasks like image classification and depth prediction, outperforming Masked Auto Encoding (MAE) in linear probing when controlling for compute."
    },
    {
      "title": "Deep Residual Learning for Image Recognition",
      "url": "/research/2015/12/10/Deep-Residual-Learning-for-Image-Recognition.html",
      "date": "December 10, 2015",
      "summary": "We introduce a residual learning framework to train deeper neural networks effectively. By reformulating layers to learn residual functions relative to inputs, our approach facilitates optimization and achieves higher accuracy with increased depth. Evaluations on ImageNet demonstrate the effectiveness of residual networks up to 152 layers deep, outperforming previous architectures. Our method achieves 3.57% error on the ImageNet test set, winning 1st place in the ILSVRC 2015 classification task. Additionally, our deep representations lead to a 28% improvement on the COCO object detection dataset. In the ILSVRC & COCO 2015 competitions, our approach secured 1st place in ImageNet detection, localization, COCO detection, and segmentation tasks."
    }
  ],
  "auto": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. This self-supervised method uses multiple crops of an image. Given a context crop the network is trained to predict the embeddings of several target crops. When applied to Vision Transformers, I-JEPA acheived strong performance in various tasks like image classification and depth prediction, outperforming Masked Auto Encoding (MAE) in linear probing when controlling for compute."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "Deep learning via Hessian-free optimization",
      "url": "/research/2010/06/21/Deep-learning-via-Hessian-free-optimization.html",
      "date": "June 21, 2010",
      "summary": "We present a novel 2nd-order optimization method inspired by the Hessian-free approach, applied to deep auto-encoders. Achieving superior results without pre-training compared to Hinton & Salakhutdinov (2006), our method is practical, user-friendly, scalable to large datasets, and versatile across various model classes. Additionally, we address the challenge of pathological curvature in deep learning, highlighting how our method effectively mitigates this issue."
    },
    {
      "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
      "url": "/research/2024/02/28/CL-MAE:-Curriculum-Learned-Masked-Autoencoders.html",
      "date": "February 28, 2024",
      "summary": "This paper proposes a curriculum learning approach that updates the masking strategy of Masked Auto Encoders (MAE) to progressively increase the complexity of the self-supervised reconstruction task. To achieve this, a novel learnable masking module is introduced, capable of generating masks of varying complexities, and integrated into masked autoencoders (MAE). This module is jointly trained with the MAE, adjusting its behavior from partner (optimizing the same reconstruction loss) to adversary (optimizing the opposite loss), with a smooth transition regulated by a factor multiplied with the reconstruction loss. This training procedure creates an easy-to-hard curriculum. The Curriculum-Learned Masked Autoencoder (CL-MAE) is trained on ImageNet and demonstrates superior representation learning compared to MAE. Empirical results on five downstream tasks confirm that curriculum learning can successfully self-supervise masked autoencoders."
    }
  ],
  "encode": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. This self-supervised method uses multiple crops of an image. Given a context crop the network is trained to predict the embeddings of several target crops. When applied to Vision Transformers, I-JEPA acheived strong performance in various tasks like image classification and depth prediction, outperforming Masked Auto Encoding (MAE) in linear probing when controlling for compute."
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel normalization technique for self-supervised transformer training that normalizes the [CLS] token and regular tokens separately. The authors find that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks compared to previous models like masked autoencoders (MAE) that use a single normalization layer for all tokens."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article provides an extensive review of vision transformer techniques applied specifically to human action recognition. These action transformers are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also examines optimizing spatio-temporal attention in transformers for longer sequences and different learning strategies like self-supervised and zero-shot learning with their respective loss functions. Additionally, it assesses progress in benchmark evaluation scores and discusses challenges and future directions in this research area."
    }
  ],
  "mae": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. This self-supervised method uses multiple crops of an image. Given a context crop the network is trained to predict the embeddings of several target crops. When applied to Vision Transformers, I-JEPA acheived strong performance in various tasks like image classification and depth prediction, outperforming Masked Auto Encoding (MAE) in linear probing when controlling for compute."
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. MAE involves masking random patches of an input image and reconstructing the missing pixels using an asymmetric encoder-decoder architecture and a lightweight decoder. Masking a significant portion of the input image creates a challenging yet informative self-supervisory task. These innovations allow for efficient training of large models, tripling training speed and enhancing accuracy. A vanilla ViT-Huge model reaches top accuracy (87.8%) on ImageNet-1K data among similar methods. The model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating promising scaling potential."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel normalization technique for self-supervised transformer training that normalizes the [CLS] token and regular tokens separately. The authors find that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks compared to previous models like masked autoencoders (MAE) that use a single normalization layer for all tokens."
    },
    {
      "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
      "url": "/research/2024/02/28/CL-MAE:-Curriculum-Learned-Masked-Autoencoders.html",
      "date": "February 28, 2024",
      "summary": "This paper proposes a curriculum learning approach that updates the masking strategy of Masked Auto Encoders (MAE) to progressively increase the complexity of the self-supervised reconstruction task. To achieve this, a novel learnable masking module is introduced, capable of generating masks of varying complexities, and integrated into masked autoencoders (MAE). This module is jointly trained with the MAE, adjusting its behavior from partner (optimizing the same reconstruction loss) to adversary (optimizing the opposite loss), with a smooth transition regulated by a factor multiplied with the reconstruction loss. This training procedure creates an easy-to-hard curriculum. The Curriculum-Learned Masked Autoencoder (CL-MAE) is trained on ImageNet and demonstrates superior representation learning compared to MAE. Empirical results on five downstream tasks confirm that curriculum learning can successfully self-supervise masked autoencoders."
    }
  ],
  "probe": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. This self-supervised method uses multiple crops of an image. Given a context crop the network is trained to predict the embeddings of several target crops. When applied to Vision Transformers, I-JEPA acheived strong performance in various tasks like image classification and depth prediction, outperforming Masked Auto Encoding (MAE) in linear probing when controlling for compute."
    }
  ],
  "compute": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. This self-supervised method uses multiple crops of an image. Given a context crop the network is trained to predict the embeddings of several target crops. When applied to Vision Transformers, I-JEPA acheived strong performance in various tasks like image classification and depth prediction, outperforming Masked Auto Encoding (MAE) in linear probing when controlling for compute."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "This paper explores the interactions between training data amount, model regularization or data augmentation (AugReg), model size, and compute budget for Vision Transformers (ViT) in various vision tasks. The authors find that using more compute and AugReg can achieve the same performance as training with significantly more data. They demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "This research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. The authors validate these approaches using the ImageNet-1k dataset and confirm their findings with the ImageNet-v2 test set, evaluating transfer performance across six additional datasets."
    },
    {
      "title": "A comprehensive review of Binary Neural Network",
      "url": "/research/2023/05/11/A-comprehensive-review-of-Binary-Neural-Network.html",
      "date": "May 11, 2023",
      "summary": "Deep learning (DL) is widely used in intelligent systems and real-life applications, but its deployment on computationally limited and energy-constrained devices requires efficient technologies like Binary Neural Networks (BNN). BNNs save significant storage, computation cost, and energy, making them ideal for small devices, despite the trade-offs in memory and performance. This article offers a comprehensive review of BNN developments, focusing on 1-bit activations and weights. It covers the evolution of BNN, from early models to advanced algorithms and design aspects, and addresses BNN optimization, deployment, computing architectures, and diverse applications. It also outlines potential research directions in BNN technology."
    },
    {
      "title": "Symbolic Discovery of Optimization Algorithms",
      "url": "/research/2023/05/08/Symbolic-Discovery-of-Optimization-Algorithms.html",
      "date": "May 8, 2023",
      "summary": "We propose a method for algorithm discovery via program search, focusing on optimizing deep neural network training. Our approach, Lion (EvoLved Sign Momentum), is memory-efficient and achieves comparable or superior performance to widely-used optimizers such as Adam and Adafactor across various tasks. Specifically, Lion enhances accuracy on tasks like image classification and vision-language contrastive learning, while reducing training compute. Notably, Lion exhibits improved performance with larger batch sizes and requires smaller learning rates compared to Adam. Despite its effectiveness, Lion has limitations, which we analyze, and provide insights into its deployment and performance. Our implementation is publicly available and has been successfully utilized in Google's search ads CTR model."
    },
    {
      "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
      "url": "/research/2022/03/19/Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training.html",
      "date": "March 19, 2022",
      "summary": "Sharpness-Aware Minimization (SAM) enhances generalization by optimizing a neighborhood-based perturbed loss, but it doesn't always favor flat minima due to both sharp and flat minima having low perturbed losses. We introduce a new measure, the surrogate gap, reflecting the dominant Hessian eigenvalue at small neighborhood radii, which is simple to compute and can be minimized during training. We propose the Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), an advancement over SAM with minimal additional computational cost. GSAM employs a dual-step approach: first, minimizing the perturbed loss similar to SAM, and second, reducing the surrogate gap without affecting the perturbed loss to target regions with low loss and sharpness, thus achieving superior generalization. GSAM is theoretically robust, showing better convergence and empirical generalization improvements, notably a +3.2% gain over SAM and +5.4% over AdamW in ImageNet accuracy for ViT-B/32."
    }
  ],
  "plasticity": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Most deep learning systems are designed to be trained once, or possibly pretrained and fintuned. These systems perform quite poorly in continual learning setups where training is ongoing. This is primarily due to two problems: catastrophic forgetting and loss of plasticity. This paper addresses the second. Various architectures and techniques were tested, with L2-regularization and shrink and perturm improving plasticity a little. This paper then introduces Continual Backpropagation, which reinitializes dead units, and seems to maintain plasticity indefinitely "
    }
  ],
  "fintune": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Most deep learning systems are designed to be trained once, or possibly pretrained and fintuned. These systems perform quite poorly in continual learning setups where training is ongoing. This is primarily due to two problems: catastrophic forgetting and loss of plasticity. This paper addresses the second. Various architectures and techniques were tested, with L2-regularization and shrink and perturm improving plasticity a little. This paper then introduces Continual Backpropagation, which reinitializes dead units, and seems to maintain plasticity indefinitely "
    }
  ],
  "setup": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Most deep learning systems are designed to be trained once, or possibly pretrained and fintuned. These systems perform quite poorly in continual learning setups where training is ongoing. This is primarily due to two problems: catastrophic forgetting and loss of plasticity. This paper addresses the second. Various architectures and techniques were tested, with L2-regularization and shrink and perturm improving plasticity a little. This paper then introduces Continual Backpropagation, which reinitializes dead units, and seems to maintain plasticity indefinitely "
    },
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/research/2020/08/03/Are-Labels-Necessary-for-Neural-Architecture-Search.html",
      "date": "August 3, 2020",
      "summary": "The paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning the necessity of human-annotated labels for discovering effective neural architectures. Two experimental setups sample-based and search-based were investigated. The sample-based approach evaluated 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. The search-based experiments employed DARTS, a recognized NAS algorithm, with various unsupervised objectives. The architectures identified without labels performed competitively compared to those found with labels. The findings suggested that image statistics alone could be sufficient for identifying efficient neural architectures, potentially eliminating the need for human-annotated labels."
    }
  ],
  "forgetting": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Most deep learning systems are designed to be trained once, or possibly pretrained and fintuned. These systems perform quite poorly in continual learning setups where training is ongoing. This is primarily due to two problems: catastrophic forgetting and loss of plasticity. This paper addresses the second. Various architectures and techniques were tested, with L2-regularization and shrink and perturm improving plasticity a little. This paper then introduces Continual Backpropagation, which reinitializes dead units, and seems to maintain plasticity indefinitely "
    },
    {
      "title": "From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning",
      "url": "/research/2023/03/16/From-MNIST-to-ImageNet-and-Back:-Benchmarking-Continual-Curriculum-Learning.html",
      "date": "March 16, 2023",
      "summary": "Continual learning (CL) is a promising trend in machine learning, aiming to develop robust models and strategies for dynamic environments by incorporating new knowledge while retaining past knowledge. CL research is fragmented, with various protocols, tasks, datasets, and metrics, often not reflecting real-world complexity and tailored to specific strategies. This work addresses this gap by introducing two novel CL benchmarks with heterogeneous tasks from six image datasets, varying in complexity and quality. These benchmarks fairly evaluate state-of-the-art CL strategies in scenarios closer to real-world conditions. The benchmarks present tasks in both increasing and decreasing complexity, assessing models' ability to exploit task structure. The work emphasizes a rigorous, reproducible evaluation protocol for measuring generalization and memory retention in models. Experimental results show that popular CL strategies perform poorly on these benchmarks, exhibit high forgetting, and struggle with curriculum task ordering. These findings underscore the need for rigorous comparisons and the development of new CL strategies for complex scenarios."
    }
  ],
  "l2regularization": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Most deep learning systems are designed to be trained once, or possibly pretrained and fintuned. These systems perform quite poorly in continual learning setups where training is ongoing. This is primarily due to two problems: catastrophic forgetting and loss of plasticity. This paper addresses the second. Various architectures and techniques were tested, with L2-regularization and shrink and perturm improving plasticity a little. This paper then introduces Continual Backpropagation, which reinitializes dead units, and seems to maintain plasticity indefinitely "
    }
  ],
  "shrink": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Most deep learning systems are designed to be trained once, or possibly pretrained and fintuned. These systems perform quite poorly in continual learning setups where training is ongoing. This is primarily due to two problems: catastrophic forgetting and loss of plasticity. This paper addresses the second. Various architectures and techniques were tested, with L2-regularization and shrink and perturm improving plasticity a little. This paper then introduces Continual Backpropagation, which reinitializes dead units, and seems to maintain plasticity indefinitely "
    },
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrives incrementally, it's common to build a sequence of models that incorporate progressively more data. It has been shown that simply continuing training of a model on new data often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    }
  ],
  "perturm": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Most deep learning systems are designed to be trained once, or possibly pretrained and fintuned. These systems perform quite poorly in continual learning setups where training is ongoing. This is primarily due to two problems: catastrophic forgetting and loss of plasticity. This paper addresses the second. Various architectures and techniques were tested, with L2-regularization and shrink and perturm improving plasticity a little. This paper then introduces Continual Backpropagation, which reinitializes dead units, and seems to maintain plasticity indefinitely "
    }
  ],
  "little": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Most deep learning systems are designed to be trained once, or possibly pretrained and fintuned. These systems perform quite poorly in continual learning setups where training is ongoing. This is primarily due to two problems: catastrophic forgetting and loss of plasticity. This paper addresses the second. Various architectures and techniques were tested, with L2-regularization and shrink and perturm improving plasticity a little. This paper then introduces Continual Backpropagation, which reinitializes dead units, and seems to maintain plasticity indefinitely "
    }
  ],
  "backpropagation": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Most deep learning systems are designed to be trained once, or possibly pretrained and fintuned. These systems perform quite poorly in continual learning setups where training is ongoing. This is primarily due to two problems: catastrophic forgetting and loss of plasticity. This paper addresses the second. Various architectures and techniques were tested, with L2-regularization and shrink and perturm improving plasticity a little. This paper then introduces Continual Backpropagation, which reinitializes dead units, and seems to maintain plasticity indefinitely "
    },
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "This paper introduces an algorithm that identifies simple and highly generalizable neural networks by searching for extensive flat minima regions in the error function, where the error rate is relatively stable. Such flat minima are associated with lower overfitting risks based on minimum description length (MDL) principles. Despite requiring second-order derivative calculations, the algorithm has a complexity level comparable to backpropagation. When tested on feedforward and recurrent networks, as well as stock market prediction tasks, this algorithm outperformed traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    }
  ],
  "reinitialize": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Most deep learning systems are designed to be trained once, or possibly pretrained and fintuned. These systems perform quite poorly in continual learning setups where training is ongoing. This is primarily due to two problems: catastrophic forgetting and loss of plasticity. This paper addresses the second. Various architectures and techniques were tested, with L2-regularization and shrink and perturm improving plasticity a little. This paper then introduces Continual Backpropagation, which reinitializes dead units, and seems to maintain plasticity indefinitely "
    }
  ],
  "seem": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Most deep learning systems are designed to be trained once, or possibly pretrained and fintuned. These systems perform quite poorly in continual learning setups where training is ongoing. This is primarily due to two problems: catastrophic forgetting and loss of plasticity. This paper addresses the second. Various architectures and techniques were tested, with L2-regularization and shrink and perturm improving plasticity a little. This paper then introduces Continual Backpropagation, which reinitializes dead units, and seems to maintain plasticity indefinitely "
    }
  ],
  "mamba": [
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "This paper introduces the Mamba archetecture, a selective state space model (SSSM) with input dependance. Mamba has better speed and scalability compared to transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences. Authors also present a hardware efficient scanning mechanism that is used in Mamba."
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a Vision archetecture that uses bidirectional Mamba blocks, challenging the necessity of self-attention in vision. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient."
    },
    {
      "title": "MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts",
      "url": "/research/2024/02/26/MoE-Mamba-Efficient-Selective-State-Space-Models-with-Mixture-of-Experts.html",
      "date": "February 26, 2024",
      "summary": "State Space Models (SSMs) are emerging as strong competitors in sequential modeling, challenging Transformers. Integrating Mixture of Experts (MoE) has enhanced Transformer-based models, including cutting-edge open models. We suggest combining SSMs with MoE to further unlock their scaling potential. Our model, MoE-Mamba, based on the SSM model Mamba, exceeds the performance of both Mamba and traditional Transformer-MoE models. Notably, MoE-Mamba achieves Mamba's performance with 2.35 times fewer training steps, maintaining Mamba's inference advantages over Transformers."
    },
    {
      "title": "VideoMamba: State Space Model for Efficient Video Understanding",
      "url": "/research/2024/03/12/VideoMamba-State-Space-Model-for-Efficient-Video-Understanding.html",
      "date": "March 12, 2024",
      "summary": "This work introduces VideoMamba, an innovative approach that addresses local redundancy and global dependencies in video understanding by adapting the Mamba framework to video analysis. VideoMamba outperforms existing 3D convolutional neural networks and video transformers through its linear-complexity operator, which facilitates efficient long-term modeling for high-resolution, long-duration videos. Its effectiveness is demonstrated across four main areas: scalability in the visual domain without needing extensive dataset pretraining, thanks to a novel self-distillation technique; the ability to recognize short-term actions with fine-grained motion differences; superior performance in long-term video understanding compared to traditional feature-based models; and robust compatibility with multiple modalities, enhancing multi-modal video analysis. VideoMamba establishes a new standard for comprehensive and efficient video understanding."
    }
  ],
  "sssm": [
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "This paper introduces the Mamba archetecture, a selective state space model (SSSM) with input dependance. Mamba has better speed and scalability compared to transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences. Authors also present a hardware efficient scanning mechanism that is used in Mamba."
    }
  ],
  "dependance": [
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "This paper introduces the Mamba archetecture, a selective state space model (SSSM) with input dependance. Mamba has better speed and scalability compared to transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences. Authors also present a hardware efficient scanning mechanism that is used in Mamba."
    }
  ],
  "scalability": [
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "This paper introduces the Mamba archetecture, a selective state space model (SSSM) with input dependance. Mamba has better speed and scalability compared to transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences. Authors also present a hardware efficient scanning mechanism that is used in Mamba."
    },
    {
      "title": "Scaling Vision Transformers to 22 Billion Parameters",
      "url": "/research/2023/02/10/Scaling-Vision-Transformers-to-22-Billion-Parameters.html",
      "date": "February 10, 2023",
      "summary": "We introduce a method for efficiently training a 22B-parameter Vision Transformer (ViT-22B) and conduct various experiments to assess its performance. Compared to existing models, ViT-22B shows improved scalability and benefits such as enhanced fairness-performance tradeoff, alignment with human visual perception, and increased robustness. Our findings suggest promising avenues for achieving large-scale language model-like capabilities in vision tasks."
    },
    {
      "title": "VideoMamba: State Space Model for Efficient Video Understanding",
      "url": "/research/2024/03/12/VideoMamba-State-Space-Model-for-Efficient-Video-Understanding.html",
      "date": "March 12, 2024",
      "summary": "This work introduces VideoMamba, an innovative approach that addresses local redundancy and global dependencies in video understanding by adapting the Mamba framework to video analysis. VideoMamba outperforms existing 3D convolutional neural networks and video transformers through its linear-complexity operator, which facilitates efficient long-term modeling for high-resolution, long-duration videos. Its effectiveness is demonstrated across four main areas: scalability in the visual domain without needing extensive dataset pretraining, thanks to a novel self-distillation technique; the ability to recognize short-term actions with fine-grained motion differences; superior performance in long-term video understanding compared to traditional feature-based models; and robust compatibility with multiple modalities, enhancing multi-modal video analysis. VideoMamba establishes a new standard for comprehensive and efficient video understanding."
    }
  ],
  "domain": [
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "This paper introduces the Mamba archetecture, a selective state space model (SSSM) with input dependance. Mamba has better speed and scalability compared to transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences. Authors also present a hardware efficient scanning mechanism that is used in Mamba."
    },
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/research/2021/05/03/Robust-and-Generalizable-Visual-Representation-Learning-via-Random-Convolutions.html",
      "date": "May 3, 2021",
      "summary": "This study introduces random convolutions for data augmentation. Random convolutions approximately preserve shape while distorting local textures. This method significantly enhances performance on domain generalization and robustness / corruption benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods."
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "This paper introduces the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a multi-directional Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions."
    },
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. Taking this to the extreme, this study trains a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss even when trained on pure noise."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. The authors find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. "
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, this paper introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, the authors enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. This simple method surpasses the current state-of-the-art in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of Vision Transformer (ViT) variants against common corruptions. The benchmarking reveals that overlapping patch embedding and convolutional feed-forward networks (FFNs) significantly enhance ViT robustness. The study also scrutinizes the effectiveness of CNN-based data augmentation strategies when applied to ViTs, finding that adversarial noise training is effective, while fourier-domain augmentation falls short. A new conditional method for generating dynamic augmentation parameters based on input images is proposed, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "SinLU: Sinu-Sigmoidal Linear Uni",
      "url": "/research/2021/12/02/SinLU-Sinu-Sigmoidal-Linear-Uni.html",
      "date": "December 2, 2021",
      "summary": "This paper introduces a non-linear activation function called the Sinu-sigmoidal Linear Unit (SinLU), formulated as SinLU(x) = (x + a sin bx) · σ(x). It incorporates a sine wave for added functionality over traditional linear units, with two trainable parameters controlling the sinusoidal participation. The authors compare SinLU's performance to ReLU, GELU, and SiLU across various domains, models, and standard datasets. The results demonstrate SinLU's robustness and superior performance due to the incorporation of the trainable sine wave parameters, facilitating easy training and fast convergence."
    },
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This paper investigates using high-quality rendering software and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle. This synthetic dataset is used to augment limited real training data for image classification and semantic segmentation tasks. While models trained solely on synthetic images showed low accuracy on real validation data, including even small amounts of real data significantly improved performance. Augmenting real training data with synthetic images outperformed using only real images. Furthermore, pretraining models on the synthetic dataset before transfer learning significantly reduced training costs by allowing most of the training to be completed upfront using the synthetic data."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "The paper introduces a system that trains deep neural networks for object detection using only synthetic images generated with domain randomization. This involves randomizing non-realistic simulator parameters like lighting and object textures, allowing the network to identify essential object features. Their findings show that networks can achieve impressive performance using just synthetic data, and further improve with fine-tuning on real data. This suggests the potential of using cost-effective synthetic data for training, avoiding the need to acquire vast amounts of real-world data or create detailed synthetic environments. They validate their method on car bounding box detection using the KITTI dataset."
    },
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "The authors introduce VideoMAE, a data-efficient self-supervised video pre-training (SSVP) approach that utilizes video masked autoencoders with a novel high-ratio video tube masking technique inspired by ImageMAE. Key findings include the effectiveness of high masking ratios (90-95%) due to video's temporal redundancy, strong performance on small datasets (~3k-4k videos) without extra data highlighting the importance of high-level structure learning, and data quality being more crucial than quantity with domain shift being significant. Notably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone without extra data."
    },
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    },
    {
      "title": "How to decay your learning rate",
      "url": "/research/2021/05/23/How-to-decay-your-learning-rate.html",
      "date": "May 23, 2021",
      "summary": "Empirical findings suggest that typical fine-tuned learning rate schedules decay the learning rate following weight norm fluctuations. This led to the development of ABEL, an automatic scheduler that adjusts the learning rate based on weight norm changes. ABEL performs comparably to tuned schedules but demonstrates greater robustness to parameter variations. Extensive experiments across various domains reveal that when the weight norm remains stable, simplified schedules yield equivalent performance to complex ones, resembling a constant learning rate with decay towards the end of training."
    },
    {
      "title": "ImageNet-D: A new challenging robustness dataset inspired by domain adaptation",
      "url": "/research/2023/05/05/ImageNet-D-A-new-challenging-robustness-dataset-inspired-by-domain-adaptation.html",
      "date": "May 5, 2023",
      "summary": "We introduce ImageNet-D, a novel dataset designed to evaluate the robustness of ImageNet-trained models across various domains. With six distinct domains including Real, Painting, Clipart, Sketch, Infograph, and Quick-draw, ImageNet-D challenges even state-of-the-art models, revealing interpretable errors. For instance, our leading EfficientNet-L2 model exhibits a significant performance decrease, dropping from 11.6% on clean ImageNet to 29.2% on the Real domain."
    },
    {
      "title": "Atomically Accurate De Novo Design of Single-domain Antibodies",
      "url": "/research/2024/03/18/Atomically-Accurate-De-Novo-Design-of-Single-domain-Antibodies.html",
      "date": "March 18, 2024",
      "summary": "Despite the critical role antibodies play in medicine, current methods for designing new antibodies targeting specific epitopes are time-consuming. Here, we showcase the effectiveness of a refined RFdiffusion network in designing novel antibody variable heavy chains (VHH’s) to target user-specified epitopes. Through experiments, we validate the binding capability of these designed VHH's to four disease-relevant epitopes. Furthermore, the cryo-EM structure analysis reveals that a designed VHH bound to influenza hemagglutinin closely matches the design model, affirming the accuracy of our approach."
    },
    {
      "title": "Chronos: Learning the Language of Time Series",
      "url": "/research/2024/03/12/Chronos-Learning-the-Language-of-Time-Series.html",
      "date": "March 12, 2024",
      "summary": "Chronos is a framework that enhances pretrained probabilistic time series models by tokenizing time series data for training with transformer-based architectures, notably the T5 family. Pretrained on a mix of public and synthetic datasets created via Gaussian processes, Chronos models outshine others in 42 benchmark datasets by showing superior performance on familiar datasets and competitive or better zero-shot capabilities on new datasets. This illustrates Chronos's ability to generalize across different domains, offering a simpler approach to forecasting tasks."
    },
    {
      "title": "HYPO: Hyperspherical Out-of-Distribution Generalization",
      "url": "/research/2024/03/19/HYPO-Hyperspherical-Out-of-Distribution-Generalization.html",
      "date": "March 19, 2024",
      "summary": "We propose HYPO, a novel framework for out-of-distribution (OOD) generalization in machine learning, which learns domain-invariant features in a hyperspherical space. Our method focuses on aligning features of the same class across different domains close to their class prototypes and separating different class prototypes maximally. We offer theoretical justifications for its improvement on OOD generalization and show through experiments on OOD benchmarks that HYPO outperforms existing baselines with superior performance."
    },
    {
      "title": "Neural Networks Learn Statistics of Increasing Complexity",
      "url": "/research/2024/02/13/Neural-Networks-Learn-Statistics-of-Increasing-Complexity.html",
      "date": "February 13, 2024",
      "summary": "The distributional simplicity bias (DSB) theory suggests neural networks first learn basic patterns in data before understanding more complex correlations. We provide new evidence supporting DSB, showing networks initially excel with data matching training set's simple statistics but this ability diminishes later. Extending DSB to discrete domains, we demonstrate an equivalence between n-gram frequencies and vector moments, also observing this bias in large language models (LLMs). Additionally, by adjusting low-level statistics of images to resemble another class, we reveal that networks in early training phases misclassify these edited images as if they belonged to the target class."
    },
    {
      "title": "VideoMamba: State Space Model for Efficient Video Understanding",
      "url": "/research/2024/03/12/VideoMamba-State-Space-Model-for-Efficient-Video-Understanding.html",
      "date": "March 12, 2024",
      "summary": "This work introduces VideoMamba, an innovative approach that addresses local redundancy and global dependencies in video understanding by adapting the Mamba framework to video analysis. VideoMamba outperforms existing 3D convolutional neural networks and video transformers through its linear-complexity operator, which facilitates efficient long-term modeling for high-resolution, long-duration videos. Its effectiveness is demonstrated across four main areas: scalability in the visual domain without needing extensive dataset pretraining, thanks to a novel self-distillation technique; the ability to recognize short-term actions with fine-grained motion differences; superior performance in long-term video understanding compared to traditional feature-based models; and robust compatibility with multiple modalities, enhancing multi-modal video analysis. VideoMamba establishes a new standard for comprehensive and efficient video understanding."
    },
    {
      "title": "The Platonic Representation Hypothesis",
      "url": "/research/2024/05/13/The-Platonic-Representation-Hypothesis.html",
      "date": "May 13, 2024",
      "summary": "This paper argues that AI model representations, especially in deep networks, are converging. It surveys examples in the literature showing how neural network representations align over time and across domains. It demonstrates convergence across data modalities, noting that as vision and language models grow, they measure distances between data points similarly. The authors hypothesize that this convergence leads to a shared statistical model of reality, termed the platonic representation and discuss possible selective pressures toward it. The paper also addresses the implications, limitations, and counterexamples to this trend."
    }
  ],
  "audio": [
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "This paper introduces the Mamba archetecture, a selective state space model (SSSM) with input dependance. Mamba has better speed and scalability compared to transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences. Authors also present a hardware efficient scanning mechanism that is used in Mamba."
    },
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. Taking this to the extreme, this study trains a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss even when trained on pure noise."
    }
  ],
  "genomic": [
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "This paper introduces the Mamba archetecture, a selective state space model (SSSM) with input dependance. Mamba has better speed and scalability compared to transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences. Authors also present a hardware efficient scanning mechanism that is used in Mamba."
    }
  ],
  "scanning": [
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "This paper introduces the Mamba archetecture, a selective state space model (SSSM) with input dependance. Mamba has better speed and scalability compared to transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences. Authors also present a hardware efficient scanning mechanism that is used in Mamba."
    }
  ],
  "mambabyte": [
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/research/2024/01/24/MambaByte-Token-free-Selective-State-Space-Model.html",
      "date": "January 24, 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    }
  ],
  "operate": [
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/research/2024/01/24/MambaByte-Token-free-Selective-State-Space-Model.html",
      "date": "January 24, 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results. The method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. A convolutional neural network (CNN) was trained on over a million color images, operating as a feed-forward pass at test time. The algorithm's effectiveness was validated through a colorization Turing test, where it deceived participants in 32% of trials. The paper also illustrates how colorization can serve as an effective pretraining task for self-supervised feature learning, achieving state-of-the-art results on several benchmarks."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "url": "/research/2014/02/19/Exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks.html",
      "date": "February 19, 2014",
      "summary": "This study aims to enhance our comprehension of deep learning dynamics by analyzing deep linear neural networks. Despite their linear input-output mapping, these networks display nonlinear gradient descent dynamics, resulting in phenomena like prolonged plateaus and swift transitions to improved solutions. Through analytical exploration, we reveal that as network depth increases indefinitely, learning speed can remain finite under specific initial conditions. Unsupervised pretraining, under certain data conditions, can discover these favorable initial conditions, unlike random Gaussian initializations. We introduce a novel class of random orthogonal initial conditions that, akin to unsupervised pretraining, enable depth-independent learning. These conditions also ensure gradient propagation in deep nonlinear networks, particularly when operating at the edge of chaos."
    },
    {
      "title": "Exponentially Faster Language Modeling",
      "url": "/research/2023/11/21/Exponentially-Faster-Language-Modeling.html",
      "date": "November 21, 2023",
      "summary": "UltraFastBERT, a BERT variant, operates with just 0.3% of its neurons—selectively using 12 out of 4095 neurons per layer—for inference, matching the performance of similar models. It replaces conventional feedforward networks with fast feedforward networks (FFFs) to achieve this efficiency. Although fully efficient conditional neural execution isn't yet practical, we offer a high-level CPU code that achieves a 78x speedup and a PyTorch implementation with a 40x speedup over standard batched feedforward inference."
    }
  ],
  "byte": [
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/research/2024/01/24/MambaByte-Token-free-Selective-State-Space-Model.html",
      "date": "January 24, 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    }
  ],
  "subword": [
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/research/2024/01/24/MambaByte-Token-free-Selective-State-Space-Model.html",
      "date": "January 24, 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    }
  ],
  "tokenization": [
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/research/2024/01/24/MambaByte-Token-free-Selective-State-Space-Model.html",
      "date": "January 24, 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    }
  ],
  "bias": [
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/research/2024/01/24/MambaByte-Token-free-Selective-State-Space-Model.html",
      "date": "January 24, 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. The authors find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. "
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Incorporating diverse image styles into CNN training data reduces texture bias, enhances shape recognition, and improves resilience against common image corruptions. This is typically explained as decreasing model reliance on high frequency texture information. This paper challenges this explaniation through large scale testing with natural images, edge information, and stylization, finding no direct link between shape bias and robustness. The enhanced corruption robustness is instead attributed to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/research/2021/08/31/Universal-Adversarial-Robustness-of-Texture-and-Shape-biased-Models.html",
      "date": "August 31, 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "This study challenges the traditional understanding of how Convolutional Neural Networks (CNNs) recognize objects, revealing that they are more biased towards recognizing textures rather than learning complex shapes. The authors found that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, training a ResNet-50 on a stylized version of ImageNet, designed to emphasize shape, aligned the network's performance more closely with human behavior. This shape-based training matched human performance in controlled experiments and enhanced object detection and robustness against image distortions, highlighting the benefits of shape-based representations in visual recognition systems."
    },
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    },
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    },
    {
      "title": "Neural Networks Learn Statistics of Increasing Complexity",
      "url": "/research/2024/02/13/Neural-Networks-Learn-Statistics-of-Increasing-Complexity.html",
      "date": "February 13, 2024",
      "summary": "The distributional simplicity bias (DSB) theory suggests neural networks first learn basic patterns in data before understanding more complex correlations. We provide new evidence supporting DSB, showing networks initially excel with data matching training set's simple statistics but this ability diminishes later. Extending DSB to discrete domains, we demonstrate an equivalence between n-gram frequencies and vector moments, also observing this bias in large language models (LLMs). Additionally, by adjusting low-level statistics of images to resemble another class, we reveal that networks in early training phases misclassify these edited images as if they belonged to the target class."
    }
  ],
  "patchdropout": [
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "This paper introduces PatchDropout, a simple training technique for Vision Transformers ViT that drops random image patches, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    }
  ],
  "economize": [
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "This paper introduces PatchDropout, a simple training technique for Vision Transformers ViT that drops random image patches, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    }
  ],
  "patch": [
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "This paper introduces PatchDropout, a simple training technique for Vision Transformers ViT that drops random image patches, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "This study introduces the Vision Transformer ViT archetecture. The paper shows that a pure transformer applied directly to sequences of image patches can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, enhances vision tasks using Vision transformers by masking and reconstructing parts of an image. The compatibility of MIM with CNNs and its operational principle are unclear. this study reveals that MIM improves generalized feature extraction through middle-order patch interactions and introduces Architecture-Agnostic Masked Image Modeling (A2MIM), compatible with both Transformers and CNNs. Extensive testing demonstrates A2MIM's ability to enhance representation learning and transferability to various tasks without specialized modifications."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of Vision Transformer (ViT) variants against common corruptions. The benchmarking reveals that overlapping patch embedding and convolutional feed-forward networks (FFNs) significantly enhance ViT robustness. The study also scrutinizes the effectiveness of CNN-based data augmentation strategies when applied to ViTs, finding that adversarial noise training is effective, while fourier-domain augmentation falls short. A new conditional method for generating dynamic augmentation parameters based on input images is proposed, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. MAE involves masking random patches of an input image and reconstructing the missing pixels using an asymmetric encoder-decoder architecture and a lightweight decoder. Masking a significant portion of the input image creates a challenging yet informative self-supervisory task. These innovations allow for efficient training of large models, tripling training speed and enhancing accuracy. A vanilla ViT-Huge model reaches top accuracy (87.8%) on ImageNet-1K data among similar methods. The model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating promising scaling potential."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "This paper introduces Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperformed CLIP models on various downstream tasks. The speedup from this method enables experimentation with larger model sizes, more data, or longer training periods."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. This encoder serves as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The DMAE ViT-Base model achieves comparable or superior certified accuracy with fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on ImageNet. The model also shows high transferability to CIFAR-10, indicating its broad applicability."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "This paper proposes Hard Patches Mining (HPM), a novel framework for masked image modeling (MIM) pre-training that goes beyond simply solving given problems. HPM aims for the model to generate more challenging tasks for itself, using reconstruction loss as a metric for task difficulty. It incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "This research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. The authors validate these approaches using the ImageNet-1k dataset and confirm their findings with the ImageNet-v2 test set, evaluating transfer performance across six additional datasets."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article provides an extensive review of vision transformer techniques applied specifically to human action recognition. These action transformers are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also examines optimizing spatio-temporal attention in transformers for longer sequences and different learning strategies like self-supervised and zero-shot learning with their respective loss functions. Additionally, it assesses progress in benchmark evaluation scores and discusses challenges and future directions in this research area."
    },
    {
      "title": "Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding",
      "url": "/research/2021/11/16/Improved-Robustness-of-Vision-Transformer-via-PreLayerNorm-in-Patch-Embedding.html",
      "date": "November 16, 2021",
      "summary": "Recent advancements in vision transformers (ViTs) have shown superior performance across diverse visual tasks, surpassing convolutional neural networks (CNNs). Given ViT's distinct architecture, understanding its behavior and reliability is imperative. This paper investigates ViT's robustness by comparing it with CNNs under various image corruptions relevant to real-world vision tasks. While ViT generally exhibits comparable or improved robustness over CNNs, it consistently underperforms in contrast enhancement tasks. Analysis suggests that positional embedding in ViT's patch embedding may malfunction with color scale changes. We propose PreLayerNorm, a modified patch embedding structure, to address this issue and ensure scale-invariant behavior in ViT. ViT with PreLayerNorm demonstrates enhanced robustness across various corruptions, particularly in contrast-varying environments."
    }
  ],
  "dropout": [
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "This paper introduces PatchDropout, a simple training technique for Vision Transformers ViT that drops random image patches, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "This paper introduces Batch Normalization, an online normalization technique which calculates mean and std across the batch dimension. Batch Norm improves training efficiency, allowing for higher learning rates, and decreasing hyper-parameter sensitivity, and sometimes removing the need for Dropout regularization. This paper suggests Batch Norm improves internal covariant shift, however later works have questioned this."
    },
    {
      "title": "Dropout as Data Augmentation",
      "url": "/research/2016/01/08/DROPOUT-AS-DATA-AUGMENTATION.html",
      "date": "January 8, 2016",
      "summary": "Dropout is typically interpreted as turning the test time network into an ensemble of the thinner training networks. This paper argues that dropout can also be interpreted as a kind of data augmentation in image space. Authors present an approach to project the dropout noise within a network back into the input space, visualising the augmented versions of the training data, and show that training a deterministic network on the augmented samples yields similar results. Authors then propose a new dropout noise scheme and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/research/2019/07/26/DropAttention-A-Regularization-Method-for-Fully-Connected-Self-Attention-Networks.html",
      "date": "July 26, 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. DropAttention uses a mask to zero out elements in the attention matrix. Experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    },
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a method for dropout in Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers."
    },
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/research/2014/01/01/Dropout-A-Simple-Way-to-Prevent-Neural-Networks-from-Overfitting.html",
      "date": "January 1, 2014",
      "summary": "This paper introduces Dropout, a technique that addresses overfitting by randomly omitting units during training, leading to an exponential number of thinned networks. This effectively turns the test time network into an ensemble."
    },
    {
      "title": "Dropout Reduces Underfitting",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. Authors then introduce two dropout schedules: early dropout which prevents underfitting, and late dropout which prevents overfitting."
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "This paper introduces DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. Authors also propose a dropout rate scheduler to optimize training."
    },
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/research/2022/10/10/Turbo-Training-with-Token-Dropout.html",
      "date": "October 10, 2022",
      "summary": "This paper introduces Turbo training, an efficient method for training Transformers on video-related tasks. The authors present a simple yet versatile training paradigm applicable to multiple video tasks and demonstrate its effectiveness across action classification, video-language representation learning, and long-video activity classification. Turbo training achieves competitive performance with up to 4× speed-up and reduced memory usage, enabling long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous resource-intensive methods.."
    },
    {
      "title": "On the Maximum Hessian Eigenvalue and Generalization",
      "url": "/research/2023/05/23/On-the-Maximum-Hessian-Eigenvalue-and-Generalization.html",
      "date": "May 23, 2023",
      "summary": "This study investigates the relationship between training interventions and the generalization of deep networks. While previous research suggests that flatter solutions generalize better than sharper ones, particularly measured by λmax, the largest eigenvalue of the Hessian of the loss, this paper challenges this notion. Through experiments, we demonstrate that larger learning rates reduce λmax for all batch sizes but do not consistently improve generalization. Additionally, scaling batch size and learning rate simultaneously can change λmax without affecting generalization. Sharpness-Aware Minimization (SAM) produces smaller λmax but does not consistently enhance generalization, especially with larger batch sizes. Excessive dropout probabilities can degrade generalization, despite promoting smaller λmax. Batch normalization, while not consistently reducing λmax, still improves generalization. These findings question λmax's role in explaining generalization in neural networks, highlighting the limits of its explanatory power."
    },
    {
      "title": "Fine-tuning with Very Large Dropout",
      "url": "/research/2024/03/01/Fine-tuning-with-Very-Large-Dropout.html",
      "date": "March 1, 2024",
      "summary": "This study challenges the notion that machine learning practices assume training and testing data have the same distribution. It explores the effectiveness of high dropout rates, as opposed to ensemble techniques, in developing rich data representations suitable for multiple distribution scenarios. These representations surpass those achieved by traditional in-distribution performance regularization and the implicit sparsity induced by common stochastic gradient methods. While training deep networks from scratch with high dropout rates is impractical, fine-tuning pre-trained models under these conditions is feasible and yields better out-of-distribution performance than ensembles and model averaging techniques like model soups. This finding is significant due to the growing relevance of fine-tuning with large pre-trained models, offering insights into the nature of rich representations and the linear characteristics of fine-tuning large networks with small datasets."
    }
  ],
  "drop": [
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "This paper introduces PatchDropout, a simple training technique for Vision Transformers ViT that drops random image patches, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a method for dropout in Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers."
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "This paper introduces DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. Authors also propose a dropout rate scheduler to optimize training."
    },
    {
      "title": "ImageNet-D: A new challenging robustness dataset inspired by domain adaptation",
      "url": "/research/2023/05/05/ImageNet-D-A-new-challenging-robustness-dataset-inspired-by-domain-adaptation.html",
      "date": "May 5, 2023",
      "summary": "We introduce ImageNet-D, a novel dataset designed to evaluate the robustness of ImageNet-trained models across various domains. With six distinct domains including Real, Painting, Clipart, Sketch, Infograph, and Quick-draw, ImageNet-D challenges even state-of-the-art models, revealing interpretable errors. For instance, our leading EfficientNet-L2 model exhibits a significant performance decrease, dropping from 11.6% on clean ImageNet to 29.2% on the Real domain."
    },
    {
      "title": "Sharpness-Aware Training for Free",
      "url": "/research/2023/03/02/Sharpness-Aware-Training-for-Free.html",
      "date": "March 2, 2023",
      "summary": "Modern deep neural networks (DNNs) excel in performance but often suffer from over-parameterization, leading to increased generalization error without tailored training strategies. Sharpness-Aware Minimization (SAM) has proven effective in reducing generalization error by minimizing sharpness in the loss landscape. However, SAM incurs a significant computational overhead. This paper introduces Sharpness-Aware Training for Free (SAF), which mitigates sharpness at nearly zero additional computational cost. SAF achieves this by preventing sudden drops in loss within sharp local minima during weight updates. A novel trajectory loss, based on KL-divergence between current and past DNN outputs, replaces SAM's sharpness measure, guiding convergence towards flat minima for enhanced generalization. Empirical results demonstrate SAF's effectiveness on ImageNet with comparable computational efficiency to the base optimizer."
    }
  ],
  "imagenet": [
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "This paper introduces PatchDropout, a simple training technique for Vision Transformers ViT that drops random image patches, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a Vision archetecture that uses bidirectional Mamba blocks, challenging the necessity of self-attention in vision. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient."
    },
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/research/2017/08/23/A-downsampled-variant-of-imagenet-as-an-alternative-to-the-cifar-datasets.html",
      "date": "August 23, 2017",
      "summary": "This paper proposes downsampled versions of ImageNet: ImageNet64x64, ImageNet32x32, and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics."
    },
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/research/2019/04/27/Benchmarking-Neural-Network-Robustness-to-Common-Corruptions-and-Surface-Variations.html",
      "date": "April 27, 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and IMAGENET-P, which assesses perturbation robustness."
    },
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "This paper intriduces the Kaiming He initialisation strategy for RELU like activation functions. This the default init in pytorch, improving model stability and decreasing training time for RELU networks. This study also introduces a Parametric Rectified Linear Unit (PReLU) activation function that extends traditional units, offering improved model fitting with negligible additional computational cost."
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/ON-INTERACTION-BETWEEN-AUGMENTATIONS-AND-CORRUPTIONS-IN-NATURAL-CORRUPTION-ROBUSTNESS.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, the correlation between data augmentations and test-time corruptions remains unclear. This paper introduces Minimal Sample Distance, and demonstrates a strong correlation between augmentation-corruption similarity and performance. Authors argue that training with augmentations that are perceptually similar to corruptions enhances test error."
    },
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets transformers as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "This study introduces the Vision Transformer ViT archetecture. The paper shows that a pure transformer applied directly to sequences of image patches can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores self-supervision through automatic colorization as an alternative to ImageNet pretraining. Self-supervised training achieved state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. This paper highlights the significance of loss formulation, training specifics, and network architecture when pretraining through colorization. It also revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and feature adaptability upon fine-tuning. The findings suggest that colorization offers a better supervisory signal comparable to various types of ImageNet pretraining."
    },
    {
      "title": "CLIP: Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns from raw text descriptions of images, moving beyond training on fixed object categories. By pre-training on 400 million (image, text) pairs using caption-image matching, the method achieves state-of-the-art image representations from scratch. This allows for zero-shot transfer to various downstream tasks using natural language, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse datasets, showing competitive results against fully supervised baselines without task-specific training, and matching the accuracy of ResNet-50 on ImageNet zero-shot without using its training examples."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. MAE involves masking random patches of an input image and reconstructing the missing pixels using an asymmetric encoder-decoder architecture and a lightweight decoder. Masking a significant portion of the input image creates a challenging yet informative self-supervisory task. These innovations allow for efficient training of large models, tripling training speed and enhancing accuracy. A vanilla ViT-Huge model reaches top accuracy (87.8%) on ImageNet-1K data among similar methods. The model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating promising scaling potential."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) uses synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. Given this perspective, a one-instance fractal database (OFDB) is developed where only a single image per category is present. Despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M comparable or superior results are obtaine to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. A ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training. Synthetic images generated by FDSL circumvent issues associated with real images, offering a promising avenue for pre-training general models. Investigations suggest that simple object contours can match fractal-based dataset performance and increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces TinyViT, a series of compact and efficient vision transformers (ViTs) designed for devices with limited resources. The authors employ a fast distillation framework to transfer knowledge from large pretrained models to smaller ones during pretraining, using sparsified logits from teacher models to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. With increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters, and demonstrates strong transferability across various downstream tasks."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets for formula-driven supervised learning (FDSL). The authors identify the optimal FDSL parameters and maximize synthetic image variety, which is crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. The authors demonstrate FDSL's potential for continuous improvement and its ability to avoid issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates how to achieve state-of-the-art certified adversarial robustness to ℓ2-norm bounded perturbations using only off-the-shelf pretrained models. The authors instantiate the denoised smoothing approach by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This method certifies 71% accuracy on ImageNet under adversarial perturbations constrained to an ℓ2 norm of ε = 0.5, improving upon the prior certified state-of-the-art by 14 percentage points and denoised smoothing by 30 percentage points, without requiring any fine-tuning or retraining of model parameters."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. This encoder serves as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The DMAE ViT-Base model achieves comparable or superior certified accuracy with fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on ImageNet. The model also shows high transferability to CIFAR-10, indicating its broad applicability."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "This paper explores the interactions between training data amount, model regularization or data augmentation (AugReg), model size, and compute budget for Vision Transformers (ViT) in various vision tasks. The authors find that using more compute and AugReg can achieve the same performance as training with significantly more data. They demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "This research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. The authors validate these approaches using the ImageNet-1k dataset and confirm their findings with the ImageNet-v2 test set, evaluating transfer performance across six additional datasets."
    },
    {
      "title": "Training data-efficient image transformers & distillation through attention",
      "url": "/research/2021/01/15/Training-data-efficient-image-transformers-&-distillation-through-attention.html",
      "date": "January 15, 2021",
      "summary": "This paper introduces DeiT, an efficient method for training vision transformers. The authors present a unique teacher-student strategy for transformers, leveraging a distillation token for efficient learning from a convolutional network teacher. This method achieves performance on par with convolutional networks, with up to 85.2% accuracy on ImageNet, and demonstrates effective transferability to other tasks."
    },
    {
      "title": "Better plain ViT baselines for ImageNet-1k",
      "url": "/research/2022/05/02/Better-plain-ViT-baselines-for-ImageNet-1k.html",
      "date": "May 2, 2022",
      "summary": "This paper challenges the common belief that the Vision Transformer (ViT) model requires sophisticated regularization techniques to excel on ImageNet-1k scale data. The authors present minor modifications to the original ViT vanilla training setting that significantly improve the performance of plain ViT models. They find that standard data augmentation is sufficient, with 90 epochs of training surpassing 76% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reaching 80% in less than one day."
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of Vision Transformers (ViTs) without relying on large-scale pre-training datasets like ImageNet and JFT or requiring modifications to the architecture or loss functions. The authors show that this approach improves ViT performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, while maintaining ViT's attention to relevant regions and robustness, despite ViT's inherent lack of inductive biases and typical dependence on large-scale pre-training."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "This study challenges the traditional understanding of how Convolutional Neural Networks (CNNs) recognize objects, revealing that they are more biased towards recognizing textures rather than learning complex shapes. The authors found that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, training a ResNet-50 on a stylized version of ImageNet, designed to emphasize shape, aligned the network's performance more closely with human behavior. This shape-based training matched human performance in controlled experiments and enhanced object detection and robustness against image distortions, highlighting the benefits of shape-based representations in visual recognition systems."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "This paper introduces Online Normalization, a novel method for normalizing neural network hidden activations that offers a batch-independent alternative with comparable accuracy to Batch Normalization. Online Normalization addresses Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. The method is applicable to recurrent, fully connected networks, and those with high activation memory requirements. The authors demonstrate its effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    },
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "The paper focuses on evaluating the performance of recent advancements in image transformers, such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, on the Tiny ImageNet dataset for transfer learning tasks. While these models are typically trained on large datasets like ImageNet-21k and then fine-tuned on ImageNet-1k, their assessments often overlook the Tiny ImageNet benchmark. The study updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. This underscores the importance of evaluating these models on the Tiny ImageNet dataset for transfer learning performance."
    },
    {
      "title": "Deep Residual Learning for Image Recognition",
      "url": "/research/2015/12/10/Deep-Residual-Learning-for-Image-Recognition.html",
      "date": "December 10, 2015",
      "summary": "We introduce a residual learning framework to train deeper neural networks effectively. By reformulating layers to learn residual functions relative to inputs, our approach facilitates optimization and achieves higher accuracy with increased depth. Evaluations on ImageNet demonstrate the effectiveness of residual networks up to 152 layers deep, outperforming previous architectures. Our method achieves 3.57% error on the ImageNet test set, winning 1st place in the ILSVRC 2015 classification task. Additionally, our deep representations lead to a 28% improvement on the COCO object detection dataset. In the ILSVRC & COCO 2015 competitions, our approach secured 1st place in ImageNet detection, localization, COCO detection, and segmentation tasks."
    },
    {
      "title": "Searching for Activation Functions",
      "url": "/research/2017/10/27/Searching-for-Activation-Functions.html",
      "date": "October 27, 2017",
      "summary": "This study explores the impact of activation functions on deep network training and performance. While Rectified Linear Unit (ReLU) is widely used, alternatives have not consistently outperformed it. We propose using automatic search techniques to discover new activation functions. Through exhaustive and reinforcement learning-based searches, we identify novel functions. Empirical evaluation shows that our best discovered function, Swish (f(x) = x · sigmoid(βx)), performs better than ReLU on deeper models across challenging datasets. Replacing ReLUs with Swish units improves classification accuracy on ImageNet, for example, by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. Swish's simplicity and similarity to ReLU facilitate its adoption in neural networks."
    },
    {
      "title": "Don't Decay the Learning Rate, Increase the Batch Size",
      "url": "/research/2017/11/01/Don't-Decay-the-Learning-Rate,-Increase-the-Batch-Size.html",
      "date": "November 1, 2017",
      "summary": "This study demonstrates that increasing the batch size during training achieves similar learning outcomes as the common practice of decaying the learning rate, applicable to stochastic gradient descent and its variants, including with momentum and Adam optimization. This approach not only matches test accuracies within the same number of epochs but also enhances parallelism and reduces training time due to fewer parameter updates. Efficiency can be further improved by adjusting the learning rate and batch size proportionally, and although increasing the momentum coefficient and scaling the batch size accordingly may slightly lower test accuracy, it enables the use of large batch training without needing to tune hyper-parameters. Using these methods, ResNet-50 was trained on ImageNet to a 76.1% validation accuracy in less than 30 minutes."
    },
    {
      "title": "ImageNet-D: A new challenging robustness dataset inspired by domain adaptation",
      "url": "/research/2023/05/05/ImageNet-D-A-new-challenging-robustness-dataset-inspired-by-domain-adaptation.html",
      "date": "May 5, 2023",
      "summary": "We introduce ImageNet-D, a novel dataset designed to evaluate the robustness of ImageNet-trained models across various domains. With six distinct domains including Real, Painting, Clipart, Sketch, Infograph, and Quick-draw, ImageNet-D challenges even state-of-the-art models, revealing interpretable errors. For instance, our leading EfficientNet-L2 model exhibits a significant performance decrease, dropping from 11.6% on clean ImageNet to 29.2% on the Real domain."
    },
    {
      "title": "Sharpness-Aware Training for Free",
      "url": "/research/2023/03/02/Sharpness-Aware-Training-for-Free.html",
      "date": "March 2, 2023",
      "summary": "Modern deep neural networks (DNNs) excel in performance but often suffer from over-parameterization, leading to increased generalization error without tailored training strategies. Sharpness-Aware Minimization (SAM) has proven effective in reducing generalization error by minimizing sharpness in the loss landscape. However, SAM incurs a significant computational overhead. This paper introduces Sharpness-Aware Training for Free (SAF), which mitigates sharpness at nearly zero additional computational cost. SAF achieves this by preventing sudden drops in loss within sharp local minima during weight updates. A novel trajectory loss, based on KL-divergence between current and past DNN outputs, replaces SAM's sharpness measure, guiding convergence towards flat minima for enhanced generalization. Empirical results demonstrate SAF's effectiveness on ImageNet with comparable computational efficiency to the base optimizer."
    },
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    },
    {
      "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
      "url": "/research/2022/03/19/Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training.html",
      "date": "March 19, 2022",
      "summary": "Sharpness-Aware Minimization (SAM) enhances generalization by optimizing a neighborhood-based perturbed loss, but it doesn't always favor flat minima due to both sharp and flat minima having low perturbed losses. We introduce a new measure, the surrogate gap, reflecting the dominant Hessian eigenvalue at small neighborhood radii, which is simple to compute and can be minimized during training. We propose the Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), an advancement over SAM with minimal additional computational cost. GSAM employs a dual-step approach: first, minimizing the perturbed loss similar to SAM, and second, reducing the surrogate gap without affecting the perturbed loss to target regions with low loss and sharpness, thus achieving superior generalization. GSAM is theoretically robust, showing better convergence and empirical generalization improvements, notably a +3.2% gain over SAM and +5.4% over AdamW in ImageNet accuracy for ViT-B/32."
    },
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    },
    {
      "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
      "url": "/research/2024/02/28/CL-MAE:-Curriculum-Learned-Masked-Autoencoders.html",
      "date": "February 28, 2024",
      "summary": "This paper proposes a curriculum learning approach that updates the masking strategy of Masked Auto Encoders (MAE) to progressively increase the complexity of the self-supervised reconstruction task. To achieve this, a novel learnable masking module is introduced, capable of generating masks of varying complexities, and integrated into masked autoencoders (MAE). This module is jointly trained with the MAE, adjusting its behavior from partner (optimizing the same reconstruction loss) to adversary (optimizing the opposite loss), with a smooth transition regulated by a factor multiplied with the reconstruction loss. This training procedure creates an easy-to-hard curriculum. The Curriculum-Learned Masked Autoencoder (CL-MAE) is trained on ImageNet and demonstrates superior representation learning compared to MAE. Empirical results on five downstream tasks confirm that curriculum learning can successfully self-supervise masked autoencoders."
    },
    {
      "title": "From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning",
      "url": "/research/2023/03/16/From-MNIST-to-ImageNet-and-Back:-Benchmarking-Continual-Curriculum-Learning.html",
      "date": "March 16, 2023",
      "summary": "Continual learning (CL) is a promising trend in machine learning, aiming to develop robust models and strategies for dynamic environments by incorporating new knowledge while retaining past knowledge. CL research is fragmented, with various protocols, tasks, datasets, and metrics, often not reflecting real-world complexity and tailored to specific strategies. This work addresses this gap by introducing two novel CL benchmarks with heterogeneous tasks from six image datasets, varying in complexity and quality. These benchmarks fairly evaluate state-of-the-art CL strategies in scenarios closer to real-world conditions. The benchmarks present tasks in both increasing and decreasing complexity, assessing models' ability to exploit task structure. The work emphasizes a rigorous, reproducible evaluation protocol for measuring generalization and memory retention in models. Experimental results show that popular CL strategies perform poorly on these benchmarks, exhibit high forgetting, and struggle with curriculum task ordering. These findings underscore the need for rigorous comparisons and the development of new CL strategies for complex scenarios."
    },
    {
      "title": "SGDR: Stochastic Gradient Descent With Warm Restarts",
      "url": "/research/2017/05/02/SGDR-Stochastic-Gradient-Descent-With-Warm-Restarts.html",
      "date": "May 2, 2017",
      "summary": "This paper introduces a warm restart technique for stochastic gradient descent aimed at enhancing anytime performance in deep neural network training. It showcases empirical performance improvements on CIFAR-10 and CIFAR-100 datasets, achieving state-of-the-art results with 3.14% and 16.21% error rates, respectively. Additionally, the technique's benefits are demonstrated on an EEG dataset and a downsampled ImageNet dataset."
    },
    {
      "title": "V-Jepa: Latent Video Prediction For Visual Representation Learning",
      "url": "/research/2023/02/15/V-Jepa-Latent-Video-Prediction-For-Visual-Representation-Learning.html",
      "date": "February 15, 2023",
      "summary": "This paper introduces V-JEPA, a self-supervised video learning method that predicts masked spatio-temporal regions in latent space, effectively applying the masked-modelling principle of large language models to video. This approach generates visual features useful across various image and video tasks without needing model adjustment, achieving significant improvements on Kinetics-400 (82.1%) and Something-Something-v2 (71.2%) benchmarks, outperforming prior video models. V-JEPA also excels in motion understanding tasks, surpassing leading image models like DINOv2 and OpenCLIP, and achieves 77.9% on ImageNet classification with video training alone, setting a new standard for video models."
    }
  ],
  "resolution": [
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "This paper introduces PatchDropout, a simple training technique for Vision Transformers ViT that drops random image patches, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "This paper introduces the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a multi-directional Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions."
    },
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/research/2017/08/23/A-downsampled-variant-of-imagenet-as-an-alternative-to-the-cifar-datasets.html",
      "date": "August 23, 2017",
      "summary": "This paper proposes downsampled versions of ImageNet: ImageNet64x64, ImageNet32x32, and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics."
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces TinyViT, a series of compact and efficient vision transformers (ViTs) designed for devices with limited resources. The authors employ a fast distillation framework to transfer knowledge from large pretrained models to smaller ones during pretraining, using sparsified logits from teacher models to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. With increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters, and demonstrates strong transferability across various downstream tasks."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "This research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. The authors validate these approaches using the ImageNet-1k dataset and confirm their findings with the ImageNet-v2 test set, evaluating transfer performance across six additional datasets."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "This paper introduces a novel approach to Radiance Field methods for novel-view synthesis that addresses the challenges of high visual quality, costly training, and real-time rendering of unbounded, complete scenes at 1080p resolution. The authors propose three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. The method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    },
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "This paper introduces DiffBIR, a framework that utilizes pretrained text-to-image diffusion models for blind image restoration. The two-stage pipeline involves pretraining a restoration module on various degradations to enhance real-world applicability, followed by the use of latent diffusion models for realistic restoration. The authors introduce a novel injective modulation sub-network, LAControlNet, for fine-tuning and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, a controllable module allows users to adjust quality and fidelity during the denoising process. Extensive tests demonstrate DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets."
    },
    {
      "title": "Stable Diffusion 3: Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "This paper focuses on improving diffusion models for high-dimensional data like images. The authors enhance noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Their large-scale study shows superior performance over established diffusion methods in high-resolution text-to-image synthesis. A new transformer-based architecture is introduced for better text comprehension, typography, and human preference in text-to-image generation. The findings demonstrate predictable scaling trends, with lower validation loss tied to improved synthesis. The largest models outperform state-of-the-art approaches."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    },
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    }
  ],
  "csaw": [
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "This paper introduces PatchDropout, a simple training technique for Vision Transformers ViT that drops random image patches, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    }
  ],
  "tuning": [
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "This paper introduces PatchDropout, a simple training technique for Vision Transformers ViT that drops random image patches, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, the authors demonstrate the close relationship between these two families. This analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Data augmentation improves generalization, and robustness against image corruptions. However, the use of complex augmentation pipelines typically involves tuning of an enormous number of hyper parameters. RandAugment addresses this by reducing the search space. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets."
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores self-supervision through automatic colorization as an alternative to ImageNet pretraining. Self-supervised training achieved state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. This paper highlights the significance of loss formulation, training specifics, and network architecture when pretraining through colorization. It also revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and feature adaptability upon fine-tuning. The findings suggest that colorization offers a better supervisory signal comparable to various types of ImageNet pretraining."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) uses synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. Given this perspective, a one-instance fractal database (OFDB) is developed where only a single image per category is present. Despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M comparable or superior results are obtaine to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. A ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training. Synthetic images generated by FDSL circumvent issues associated with real images, offering a promising avenue for pre-training general models. Investigations suggest that simple object contours can match fractal-based dataset performance and increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey provides a comprehensive review of transformer-based visual segmentation, a crucial field for various applications. The authors cover the evolution from convolutional methods to vision transformers, offering a unified framework to simplify understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas such as 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. The authors re-evaluate these methods on established datasets, outline current challenges, and suggest future research directions."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates how to achieve state-of-the-art certified adversarial robustness to ℓ2-norm bounded perturbations using only off-the-shelf pretrained models. The authors instantiate the denoised smoothing approach by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This method certifies 71% accuracy on ImageNet under adversarial perturbations constrained to an ℓ2 norm of ε = 0.5, improving upon the prior certified state-of-the-art by 14 percentage points and denoised smoothing by 30 percentage points, without requiring any fine-tuning or retraining of model parameters."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "This research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. The authors validate these approaches using the ImageNet-1k dataset and confirm their findings with the ImageNet-v2 test set, evaluating transfer performance across six additional datasets."
    },
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "This paper introduces DiffBIR, a framework that utilizes pretrained text-to-image diffusion models for blind image restoration. The two-stage pipeline involves pretraining a restoration module on various degradations to enhance real-world applicability, followed by the use of latent diffusion models for realistic restoration. The authors introduce a novel injective modulation sub-network, LAControlNet, for fine-tuning and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, a controllable module allows users to adjust quality and fidelity during the denoising process. Extensive tests demonstrate DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets."
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of Vision Transformers (ViTs) without relying on large-scale pre-training datasets like ImageNet and JFT or requiring modifications to the architecture or loss functions. The authors show that this approach improves ViT performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, while maintaining ViT's attention to relevant regions and robustness, despite ViT's inherent lack of inductive biases and typical dependence on large-scale pre-training."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "The paper introduces a system that trains deep neural networks for object detection using only synthetic images generated with domain randomization. This involves randomizing non-realistic simulator parameters like lighting and object textures, allowing the network to identify essential object features. Their findings show that networks can achieve impressive performance using just synthetic data, and further improve with fine-tuning on real data. This suggests the potential of using cost-effective synthetic data for training, avoiding the need to acquire vast amounts of real-world data or create detailed synthetic environments. They validate their method on car bounding box detection using the KITTI dataset."
    },
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "The paper focuses on evaluating the performance of recent advancements in image transformers, such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, on the Tiny ImageNet dataset for transfer learning tasks. While these models are typically trained on large datasets like ImageNet-21k and then fine-tuned on ImageNet-1k, their assessments often overlook the Tiny ImageNet benchmark. The study updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. This underscores the importance of evaluating these models on the Tiny ImageNet dataset for transfer learning performance."
    },
    {
      "title": "Automated Learning Rate Scheduler for Large-batch",
      "url": "/research/2021/07/13/Automated-Learning-Rate-Scheduler-for-Large-batch.html",
      "date": "July 13, 2021",
      "summary": "Large-batch training is crucial for deep learning with large datasets and models, but requires specific learning rate (LR) schedules for optimal performance, especially under limited training epochs. This work introduces an automated LR scheduling algorithm for large-batch neural network training within a fixed epoch budget, consisting of adaptive warmup and predefined decay phases. The LR is dynamically adjusted based on training loss monitored through Gaussian process smoothing, facilitating low computational overhead. When integrated with adaptive stochastic optimizers like AdamP and LAMB, this scheduler eliminates the need for extensive hyperparameter tuning and achieves competitive or superior results on various image classification tasks across different batch sizes and architectures."
    },
    {
      "title": "Don't Decay the Learning Rate, Increase the Batch Size",
      "url": "/research/2017/11/01/Don't-Decay-the-Learning-Rate,-Increase-the-Batch-Size.html",
      "date": "November 1, 2017",
      "summary": "This study demonstrates that increasing the batch size during training achieves similar learning outcomes as the common practice of decaying the learning rate, applicable to stochastic gradient descent and its variants, including with momentum and Adam optimization. This approach not only matches test accuracies within the same number of epochs but also enhances parallelism and reduces training time due to fewer parameter updates. Efficiency can be further improved by adjusting the learning rate and batch size proportionally, and although increasing the momentum coefficient and scaling the batch size accordingly may slightly lower test accuracy, it enables the use of large batch training without needing to tune hyper-parameters. Using these methods, ResNet-50 was trained on ImageNet to a 76.1% validation accuracy in less than 30 minutes."
    },
    {
      "title": "How to decay your learning rate",
      "url": "/research/2021/05/23/How-to-decay-your-learning-rate.html",
      "date": "May 23, 2021",
      "summary": "Empirical findings suggest that typical fine-tuned learning rate schedules decay the learning rate following weight norm fluctuations. This led to the development of ABEL, an automatic scheduler that adjusts the learning rate based on weight norm changes. ABEL performs comparably to tuned schedules but demonstrates greater robustness to parameter variations. Extensive experiments across various domains reveal that when the weight norm remains stable, simplified schedules yield equivalent performance to complex ones, resembling a constant learning rate with decay towards the end of training."
    },
    {
      "title": "On Layer Normalization in the Transformer Architecture",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architecture.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is widely used in natural language processing. Training it typically requires a carefully designed learning rate warm-up stage, crucial for performance but slowing optimization and requiring more hyperparameter tuning. We provide theoretical insights into the necessity of the warm-up stage, demonstrating that the location of layer normalization affects gradient behavior. For Post-LN Transformers, with layer normalization between residual blocks, large gradients near the output layer at initialization make training unstable without warm-up. Conversely, for Pre-LN Transformers, with layer normalization inside residual blocks, gradients are well-behaved, suggesting the removal of the warm-up stage. Experimental results show that Pre-LN Transformers without warm-up achieve comparable performance with less training time and hyperparameter tuning."
    },
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    },
    {
      "title": "Fine-tuning with Very Large Dropout",
      "url": "/research/2024/03/01/Fine-tuning-with-Very-Large-Dropout.html",
      "date": "March 1, 2024",
      "summary": "This study challenges the notion that machine learning practices assume training and testing data have the same distribution. It explores the effectiveness of high dropout rates, as opposed to ensemble techniques, in developing rich data representations suitable for multiple distribution scenarios. These representations surpass those achieved by traditional in-distribution performance regularization and the implicit sparsity induced by common stochastic gradient methods. While training deep networks from scratch with high dropout rates is impractical, fine-tuning pre-trained models under these conditions is feasible and yields better out-of-distribution performance than ensembles and model averaging techniques like model soups. This finding is significant due to the growing relevance of fine-tuning with large pre-trained models, offering insights into the nature of rich representations and the linear characteristics of fine-tuning large networks with small datasets."
    },
    {
      "title": "Suppressing Pink Elephants with Direct Principle Feedback",
      "url": "/research/2024/02/13/Suppressing-Pink-Elephants-with-Direct-Principle-Feedback.html",
      "date": "February 13, 2024",
      "summary": "Existing methods for controlling language models focus on training desired behaviors, but often lack the flexibility needed for diverse applications. The authors address this with the Pink Elephant Problem, demonstrating the need for language models to adapt to different contexts by avoiding certain topics (the Pink Elephant) in favor of others (Grey Elephant). The authors introduce Direct Principle Feedback (DPF), an adaptation of Constitutional AI that improves control by directly applying critiques and revisions without ranking responses. Our study shows that a 13B LLaMA 2 model fine-tuned with DPF on a synthetic dataset outperforms existing models and matches GPT-4 in managing the Pink Elephant Problem."
    }
  ],
  "budget": [
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "This paper introduces PatchDropout, a simple training technique for Vision Transformers ViT that drops random image patches, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "This study introduces a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. The method involves creating spaces for network design that parameterize multiple network populations. A low-dimensional, efficient design space called RegNet is developed, based on modeling network widths and depths through a quantized linear function. The analysis of the RegNet space challenges existing design practices, offering simpler and faster networks effective across various computational budgets. RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs under similar conditions."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "This paper explores the interactions between training data amount, model regularization or data augmentation (AugReg), model size, and compute budget for Vision Transformers (ViT) in various vision tasks. The authors find that using more compute and AugReg can achieve the same performance as training with significantly more data. They demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    },
    {
      "title": "Automated Learning Rate Scheduler for Large-batch",
      "url": "/research/2021/07/13/Automated-Learning-Rate-Scheduler-for-Large-batch.html",
      "date": "July 13, 2021",
      "summary": "Large-batch training is crucial for deep learning with large datasets and models, but requires specific learning rate (LR) schedules for optimal performance, especially under limited training epochs. This work introduces an automated LR scheduling algorithm for large-batch neural network training within a fixed epoch budget, consisting of adaptive warmup and predefined decay phases. The LR is dynamically adjusted based on training loss monitored through Gaussian process smoothing, facilitating low computational overhead. When integrated with adaptive stochastic optimizers like AdamP and LAMB, this scheduler eliminates the need for extensive hyperparameter tuning and achieves competitive or superior results on various image classification tasks across different batch sizes and architectures."
    },
    {
      "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
      "url": "/research/2024/03/13/Simple-and-Scalable-Strategies-to-Continually-Pre-train-Large-Language-Models.html",
      "date": "March 13, 2024",
      "summary": "Large language models (LLMs) often require re-training on new data, consuming extensive computational resources. We present an efficient method that combines learning rate re-warming, re-decaying, and data replay, effectively maintaining performance without full re-training. This approach works well across different data distributions, including minor shifts (English→English) and significant shifts (English→German), tested up to 405M and 10B parameter models. Our findings suggest that continual learning strategies can update LLMs with minimal computational cost, rivaling traditional re-training methods. Additionally, we propose alternatives to the cosine learning rate schedule to reduce forgetting, offering more flexibility in learning without a fixed token budget."
    }
  ],
  "shape": [
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/research/2021/05/03/Robust-and-Generalizable-Visual-Representation-Learning-via-Random-Convolutions.html",
      "date": "May 3, 2021",
      "summary": "This study introduces random convolutions for data augmentation. Random convolutions approximately preserve shape while distorting local textures. This method significantly enhances performance on domain generalization and robustness / corruption benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. The authors find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. "
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Incorporating diverse image styles into CNN training data reduces texture bias, enhances shape recognition, and improves resilience against common image corruptions. This is typically explained as decreasing model reliance on high frequency texture information. This paper challenges this explaniation through large scale testing with natural images, edge information, and stylization, finding no direct link between shape bias and robustness. The enhanced corruption robustness is instead attributed to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/research/2021/08/31/Universal-Adversarial-Robustness-of-Texture-and-Shape-biased-Models.html",
      "date": "August 31, 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "This study challenges the traditional understanding of how Convolutional Neural Networks (CNNs) recognize objects, revealing that they are more biased towards recognizing textures rather than learning complex shapes. The authors found that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, training a ResNet-50 on a stylized version of ImageNet, designed to emphasize shape, aligned the network's performance more closely with human behavior. This shape-based training matched human performance in controlled experiments and enhanced object detection and robustness against image distortions, highlighting the benefits of shape-based representations in visual recognition systems."
    },
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    },
    {
      "title": "Transformers with Learnable Activation Functions",
      "url": "/research/2023/02/14/Transformers-with-Learnable-Activation-Functions.html",
      "date": "February 14, 2023",
      "summary": "Activation functions significantly affect model performance by reducing data complexity, yet their selection in Transformer-based language models is often overlooked. This paper explores the impact of using rational activation functions (RAFs), which unlike fixed activation functions (FAFs), can learn optimal functions from data. Our experiments demonstrate that Transformer models with RAFs (RAFT) outperform those with FAFs (FAFT), achieving a 5.71 point higher score on the GLUE benchmark with only 100 training examples and a 2.05 point increase on SQuAD with full data. The varied shapes of learned RAFs across layers and tasks suggest a new method for analyzing and understanding large pre-trained language models."
    }
  ],
  "distort": [
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/research/2021/05/03/Robust-and-Generalizable-Visual-Representation-Learning-via-Random-Convolutions.html",
      "date": "May 3, 2021",
      "summary": "This study introduces random convolutions for data augmentation. Random convolutions approximately preserve shape while distorting local textures. This method significantly enhances performance on domain generalization and robustness / corruption benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods."
    }
  ],
  "texture": [
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/research/2021/05/03/Robust-and-Generalizable-Visual-Representation-Learning-via-Random-Convolutions.html",
      "date": "May 3, 2021",
      "summary": "This study introduces random convolutions for data augmentation. Random convolutions approximately preserve shape while distorting local textures. This method significantly enhances performance on domain generalization and robustness / corruption benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. The authors find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. "
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, this paper introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, the authors enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. This simple method surpasses the current state-of-the-art in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Incorporating diverse image styles into CNN training data reduces texture bias, enhances shape recognition, and improves resilience against common image corruptions. This is typically explained as decreasing model reliance on high frequency texture information. This paper challenges this explaniation through large scale testing with natural images, edge information, and stylization, finding no direct link between shape bias and robustness. The enhanced corruption robustness is instead attributed to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/research/2021/08/31/Universal-Adversarial-Robustness-of-Texture-and-Shape-biased-Models.html",
      "date": "August 31, 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "This study challenges the traditional understanding of how Convolutional Neural Networks (CNNs) recognize objects, revealing that they are more biased towards recognizing textures rather than learning complex shapes. The authors found that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, training a ResNet-50 on a stylized version of ImageNet, designed to emphasize shape, aligned the network's performance more closely with human behavior. This shape-based training matched human performance in controlled experiments and enhanced object detection and robustness against image distortions, highlighting the benefits of shape-based representations in visual recognition systems."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "The paper introduces a system that trains deep neural networks for object detection using only synthetic images generated with domain randomization. This involves randomizing non-realistic simulator parameters like lighting and object textures, allowing the network to identify essential object features. Their findings show that networks can achieve impressive performance using just synthetic data, and further improve with fine-tuning on real data. This suggests the potential of using cost-effective synthetic data for training, avoiding the need to acquire vast amounts of real-world data or create detailed synthetic environments. They validate their method on car bounding box detection using the KITTI dataset."
    },
    {
      "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
      "url": "/research/2021/07/24/The-Many-Faces-of-Robustness-A-Critical-Analysis-of-Out-of-Distribution-Generalization.html",
      "date": "July 24, 2021",
      "summary": "We present four novel real-world distribution shift datasets encompassing changes in image style, blurriness, location, camera settings, and more. Evaluating existing methods for enhancing out-of-distribution robustness, we discover that employing larger models and artificial data augmentations can enhance robustness against real-world distribution shifts, contradicting prior claims. Our findings demonstrate that improvements in artificial robustness benchmarks can indeed transfer to real-world distribution shifts, contrary to prior assumptions. Additionally, we introduce a novel data augmentation technique that surpasses models pretrained with significantly more labeled data, emphasizing its efficacy in addressing real-world distribution shifts. While some methods consistently mitigate texture and local image statistics shifts, they fail to address other shifts like geographic changes. Our results underscore the necessity for future research to examine multiple distribution shifts concurrently, as no single method consistently improves robustness across all evaluated scenarios."
    }
  ],
  "corruption": [
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/research/2021/05/03/Robust-and-Generalizable-Visual-Representation-Learning-via-Random-Convolutions.html",
      "date": "May 3, 2021",
      "summary": "This study introduces random convolutions for data augmentation. Random convolutions approximately preserve shape while distorting local textures. This method significantly enhances performance on domain generalization and robustness / corruption benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods."
    },
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/research/2019/04/27/Benchmarking-Neural-Network-Robustness-to-Common-Corruptions-and-Surface-Variations.html",
      "date": "April 27, 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and IMAGENET-P, which assesses perturbation robustness."
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/ON-INTERACTION-BETWEEN-AUGMENTATIONS-AND-CORRUPTIONS-IN-NATURAL-CORRUPTION-ROBUSTNESS.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, the correlation between data augmentations and test-time corruptions remains unclear. This paper introduces Minimal Sample Distance, and demonstrates a strong correlation between augmentation-corruption similarity and performance. Authors argue that training with augmentations that are perceptually similar to corruptions enhances test error."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Data augmentation improves generalization, and robustness against image corruptions. However, the use of complex augmentation pipelines typically involves tuning of an enormous number of hyper parameters. RandAugment addresses this by reducing the search space. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of Vision Transformer (ViT) variants against common corruptions. The benchmarking reveals that overlapping patch embedding and convolutional feed-forward networks (FFNs) significantly enhance ViT robustness. The study also scrutinizes the effectiveness of CNN-based data augmentation strategies when applied to ViTs, finding that adversarial noise training is effective, while fourier-domain augmentation falls short. A new conditional method for generating dynamic augmentation parameters based on input images is proposed, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Incorporating diverse image styles into CNN training data reduces texture bias, enhances shape recognition, and improves resilience against common image corruptions. This is typically explained as decreasing model reliance on high frequency texture information. This paper challenges this explaniation through large scale testing with natural images, edge information, and stylization, finding no direct link between shape bias and robustness. The enhanced corruption robustness is instead attributed to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding",
      "url": "/research/2021/11/16/Improved-Robustness-of-Vision-Transformer-via-PreLayerNorm-in-Patch-Embedding.html",
      "date": "November 16, 2021",
      "summary": "Recent advancements in vision transformers (ViTs) have shown superior performance across diverse visual tasks, surpassing convolutional neural networks (CNNs). Given ViT's distinct architecture, understanding its behavior and reliability is imperative. This paper investigates ViT's robustness by comparing it with CNNs under various image corruptions relevant to real-world vision tasks. While ViT generally exhibits comparable or improved robustness over CNNs, it consistently underperforms in contrast enhancement tasks. Analysis suggests that positional embedding in ViT's patch embedding may malfunction with color scale changes. We propose PreLayerNorm, a modified patch embedding structure, to address this issue and ensure scale-invariant behavior in ViT. ViT with PreLayerNorm demonstrates enhanced robustness across various corruptions, particularly in contrast-varying environments."
    },
    {
      "title": "Learning and Leveraging World Models in Visual Representation Learning",
      "url": "/research/2024/03/01/Learning-and-Leveraging-World-Models-in-Visual-Representation-Learning.html",
      "date": "March 1, 2024",
      "summary": "The Joint-Embedding Predictive Architecture (JEPA) is a self-supervised method previously used for predicting missing input parts. This study expands JEPA to predict a wider range of corruptions by introducing Image World Models (IWM), which learn the effects of global photometric transformations in latent space. Key to effective IWM learning are conditioning, prediction difficulty, and model capacity. The study demonstrates that IWM's predictive world model, when fine-tuned, can tackle diverse tasks and either matches or outperforms existing self-supervised techniques. Additionally, it enables control over the abstraction level of learned representations, achieving either invariant or equivariant representations, similar to contrastive methods or masked image modeling, respectively."
    }
  ],
  "sketch": [
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/research/2021/05/03/Robust-and-Generalizable-Visual-Representation-Learning-via-Random-Convolutions.html",
      "date": "May 3, 2021",
      "summary": "This study introduces random convolutions for data augmentation. Random convolutions approximately preserve shape while distorting local textures. This method significantly enhances performance on domain generalization and robustness / corruption benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods."
    },
    {
      "title": "ImageNet-D: A new challenging robustness dataset inspired by domain adaptation",
      "url": "/research/2023/05/05/ImageNet-D-A-new-challenging-robustness-dataset-inspired-by-domain-adaptation.html",
      "date": "May 5, 2023",
      "summary": "We introduce ImageNet-D, a novel dataset designed to evaluate the robustness of ImageNet-trained models across various domains. With six distinct domains including Real, Painting, Clipart, Sketch, Infograph, and Quick-draw, ImageNet-D challenges even state-of-the-art models, revealing interpretable errors. For instance, our leading EfficientNet-L2 model exhibits a significant performance decrease, dropping from 11.6% on clean ImageNet to 29.2% on the Real domain."
    }
  ],
  "vmamba": [
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "This paper introduces the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a multi-directional Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions."
    }
  ],
  "fields": [
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "This paper introduces the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a multi-directional Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. The authors find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. "
    }
  ],
  "addition": [
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "This paper introduces the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a multi-directional Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions."
    }
  ],
  "scan": [
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "This paper introduces the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a multi-directional Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions."
    }
  ],
  "csm": [
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "This paper introduces the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a multi-directional Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions."
    }
  ],
  "sensitivity": [
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "This paper introduces the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a multi-directional Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions."
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "This paper introduces Batch Normalization, an online normalization technique which calculates mean and std across the batch dimension. Batch Norm improves training efficiency, allowing for higher learning rates, and decreasing hyper-parameter sensitivity, and sometimes removing the need for Dropout regularization. This paper suggests Batch Norm improves internal covariant shift, however later works have questioned this."
    },
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    },
    {
      "title": "ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Network",
      "url": "/research/2021/06/29/ASAM-Adaptive-Sharpness-Aware-Minimization-for-Scale-Invariant-Learning-of-Deep-Neural-Network.html",
      "date": "June 29, 2021",
      "summary": "Recent advances in learning algorithms have highlighted the importance of the sharpness of the loss surface as a key indicator of the generalization gap, achieving state-of-the-art results. However, traditional sharpness measures, defined within a fixed region, suffer from sensitivity to parameter scaling, weakening their predictive value for the generalization gap. This paper introduces a scale-invariant measure, adaptive sharpness, along with a new generalization bound. We present a new learning method, adaptive sharpness-aware minimization (ASAM), which leverages this bound. Our experiments across various benchmark datasets demonstrate that ASAM significantly enhances model generalization."
    }
  ],
  "traversal": [
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "This paper introduces the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a multi-directional Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions."
    }
  ],
  "mode": [
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a Vision archetecture that uses bidirectional Mamba blocks, challenging the necessity of self-attention in vision. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient."
    }
  ],
  "vim": [
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a Vision archetecture that uses bidirectional Mamba blocks, challenging the necessity of self-attention in vision. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient."
    }
  ],
  "coco": [
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a Vision archetecture that uses bidirectional Mamba blocks, challenging the necessity of self-attention in vision. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient."
    },
    {
      "title": "Deep Residual Learning for Image Recognition",
      "url": "/research/2015/12/10/Deep-Residual-Learning-for-Image-Recognition.html",
      "date": "December 10, 2015",
      "summary": "We introduce a residual learning framework to train deeper neural networks effectively. By reformulating layers to learn residual functions relative to inputs, our approach facilitates optimization and achieves higher accuracy with increased depth. Evaluations on ImageNet demonstrate the effectiveness of residual networks up to 152 layers deep, outperforming previous architectures. Our method achieves 3.57% error on the ImageNet test set, winning 1st place in the ILSVRC 2015 classification task. Additionally, our deep representations lead to a 28% improvement on the COCO object detection dataset. In the ILSVRC & COCO 2015 competitions, our approach secured 1st place in ImageNet detection, localization, COCO detection, and segmentation tasks."
    }
  ],
  "ade20k": [
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a Vision archetecture that uses bidirectional Mamba blocks, challenging the necessity of self-attention in vision. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient."
    }
  ],
  "deit": [
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a Vision archetecture that uses bidirectional Mamba blocks, challenging the necessity of self-attention in vision. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient."
    },
    {
      "title": "Training data-efficient image transformers & distillation through attention",
      "url": "/research/2021/01/15/Training-data-efficient-image-transformers-&-distillation-through-attention.html",
      "date": "January 15, 2021",
      "summary": "This paper introduces DeiT, an efficient method for training vision transformers. The authors present a unique teacher-student strategy for transformers, leveraging a distillation token for efficient learning from a convolutional network teacher. This method achieves performance on par with convolutional networks, with up to 85.2% accuracy on ImageNet, and demonstrates effective transferability to other tasks."
    },
    {
      "title": "DeiT III: Revenge of the ViT",
      "url": "/research/2023/01/18/DeiT-III-Revenge-of-the-ViT.html",
      "date": "January 18, 2023",
      "summary": "This paper explores the supervised training of Vision Transformers (ViTs) using a simplified training approach adapted from ResNet-50 that includes a novel data-augmentation method with just 3 augmentations. The study demonstrates that this method significantly improves ViTs' performance in image classification, transfer learning, and semantic segmentation over previous supervised training techniques. Moreover, it shows ViTs' performance can match newer architectures, providing a new benchmark for evaluating self-supervised methods on ViTs."
    },
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "The paper focuses on evaluating the performance of recent advancements in image transformers, such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, on the Tiny ImageNet dataset for transfer learning tasks. While these models are typically trained on large datasets like ImageNet-21k and then fine-tuned on ImageNet-1k, their assessments often overlook the Tiny ImageNet benchmark. The study updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. This underscores the importance of evaluating these models on the Tiny ImageNet dataset for transfer learning performance."
    }
  ],
  "downsample": [
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/research/2017/08/23/A-downsampled-variant-of-imagenet-as-an-alternative-to-the-cifar-datasets.html",
      "date": "August 23, 2017",
      "summary": "This paper proposes downsampled versions of ImageNet: ImageNet64x64, ImageNet32x32, and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics."
    },
    {
      "title": "SGDR: Stochastic Gradient Descent With Warm Restarts",
      "url": "/research/2017/05/02/SGDR-Stochastic-Gradient-Descent-With-Warm-Restarts.html",
      "date": "May 2, 2017",
      "summary": "This paper introduces a warm restart technique for stochastic gradient descent aimed at enhancing anytime performance in deep neural network training. It showcases empirical performance improvements on CIFAR-10 and CIFAR-100 datasets, achieving state-of-the-art results with 3.14% and 16.21% error rates, respectively. Additionally, the technique's benefits are demonstrated on an EEG dataset and a downsampled ImageNet dataset."
    }
  ],
  "proposes": [
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/research/2017/08/23/A-downsampled-variant-of-imagenet-as-an-alternative-to-the-cifar-datasets.html",
      "date": "August 23, 2017",
      "summary": "This paper proposes downsampled versions of ImageNet: ImageNet64x64, ImageNet32x32, and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics."
    }
  ],
  "imagenet64x64": [
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/research/2017/08/23/A-downsampled-variant-of-imagenet-as-an-alternative-to-the-cifar-datasets.html",
      "date": "August 23, 2017",
      "summary": "This paper proposes downsampled versions of ImageNet: ImageNet64x64, ImageNet32x32, and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics."
    }
  ],
  "imagenet32x32": [
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/research/2017/08/23/A-downsampled-variant-of-imagenet-as-an-alternative-to-the-cifar-datasets.html",
      "date": "August 23, 2017",
      "summary": "This paper proposes downsampled versions of ImageNet: ImageNet64x64, ImageNet32x32, and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics."
    }
  ],
  "imagenet16x16": [
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/research/2017/08/23/A-downsampled-variant-of-imagenet-as-an-alternative-to-the-cifar-datasets.html",
      "date": "August 23, 2017",
      "summary": "This paper proposes downsampled versions of ImageNet: ImageNet64x64, ImageNet32x32, and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics."
    }
  ],
  "class": [
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/research/2017/08/23/A-downsampled-variant-of-imagenet-as-an-alternative-to-the-cifar-datasets.html",
      "date": "August 23, 2017",
      "summary": "This paper proposes downsampled versions of ImageNet: ImageNet64x64, ImageNet32x32, and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This study suggest that label smoothing not only boosts generalization but also enhances model calibration. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. Label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results. The method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. A convolutional neural network (CNN) was trained on over a million color images, operating as a feed-forward pass at test time. The algorithm's effectiveness was validated through a colorization Turing test, where it deceived participants in 32% of trials. The paper also illustrates how colorization can serve as an effective pretraining task for self-supervised feature learning, achieving state-of-the-art results on several benchmarks."
    },
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "This paper introduces FineAction, a large-scale, fine-grained video dataset designed to address the limitations of current temporal action localization (TAL) benchmarks, which rely on coarse action classes and lead to model overfitting and ambiguous annotations. FineAction contains 103K instances across 106 action categories in 17K videos, offering a diverse and densely annotated dataset with co-occurring actions that pose new challenges for TAL. The authors evaluate popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and propose a baseline method achieving a 13.17% mAP. FineAction aims to advance TAL research and is accessible online."
    },
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "This paper introduces the Kinetics Human Action Video dataset. The authors note that the limited number of videos in existing datasets like UCF-101 and HMDB-51 hinders the ability to effectively evaluate architectures due to similar performance across these small benchmarks. Kinetics contains 400 classes with over 400 video clips per class from challenging YouTube videos. The paper analyzes the impact of this larger dataset on the performance of existing architectures as well as the benefits of pre-training models on Kinetics. The authors introduce the Two-Stream Inflated 3D ConvNet (I3D), an extension of 2D ConvNets to 3D for improved video feature extraction. When pre-trained on Kinetics, this I3D model sets new benchmarks for action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    },
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "The paper focuses on evaluating the performance of recent advancements in image transformers, such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, on the Tiny ImageNet dataset for transfer learning tasks. While these models are typically trained on large datasets like ImageNet-21k and then fine-tuned on ImageNet-1k, their assessments often overlook the Tiny ImageNet benchmark. The study updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. This underscores the importance of evaluating these models on the Tiny ImageNet dataset for transfer learning performance."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Deep learning via Hessian-free optimization",
      "url": "/research/2010/06/21/Deep-learning-via-Hessian-free-optimization.html",
      "date": "June 21, 2010",
      "summary": "We present a novel 2nd-order optimization method inspired by the Hessian-free approach, applied to deep auto-encoders. Achieving superior results without pre-training compared to Hinton & Salakhutdinov (2006), our method is practical, user-friendly, scalable to large datasets, and versatile across various model classes. Additionally, we address the challenge of pathological curvature in deep learning, highlighting how our method effectively mitigates this issue."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "url": "/research/2014/02/19/Exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks.html",
      "date": "February 19, 2014",
      "summary": "This study aims to enhance our comprehension of deep learning dynamics by analyzing deep linear neural networks. Despite their linear input-output mapping, these networks display nonlinear gradient descent dynamics, resulting in phenomena like prolonged plateaus and swift transitions to improved solutions. Through analytical exploration, we reveal that as network depth increases indefinitely, learning speed can remain finite under specific initial conditions. Unsupervised pretraining, under certain data conditions, can discover these favorable initial conditions, unlike random Gaussian initializations. We introduce a novel class of random orthogonal initial conditions that, akin to unsupervised pretraining, enable depth-independent learning. These conditions also ensure gradient propagation in deep nonlinear networks, particularly when operating at the edge of chaos."
    },
    {
      "title": "HYPO: Hyperspherical Out-of-Distribution Generalization",
      "url": "/research/2024/03/19/HYPO-Hyperspherical-Out-of-Distribution-Generalization.html",
      "date": "March 19, 2024",
      "summary": "We propose HYPO, a novel framework for out-of-distribution (OOD) generalization in machine learning, which learns domain-invariant features in a hyperspherical space. Our method focuses on aligning features of the same class across different domains close to their class prototypes and separating different class prototypes maximally. We offer theoretical justifications for its improvement on OOD generalization and show through experiments on OOD benchmarks that HYPO outperforms existing baselines with superior performance."
    },
    {
      "title": "Neural Networks Learn Statistics of Increasing Complexity",
      "url": "/research/2024/02/13/Neural-Networks-Learn-Statistics-of-Increasing-Complexity.html",
      "date": "February 13, 2024",
      "summary": "The distributional simplicity bias (DSB) theory suggests neural networks first learn basic patterns in data before understanding more complex correlations. We provide new evidence supporting DSB, showing networks initially excel with data matching training set's simple statistics but this ability diminishes later. Extending DSB to discrete domains, we demonstrate an equivalence between n-gram frequencies and vector moments, also observing this bias in large language models (LLMs). Additionally, by adjusting low-level statistics of images to resemble another class, we reveal that networks in early training phases misclassify these edited images as if they belonged to the target class."
    },
    {
      "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum",
      "url": "/research/2023/09/30/When-to-Learn-What:-Model-Adaptive-Data-Augmentation-Curriculum.html",
      "date": "September 30, 2023",
      "summary": "Data augmentation (DA) improves neural network generalization by enforcing invariances and symmetries to predefined transformations applied to input data. However, fixed augmentation policies affect samples differently at various training stages, and existing approaches cannot adapt policies to individual samples and the training model. This paper proposes Model-Adaptive Data Augmentation (MADAug) which trains an augmentation policy network to determine when to learn what. Unlike previous work, MADAug selects augmentation operators for each input image with a model-adaptive policy that varies between training stages, creating an optimized data augmentation curriculum. The policy is trained using a bi-level optimization scheme to minimize validation-set loss of a model trained with policy-produced augmentations. Extensive evaluations on multiple image classification tasks and network architectures show that MADAug outperforms or matches existing DA approaches, enhances fairness by improving all classes, particularly the difficult ones, and performs better when transferred to fine-grained datasets. Additionally, the auto-optimized policy in MADAug gradually increases perturbations, forming an easy-to-hard curriculum."
    }
  ],
  "hyperparameters": [
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/research/2017/08/23/A-downsampled-variant-of-imagenet-as-an-alternative-to-the-cifar-datasets.html",
      "date": "August 23, 2017",
      "summary": "This paper proposes downsampled versions of ImageNet: ImageNet64x64, ImageNet32x32, and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics."
    }
  ],
  "characteristic": [
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/research/2017/08/23/A-downsampled-variant-of-imagenet-as-an-alternative-to-the-cifar-datasets.html",
      "date": "August 23, 2017",
      "summary": "This paper proposes downsampled versions of ImageNet: ImageNet64x64, ImageNet32x32, and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics."
    },
    {
      "title": "Fine-tuning with Very Large Dropout",
      "url": "/research/2024/03/01/Fine-tuning-with-Very-Large-Dropout.html",
      "date": "March 1, 2024",
      "summary": "This study challenges the notion that machine learning practices assume training and testing data have the same distribution. It explores the effectiveness of high dropout rates, as opposed to ensemble techniques, in developing rich data representations suitable for multiple distribution scenarios. These representations surpass those achieved by traditional in-distribution performance regularization and the implicit sparsity induced by common stochastic gradient methods. While training deep networks from scratch with high dropout rates is impractical, fine-tuning pre-trained models under these conditions is feasible and yields better out-of-distribution performance than ensembles and model averaging techniques like model soups. This finding is significant due to the growing relevance of fine-tuning with large pre-trained models, offering insights into the nature of rich representations and the linear characteristics of fine-tuning large networks with small datasets."
    }
  ],
  "std": [
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "This paper introduces Batch Normalization, an online normalization technique which calculates mean and std across the batch dimension. Batch Norm improves training efficiency, allowing for higher learning rates, and decreasing hyper-parameter sensitivity, and sometimes removing the need for Dropout regularization. This paper suggests Batch Norm improves internal covariant shift, however later works have questioned this."
    },
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/research/2017/11/06/Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization.html",
      "date": "November 6, 2017",
      "summary": "This paper itroduces Instance Normalization, an online normalization technique that calculates the mean and std across individual data instances. This is shown to work well in situations where there is very high vatiation within batches, such as in style transfer."
    }
  ],
  "dimension": [
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "This paper introduces Batch Normalization, an online normalization technique which calculates mean and std across the batch dimension. Batch Norm improves training efficiency, allowing for higher learning rates, and decreasing hyper-parameter sensitivity, and sometimes removing the need for Dropout regularization. This paper suggests Batch Norm improves internal covariant shift, however later works have questioned this."
    },
    {
      "title": "Learning RelU Networks To High Uniform Accuracy Is Intractable",
      "url": "/research/2023/02/28/Learning-Relu-Networks-To-High-Uniform-Accuracy-Is-Intractable.html",
      "date": "February 28, 2023",
      "summary": "Statistical learning theory provides guidelines on the number of training samples needed for achieving desired accuracy in learning problems. However, this is not always adequate, particularly in security-sensitive areas or computational sciences, where uniform accuracy across all inputs is necessary. This paper quantifies the training samples required for uniform accuracy in learning problems involving ReLU neural networks, revealing that the number of samples needed exponentially increases with the network's depth and input dimension."
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, the authors demonstrate the close relationship between these two families. This analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "This paper investigates the geometric and statistical properties of representations across layers in large transformers used for self-supervised learning on various data types. The authors observe common evolution patterns, with data manifolds expanding initially, contracting at intermediate layers, and the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. They find that semantic information peaks after initial expansion, a trend consistent across models and datasets. The study proposes an unsupervised method to identify layers richest in semantic content, suggesting that those at a relative ID minimum are optimal for downstream tasks."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article provides an extensive review of vision transformer techniques applied specifically to human action recognition. These action transformers are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also examines optimizing spatio-temporal attention in transformers for longer sequences and different learning strategies like self-supervised and zero-shot learning with their respective loss functions. Additionally, it assesses progress in benchmark evaluation scores and discusses challenges and future directions in this research area."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    },
    {
      "title": "Towards a Unified Theoretical Understanding of Non-contrastive Learning via Rank Differential Mechanism",
      "url": "/research/2023/03/04/Towards-a-Unified-Theoretical-Understanding-of-Non-contrastive-Learning-via-Rank-Differential-Mechanism.html",
      "date": "March 4, 2023",
      "summary": "Recent non-contrastive learning methods like BYOL, SimSiam, SwAV, and DINO have shown that asymmetric architectural designs can achieve good self-supervised visual learning performance by aligning positive pairs alone, despite a lack of unified theoretical understanding on how these designs prevent feature collapse. This study introduces the Rank Differential Mechanism (RDM) as a unified theoretical framework for non-contrastive learning. RDM demonstrates that these asymmetric designs maintain a consistent rank difference in output features, enhancing effective dimensionality and preventing feature collapse. Unlike previous theories, RDM applies to designs both with and without a predictor, offering a comprehensive understanding of non-contrastive learning methods and guiding the development of new variants. Experiments confirm that these variants perform comparably, if not better than, existing methods on benchmark datasets."
    }
  ],
  "improves": [
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "This paper introduces Batch Normalization, an online normalization technique which calculates mean and std across the batch dimension. Batch Norm improves training efficiency, allowing for higher learning rates, and decreasing hyper-parameter sensitivity, and sometimes removing the need for Dropout regularization. This paper suggests Batch Norm improves internal covariant shift, however later works have questioned this."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, enhances vision tasks using Vision transformers by masking and reconstructing parts of an image. The compatibility of MIM with CNNs and its operational principle are unclear. this study reveals that MIM improves generalized feature extraction through middle-order patch interactions and introduces Architecture-Agnostic Masked Image Modeling (A2MIM), compatible with both Transformers and CNNs. Extensive testing demonstrates A2MIM's ability to enhance representation learning and transferability to various tasks without specialized modifications."
    }
  ],
  "remove": [
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "This paper introduces Batch Normalization, an online normalization technique which calculates mean and std across the batch dimension. Batch Norm improves training efficiency, allowing for higher learning rates, and decreasing hyper-parameter sensitivity, and sometimes removing the need for Dropout regularization. This paper suggests Batch Norm improves internal covariant shift, however later works have questioned this."
    }
  ],
  "peturbation": [
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/research/2019/04/27/Benchmarking-Neural-Network-Robustness-to-Common-Corruptions-and-Surface-Variations.html",
      "date": "April 27, 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and IMAGENET-P, which assesses perturbation robustness."
    }
  ],
  "perturbation": [
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/research/2019/04/27/Benchmarking-Neural-Network-Robustness-to-Common-Corruptions-and-Surface-Variations.html",
      "date": "April 27, 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and IMAGENET-P, which assesses perturbation robustness."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. The authors find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. "
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/research/2021/08/31/Universal-Adversarial-Robustness-of-Texture-and-Shape-biased-Models.html",
      "date": "August 31, 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates how to achieve state-of-the-art certified adversarial robustness to ℓ2-norm bounded perturbations using only off-the-shelf pretrained models. The authors instantiate the denoised smoothing approach by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This method certifies 71% accuracy on ImageNet under adversarial perturbations constrained to an ℓ2 norm of ε = 0.5, improving upon the prior certified state-of-the-art by 14 percentage points and denoised smoothing by 30 percentage points, without requiring any fine-tuning or retraining of model parameters."
    },
    {
      "title": "Learning Rate Perturbation: A Generic Plugin of Learning Rate schedule towards Flatter Local Minima",
      "url": "/research/2022/08/25/Learning-Rate-Perturbation-A-Generic-Plugin-of-Learning-Rate-schedule-towards-Flatter-Local-Minima.html",
      "date": "August 25, 2022",
      "summary": "Learning rate is crucial in neural network training, yet existing schedules lack theoretical backing, often leading to suboptimal choices made through trial and error. To address this, we propose LEAP, a plugin enhancing various learning rate schedules by introducing perturbations. This simple yet effective strategy favors flat minima, ensuring better generalization. Extensive experiments demonstrate LEAP's ability to improve performance across diverse datasets and learning rate schedules, including constant ones."
    },
    {
      "title": "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization",
      "url": "/research/2023/07/04/Gradient-Norm-Aware-Minimization-Seeks-First-Order-Flatness-and-Improves-Generalization.html",
      "date": "July 4, 2023",
      "summary": "Recent advancements have highlighted the effectiveness of flat minima in enhancing generalization, particularly through Sharpness-Aware Minimization (SAM). However, existing definitions of flatness, such as zeroth-order flatness, have limitations in discerning between minima with low and high generalization errors. To address this, we propose first-order flatness, which considers maximal gradient norm within a perturbation radius. We introduce Gradient norm Aware Minimization (GAM) as a novel training approach to achieve uniformly small curvature across all directions. Experimental results demonstrate GAM's ability to enhance generalization compared to standard optimizers like SGD and AdamW across various datasets and networks. Moreover, GAM facilitates SAM in identifying flatter minima, leading to improved generalization."
    },
    {
      "title": "AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks",
      "url": "/research/2023/03/01/AdaSAM-Boosting-Sharpness-Aware-Minimization-with-Adaptive-Learning-Rate-and-Momentum-for-Training-Deep-Neural-Networks.html",
      "date": "March 1, 2023",
      "summary": "The Sharpness Aware Minimization (SAM) optimizer, known for enhancing the generalization of deep neural networks by introducing extra perturbation steps, is further developed into AdaSAM by integrating adaptive learning rates and momentum acceleration. Although AdaSAM has been empirically applied to large-scale networks, a theoretical understanding of its performance, considering the complexity of its components, was lacking. This study presents a theoretical analysis of AdaSAM's convergence in stochastic non-convex settings, demonstrating a convergence rate of O(1/√bT) that scales linearly with mini-batch size. By introducing a delayed second-order momentum term, the study successfully decouples and analyzes the intertwined effects of stochastic gradients, adaptive learning rates, and perturbations. This is the first work to offer a detailed convergence rate for SAM with adaptive mechanisms. Experimental results on various NLP tasks indicate AdaSAM's superior performance over SGD, AMSGrad, and SAM optimizers."
    },
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    },
    {
      "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum",
      "url": "/research/2023/09/30/When-to-Learn-What:-Model-Adaptive-Data-Augmentation-Curriculum.html",
      "date": "September 30, 2023",
      "summary": "Data augmentation (DA) improves neural network generalization by enforcing invariances and symmetries to predefined transformations applied to input data. However, fixed augmentation policies affect samples differently at various training stages, and existing approaches cannot adapt policies to individual samples and the training model. This paper proposes Model-Adaptive Data Augmentation (MADAug) which trains an augmentation policy network to determine when to learn what. Unlike previous work, MADAug selects augmentation operators for each input image with a model-adaptive policy that varies between training stages, creating an optimized data augmentation curriculum. The policy is trained using a bi-level optimization scheme to minimize validation-set loss of a model trained with policy-produced augmentations. Extensive evaluations on multiple image classification tasks and network architectures show that MADAug outperforms or matches existing DA approaches, enhances fairness by improving all classes, particularly the difficult ones, and performs better when transferred to fine-grained datasets. Additionally, the auto-optimized policy in MADAug gradually increases perturbations, forming an easy-to-hard curriculum."
    }
  ],
  "p": [
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/research/2019/04/27/Benchmarking-Neural-Network-Robustness-to-Common-Corruptions-and-Surface-Variations.html",
      "date": "April 27, 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and IMAGENET-P, which assesses perturbation robustness."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "This paper introduces a novel approach to Radiance Field methods for novel-view synthesis that addresses the challenges of high visual quality, costly training, and real-time rendering of unbounded, complete scenes at 1080p resolution. The authors propose three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. The method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "turning": [
    {
      "title": "Dropout as Data Augmentation",
      "url": "/research/2016/01/08/DROPOUT-AS-DATA-AUGMENTATION.html",
      "date": "January 8, 2016",
      "summary": "Dropout is typically interpreted as turning the test time network into an ensemble of the thinner training networks. This paper argues that dropout can also be interpreted as a kind of data augmentation in image space. Authors present an approach to project the dropout noise within a network back into the input space, visualising the augmented versions of the training data, and show that training a deterministic network on the augmented samples yields similar results. Authors then propose a new dropout noise scheme and show that it improves dropout results without adding significant computational cos"
    }
  ],
  "ensemble": [
    {
      "title": "Dropout as Data Augmentation",
      "url": "/research/2016/01/08/DROPOUT-AS-DATA-AUGMENTATION.html",
      "date": "January 8, 2016",
      "summary": "Dropout is typically interpreted as turning the test time network into an ensemble of the thinner training networks. This paper argues that dropout can also be interpreted as a kind of data augmentation in image space. Authors present an approach to project the dropout noise within a network back into the input space, visualising the augmented versions of the training data, and show that training a deterministic network on the augmented samples yields similar results. Authors then propose a new dropout noise scheme and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/research/2014/01/01/Dropout-A-Simple-Way-to-Prevent-Neural-Networks-from-Overfitting.html",
      "date": "January 1, 2014",
      "summary": "This paper introduces Dropout, a technique that addresses overfitting by randomly omitting units during training, leading to an exponential number of thinned networks. This effectively turns the test time network into an ensemble."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. The authors find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. "
    },
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/research/2021/08/31/Universal-Adversarial-Robustness-of-Texture-and-Shape-biased-Models.html",
      "date": "August 31, 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    },
    {
      "title": "Fine-tuning with Very Large Dropout",
      "url": "/research/2024/03/01/Fine-tuning-with-Very-Large-Dropout.html",
      "date": "March 1, 2024",
      "summary": "This study challenges the notion that machine learning practices assume training and testing data have the same distribution. It explores the effectiveness of high dropout rates, as opposed to ensemble techniques, in developing rich data representations suitable for multiple distribution scenarios. These representations surpass those achieved by traditional in-distribution performance regularization and the implicit sparsity induced by common stochastic gradient methods. While training deep networks from scratch with high dropout rates is impractical, fine-tuning pre-trained models under these conditions is feasible and yields better out-of-distribution performance than ensembles and model averaging techniques like model soups. This finding is significant due to the growing relevance of fine-tuning with large pre-trained models, offering insights into the nature of rich representations and the linear characteristics of fine-tuning large networks with small datasets."
    }
  ],
  "kind": [
    {
      "title": "Dropout as Data Augmentation",
      "url": "/research/2016/01/08/DROPOUT-AS-DATA-AUGMENTATION.html",
      "date": "January 8, 2016",
      "summary": "Dropout is typically interpreted as turning the test time network into an ensemble of the thinner training networks. This paper argues that dropout can also be interpreted as a kind of data augmentation in image space. Authors present an approach to project the dropout noise within a network back into the input space, visualising the augmented versions of the training data, and show that training a deterministic network on the augmented samples yields similar results. Authors then propose a new dropout noise scheme and show that it improves dropout results without adding significant computational cos"
    }
  ],
  "visualise": [
    {
      "title": "Dropout as Data Augmentation",
      "url": "/research/2016/01/08/DROPOUT-AS-DATA-AUGMENTATION.html",
      "date": "January 8, 2016",
      "summary": "Dropout is typically interpreted as turning the test time network into an ensemble of the thinner training networks. This paper argues that dropout can also be interpreted as a kind of data augmentation in image space. Authors present an approach to project the dropout noise within a network back into the input space, visualising the augmented versions of the training data, and show that training a deterministic network on the augmented samples yields similar results. Authors then propose a new dropout noise scheme and show that it improves dropout results without adding significant computational cos"
    }
  ],
  "yield": [
    {
      "title": "Dropout as Data Augmentation",
      "url": "/research/2016/01/08/DROPOUT-AS-DATA-AUGMENTATION.html",
      "date": "January 8, 2016",
      "summary": "Dropout is typically interpreted as turning the test time network into an ensemble of the thinner training networks. This paper argues that dropout can also be interpreted as a kind of data augmentation in image space. Authors present an approach to project the dropout noise within a network back into the input space, visualising the augmented versions of the training data, and show that training a deterministic network on the augmented samples yields similar results. Authors then propose a new dropout noise scheme and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets transformers as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet."
    },
    {
      "title": "Is Cosine-Similarity of Embeddings Really About Similarity?",
      "url": "/research/2024/03/08/Is-Cosine-Similarity-of-Embeddings-Really-About-Similarity.html",
      "date": "March 8, 2024",
      "summary": "Cosine similarity, often used to gauge semantic similarity between high-dimensional objects by comparing low-dimensional feature embeddings, can yield variable results compared to unnormalized dot products. We investigate this phenomenon by analyzing embeddings from regularized linear models, revealing that cosine similarity can produce arbitrary and even meaningless similarities. This applies not only to linear models but also to deep models due to the implicit effects of various regularizations. Consequently, we advise against relying solely on cosine similarity and suggest exploring alternative approaches."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "How to decay your learning rate",
      "url": "/research/2021/05/23/How-to-decay-your-learning-rate.html",
      "date": "May 23, 2021",
      "summary": "Empirical findings suggest that typical fine-tuned learning rate schedules decay the learning rate following weight norm fluctuations. This led to the development of ABEL, an automatic scheduler that adjusts the learning rate based on weight norm changes. ABEL performs comparably to tuned schedules but demonstrates greater robustness to parameter variations. Extensive experiments across various domains reveal that when the weight norm remains stable, simplified schedules yield equivalent performance to complex ones, resembling a constant learning rate with decay towards the end of training."
    },
    {
      "title": "Fine-tuning with Very Large Dropout",
      "url": "/research/2024/03/01/Fine-tuning-with-Very-Large-Dropout.html",
      "date": "March 1, 2024",
      "summary": "This study challenges the notion that machine learning practices assume training and testing data have the same distribution. It explores the effectiveness of high dropout rates, as opposed to ensemble techniques, in developing rich data representations suitable for multiple distribution scenarios. These representations surpass those achieved by traditional in-distribution performance regularization and the implicit sparsity induced by common stochastic gradient methods. While training deep networks from scratch with high dropout rates is impractical, fine-tuning pre-trained models under these conditions is feasible and yields better out-of-distribution performance than ensembles and model averaging techniques like model soups. This finding is significant due to the growing relevance of fine-tuning with large pre-trained models, offering insights into the nature of rich representations and the linear characteristics of fine-tuning large networks with small datasets."
    }
  ],
  "overrate": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. Taking this to the extreme, this study trains a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss even when trained on pure noise."
    }
  ],
  "metric": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. Taking this to the extreme, this study trains a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss even when trained on pure noise."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "This paper proposes Hard Patches Mining (HPM), a novel framework for masked image modeling (MIM) pre-training that goes beyond simply solving given problems. HPM aims for the model to generate more challenging tasks for itself, using reconstruction loss as a metric for task difficulty. It incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "The study presents a novel approach for detection identification of Holstein-Friesian cattle. This technique offers a completely hands-off solution for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training when new cattle are introduced. The system utilizes convolutional neural networks and deep metric learning, achieving high accuracy with a 93.8% success rate in identifying cattle unseen during training, using only half of the population."
    },
    {
      "title": "From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning",
      "url": "/research/2023/03/16/From-MNIST-to-ImageNet-and-Back:-Benchmarking-Continual-Curriculum-Learning.html",
      "date": "March 16, 2023",
      "summary": "Continual learning (CL) is a promising trend in machine learning, aiming to develop robust models and strategies for dynamic environments by incorporating new knowledge while retaining past knowledge. CL research is fragmented, with various protocols, tasks, datasets, and metrics, often not reflecting real-world complexity and tailored to specific strategies. This work addresses this gap by introducing two novel CL benchmarks with heterogeneous tasks from six image datasets, varying in complexity and quality. These benchmarks fairly evaluate state-of-the-art CL strategies in scenarios closer to real-world conditions. The benchmarks present tasks in both increasing and decreasing complexity, assessing models' ability to exploit task structure. The work emphasizes a rigorous, reproducible evaluation protocol for measuring generalization and memory retention in models. Experimental results show that popular CL strategies perform poorly on these benchmarks, exhibit high forgetting, and struggle with curriculum task ordering. These findings underscore the need for rigorous comparisons and the development of new CL strategies for complex scenarios."
    }
  ],
  "absence": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. Taking this to the extreme, this study trains a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss even when trained on pure noise."
    }
  ],
  "human": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. Taking this to the extreme, this study trains a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss even when trained on pure noise."
    },
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "This paper intriduces the Kaiming He initialisation strategy for RELU like activation functions. This the default init in pytorch, improving model stability and decreasing training time for RELU networks. This study also introduces a Parametric Rectified Linear Unit (PReLU) activation function that extends traditional units, offering improved model fitting with negligible additional computational cost."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. The authors find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. "
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. A ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training. Synthetic images generated by FDSL circumvent issues associated with real images, offering a promising avenue for pre-training general models. Investigations suggest that simple object contours can match fractal-based dataset performance and increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "This study challenges the traditional understanding of how Convolutional Neural Networks (CNNs) recognize objects, revealing that they are more biased towards recognizing textures rather than learning complex shapes. The authors found that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, training a ResNet-50 on a stylized version of ImageNet, designed to emphasize shape, aligned the network's performance more closely with human behavior. This shape-based training matched human performance in controlled experiments and enhanced object detection and robustness against image distortions, highlighting the benefits of shape-based representations in visual recognition systems."
    },
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "This paper introduces QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU while maintaining 16-bit task performance. QLORA utilizes 4-bit quantized language models and Low Rank Adapters (LoRA) and incorporates several memory-saving innovations without compromising performance. The authors' best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. The authors applied QLORA to finetune over 1,000 models and analyzed performance across various datasets, model types, and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Their findings suggest that GPT-4 evaluations are a viable substitute for human assessments and question the reliability of current chatbot benchmarks. The authors make their models and 4-bit training code public."
    },
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "This paper introduces the Kinetics Human Action Video dataset. The authors note that the limited number of videos in existing datasets like UCF-101 and HMDB-51 hinders the ability to effectively evaluate architectures due to similar performance across these small benchmarks. Kinetics contains 400 classes with over 400 video clips per class from challenging YouTube videos. The paper analyzes the impact of this larger dataset on the performance of existing architectures as well as the benefits of pre-training models on Kinetics. The authors introduce the Two-Stream Inflated 3D ConvNet (I3D), an extension of 2D ConvNets to 3D for improved video feature extraction. When pre-trained on Kinetics, this I3D model sets new benchmarks for action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    },
    {
      "title": "Stable Diffusion 3: Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "This paper focuses on improving diffusion models for high-dimensional data like images. The authors enhance noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Their large-scale study shows superior performance over established diffusion methods in high-resolution text-to-image synthesis. A new transformer-based architecture is introduced for better text comprehension, typography, and human preference in text-to-image generation. The findings demonstrate predictable scaling trends, with lower validation loss tied to improved synthesis. The largest models outperform state-of-the-art approaches."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article provides an extensive review of vision transformer techniques applied specifically to human action recognition. These action transformers are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also examines optimizing spatio-temporal attention in transformers for longer sequences and different learning strategies like self-supervised and zero-shot learning with their respective loss functions. Additionally, it assesses progress in benchmark evaluation scores and discusses challenges and future directions in this research area."
    },
    {
      "title": "Are Convolutional Neural Networks or Transformers more like human vision?",
      "url": "/research/2021/07/01/Are-Convolutional-Neural-Networks-or-Transformers-more-like-human-vision.html",
      "date": "July 1, 2021",
      "summary": "This study investigates the performance of Convolutional Neural Networks (CNNs) and attention-based Vision Transformers (ViTs) in computer vision tasks. While CNNs excel in accuracy, ViTs offer a different approach with weaker inductive biases. By analyzing error patterns, we find that ViTs exhibit consistency with human errors, suggesting potential for more human-like vision models and insights into human object recognition."
    },
    {
      "title": "Scaling Vision Transformers to 22 Billion Parameters",
      "url": "/research/2023/02/10/Scaling-Vision-Transformers-to-22-Billion-Parameters.html",
      "date": "February 10, 2023",
      "summary": "We introduce a method for efficiently training a 22B-parameter Vision Transformer (ViT-22B) and conduct various experiments to assess its performance. Compared to existing models, ViT-22B shows improved scalability and benefits such as enhanced fairness-performance tradeoff, alignment with human visual perception, and increased robustness. Our findings suggest promising avenues for achieving large-scale language model-like capabilities in vision tasks."
    },
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    },
    {
      "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
      "url": "/research/2024/01/24/Better-Call-GPT,-Comparing-Large-Language-Models-Against-Lawyers.html",
      "date": "January 24, 2024",
      "summary": "This paper compares Large Language Models (LLMs) with traditional legal contract reviewers—Junior Lawyers and Legal Process Outsourcers (LPOs). It evaluates whether LLMs outperform humans in accuracy, speed, and cost-efficiency during contract review. Empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, revealing that advanced models match or exceed human accuracy in identifying legal issues. LLMs complete reviews in seconds, vastly outpacing the hours required by humans. Cost-wise, LLMs achieve a 99.97 percent reduction compared to traditional methods. These findings indicate a transformative shift in legal practice, with LLMs enhancing the accessibility and efficiency of legal services. The research suggests that LLM dominance in legal contract review challenges the status quo, necessitating a reimagined future for legal workflows."
    }
  ],
  "behavior": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. Taking this to the extreme, this study trains a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss even when trained on pure noise."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "This study challenges the traditional understanding of how Convolutional Neural Networks (CNNs) recognize objects, revealing that they are more biased towards recognizing textures rather than learning complex shapes. The authors found that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, training a ResNet-50 on a stylized version of ImageNet, designed to emphasize shape, aligned the network's performance more closely with human behavior. This shape-based training matched human performance in controlled experiments and enhanced object detection and robustness against image distortions, highlighting the benefits of shape-based representations in visual recognition systems."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "This paper demonstrates that artificial neural networks (ANNs) are equivalent to Gaussian processes at initialization in the infinite-width limit and introduces the Neural Tangent Kernel (NTK), which describes ANNs' behavior during training. The NTK stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. The authors prove the positive-definiteness of the limiting NTK under certain conditions and show that the network function follows a linear differential equation during training for least-squares regression. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    },
    {
      "title": "Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding",
      "url": "/research/2021/11/16/Improved-Robustness-of-Vision-Transformer-via-PreLayerNorm-in-Patch-Embedding.html",
      "date": "November 16, 2021",
      "summary": "Recent advancements in vision transformers (ViTs) have shown superior performance across diverse visual tasks, surpassing convolutional neural networks (CNNs). Given ViT's distinct architecture, understanding its behavior and reliability is imperative. This paper investigates ViT's robustness by comparing it with CNNs under various image corruptions relevant to real-world vision tasks. While ViT generally exhibits comparable or improved robustness over CNNs, it consistently underperforms in contrast enhancement tasks. Analysis suggests that positional embedding in ViT's patch embedding may malfunction with color scale changes. We propose PreLayerNorm, a modified patch embedding structure, to address this issue and ensure scale-invariant behavior in ViT. ViT with PreLayerNorm demonstrates enhanced robustness across various corruptions, particularly in contrast-varying environments."
    },
    {
      "title": "On Layer Normalization in the Transformer Architecture",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architecture.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is widely used in natural language processing. Training it typically requires a carefully designed learning rate warm-up stage, crucial for performance but slowing optimization and requiring more hyperparameter tuning. We provide theoretical insights into the necessity of the warm-up stage, demonstrating that the location of layer normalization affects gradient behavior. For Post-LN Transformers, with layer normalization between residual blocks, large gradients near the output layer at initialization make training unstable without warm-up. Conversely, for Pre-LN Transformers, with layer normalization inside residual blocks, gradients are well-behaved, suggesting the removal of the warm-up stage. Experimental results show that Pre-LN Transformers without warm-up achieve comparable performance with less training time and hyperparameter tuning."
    },
    {
      "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
      "url": "/research/2024/02/28/CL-MAE:-Curriculum-Learned-Masked-Autoencoders.html",
      "date": "February 28, 2024",
      "summary": "This paper proposes a curriculum learning approach that updates the masking strategy of Masked Auto Encoders (MAE) to progressively increase the complexity of the self-supervised reconstruction task. To achieve this, a novel learnable masking module is introduced, capable of generating masks of varying complexities, and integrated into masked autoencoders (MAE). This module is jointly trained with the MAE, adjusting its behavior from partner (optimizing the same reconstruction loss) to adversary (optimizing the opposite loss), with a smooth transition regulated by a factor multiplied with the reconstruction loss. This training procedure creates an easy-to-hard curriculum. The Curriculum-Learned Masked Autoencoder (CL-MAE) is trained on ImageNet and demonstrates superior representation learning compared to MAE. Empirical results on five downstream tasks confirm that curriculum learning can successfully self-supervise masked autoencoders."
    },
    {
      "title": "Suppressing Pink Elephants with Direct Principle Feedback",
      "url": "/research/2024/02/13/Suppressing-Pink-Elephants-with-Direct-Principle-Feedback.html",
      "date": "February 13, 2024",
      "summary": "Existing methods for controlling language models focus on training desired behaviors, but often lack the flexibility needed for diverse applications. The authors address this with the Pink Elephant Problem, demonstrating the need for language models to adapt to different contexts by avoiding certain topics (the Pink Elephant) in favor of others (Grey Elephant). The authors introduce Direct Principle Feedback (DPF), an adaptation of Constitutional AI that improves control by directly applying critiques and revisions without ranking responses. Our study shows that a 13B LLaMA 2 model fine-tuned with DPF on a synthetic dataset outperforms existing models and matches GPT-4 in managing the Pink Elephant Problem."
    }
  ],
  "take": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. Taking this to the extreme, this study trains a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss even when trained on pure noise."
    }
  ],
  "extreme": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. Taking this to the extreme, this study trains a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss even when trained on pure noise."
    }
  ],
  "uniform": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. Taking this to the extreme, this study trains a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss even when trained on pure noise."
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing is a regularization technique that reduces overfitting and enhances classification performance. Label smothing involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution. This significantly improves classification accuracy and model robustness to noisy labels."
    },
    {
      "title": "Learning RelU Networks To High Uniform Accuracy Is Intractable",
      "url": "/research/2023/02/28/Learning-Relu-Networks-To-High-Uniform-Accuracy-Is-Intractable.html",
      "date": "February 28, 2023",
      "summary": "Statistical learning theory provides guidelines on the number of training samples needed for achieving desired accuracy in learning problems. However, this is not always adequate, particularly in security-sensitive areas or computational sciences, where uniform accuracy across all inputs is necessary. This paper quantifies the training samples required for uniform accuracy in learning problems involving ReLU neural networks, revealing that the number of samples needed exponentially increases with the network's depth and input dimension."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This study suggest that label smoothing not only boosts generalization but also enhances model calibration. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. Label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel normalization technique for self-supervised transformer training that normalizes the [CLS] token and regular tokens separately. The authors find that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks compared to previous models like masked autoencoders (MAE) that use a single normalization layer for all tokens."
    }
  ],
  "reconstruct": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. Taking this to the extreme, this study trains a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss even when trained on pure noise."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "This paper proposes Hard Patches Mining (HPM), a novel framework for masked image modeling (MIM) pre-training that goes beyond simply solving given problems. HPM aims for the model to generate more challenging tasks for itself, using reconstruction loss as a metric for task difficulty. It incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
      "url": "/research/2024/02/28/CL-MAE:-Curriculum-Learned-Masked-Autoencoders.html",
      "date": "February 28, 2024",
      "summary": "This paper proposes a curriculum learning approach that updates the masking strategy of Masked Auto Encoders (MAE) to progressively increase the complexity of the self-supervised reconstruction task. To achieve this, a novel learnable masking module is introduced, capable of generating masks of varying complexities, and integrated into masked autoencoders (MAE). This module is jointly trained with the MAE, adjusting its behavior from partner (optimizing the same reconstruction loss) to adversary (optimizing the opposite loss), with a smooth transition regulated by a factor multiplied with the reconstruction loss. This training procedure creates an easy-to-hard curriculum. The Curriculum-Learned Masked Autoencoder (CL-MAE) is trained on ImageNet and demonstrates superior representation learning compared to MAE. Empirical results on five downstream tasks confirm that curriculum learning can successfully self-supervise masked autoencoders."
    }
  ],
  "spectrogram": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. Taking this to the extreme, this study trains a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss even when trained on pure noise."
    }
  ],
  "resynthesize": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. Taking this to the extreme, this study trains a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss even when trained on pure noise."
    }
  ],
  "euclidean": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. Taking this to the extreme, this study trains a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss even when trained on pure noise."
    }
  ],
  "delve": [
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing is a regularization technique that reduces overfitting and enhances classification performance. Label smothing involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution. This significantly improves classification accuracy and model robustness to noisy labels."
    },
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "This paper intriduces the Kaiming He initialisation strategy for RELU like activation functions. This the default init in pytorch, improving model stability and decreasing training time for RELU networks. This study also introduces a Parametric Rectified Linear Unit (PReLU) activation function that extends traditional units, offering improved model fitting with negligible additional computational cost."
    },
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    }
  ],
  "smoothing": [
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing is a regularization technique that reduces overfitting and enhances classification performance. Label smothing involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution. This significantly improves classification accuracy and model robustness to noisy labels."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    }
  ],
  "reduces": [
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing is a regularization technique that reduces overfitting and enhances classification performance. Label smothing involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution. This significantly improves classification accuracy and model robustness to noisy labels."
    },
    {
      "title": "Don't Decay the Learning Rate, Increase the Batch Size",
      "url": "/research/2017/11/01/Don't-Decay-the-Learning-Rate,-Increase-the-Batch-Size.html",
      "date": "November 1, 2017",
      "summary": "This study demonstrates that increasing the batch size during training achieves similar learning outcomes as the common practice of decaying the learning rate, applicable to stochastic gradient descent and its variants, including with momentum and Adam optimization. This approach not only matches test accuracies within the same number of epochs but also enhances parallelism and reduces training time due to fewer parameter updates. Efficiency can be further improved by adjusting the learning rate and batch size proportionally, and although increasing the momentum coefficient and scaling the batch size accordingly may slightly lower test accuracy, it enables the use of large batch training without needing to tune hyper-parameters. Using these methods, ResNet-50 was trained on ImageNet to a 76.1% validation accuracy in less than 30 minutes."
    }
  ],
  "smoth": [
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing is a regularization technique that reduces overfitting and enhances classification performance. Label smothing involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution. This significantly improves classification accuracy and model robustness to noisy labels."
    }
  ],
  "blend": [
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing is a regularization technique that reduces overfitting and enhances classification performance. Label smothing involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution. This significantly improves classification accuracy and model robustness to noisy labels."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This study suggest that label smoothing not only boosts generalization but also enhances model calibration. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. Label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "DeepNet: Scaling Transformers to 1,000 Layers",
      "url": "/research/2022/03/01/DeepNet-Scaling-Transformers-to-1,000-Layers.html",
      "date": "March 1, 2022",
      "summary": "This paper introduces DEEPNORM, a novel method to stabilize extremely deep Transformers through a new normalization function and a theoretically derived initialization for the residual connection. DEEPNORM offers a stable and efficient training regime, blending the benefits of Post-LN's performance with Pre-LN's stability, allowing for the scaling of Transformers up to 1,000 layers without difficulty. This represents a significant advancement over previous models, with a 200-layer, 3.2B parameter model outperforming a 48-layer, 12B parameter state-of-the-art model by 5 BLEU points on a multilingual benchmark with 7,482 translation directions, suggesting a promising direction for scaling."
    }
  ],
  "distribution": [
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing is a regularization technique that reduces overfitting and enhances classification performance. Label smothing involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution. This significantly improves classification accuracy and model robustness to noisy labels."
    },
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/research/2023/06/30/Impact-of-Noise-on-Calibration-and-Generalisation-of-Neural-Networks.html",
      "date": "June 30, 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. Activation noise is shown to significantly improve generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This study suggest that label smoothing not only boosts generalization but also enhances model calibration. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. Label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets transformers as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet."
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel normalization technique for self-supervised transformer training that normalizes the [CLS] token and regular tokens separately. The authors find that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks compared to previous models like masked autoencoders (MAE) that use a single normalization layer for all tokens."
    },
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "This paper introduces the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. The authors' empirical evaluation across computer vision, natural language processing, and speech tasks demonstrates that GELU offers performance improvements over ReLU and ELU activations."
    },
    {
      "title": "Eigencurve: Optimal Learning Rate Schedule For Sgd on Quadratic Objectives With Skewed Hessian Spectrums",
      "url": "/research/2022/06/14/Eigencurve-Optimal-Learning-Rate-Schedule-For-Sgd-on-Quadratic-Objectives-With-Skewed-Hessian-Spectrums.html",
      "date": "June 14, 2022",
      "summary": "Learning rate schedulers are essential in deep neural network training, but there's a gap between practical usage and theoretical understanding. This paper introduces Eigencurve, the first set of learning rate schedules achieving minimax optimal convergence rates for SGD on quadratic objectives with skewed eigenvalue distributions of the Hessian matrix. This condition is common in practice. Experimental results on CIFAR-10 image classification tasks demonstrate Eigencurve's superiority over step decay, particularly with fewer epochs. The theory inspires practical schedulers approximating Eigencurve, resembling cosine decay for some problems while outperforming it in others."
    },
    {
      "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
      "url": "/research/2021/07/24/The-Many-Faces-of-Robustness-A-Critical-Analysis-of-Out-of-Distribution-Generalization.html",
      "date": "July 24, 2021",
      "summary": "We present four novel real-world distribution shift datasets encompassing changes in image style, blurriness, location, camera settings, and more. Evaluating existing methods for enhancing out-of-distribution robustness, we discover that employing larger models and artificial data augmentations can enhance robustness against real-world distribution shifts, contradicting prior claims. Our findings demonstrate that improvements in artificial robustness benchmarks can indeed transfer to real-world distribution shifts, contrary to prior assumptions. Additionally, we introduce a novel data augmentation technique that surpasses models pretrained with significantly more labeled data, emphasizing its efficacy in addressing real-world distribution shifts. While some methods consistently mitigate texture and local image statistics shifts, they fail to address other shifts like geographic changes. Our results underscore the necessity for future research to examine multiple distribution shifts concurrently, as no single method consistently improves robustness across all evaluated scenarios."
    },
    {
      "title": "Fine-tuning with Very Large Dropout",
      "url": "/research/2024/03/01/Fine-tuning-with-Very-Large-Dropout.html",
      "date": "March 1, 2024",
      "summary": "This study challenges the notion that machine learning practices assume training and testing data have the same distribution. It explores the effectiveness of high dropout rates, as opposed to ensemble techniques, in developing rich data representations suitable for multiple distribution scenarios. These representations surpass those achieved by traditional in-distribution performance regularization and the implicit sparsity induced by common stochastic gradient methods. While training deep networks from scratch with high dropout rates is impractical, fine-tuning pre-trained models under these conditions is feasible and yields better out-of-distribution performance than ensembles and model averaging techniques like model soups. This finding is significant due to the growing relevance of fine-tuning with large pre-trained models, offering insights into the nature of rich representations and the linear characteristics of fine-tuning large networks with small datasets."
    },
    {
      "title": "HYPO: Hyperspherical Out-of-Distribution Generalization",
      "url": "/research/2024/03/19/HYPO-Hyperspherical-Out-of-Distribution-Generalization.html",
      "date": "March 19, 2024",
      "summary": "We propose HYPO, a novel framework for out-of-distribution (OOD) generalization in machine learning, which learns domain-invariant features in a hyperspherical space. Our method focuses on aligning features of the same class across different domains close to their class prototypes and separating different class prototypes maximally. We offer theoretical justifications for its improvement on OOD generalization and show through experiments on OOD benchmarks that HYPO outperforms existing baselines with superior performance."
    },
    {
      "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
      "url": "/research/2024/03/13/Simple-and-Scalable-Strategies-to-Continually-Pre-train-Large-Language-Models.html",
      "date": "March 13, 2024",
      "summary": "Large language models (LLMs) often require re-training on new data, consuming extensive computational resources. We present an efficient method that combines learning rate re-warming, re-decaying, and data replay, effectively maintaining performance without full re-training. This approach works well across different data distributions, including minor shifts (English→English) and significant shifts (English→German), tested up to 405M and 10B parameter models. Our findings suggest that continual learning strategies can update LLMs with minimal computational cost, rivaling traditional re-training methods. Additionally, we propose alternatives to the cosine learning rate schedule to reduce forgetting, offering more flexibility in learning without a fixed token budget."
    }
  ],
  "ols": [
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing is a regularization technique that reduces overfitting and enhances classification performance. Label smothing involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution. This significantly improves classification accuracy and model robustness to noisy labels."
    }
  ],
  "generation": [
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing is a regularization technique that reduces overfitting and enhances classification performance. Label smothing involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution. This significantly improves classification accuracy and model robustness to noisy labels."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of Vision Transformer (ViT) variants against common corruptions. The benchmarking reveals that overlapping patch embedding and convolutional feed-forward networks (FFNs) significantly enhance ViT robustness. The study also scrutinizes the effectiveness of CNN-based data augmentation strategies when applied to ViTs, finding that adversarial noise training is effective, while fourier-domain augmentation falls short. A new conditional method for generating dynamic augmentation parameters based on input images is proposed, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. A ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training. Synthetic images generated by FDSL circumvent issues associated with real images, offering a promising avenue for pre-training general models. Investigations suggest that simple object contours can match fractal-based dataset performance and increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "This paper introduces Mask-Reconstruct Augmentation (MRA), an image augmentation technique that leverages self-supervised masked autoencoders to generate distorted inputs and address the issue of overfitting in deep neural networks. Inspired by the success of masked image modeling in self-supervised learning, MRA uses nonlinear transformations for regularization, as opposed to current hand-crafted linear techniques like scale, flip, and color jitter. Extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "This paper introduces FixMatch, a streamlined semi-supervised learning (SSL) algorithm. FixMatch generates pseudo-labels using a model's predictions on weakly-augmented unlabeled images, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness, and the code is publicly available."
    },
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper introduces the concept of grokking, a sudden onset of perfect generalisation long after overfitting, on small, algorithmically generated datasets. The study also finds that generalization on smaller datasets requires more optimization. The authors suggest that such datasets are ideal for investigating the puzzling phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    },
    {
      "title": "Stable Diffusion 3: Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "This paper focuses on improving diffusion models for high-dimensional data like images. The authors enhance noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Their large-scale study shows superior performance over established diffusion methods in high-resolution text-to-image synthesis. A new transformer-based architecture is introduced for better text comprehension, typography, and human preference in text-to-image generation. The findings demonstrate predictable scaling trends, with lower validation loss tied to improved synthesis. The largest models outperform state-of-the-art approaches."
    },
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces an iterative self-training framework for sim-to-real 6D object pose estimation to enable cost-effective robotic bin-picking. The authors create a photo-realistic simulator to synthesize virtual data for training an initial pose estimation network (teacher model). This teacher predicts poses on unlabeled real data, and an adaptive selection scheme filters reliable predictions to generate pseudo-labels for training a student model on real data. Iteratively refining the teacher with the student improves pseudo-label quality. Evaluated on public benchmarks and a new dataset, their method shows 11.49% and 22.62% ADD(-S) improvements and a 19.54% increase in bin-picking success, demonstrating the effectiveness of this iterative sim-to-real approach."
    },
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This paper investigates using high-quality rendering software and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle. This synthetic dataset is used to augment limited real training data for image classification and semantic segmentation tasks. While models trained solely on synthetic images showed low accuracy on real validation data, including even small amounts of real data significantly improved performance. Augmenting real training data with synthetic images outperformed using only real images. Furthermore, pretraining models on the synthetic dataset before transfer learning significantly reduced training costs by allowing most of the training to be completed upfront using the synthetic data."
    },
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "The paper investigates the use of synthetic data generated by graphics simulators for pre-training computer vision models. The authors find that model performance on downstream tasks varies with different simulation parameters used to generate the synthetic data. They introduce Task2Sim, a model that maps the requirements of a downstream task to the optimal simulation parameters for generating synthetic pre-training data."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "The paper introduces a system that trains deep neural networks for object detection using only synthetic images generated with domain randomization. This involves randomizing non-realistic simulator parameters like lighting and object textures, allowing the network to identify essential object features. Their findings show that networks can achieve impressive performance using just synthetic data, and further improve with fine-tuning on real data. This suggests the potential of using cost-effective synthetic data for training, avoiding the need to acquire vast amounts of real-world data or create detailed synthetic environments. They validate their method on car bounding box detection using the KITTI dataset."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    },
    {
      "title": "Neural Network Diffusion",
      "url": "/research/2024/02/20/Neural-Network-Diffusion.html",
      "date": "February 20, 2024",
      "summary": "Diffusion models, known for their success in image and video generation, can also generate high-performing neural network parameters, as demonstrated in this study. By employing a simple combination of an autoencoder and a standard latent diffusion model, our method involves extracting latent representations of network parameters, which are then synthesized from random noise by the diffusion model. These new representations, processed through the autoencoder's decoder, serve as fresh network parameters. Tested across various architectures and datasets, this diffusion approach consistently produces models with comparable or superior performance to traditionally trained networks at minimal extra cost. Importantly, the generated models show distinct performance differences from the trained ones, suggesting further exploration into the versatile applications of diffusion models."
    },
    {
      "title": "Non-negative Contrastive Learning",
      "url": "/research/2024/03/19/Non-negative-Contrastive-Learning.html",
      "date": "March 19, 2024",
      "summary": "Deep representations offer promising performance for downstream tasks but lack interpretability, posing a significant challenge. In this paper, we introduce Non-negative Contrastive Learning (NCL), a refinement of Non-negative Matrix Factorization (NMF) aimed at generating interpretable features. NCL enforces non-negativity constraints, akin to NMF, resulting in sparse and disentangled representations, unlike standard contrastive learning (CL). We establish theoretical guarantees on NCL's identifiability and downstream generalization. Empirically, NCL outperforms CL in feature disentanglement, selection, and downstream classification tasks. Moreover, NCL can be extended to other learning scenarios, benefiting supervised learning."
    },
    {
      "title": "V-Jepa: Latent Video Prediction For Visual Representation Learning",
      "url": "/research/2023/02/15/V-Jepa-Latent-Video-Prediction-For-Visual-Representation-Learning.html",
      "date": "February 15, 2023",
      "summary": "This paper introduces V-JEPA, a self-supervised video learning method that predicts masked spatio-temporal regions in latent space, effectively applying the masked-modelling principle of large language models to video. This approach generates visual features useful across various image and video tasks without needing model adjustment, achieving significant improvements on Kinetics-400 (82.1%) and Something-Something-v2 (71.2%) benchmarks, outperforming prior video models. V-JEPA also excels in motion understanding tasks, surpassing leading image models like DINOv2 and OpenCLIP, and achieves 77.9% on ImageNet classification with video training alone, setting a new standard for video models."
    }
  ],
  "statistics": [
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing is a regularization technique that reduces overfitting and enhances classification performance. Label smothing involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution. This significantly improves classification accuracy and model robustness to noisy labels."
    },
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes."
    },
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/research/2020/08/03/Are-Labels-Necessary-for-Neural-Architecture-Search.html",
      "date": "August 3, 2020",
      "summary": "The paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning the necessity of human-annotated labels for discovering effective neural architectures. Two experimental setups sample-based and search-based were investigated. The sample-based approach evaluated 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. The search-based experiments employed DARTS, a recognized NAS algorithm, with various unsupervised objectives. The architectures identified without labels performed competitively compared to those found with labels. The findings suggested that image statistics alone could be sufficient for identifying efficient neural architectures, potentially eliminating the need for human-annotated labels."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/2020/06/17/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "June 17, 2020",
      "summary": "Implicit neural representations, parameterized by continuous, differentiable neural networks, offer numerous advantages over conventional representations. However, existing architectures struggle with fine detail and fail to capture spatial and temporal derivatives, crucial for many physical signals described by partial differential equations. We introduce sinusoidal representation networks (SIRENs) utilizing periodic activation functions, ideal for representing complex signals and their derivatives. Analyzing SIREN activation statistics informs a principled initialization scheme. We demonstrate SIRENs' efficacy in representing various signals and solving boundary value problems, including Eikonal, Poisson, Helmholtz, and wave equations. Additionally, we integrate SIRENs with hypernetworks to learn priors over the SIREN function space. Visit the project website for detailed demonstrations."
    },
    {
      "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
      "url": "/research/2021/07/24/The-Many-Faces-of-Robustness-A-Critical-Analysis-of-Out-of-Distribution-Generalization.html",
      "date": "July 24, 2021",
      "summary": "We present four novel real-world distribution shift datasets encompassing changes in image style, blurriness, location, camera settings, and more. Evaluating existing methods for enhancing out-of-distribution robustness, we discover that employing larger models and artificial data augmentations can enhance robustness against real-world distribution shifts, contradicting prior claims. Our findings demonstrate that improvements in artificial robustness benchmarks can indeed transfer to real-world distribution shifts, contrary to prior assumptions. Additionally, we introduce a novel data augmentation technique that surpasses models pretrained with significantly more labeled data, emphasizing its efficacy in addressing real-world distribution shifts. While some methods consistently mitigate texture and local image statistics shifts, they fail to address other shifts like geographic changes. Our results underscore the necessity for future research to examine multiple distribution shifts concurrently, as no single method consistently improves robustness across all evaluated scenarios."
    },
    {
      "title": "Neural Networks Learn Statistics of Increasing Complexity",
      "url": "/research/2024/02/13/Neural-Networks-Learn-Statistics-of-Increasing-Complexity.html",
      "date": "February 13, 2024",
      "summary": "The distributional simplicity bias (DSB) theory suggests neural networks first learn basic patterns in data before understanding more complex correlations. We provide new evidence supporting DSB, showing networks initially excel with data matching training set's simple statistics but this ability diminishes later. Extending DSB to discrete domains, we demonstrate an equivalence between n-gram frequencies and vector moments, also observing this bias in large language models (LLMs). Additionally, by adjusting low-level statistics of images to resemble another class, we reveal that networks in early training phases misclassify these edited images as if they belonged to the target class."
    }
  ],
  "probability": [
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing is a regularization technique that reduces overfitting and enhances classification performance. Label smothing involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution. This significantly improves classification accuracy and model robustness to noisy labels."
    },
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a method for dropout in Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers."
    },
    {
      "title": "On the Maximum Hessian Eigenvalue and Generalization",
      "url": "/research/2023/05/23/On-the-Maximum-Hessian-Eigenvalue-and-Generalization.html",
      "date": "May 23, 2023",
      "summary": "This study investigates the relationship between training interventions and the generalization of deep networks. While previous research suggests that flatter solutions generalize better than sharper ones, particularly measured by λmax, the largest eigenvalue of the Hessian of the loss, this paper challenges this notion. Through experiments, we demonstrate that larger learning rates reduce λmax for all batch sizes but do not consistently improve generalization. Additionally, scaling batch size and learning rate simultaneously can change λmax without affecting generalization. Sharpness-Aware Minimization (SAM) produces smaller λmax but does not consistently enhance generalization, especially with larger batch sizes. Excessive dropout probabilities can degrade generalization, despite promoting smaller λmax. Batch normalization, while not consistently reducing λmax, still improves generalization. These findings question λmax's role in explaining generalization in neural networks, highlighting the limits of its explanatory power."
    },
    {
      "title": "Normalized Attention Without Probability Cage",
      "url": "/research/2020/05/19/Normalized-Attention-Without-Probability-Cage.html",
      "date": "May 19, 2020",
      "summary": "Softmax-attention architectures, especially popularized by Transformers, have seen significant advancements in various tasks. However, the geometric implications of softmax-attention remain underexplored. In this study, we demonstrate limitations arising from constraining attention weights to the probability simplex and its impact on the convex hull of value vectors. We reveal sequence length-dependent biases in Transformers towards token isolation at initialization and compare them with max- and sum-pooling, which are strong but often overlooked baselines. To address these issues, we propose a novel approach of replacing softmax with normalization in self-attention, resulting in a robust and widely applicable architecture. Our findings are supported by empirical results from over 25,000 trained models, and all results and implementations are publicly available."
    },
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    }
  ],
  "rectifiers": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "This paper intriduces the Kaiming He initialisation strategy for RELU like activation functions. This the default init in pytorch, improving model stability and decreasing training time for RELU networks. This study also introduces a Parametric Rectified Linear Unit (PReLU) activation function that extends traditional units, offering improved model fitting with negligible additional computational cost."
    }
  ],
  "surpassing": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "This paper intriduces the Kaiming He initialisation strategy for RELU like activation functions. This the default init in pytorch, improving model stability and decreasing training time for RELU networks. This study also introduces a Parametric Rectified Linear Unit (PReLU) activation function that extends traditional units, offering improved model fitting with negligible additional computational cost."
    }
  ],
  "classificatio": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "This paper intriduces the Kaiming He initialisation strategy for RELU like activation functions. This the default init in pytorch, improving model stability and decreasing training time for RELU networks. This study also introduces a Parametric Rectified Linear Unit (PReLU) activation function that extends traditional units, offering improved model fitting with negligible additional computational cost."
    }
  ],
  "intriduce": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "This paper intriduces the Kaiming He initialisation strategy for RELU like activation functions. This the default init in pytorch, improving model stability and decreasing training time for RELU networks. This study also introduces a Parametric Rectified Linear Unit (PReLU) activation function that extends traditional units, offering improved model fitting with negligible additional computational cost."
    }
  ],
  "initialisation": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "This paper intriduces the Kaiming He initialisation strategy for RELU like activation functions. This the default init in pytorch, improving model stability and decreasing training time for RELU networks. This study also introduces a Parametric Rectified Linear Unit (PReLU) activation function that extends traditional units, offering improved model fitting with negligible additional computational cost."
    }
  ],
  "relu": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "This paper intriduces the Kaiming He initialisation strategy for RELU like activation functions. This the default init in pytorch, improving model stability and decreasing training time for RELU networks. This study also introduces a Parametric Rectified Linear Unit (PReLU) activation function that extends traditional units, offering improved model fitting with negligible additional computational cost."
    },
    {
      "title": "Learning RelU Networks To High Uniform Accuracy Is Intractable",
      "url": "/research/2023/02/28/Learning-Relu-Networks-To-High-Uniform-Accuracy-Is-Intractable.html",
      "date": "February 28, 2023",
      "summary": "Statistical learning theory provides guidelines on the number of training samples needed for achieving desired accuracy in learning problems. However, this is not always adequate, particularly in security-sensitive areas or computational sciences, where uniform accuracy across all inputs is necessary. This paper quantifies the training samples required for uniform accuracy in learning problems involving ReLU neural networks, revealing that the number of samples needed exponentially increases with the network's depth and input dimension."
    },
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "This paper explores Gated Linear Units (GLUs), which involve the component-wise product of two linear projections, with one undergoing a sigmoid function. The authors investigate GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers. They find that certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    },
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "This paper introduces the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. The authors' empirical evaluation across computer vision, natural language processing, and speech tasks demonstrates that GELU offers performance improvements over ReLU and ELU activations."
    },
    {
      "title": "SinLU: Sinu-Sigmoidal Linear Uni",
      "url": "/research/2021/12/02/SinLU-Sinu-Sigmoidal-Linear-Uni.html",
      "date": "December 2, 2021",
      "summary": "This paper introduces a non-linear activation function called the Sinu-sigmoidal Linear Unit (SinLU), formulated as SinLU(x) = (x + a sin bx) · σ(x). It incorporates a sine wave for added functionality over traditional linear units, with two trainable parameters controlling the sinusoidal participation. The authors compare SinLU's performance to ReLU, GELU, and SiLU across various domains, models, and standard datasets. The results demonstrate SinLU's robustness and superior performance due to the incorporation of the trainable sine wave parameters, facilitating easy training and fast convergence."
    },
    {
      "title": "Searching for Activation Functions",
      "url": "/research/2017/10/27/Searching-for-Activation-Functions.html",
      "date": "October 27, 2017",
      "summary": "This study explores the impact of activation functions on deep network training and performance. While Rectified Linear Unit (ReLU) is widely used, alternatives have not consistently outperformed it. We propose using automatic search techniques to discover new activation functions. Through exhaustive and reinforcement learning-based searches, we identify novel functions. Empirical evaluation shows that our best discovered function, Swish (f(x) = x · sigmoid(βx)), performs better than ReLU on deeper models across challenging datasets. Replacing ReLUs with Swish units improves classification accuracy on ImageNet, for example, by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. Swish's simplicity and similarity to ReLU facilitate its adoption in neural networks."
    },
    {
      "title": "Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks",
      "url": "/research/2020/06/11/Batch-Normalization-Provably-Avoids-Rank-Collapse-for-Randomly-Initialised-Deep-Networks.html",
      "date": "June 11, 2020",
      "summary": "We investigate the challenge of training randomly initialized deep neural networks due to spectral instabilities in products of random matrices. Batch normalization emerges as an effective solution to prevent rank collapse in both linear and ReLU networks. Leveraging tools from Markov chain theory, we establish a lower rank bound for deep linear networks. Empirical findings show that this rank robustness extends to ReLU networks. Our experiments on real-world datasets underscore the significance of rank stability in training modern deep neural architectures."
    }
  ],
  "activation": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "This paper intriduces the Kaiming He initialisation strategy for RELU like activation functions. This the default init in pytorch, improving model stability and decreasing training time for RELU networks. This study also introduces a Parametric Rectified Linear Unit (PReLU) activation function that extends traditional units, offering improved model fitting with negligible additional computational cost."
    },
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/research/2023/06/30/Impact-of-Noise-on-Calibration-and-Generalisation-of-Neural-Networks.html",
      "date": "June 30, 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. Activation noise is shown to significantly improve generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets."
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "This paper introduces Xavier Glorot init for Sigmoid like activation functions. The study investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial."
    },
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "This paper introdices LOST, a method for unsupervised object localization in images. LOST utilizes activation features from a self-supervised pre-trained vision transformer. Unlike other methods, LOST works on individual images without relying on external object proposals or image collection exploration. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, demonstrating its effectiveness in unsupervised object discovery."
    },
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "This paper introduces a Fourier neural network (FNN) that aligns with Fourier decomposition by utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design allows for easy integration with more complex networks for data processing tasks. The authors demonstrate the FNN's efficacy on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits of the FNN include solution validity beyond the training scope, model interpretability, and ease of use."
    },
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "This paper explores Gated Linear Units (GLUs), which involve the component-wise product of two linear projections, with one undergoing a sigmoid function. The authors investigate GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers. They find that certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    },
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "This paper introduces the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. The authors' empirical evaluation across computer vision, natural language processing, and speech tasks demonstrates that GELU offers performance improvements over ReLU and ELU activations."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces sinusoidal representation networks (SIRENs), which utilize periodic activation functions like Sin to effectively capture complex natural signals and their derivatives, addressing the limitations of neural networks parameterized for continuous, differentiable signal representations. The authors' analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. SIRENs are also applied in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations. The authors extend SIRENs' use with hypernetworks to learn priors for SIREN functions."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "This paper introduces Online Normalization, a novel method for normalizing neural network hidden activations that offers a batch-independent alternative with comparable accuracy to Batch Normalization. Online Normalization addresses Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. The method is applicable to recurrent, fully connected networks, and those with high activation memory requirements. The authors demonstrate its effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    },
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions, SiLU and dSiLU. It challenges the need for experience replay and separate target networks in deep reinforcement learning. By using on-policy learning with eligibility traces and softmax action selection, it achieves state-of-the-art results in stochastic SZ-Tetris and small-board Tetris with TD(λ) learning and shallow dSiLU agents. It also outperforms DQN in Atari 2600 with a deep Sarsa(λ) agent using SiLU/dSiLU, suggesting an alternative to traditional DQN approaches."
    },
    {
      "title": "SinLU: Sinu-Sigmoidal Linear Uni",
      "url": "/research/2021/12/02/SinLU-Sinu-Sigmoidal-Linear-Uni.html",
      "date": "December 2, 2021",
      "summary": "This paper introduces a non-linear activation function called the Sinu-sigmoidal Linear Unit (SinLU), formulated as SinLU(x) = (x + a sin bx) · σ(x). It incorporates a sine wave for added functionality over traditional linear units, with two trainable parameters controlling the sinusoidal participation. The authors compare SinLU's performance to ReLU, GELU, and SiLU across various domains, models, and standard datasets. The results demonstrate SinLU's robustness and superior performance due to the incorporation of the trainable sine wave parameters, facilitating easy training and fast convergence."
    },
    {
      "title": "Taming the Waves: Sine as Activation Function in Deep Neural Networks",
      "url": "/research/2016/11/04/Taming-the-Waves-Sine-as-Activation-Function-in-Deep-Neural-Networks.html",
      "date": "November 4, 2016",
      "summary": "This paper explores the challenges in training deep neural networks that use sinusoidal activation functions. The authors explain that the difficulty arises from the emergence of numerous shallow local minima in the loss landscape. The study reveals that successful learning in typical classification tasks occurs when the network effectively ignores the periodic cycles of the sinusoidal functions. However, for certain non-trivial tasks, networks with sinusoidal activations can outperform those using traditional monotonic activation functions."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/2020/06/17/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "June 17, 2020",
      "summary": "Implicit neural representations, parameterized by continuous, differentiable neural networks, offer numerous advantages over conventional representations. However, existing architectures struggle with fine detail and fail to capture spatial and temporal derivatives, crucial for many physical signals described by partial differential equations. We introduce sinusoidal representation networks (SIRENs) utilizing periodic activation functions, ideal for representing complex signals and their derivatives. Analyzing SIREN activation statistics informs a principled initialization scheme. We demonstrate SIRENs' efficacy in representing various signals and solving boundary value problems, including Eikonal, Poisson, Helmholtz, and wave equations. Additionally, we integrate SIRENs with hypernetworks to learn priors over the SIREN function space. Visit the project website for detailed demonstrations."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Searching for Activation Functions",
      "url": "/research/2017/10/27/Searching-for-Activation-Functions.html",
      "date": "October 27, 2017",
      "summary": "This study explores the impact of activation functions on deep network training and performance. While Rectified Linear Unit (ReLU) is widely used, alternatives have not consistently outperformed it. We propose using automatic search techniques to discover new activation functions. Through exhaustive and reinforcement learning-based searches, we identify novel functions. Empirical evaluation shows that our best discovered function, Swish (f(x) = x · sigmoid(βx)), performs better than ReLU on deeper models across challenging datasets. Replacing ReLUs with Swish units improves classification accuracy on ImageNet, for example, by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. Swish's simplicity and similarity to ReLU facilitate its adoption in neural networks."
    },
    {
      "title": "A comprehensive review of Binary Neural Network",
      "url": "/research/2023/05/11/A-comprehensive-review-of-Binary-Neural-Network.html",
      "date": "May 11, 2023",
      "summary": "Deep learning (DL) is widely used in intelligent systems and real-life applications, but its deployment on computationally limited and energy-constrained devices requires efficient technologies like Binary Neural Networks (BNN). BNNs save significant storage, computation cost, and energy, making them ideal for small devices, despite the trade-offs in memory and performance. This article offers a comprehensive review of BNN developments, focusing on 1-bit activations and weights. It covers the evolution of BNN, from early models to advanced algorithms and design aspects, and addresses BNN optimization, deployment, computing architectures, and diverse applications. It also outlines potential research directions in BNN technology."
    },
    {
      "title": "Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks",
      "url": "/research/2017/10/23/Cosine-Normalization-Using-Cosine-Similarity-Instead-of-Dot-Product-in-Neural-Networks.html",
      "date": "October 23, 2017",
      "summary": "In traditional multi-layer neural networks, the dot product between the output and weight vectors of preceding layers serves as input to the activation function, resulting in unbounded outputs and increased variance. This variance can lead to poor generalization and hinder training by exacerbating internal covariate shift. To address this, we propose cosine normalization, which replaces the dot product with cosine similarity or centered cosine similarity (Pearson Correlation Coefficient). We evaluate cosine normalization against batch, weight, and layer normalization in fully-connected and convolutional neural networks across various datasets. Our experiments demonstrate that cosine normalization outperforms other normalization techniques."
    },
    {
      "title": "Transformers with Learnable Activation Functions",
      "url": "/research/2023/02/14/Transformers-with-Learnable-Activation-Functions.html",
      "date": "February 14, 2023",
      "summary": "Activation functions significantly affect model performance by reducing data complexity, yet their selection in Transformer-based language models is often overlooked. This paper explores the impact of using rational activation functions (RAFs), which unlike fixed activation functions (FAFs), can learn optimal functions from data. Our experiments demonstrate that Transformer models with RAFs (RAFT) outperform those with FAFs (FAFT), achieving a 5.71 point higher score on the GLUE benchmark with only 100 training examples and a 2.05 point increase on SQuAD with full data. The varied shapes of learned RAFs across layers and tasks suggest a new method for analyzing and understanding large pre-trained language models."
    },
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "default": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "This paper intriduces the Kaiming He initialisation strategy for RELU like activation functions. This the default init in pytorch, improving model stability and decreasing training time for RELU networks. This study also introduces a Parametric Rectified Linear Unit (PReLU) activation function that extends traditional units, offering improved model fitting with negligible additional computational cost."
    }
  ],
  "initialize": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "This paper intriduces the Kaiming He initialisation strategy for RELU like activation functions. This the default init in pytorch, improving model stability and decreasing training time for RELU networks. This study also introduces a Parametric Rectified Linear Unit (PReLU) activation function that extends traditional units, offering improved model fitting with negligible additional computational cost."
    },
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrives incrementally, it's common to build a sequence of models that incorporate progressively more data. It has been shown that simply continuing training of a model on new data often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "This paper introduces Xavier Glorot init for Sigmoid like activation functions. The study investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial."
    },
    {
      "title": "Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks",
      "url": "/research/2020/06/11/Batch-Normalization-Provably-Avoids-Rank-Collapse-for-Randomly-Initialised-Deep-Networks.html",
      "date": "June 11, 2020",
      "summary": "We investigate the challenge of training randomly initialized deep neural networks due to spectral instabilities in products of random matrices. Batch normalization emerges as an effective solution to prevent rank collapse in both linear and ReLU networks. Leveraging tools from Markov chain theory, we establish a lower rank bound for deep linear networks. Empirical findings show that this rank robustness extends to ReLU networks. Our experiments on real-world datasets underscore the significance of rank stability in training modern deep neural architectures."
    }
  ],
  "pytorch": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "This paper intriduces the Kaiming He initialisation strategy for RELU like activation functions. This the default init in pytorch, improving model stability and decreasing training time for RELU networks. This study also introduces a Parametric Rectified Linear Unit (PReLU) activation function that extends traditional units, offering improved model fitting with negligible additional computational cost."
    },
    {
      "title": "Exponentially Faster Language Modeling",
      "url": "/research/2023/11/21/Exponentially-Faster-Language-Modeling.html",
      "date": "November 21, 2023",
      "summary": "UltraFastBERT, a BERT variant, operates with just 0.3% of its neurons—selectively using 12 out of 4095 neurons per layer—for inference, matching the performance of similar models. It replaces conventional feedforward networks with fast feedforward networks (FFFs) to achieve this efficiency. Although fully efficient conditional neural execution isn't yet practical, we offer a high-level CPU code that achieves a 78x speedup and a PyTorch implementation with a 40x speedup over standard batched feedforward inference."
    }
  ],
  "stability": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "This paper intriduces the Kaiming He initialisation strategy for RELU like activation functions. This the default init in pytorch, improving model stability and decreasing training time for RELU networks. This study also introduces a Parametric Rectified Linear Unit (PReLU) activation function that extends traditional units, offering improved model fitting with negligible additional computational cost."
    },
    {
      "title": "Dropout Reduces Underfitting",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. Authors then introduce two dropout schedules: early dropout which prevents underfitting, and late dropout which prevents overfitting."
    },
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/research/2021/08/16/An-Empirical-Study-of-Training-Self-Supervised-Vision-Transformers.html",
      "date": "August 16, 2021",
      "summary": "This study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks."
    },
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "This paper investigates the importance of the warm-up stage in training transformers, and the impact of layer normalization placement. Using mean field theory, the authors demonstrate that the original Post-LN Transformer's design results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    },
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    },
    {
      "title": "Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks",
      "url": "/research/2020/06/11/Batch-Normalization-Provably-Avoids-Rank-Collapse-for-Randomly-Initialised-Deep-Networks.html",
      "date": "June 11, 2020",
      "summary": "We investigate the challenge of training randomly initialized deep neural networks due to spectral instabilities in products of random matrices. Batch normalization emerges as an effective solution to prevent rank collapse in both linear and ReLU networks. Leveraging tools from Markov chain theory, we establish a lower rank bound for deep linear networks. Empirical findings show that this rank robustness extends to ReLU networks. Our experiments on real-world datasets underscore the significance of rank stability in training modern deep neural architectures."
    },
    {
      "title": "How to decay your learning rate",
      "url": "/research/2021/05/23/How-to-decay-your-learning-rate.html",
      "date": "May 23, 2021",
      "summary": "Empirical findings suggest that typical fine-tuned learning rate schedules decay the learning rate following weight norm fluctuations. This led to the development of ABEL, an automatic scheduler that adjusts the learning rate based on weight norm changes. ABEL performs comparably to tuned schedules but demonstrates greater robustness to parameter variations. Extensive experiments across various domains reveal that when the weight norm remains stable, simplified schedules yield equivalent performance to complex ones, resembling a constant learning rate with decay towards the end of training."
    },
    {
      "title": "Root Mean Square Layer Normalization",
      "url": "/research/2019/10/16/Root-Mean-Square-Layer-Normalization.html",
      "date": "October 16, 2019",
      "summary": "Layer normalization (LayerNorm) enhances deep neural network stability and convergence by re-centering and re-scaling inputs and weight matrices. However, its computational overhead slows networks, particularly RNNs. We introduce RMSNorm, which replaces re-centering with root mean square (RMS) regularization. RMSNorm maintains re-scaling invariance and adapts learning rates implicitly, while being computationally simpler than LayerNorm. We also propose partial RMSNorm (pRMSNorm), estimating RMS from a subset of inputs. Empirical results across various tasks and architectures demonstrate that RMSNorm achieves comparable performance to LayerNorm while reducing running time by 7%∼64%."
    },
    {
      "title": "DeepNet: Scaling Transformers to 1,000 Layers",
      "url": "/research/2022/03/01/DeepNet-Scaling-Transformers-to-1,000-Layers.html",
      "date": "March 1, 2022",
      "summary": "This paper introduces DEEPNORM, a novel method to stabilize extremely deep Transformers through a new normalization function and a theoretically derived initialization for the residual connection. DEEPNORM offers a stable and efficient training regime, blending the benefits of Post-LN's performance with Pre-LN's stability, allowing for the scaling of Transformers up to 1,000 layers without difficulty. This represents a significant advancement over previous models, with a 200-layer, 3.2B parameter model outperforming a 48-layer, 12B parameter state-of-the-art model by 5 BLEU points on a multilingual benchmark with 7,482 translation directions, suggesting a promising direction for scaling."
    }
  ],
  "rectify": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "This paper intriduces the Kaiming He initialisation strategy for RELU like activation functions. This the default init in pytorch, improving model stability and decreasing training time for RELU networks. This study also introduces a Parametric Rectified Linear Unit (PReLU) activation function that extends traditional units, offering improved model fitting with negligible additional computational cost."
    },
    {
      "title": "Stable Diffusion 3: Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "This paper focuses on improving diffusion models for high-dimensional data like images. The authors enhance noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Their large-scale study shows superior performance over established diffusion methods in high-resolution text-to-image synthesis. A new transformer-based architecture is introduced for better text comprehension, typography, and human preference in text-to-image generation. The findings demonstrate predictable scaling trends, with lower validation loss tied to improved synthesis. The largest models outperform state-of-the-art approaches."
    },
    {
      "title": "Searching for Activation Functions",
      "url": "/research/2017/10/27/Searching-for-Activation-Functions.html",
      "date": "October 27, 2017",
      "summary": "This study explores the impact of activation functions on deep network training and performance. While Rectified Linear Unit (ReLU) is widely used, alternatives have not consistently outperformed it. We propose using automatic search techniques to discover new activation functions. Through exhaustive and reinforcement learning-based searches, we identify novel functions. Empirical evaluation shows that our best discovered function, Swish (f(x) = x · sigmoid(βx)), performs better than ReLU on deeper models across challenging datasets. Replacing ReLUs with Swish units improves classification accuracy on ImageNet, for example, by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. Swish's simplicity and similarity to ReLU facilitate its adoption in neural networks."
    }
  ],
  "prelu": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "This paper intriduces the Kaiming He initialisation strategy for RELU like activation functions. This the default init in pytorch, improving model stability and decreasing training time for RELU networks. This study also introduces a Parametric Rectified Linear Unit (PReLU) activation function that extends traditional units, offering improved model fitting with negligible additional computational cost."
    }
  ],
  "fit": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "This paper intriduces the Kaiming He initialisation strategy for RELU like activation functions. This the default init in pytorch, improving model stability and decreasing training time for RELU networks. This study also introduces a Parametric Rectified Linear Unit (PReLU) activation function that extends traditional units, offering improved model fitting with negligible additional computational cost."
    },
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "cost": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "This paper intriduces the Kaiming He initialisation strategy for RELU like activation functions. This the default init in pytorch, improving model stability and decreasing training time for RELU networks. This study also introduces a Parametric Rectified Linear Unit (PReLU) activation function that extends traditional units, offering improved model fitting with negligible additional computational cost."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "This study introduces the Vision Transformer ViT archetecture. The paper shows that a pure transformer applied directly to sequences of image patches can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces TinyViT, a series of compact and efficient vision transformers (ViTs) designed for devices with limited resources. The authors employ a fast distillation framework to transfer knowledge from large pretrained models to smaller ones during pretraining, using sparsified logits from teacher models to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. With increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters, and demonstrates strong transferability across various downstream tasks."
    },
    {
      "title": "Curriculum Learning by Dynamic Instance Hardness",
      "url": "/research/2020/12/06/Curriculum-Learning-by-Dynamic-Instance-Hardness.html",
      "date": "December 6, 2020",
      "summary": "This paper improves curriculum learning via introduces Dynamic Instance Hardness (DIH), a method that measures a sample's learning difficulty over time. DIH provides a stable indicator of learning progress by tracking the exponential moving average of a sample's hardness. DIHCL enhances learning efficiency and model accuracy without extra computational costs by leveraging data from the training process itself. Tested on 11 datasets, DIHCL surpasses traditional training methods and recent curriculum learning techniques in both efficiency and effectiveness."
    },
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This paper investigates using high-quality rendering software and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle. This synthetic dataset is used to augment limited real training data for image classification and semantic segmentation tasks. While models trained solely on synthetic images showed low accuracy on real validation data, including even small amounts of real data significantly improved performance. Augmenting real training data with synthetic images outperformed using only real images. Furthermore, pretraining models on the synthetic dataset before transfer learning significantly reduced training costs by allowing most of the training to be completed upfront using the synthetic data."
    },
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "The paper introduces TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method that applies a single augmentation to each image. Despite its minimal complexity and cost, TrivialAugment outperforms existing methods across various image classification scenarios, as validated through extensive experiments against state-of-the-art methods and multiple ablation studies involving different augmentation spaces and methods. The work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Noting a stagnation in automatic augmentation research, the authors conclude by proposing best practices for future advancements in the field."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    },
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "A comprehensive review of Binary Neural Network",
      "url": "/research/2023/05/11/A-comprehensive-review-of-Binary-Neural-Network.html",
      "date": "May 11, 2023",
      "summary": "Deep learning (DL) is widely used in intelligent systems and real-life applications, but its deployment on computationally limited and energy-constrained devices requires efficient technologies like Binary Neural Networks (BNN). BNNs save significant storage, computation cost, and energy, making them ideal for small devices, despite the trade-offs in memory and performance. This article offers a comprehensive review of BNN developments, focusing on 1-bit activations and weights. It covers the evolution of BNN, from early models to advanced algorithms and design aspects, and addresses BNN optimization, deployment, computing architectures, and diverse applications. It also outlines potential research directions in BNN technology."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "Sharpness-Aware Training for Free",
      "url": "/research/2023/03/02/Sharpness-Aware-Training-for-Free.html",
      "date": "March 2, 2023",
      "summary": "Modern deep neural networks (DNNs) excel in performance but often suffer from over-parameterization, leading to increased generalization error without tailored training strategies. Sharpness-Aware Minimization (SAM) has proven effective in reducing generalization error by minimizing sharpness in the loss landscape. However, SAM incurs a significant computational overhead. This paper introduces Sharpness-Aware Training for Free (SAF), which mitigates sharpness at nearly zero additional computational cost. SAF achieves this by preventing sudden drops in loss within sharp local minima during weight updates. A novel trajectory loss, based on KL-divergence between current and past DNN outputs, replaces SAM's sharpness measure, guiding convergence towards flat minima for enhanced generalization. Empirical results demonstrate SAF's effectiveness on ImageNet with comparable computational efficiency to the base optimizer."
    },
    {
      "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
      "url": "/research/2022/03/19/Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training.html",
      "date": "March 19, 2022",
      "summary": "Sharpness-Aware Minimization (SAM) enhances generalization by optimizing a neighborhood-based perturbed loss, but it doesn't always favor flat minima due to both sharp and flat minima having low perturbed losses. We introduce a new measure, the surrogate gap, reflecting the dominant Hessian eigenvalue at small neighborhood radii, which is simple to compute and can be minimized during training. We propose the Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), an advancement over SAM with minimal additional computational cost. GSAM employs a dual-step approach: first, minimizing the perturbed loss similar to SAM, and second, reducing the surrogate gap without affecting the perturbed loss to target regions with low loss and sharpness, thus achieving superior generalization. GSAM is theoretically robust, showing better convergence and empirical generalization improvements, notably a +3.2% gain over SAM and +5.4% over AdamW in ImageNet accuracy for ViT-B/32."
    },
    {
      "title": "Neural Network Diffusion",
      "url": "/research/2024/02/20/Neural-Network-Diffusion.html",
      "date": "February 20, 2024",
      "summary": "Diffusion models, known for their success in image and video generation, can also generate high-performing neural network parameters, as demonstrated in this study. By employing a simple combination of an autoencoder and a standard latent diffusion model, our method involves extracting latent representations of network parameters, which are then synthesized from random noise by the diffusion model. These new representations, processed through the autoencoder's decoder, serve as fresh network parameters. Tested across various architectures and datasets, this diffusion approach consistently produces models with comparable or superior performance to traditionally trained networks at minimal extra cost. Importantly, the generated models show distinct performance differences from the trained ones, suggesting further exploration into the versatile applications of diffusion models."
    },
    {
      "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
      "url": "/research/2024/03/13/Simple-and-Scalable-Strategies-to-Continually-Pre-train-Large-Language-Models.html",
      "date": "March 13, 2024",
      "summary": "Large language models (LLMs) often require re-training on new data, consuming extensive computational resources. We present an efficient method that combines learning rate re-warming, re-decaying, and data replay, effectively maintaining performance without full re-training. This approach works well across different data distributions, including minor shifts (English→English) and significant shifts (English→German), tested up to 405M and 10B parameter models. Our findings suggest that continual learning strategies can update LLMs with minimal computational cost, rivaling traditional re-training methods. Additionally, we propose alternatives to the cosine learning rate schedule to reduce forgetting, offering more flexibility in learning without a fixed token budget."
    }
  ],
  "dropattention": [
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/research/2019/07/26/DropAttention-A-Regularization-Method-for-Fully-Connected-Self-Attention-Networks.html",
      "date": "July 26, 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. DropAttention uses a mask to zero out elements in the attention matrix. Experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    }
  ],
  "zero": [
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/research/2019/07/26/DropAttention-A-Regularization-Method-for-Fully-Connected-Self-Attention-Networks.html",
      "date": "July 26, 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. DropAttention uses a mask to zero out elements in the attention matrix. Experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    }
  ],
  "element": [
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/research/2019/07/26/DropAttention-A-Regularization-Method-for-Fully-Connected-Self-Attention-Networks.html",
      "date": "July 26, 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. DropAttention uses a mask to zero out elements in the attention matrix. Experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    }
  ],
  "matrix": [
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/research/2019/07/26/DropAttention-A-Regularization-Method-for-Fully-Connected-Self-Attention-Networks.html",
      "date": "July 26, 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. DropAttention uses a mask to zero out elements in the attention matrix. Experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    },
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a method for dropout in Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks",
      "url": "/research/2020/06/11/Batch-Normalization-Provably-Avoids-Rank-Collapse-for-Randomly-Initialised-Deep-Networks.html",
      "date": "June 11, 2020",
      "summary": "We investigate the challenge of training randomly initialized deep neural networks due to spectral instabilities in products of random matrices. Batch normalization emerges as an effective solution to prevent rank collapse in both linear and ReLU networks. Leveraging tools from Markov chain theory, we establish a lower rank bound for deep linear networks. Empirical findings show that this rank robustness extends to ReLU networks. Our experiments on real-world datasets underscore the significance of rank stability in training modern deep neural architectures."
    },
    {
      "title": "Eigencurve: Optimal Learning Rate Schedule For Sgd on Quadratic Objectives With Skewed Hessian Spectrums",
      "url": "/research/2022/06/14/Eigencurve-Optimal-Learning-Rate-Schedule-For-Sgd-on-Quadratic-Objectives-With-Skewed-Hessian-Spectrums.html",
      "date": "June 14, 2022",
      "summary": "Learning rate schedulers are essential in deep neural network training, but there's a gap between practical usage and theoretical understanding. This paper introduces Eigencurve, the first set of learning rate schedules achieving minimax optimal convergence rates for SGD on quadratic objectives with skewed eigenvalue distributions of the Hessian matrix. This condition is common in practice. Experimental results on CIFAR-10 image classification tasks demonstrate Eigencurve's superiority over step decay, particularly with fewer epochs. The theory inspires practical schedulers approximating Eigencurve, resembling cosine decay for some problems while outperforming it in others."
    },
    {
      "title": "Root Mean Square Layer Normalization",
      "url": "/research/2019/10/16/Root-Mean-Square-Layer-Normalization.html",
      "date": "October 16, 2019",
      "summary": "Layer normalization (LayerNorm) enhances deep neural network stability and convergence by re-centering and re-scaling inputs and weight matrices. However, its computational overhead slows networks, particularly RNNs. We introduce RMSNorm, which replaces re-centering with root mean square (RMS) regularization. RMSNorm maintains re-scaling invariance and adapts learning rates implicitly, while being computationally simpler than LayerNorm. We also propose partial RMSNorm (pRMSNorm), estimating RMS from a subset of inputs. Empirical results across various tasks and architectures demonstrate that RMSNorm achieves comparable performance to LayerNorm while reducing running time by 7%∼64%."
    },
    {
      "title": "A Phase Transition between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention",
      "url": "/research/2024/02/06/A-Phase-Transition-between-Positional-and-Semantic-Learning-in-a-Solvable-Model-of-Dot-Product-Attention.html",
      "date": "February 6, 2024",
      "summary": "We explore the learning process of a dot-product attention layer, which learns both positional and semantic attention matrices, enabling tokens to attend based on position or meaning. Through experiments on an algorithmic task, we demonstrate that this architecture can use either mechanism for solving the task. Theoretically, we examine a non-linear self-attention layer with special query and key matrices, offering a closed-form solution for its non-convex loss landscape in high-dimensional data, which reveals a phase transition from positional to semantic mechanisms as sample complexity increases. We also show that the dot-product attention layer surpasses a linear positional baseline through the semantic mechanism with adequate data."
    },
    {
      "title": "Non-negative Contrastive Learning",
      "url": "/research/2024/03/19/Non-negative-Contrastive-Learning.html",
      "date": "March 19, 2024",
      "summary": "Deep representations offer promising performance for downstream tasks but lack interpretability, posing a significant challenge. In this paper, we introduce Non-negative Contrastive Learning (NCL), a refinement of Non-negative Matrix Factorization (NMF) aimed at generating interpretable features. NCL enforces non-negativity constraints, akin to NMF, resulting in sparse and disentangled representations, unlike standard contrastive learning (CL). We establish theoretical guarantees on NCL's identifiability and downstream generalization. Empirically, NCL outperforms CL in feature disentanglement, selection, and downstream classification tasks. Moreover, NCL can be extended to other learning scenarios, benefiting supervised learning."
    },
    {
      "title": "The loss landscape of deep linear neural networks: a second-order analysis",
      "url": "/research/2022/03/11/The-loss-landscape-of-deep-linear-neural-networks-a-second-order-analysis.html",
      "date": "March 11, 2022",
      "summary": "This study investigates the optimization landscape of deep linear neural networks with square loss, focusing on the role of non-strict saddle points in algorithm dynamics, which has been previously understudied. the authors conduct a comprehensive second-order analysis, identifying global minimizers, strict, and non-strict saddle points among all critical points, alongside their critical values. Our findings, based on conditions related to the ranks of partial matrix products, contribute insights into global convergence and implicit regularization observed in optimizing linear neural networks. Additionally, the authors offer an explicit parameterization of global minimizers and identify extensive sets of strict and non-strict saddle points."
    },
    {
      "title": "Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes",
      "url": "/research/2017/11/28/Towards-Understanding-Generalization-of-Deep-Learning-Perspective-of-Loss-Landscapes.html",
      "date": "November 28, 2017",
      "summary": "This study examines why deep learning models, despite having more parameters than training samples, generalize well. It highlights the importance of the loss function's landscape, showing that areas leading to better generalization (good minima) are more prevalent than those leading to poor outcomes. This predominance facilitates the convergence of optimization methods to these beneficial minima. The research provides theoretical backing by analyzing 2-layer neural networks, noting that solutions with better generalization have a smaller Hessian matrix norm. For deeper networks, extensive numerical data supports these conclusions."
    },
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "providing": [
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/research/2019/07/26/DropAttention-A-Regularization-Method-for-Fully-Connected-Self-Attention-Networks.html",
      "date": "July 26, 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. DropAttention uses a mask to zero out elements in the attention matrix. Experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    }
  ],
  "dropkey": [
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a method for dropout in Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers."
    }
  ],
  "ignore": [
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a method for dropout in Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers."
    },
    {
      "title": "Taming the Waves: Sine as Activation Function in Deep Neural Networks",
      "url": "/research/2016/11/04/Taming-the-Waves-Sine-as-Activation-Function-in-Deep-Neural-Networks.html",
      "date": "November 4, 2016",
      "summary": "This paper explores the challenges in training deep neural networks that use sinusoidal activation functions. The authors explain that the difficulty arises from the emergence of numerous shallow local minima in the loss landscape. The study reveals that successful learning in typical classification tasks occurs when the network effectively ignores the periodic cycles of the sinusoidal functions. However, for certain non-trivial tasks, networks with sinusoidal activations can outperform those using traditional monotonic activation functions."
    }
  ],
  "softmax": [
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a method for dropout in Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers."
    },
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions, SiLU and dSiLU. It challenges the need for experience replay and separate target networks in deep reinforcement learning. By using on-policy learning with eligibility traces and softmax action selection, it achieves state-of-the-art results in stochastic SZ-Tetris and small-board Tetris with TD(λ) learning and shallow dSiLU agents. It also outperforms DQN in Atari 2600 with a deep Sarsa(λ) agent using SiLU/dSiLU, suggesting an alternative to traditional DQN approaches."
    },
    {
      "title": "Normalized Attention Without Probability Cage",
      "url": "/research/2020/05/19/Normalized-Attention-Without-Probability-Cage.html",
      "date": "May 19, 2020",
      "summary": "Softmax-attention architectures, especially popularized by Transformers, have seen significant advancements in various tasks. However, the geometric implications of softmax-attention remain underexplored. In this study, we demonstrate limitations arising from constraining attention weights to the probability simplex and its impact on the convex hull of value vectors. We reveal sequence length-dependent biases in Transformers towards token isolation at initialization and compare them with max- and sum-pooling, which are strong but often overlooked baselines. To address these issues, we propose a novel approach of replacing softmax with normalization in self-attention, resulting in a robust and widely applicable architecture. Our findings are supported by empirical results from over 25,000 trained models, and all results and implementations are publicly available."
    }
  ],
  "ratio": [
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a method for dropout in Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "The authors introduce VideoMAE, a data-efficient self-supervised video pre-training (SSVP) approach that utilizes video masked autoencoders with a novel high-ratio video tube masking technique inspired by ImageMAE. Key findings include the effectiveness of high masking ratios (90-95%) due to video's temporal redundancy, strong performance on small datasets (~3k-4k videos) without extra data highlighting the importance of high-level structure learning, and data quality being more crucial than quantity with domain shift being significant. Notably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone without extra data."
    }
  ],
  "schedule": [
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a method for dropout in Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers."
    },
    {
      "title": "Dropout Reduces Underfitting",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. Authors then introduce two dropout schedules: early dropout which prevents underfitting, and late dropout which prevents overfitting."
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "This paper introduces DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. Authors also propose a dropout rate scheduler to optimize training."
    },
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/research/2022/10/10/Turbo-Training-with-Token-Dropout.html",
      "date": "October 10, 2022",
      "summary": "This paper introduces Turbo training, an efficient method for training Transformers on video-related tasks. The authors present a simple yet versatile training paradigm applicable to multiple video tasks and demonstrate its effectiveness across action classification, video-language representation learning, and long-video activity classification. Turbo training achieves competitive performance with up to 4× speed-up and reduced memory usage, enabling long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous resource-intensive methods.."
    },
    {
      "title": "Automated Learning Rate Scheduler for Large-batch",
      "url": "/research/2021/07/13/Automated-Learning-Rate-Scheduler-for-Large-batch.html",
      "date": "July 13, 2021",
      "summary": "Large-batch training is crucial for deep learning with large datasets and models, but requires specific learning rate (LR) schedules for optimal performance, especially under limited training epochs. This work introduces an automated LR scheduling algorithm for large-batch neural network training within a fixed epoch budget, consisting of adaptive warmup and predefined decay phases. The LR is dynamically adjusted based on training loss monitored through Gaussian process smoothing, facilitating low computational overhead. When integrated with adaptive stochastic optimizers like AdamP and LAMB, this scheduler eliminates the need for extensive hyperparameter tuning and achieves competitive or superior results on various image classification tasks across different batch sizes and architectures."
    },
    {
      "title": "Eigencurve: Optimal Learning Rate Schedule For Sgd on Quadratic Objectives With Skewed Hessian Spectrums",
      "url": "/research/2022/06/14/Eigencurve-Optimal-Learning-Rate-Schedule-For-Sgd-on-Quadratic-Objectives-With-Skewed-Hessian-Spectrums.html",
      "date": "June 14, 2022",
      "summary": "Learning rate schedulers are essential in deep neural network training, but there's a gap between practical usage and theoretical understanding. This paper introduces Eigencurve, the first set of learning rate schedules achieving minimax optimal convergence rates for SGD on quadratic objectives with skewed eigenvalue distributions of the Hessian matrix. This condition is common in practice. Experimental results on CIFAR-10 image classification tasks demonstrate Eigencurve's superiority over step decay, particularly with fewer epochs. The theory inspires practical schedulers approximating Eigencurve, resembling cosine decay for some problems while outperforming it in others."
    },
    {
      "title": "How to decay your learning rate",
      "url": "/research/2021/05/23/How-to-decay-your-learning-rate.html",
      "date": "May 23, 2021",
      "summary": "Empirical findings suggest that typical fine-tuned learning rate schedules decay the learning rate following weight norm fluctuations. This led to the development of ABEL, an automatic scheduler that adjusts the learning rate based on weight norm changes. ABEL performs comparably to tuned schedules but demonstrates greater robustness to parameter variations. Extensive experiments across various domains reveal that when the weight norm remains stable, simplified schedules yield equivalent performance to complex ones, resembling a constant learning rate with decay towards the end of training."
    },
    {
      "title": "Learning Rate Perturbation: A Generic Plugin of Learning Rate schedule towards Flatter Local Minima",
      "url": "/research/2022/08/25/Learning-Rate-Perturbation-A-Generic-Plugin-of-Learning-Rate-schedule-towards-Flatter-Local-Minima.html",
      "date": "August 25, 2022",
      "summary": "Learning rate is crucial in neural network training, yet existing schedules lack theoretical backing, often leading to suboptimal choices made through trial and error. To address this, we propose LEAP, a plugin enhancing various learning rate schedules by introducing perturbations. This simple yet effective strategy favors flat minima, ensuring better generalization. Extensive experiments demonstrate LEAP's ability to improve performance across diverse datasets and learning rate schedules, including constant ones."
    },
    {
      "title": "Optimal learning rate schedules in high-dimensional non-convex optimization problems",
      "url": "/research/2022/02/09/Optimal-learning-rate-schedules-in-high-dimensional-non-convex-optimization-problems.html",
      "date": "February 9, 2022",
      "summary": "This paper explores the role of learning rate schedules in high-dimensional and non-convex optimization problems, focusing on Langevin optimization with a decaying learning rate. Analyzing models with Gaussian random functions on N-dimensional spheres, the study reveals that to accelerate optimization without getting trapped in saddles, a decay rate β < 1 is optimal, contrary to convex settings where β = 1 is preferred. Introducing a signal recovery component, the dynamics involve an exploration phase navigating through rough landscape parts and a convergence phase entering convex basins. It's found optimal to maintain a large learning rate during exploration to swiftly exit non-convex regions, then transition to β = 1 for rapid convergence to the solution. These findings are validated in a neural network regression task."
    },
    {
      "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
      "url": "/research/2024/03/13/Simple-and-Scalable-Strategies-to-Continually-Pre-train-Large-Language-Models.html",
      "date": "March 13, 2024",
      "summary": "Large language models (LLMs) often require re-training on new data, consuming extensive computational resources. We present an efficient method that combines learning rate re-warming, re-decaying, and data replay, effectively maintaining performance without full re-training. This approach works well across different data distributions, including minor shifts (English→English) and significant shifts (English→German), tested up to 405M and 10B parameter models. Our findings suggest that continual learning strategies can update LLMs with minimal computational cost, rivaling traditional re-training methods. Additionally, we propose alternatives to the cosine learning rate schedule to reduce forgetting, offering more flexibility in learning without a fixed token budget."
    }
  ],
  "balance": [
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a method for dropout in Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers."
    }
  ],
  "retention": [
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a method for dropout in Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers."
    },
    {
      "title": "From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning",
      "url": "/research/2023/03/16/From-MNIST-to-ImageNet-and-Back:-Benchmarking-Continual-Curriculum-Learning.html",
      "date": "March 16, 2023",
      "summary": "Continual learning (CL) is a promising trend in machine learning, aiming to develop robust models and strategies for dynamic environments by incorporating new knowledge while retaining past knowledge. CL research is fragmented, with various protocols, tasks, datasets, and metrics, often not reflecting real-world complexity and tailored to specific strategies. This work addresses this gap by introducing two novel CL benchmarks with heterogeneous tasks from six image datasets, varying in complexity and quality. These benchmarks fairly evaluate state-of-the-art CL strategies in scenarios closer to real-world conditions. The benchmarks present tasks in both increasing and decreasing complexity, assessing models' ability to exploit task structure. The work emphasizes a rigorous, reproducible evaluation protocol for measuring generalization and memory retention in models. Experimental results show that popular CL strategies perform poorly on these benchmarks, exhibit high forgetting, and struggle with curriculum task ordering. These findings underscore the need for rigorous comparisons and the development of new CL strategies for complex scenarios."
    }
  ],
  "structured": [
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a method for dropout in Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers."
    }
  ],
  "way": [
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/research/2014/01/01/Dropout-A-Simple-Way-to-Prevent-Neural-Networks-from-Overfitting.html",
      "date": "January 1, 2014",
      "summary": "This paper introduces Dropout, a technique that addresses overfitting by randomly omitting units during training, leading to an exponential number of thinned networks. This effectively turns the test time network into an ensemble."
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM) for self-supervised visual representation in vision transformers ViT. Five different learning objectives were proposed, which degraded the input image in various ways similar to masking. Design principles are proposed for  token-based pre-training of vision transformers. The most effective strategy combined preserving the original image style with spatial misalignment and masking. This approach outperformed traditional MIM on downstream recognition tasks without increasing computational demands."
    },
    {
      "title": "SAM as an Optimal Relaxation of Bayes",
      "url": "/research/2023/12/10/SAM-as-an-Optimal-Relaxation-of-Bayes.html",
      "date": "December 10, 2023",
      "summary": "We introduce SAM, a method enhancing generalization in deep learning, as a relaxation of the Bayes objective. SAM replaces the expected negative-loss with an optimal convex lower bound derived using the Fenchel biconjugate. This connection enables an Adam-like extension of SAM, offering automatic uncertainty estimates and potential accuracy improvements. Bridging adversarial and Bayesian methods, our work paves the way for robustness enhancement."
    }
  ],
  "overfitting": [
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/research/2014/01/01/Dropout-A-Simple-Way-to-Prevent-Neural-Networks-from-Overfitting.html",
      "date": "January 1, 2014",
      "summary": "This paper introduces Dropout, a technique that addresses overfitting by randomly omitting units during training, leading to an exponential number of thinned networks. This effectively turns the test time network into an ensemble."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "This paper proposes Hard Patches Mining (HPM), a novel framework for masked image modeling (MIM) pre-training that goes beyond simply solving given problems. HPM aims for the model to generate more challenging tasks for itself, using reconstruction loss as a metric for task difficulty. It incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper introduces the concept of grokking, a sudden onset of perfect generalisation long after overfitting, on small, algorithmically generated datasets. The study also finds that generalization on smaller datasets requires more optimization. The authors suggest that such datasets are ideal for investigating the puzzling phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    },
    {
      "title": "Understanding Deep Learning Requires Re- Thinking Generalization",
      "url": "/research/2017/02/26/Understanding-Deep-Learning-Requires-Re--Thinking-Generalization.html",
      "date": "February 26, 2017",
      "summary": "Extensive experiments reveal that traditional explanations for the strong generalization performance of large neural networks fall short. Despite conventional wisdom attributing success to model properties or regularization techniques, our findings show that state-of-the-art convolutional networks for image classification can easily overfit random labels. This phenomenon persists regardless of explicit regularization or substituting true images with random noise. Theoretical analysis suggests that shallow neural networks achieve perfect finite sample expressivity when parameters outnumber data points, as commonly observed. We interpret these results in contrast to traditional models."
    }
  ],
  "omit": [
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/research/2014/01/01/Dropout-A-Simple-Way-to-Prevent-Neural-Networks-from-Overfitting.html",
      "date": "January 1, 2014",
      "summary": "This paper introduces Dropout, a technique that addresses overfitting by randomly omitting units during training, leading to an exponential number of thinned networks. This effectively turns the test time network into an ensemble."
    }
  ],
  "turn": [
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/research/2014/01/01/Dropout-A-Simple-Way-to-Prevent-Neural-Networks-from-Overfitting.html",
      "date": "January 1, 2014",
      "summary": "This paper introduces Dropout, a technique that addresses overfitting by randomly omitting units during training, leading to an exponential number of thinned networks. This effectively turns the test time network into an ensemble."
    }
  ],
  "well": [
    {
      "title": "Dropout Reduces Underfitting",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. Authors then introduce two dropout schedules: early dropout which prevents underfitting, and late dropout which prevents overfitting."
    }
  ],
  "variance": [
    {
      "title": "Dropout Reduces Underfitting",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. Authors then introduce two dropout schedules: early dropout which prevents underfitting, and late dropout which prevents overfitting."
    },
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks",
      "url": "/research/2017/10/23/Cosine-Normalization-Using-Cosine-Similarity-Instead-of-Dot-Product-in-Neural-Networks.html",
      "date": "October 23, 2017",
      "summary": "In traditional multi-layer neural networks, the dot product between the output and weight vectors of preceding layers serves as input to the activation function, resulting in unbounded outputs and increased variance. This variance can lead to poor generalization and hinder training by exacerbating internal covariate shift. To address this, we propose cosine normalization, which replaces the dot product with cosine similarity or centered cosine similarity (Pearson Correlation Coefficient). We evaluate cosine normalization against batch, weight, and layer normalization in fully-connected and convolutional neural networks across various datasets. Our experiments demonstrate that cosine normalization outperforms other normalization techniques."
    }
  ],
  "align": [
    {
      "title": "Dropout Reduces Underfitting",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. Authors then introduce two dropout schedules: early dropout which prevents underfitting, and late dropout which prevents overfitting."
    },
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "This paper introduces a Fourier neural network (FNN) that aligns with Fourier decomposition by utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design allows for easy integration with more complex networks for data processing tasks. The authors demonstrate the FNN's efficacy on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits of the FNN include solution validity beyond the training scope, model interpretability, and ease of use."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "This study challenges the traditional understanding of how Convolutional Neural Networks (CNNs) recognize objects, revealing that they are more biased towards recognizing textures rather than learning complex shapes. The authors found that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, training a ResNet-50 on a stylized version of ImageNet, designed to emphasize shape, aligned the network's performance more closely with human behavior. This shape-based training matched human performance in controlled experiments and enhanced object detection and robustness against image distortions, highlighting the benefits of shape-based representations in visual recognition systems."
    },
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "This paper introduces a curriculum learning framework for image-caption pretraining, inspired by children's language learning from cognitive science, to address the challenges of aligning multiple concepts from captions to objects in images. The method starts with simple image-caption pairs and gradually increases complexity by adding more concepts, leveraging knowledge from each phase for subsequent learning. This approach outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    },
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    },
    {
      "title": "HYPO: Hyperspherical Out-of-Distribution Generalization",
      "url": "/research/2024/03/19/HYPO-Hyperspherical-Out-of-Distribution-Generalization.html",
      "date": "March 19, 2024",
      "summary": "We propose HYPO, a novel framework for out-of-distribution (OOD) generalization in machine learning, which learns domain-invariant features in a hyperspherical space. Our method focuses on aligning features of the same class across different domains close to their class prototypes and separating different class prototypes maximally. We offer theoretical justifications for its improvement on OOD generalization and show through experiments on OOD benchmarks that HYPO outperforms existing baselines with superior performance."
    },
    {
      "title": "How Does Sharpness-Aware Minimization Minimize Sharpness?",
      "url": "/research/2023/01/05/How-Does-Sharpness-Aware-Minimization-Minimize-Sharpness.html",
      "date": "January 5, 2023",
      "summary": "Sharpness-Aware Minimization (SAM) enhances deep neural networks' generalization across various settings. Although SAM aims to penalize model sharpness using a computationally efficient approach, its exact operational mechanism and the sharpness notion it regularizes remain unclear, partly due to differing sharpness concepts used in its theoretical framework and empirical validation. This study identifies the precise sharpness concept SAM regulates and explains its mechanism. It reveals that the combined effect of SAM's two-step approximations, despite being individually misleading, correctly improves generalization when using full-batch gradients. Moreover, we demonstrate that the stochastic SAM version indeed regularizes a third sharpness notion, aligning more closely with practical performance. This effectiveness is attributed to the gradient's alignment with the Hessian's top eigenvector under SAM."
    },
    {
      "title": "Towards a Unified Theoretical Understanding of Non-contrastive Learning via Rank Differential Mechanism",
      "url": "/research/2023/03/04/Towards-a-Unified-Theoretical-Understanding-of-Non-contrastive-Learning-via-Rank-Differential-Mechanism.html",
      "date": "March 4, 2023",
      "summary": "Recent non-contrastive learning methods like BYOL, SimSiam, SwAV, and DINO have shown that asymmetric architectural designs can achieve good self-supervised visual learning performance by aligning positive pairs alone, despite a lack of unified theoretical understanding on how these designs prevent feature collapse. This study introduces the Rank Differential Mechanism (RDM) as a unified theoretical framework for non-contrastive learning. RDM demonstrates that these asymmetric designs maintain a consistent rank difference in output features, enhancing effective dimensionality and preventing feature collapse. Unlike previous theories, RDM applies to designs both with and without a predictor, offering a comprehensive understanding of non-contrastive learning methods and guiding the development of new variants. Experiments confirm that these variants perform comparably, if not better than, existing methods on benchmark datasets."
    },
    {
      "title": "The Platonic Representation Hypothesis",
      "url": "/research/2024/05/13/The-Platonic-Representation-Hypothesis.html",
      "date": "May 13, 2024",
      "summary": "This paper argues that AI model representations, especially in deep networks, are converging. It surveys examples in the literature showing how neural network representations align over time and across domains. It demonstrates convergence across data modalities, noting that as vision and language models grow, they measure distances between data points similarly. The authors hypothesize that this convergence leads to a shared statistical model of reality, termed the platonic representation and discuss possible selective pressures toward it. The paper also addresses the implications, limitations, and counterexamples to this trend."
    }
  ],
  "prevents": [
    {
      "title": "Dropout Reduces Underfitting",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. Authors then introduce two dropout schedules: early dropout which prevents underfitting, and late dropout which prevents overfitting."
    }
  ],
  "group": [
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes."
    }
  ],
  "bn": [
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes."
    }
  ],
  "estimation": [
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "This paper introduces a self-supervised vision model pretraining framework inspired by energy-based models (EBMs), leveraging symmetric mappings in deep networks without auxiliary components. The framework models energy estimation and data restoration through the network's forward and backward passes, assigning low energy to unlabeled dataset samples and using gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting various pretext tasks. Experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "This paper introduces a novel approach called mixed-order self-paced curriculum learning (Mo-SCL) to address the challenges faced by self-paced curriculum learning (SCL) in medical image analysis tasks, such as universal lesion detection. These challenges include inaccurate difficulty estimation and the under-utilization of hard samples. Mo-SCL combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches. Through theoretical analysis and experiments on the DeepLesion dataset, the authors demonstrate that Mo-SCL enhances lesion detection accuracy in state-of-the-art methods without requiring additional network modifications."
    },
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces an iterative self-training framework for sim-to-real 6D object pose estimation to enable cost-effective robotic bin-picking. The authors create a photo-realistic simulator to synthesize virtual data for training an initial pose estimation network (teacher model). This teacher predicts poses on unlabeled real data, and an adaptive selection scheme filters reliable predictions to generate pseudo-labels for training a student model on real data. Iteratively refining the teacher with the student improves pseudo-label quality. Evaluated on public benchmarks and a new dataset, their method shows 11.49% and 22.62% ADD(-S) improvements and a 19.54% increase in bin-picking success, demonstrating the effectiveness of this iterative sim-to-real approach."
    },
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "This paper introduces tools for training monocular depth estimation models with multiple datasets despite incompatible annotations. Their approach uses a robust training objective, multi-objective learning for data integration, and pretraining encoders on auxiliary tasks. Tested across five datasets, including 3D films as a novel data source, their methods achieve superior zero-shot cross-dataset generalization, outperforming existing benchmarks through principled multi-dataset training."
    }
  ],
  "constrain": [
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates how to achieve state-of-the-art certified adversarial robustness to ℓ2-norm bounded perturbations using only off-the-shelf pretrained models. The authors instantiate the denoised smoothing approach by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This method certifies 71% accuracy on ImageNet under adversarial perturbations constrained to an ℓ2 norm of ε = 0.5, improving upon the prior certified state-of-the-art by 14 percentage points and denoised smoothing by 30 percentage points, without requiring any fine-tuning or retraining of model parameters."
    },
    {
      "title": "Normalized Attention Without Probability Cage",
      "url": "/research/2020/05/19/Normalized-Attention-Without-Probability-Cage.html",
      "date": "May 19, 2020",
      "summary": "Softmax-attention architectures, especially popularized by Transformers, have seen significant advancements in various tasks. However, the geometric implications of softmax-attention remain underexplored. In this study, we demonstrate limitations arising from constraining attention weights to the probability simplex and its impact on the convex hull of value vectors. We reveal sequence length-dependent biases in Transformers towards token isolation at initialization and compare them with max- and sum-pooling, which are strong but often overlooked baselines. To address these issues, we propose a novel approach of replacing softmax with normalization in self-attention, resulting in a robust and widely applicable architecture. Our findings are supported by empirical results from over 25,000 trained models, and all results and implementations are publicly available."
    },
    {
      "title": "Penalizing Gradient Norm for Efficiently Improving Generalization in Deep Learning",
      "url": "/research/2022/06/26/Penalizing-Gradient-Norm-for-Efficiently-Improving-Generalization-in-Deep-Learning.html",
      "date": "June 26, 2022",
      "summary": "This paper proposes a method to enhance the generalization of deep neural networks (DNNs) by penalizing the gradient norm of the loss function during optimization. By constraining the gradient norm, the optimizers tend to find flat minima, improving generalization. We efficiently implement this method using first-order approximation within the gradient descent framework. Experimental results demonstrate improved generalization across various models and datasets. Additionally, we show that a recent method, sharpness-aware minimization, is a special case of our approach, with our method achieving new state-of-the-art performance on tested tasks."
    }
  ],
  "gn": [
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes."
    }
  ],
  "divide": [
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes."
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, the authors demonstrate the close relationship between these two families. This analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    }
  ],
  "channel": [
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes."
    },
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "This paper introduces SlowFast networks, a new convolutional architecture for video recognition tasks. These networks have two pathways: a Slow pathway operating at low frame rates to capture spatial semantics, and a Fast pathway working at high frame rates with reduced channel capacity to efficiently capture motion details. The SlowFast models achieve top accuracy on major video benchmarks like Kinetics, Charades, and AVA, significantly improving performance for action classification and detection tasks."
    }
  ],
  "calibration": [
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/research/2023/06/30/Impact-of-Noise-on-Calibration-and-Generalisation-of-Neural-Networks.html",
      "date": "June 30, 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. Activation noise is shown to significantly improve generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This study suggest that label smoothing not only boosts generalization but also enhances model calibration. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. Label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "This paper introduces a novel approach to Radiance Field methods for novel-view synthesis that addresses the challenges of high visual quality, costly training, and real-time rendering of unbounded, complete scenes at 1080p resolution. The authors propose three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. The method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    },
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    }
  ],
  "generalisation": [
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/research/2023/06/30/Impact-of-Noise-on-Calibration-and-Generalisation-of-Neural-Networks.html",
      "date": "June 30, 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. Activation noise is shown to significantly improve generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    },
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper introduces the concept of grokking, a sudden onset of perfect generalisation long after overfitting, on small, algorithmically generated datasets. The study also finds that generalization on smaller datasets requires more optimization. The authors suggest that such datasets are ideal for investigating the puzzling phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    }
  ],
  "injection": [
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/research/2023/06/30/Impact-of-Noise-on-Calibration-and-Generalisation-of-Neural-Networks.html",
      "date": "June 30, 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. Activation noise is shown to significantly improve generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    }
  ],
  "nn": [
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/research/2023/06/30/Impact-of-Noise-on-Calibration-and-Generalisation-of-Neural-Networks.html",
      "date": "June 30, 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. Activation noise is shown to significantly improve generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    }
  ],
  "ingredient": [
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/research/2017/11/06/Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization.html",
      "date": "November 6, 2017",
      "summary": "This paper itroduces Instance Normalization, an online normalization technique that calculates the mean and std across individual data instances. This is shown to work well in situations where there is very high vatiation within batches, such as in style transfer."
    }
  ],
  "stylization": [
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/research/2017/11/06/Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization.html",
      "date": "November 6, 2017",
      "summary": "This paper itroduces Instance Normalization, an online normalization technique that calculates the mean and std across individual data instances. This is shown to work well in situations where there is very high vatiation within batches, such as in style transfer."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Incorporating diverse image styles into CNN training data reduces texture bias, enhances shape recognition, and improves resilience against common image corruptions. This is typically explained as decreasing model reliance on high frequency texture information. This paper challenges this explaniation through large scale testing with natural images, edge information, and stylization, finding no direct link between shape bias and robustness. The enhanced corruption robustness is instead attributed to style variation in data augmentation, with increased shape bias being an indirect effect."
    }
  ],
  "itroduces": [
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/research/2017/11/06/Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization.html",
      "date": "November 6, 2017",
      "summary": "This paper itroduces Instance Normalization, an online normalization technique that calculates the mean and std across individual data instances. This is shown to work well in situations where there is very high vatiation within batches, such as in style transfer."
    }
  ],
  "situation": [
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/research/2017/11/06/Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization.html",
      "date": "November 6, 2017",
      "summary": "This paper itroduces Instance Normalization, an online normalization technique that calculates the mean and std across individual data instances. This is shown to work well in situations where there is very high vatiation within batches, such as in style transfer."
    }
  ],
  "there": [
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/research/2017/11/06/Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization.html",
      "date": "November 6, 2017",
      "summary": "This paper itroduces Instance Normalization, an online normalization technique that calculates the mean and std across individual data instances. This is shown to work well in situations where there is very high vatiation within batches, such as in style transfer."
    },
    {
      "title": "Eigencurve: Optimal Learning Rate Schedule For Sgd on Quadratic Objectives With Skewed Hessian Spectrums",
      "url": "/research/2022/06/14/Eigencurve-Optimal-Learning-Rate-Schedule-For-Sgd-on-Quadratic-Objectives-With-Skewed-Hessian-Spectrums.html",
      "date": "June 14, 2022",
      "summary": "Learning rate schedulers are essential in deep neural network training, but there's a gap between practical usage and theoretical understanding. This paper introduces Eigencurve, the first set of learning rate schedules achieving minimax optimal convergence rates for SGD on quadratic objectives with skewed eigenvalue distributions of the Hessian matrix. This condition is common in practice. Experimental results on CIFAR-10 image classification tasks demonstrate Eigencurve's superiority over step decay, particularly with fewer epochs. The theory inspires practical schedulers approximating Eigencurve, resembling cosine decay for some problems while outperforming it in others."
    }
  ],
  "vatiation": [
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/research/2017/11/06/Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization.html",
      "date": "November 6, 2017",
      "summary": "This paper itroduces Instance Normalization, an online normalization technique that calculates the mean and std across individual data instances. This is shown to work well in situations where there is very high vatiation within batches, such as in style transfer."
    }
  ],
  "style": [
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/research/2017/11/06/Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization.html",
      "date": "November 6, 2017",
      "summary": "This paper itroduces Instance Normalization, an online normalization technique that calculates the mean and std across individual data instances. This is shown to work well in situations where there is very high vatiation within batches, such as in style transfer."
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, this paper introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, the authors enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. This simple method surpasses the current state-of-the-art in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM) for self-supervised visual representation in vision transformers ViT. Five different learning objectives were proposed, which degraded the input image in various ways similar to masking. Design principles are proposed for  token-based pre-training of vision transformers. The most effective strategy combined preserving the original image style with spatial misalignment and masking. This approach outperformed traditional MIM on downstream recognition tasks without increasing computational demands."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Incorporating diverse image styles into CNN training data reduces texture bias, enhances shape recognition, and improves resilience against common image corruptions. This is typically explained as decreasing model reliance on high frequency texture information. This paper challenges this explaniation through large scale testing with natural images, edge information, and stylization, finding no direct link between shape bias and robustness. The enhanced corruption robustness is instead attributed to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
      "url": "/research/2021/07/24/The-Many-Faces-of-Robustness-A-Critical-Analysis-of-Out-of-Distribution-Generalization.html",
      "date": "July 24, 2021",
      "summary": "We present four novel real-world distribution shift datasets encompassing changes in image style, blurriness, location, camera settings, and more. Evaluating existing methods for enhancing out-of-distribution robustness, we discover that employing larger models and artificial data augmentations can enhance robustness against real-world distribution shifts, contradicting prior claims. Our findings demonstrate that improvements in artificial robustness benchmarks can indeed transfer to real-world distribution shifts, contrary to prior assumptions. Additionally, we introduce a novel data augmentation technique that surpasses models pretrained with significantly more labeled data, emphasizing its efficacy in addressing real-world distribution shifts. While some methods consistently mitigate texture and local image statistics shifts, they fail to address other shifts like geographic changes. Our results underscore the necessity for future research to examine multiple distribution shifts concurrently, as no single method consistently improves robustness across all evaluated scenarios."
    }
  ],
  "flexibility": [
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. The authors find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. "
    },
    {
      "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
      "url": "/research/2024/03/13/Simple-and-Scalable-Strategies-to-Continually-Pre-train-Large-Language-Models.html",
      "date": "March 13, 2024",
      "summary": "Large language models (LLMs) often require re-training on new data, consuming extensive computational resources. We present an efficient method that combines learning rate re-warming, re-decaying, and data replay, effectively maintaining performance without full re-training. This approach works well across different data distributions, including minor shifts (English→English) and significant shifts (English→German), tested up to 405M and 10B parameter models. Our findings suggest that continual learning strategies can update LLMs with minimal computational cost, rivaling traditional re-training methods. Additionally, we propose alternatives to the cosine learning rate schedule to reduce forgetting, offering more flexibility in learning without a fixed token budget."
    },
    {
      "title": "Suppressing Pink Elephants with Direct Principle Feedback",
      "url": "/research/2024/02/13/Suppressing-Pink-Elephants-with-Direct-Principle-Feedback.html",
      "date": "February 13, 2024",
      "summary": "Existing methods for controlling language models focus on training desired behaviors, but often lack the flexibility needed for diverse applications. The authors address this with the Pink Elephant Problem, demonstrating the need for language models to adapt to different contexts by avoiding certain topics (the Pink Elephant) in favor of others (Grey Elephant). The authors introduce Direct Principle Feedback (DPF), an adaptation of Constitutional AI that improves control by directly applying critiques and revisions without ranking responses. Our study shows that a 13B LLaMA 2 model fine-tuned with DPF on a synthetic dataset outperforms existing models and matches GPT-4 in managing the Pink Elephant Problem."
    }
  ],
  "aid": [
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. The authors find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. "
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    }
  ],
  "occlusions": [
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. The authors find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. "
    }
  ],
  "recognition": [
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. The authors find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. "
    },
    {
      "title": "SVHN: Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View House Numbers."
    },
    {
      "title": "Rethinking Batch in BatchNorm",
      "url": "/research/2021/05/17/Rethinking-Batch-in-BatchNorm.html",
      "date": "May 17, 2021",
      "summary": "BatchNorm, essential in convolutional neural networks, behaves uniquely due to its batch-based operation, leading to performance issues in visual recognition tasks. This paper identifies these issues and suggests reevaluating the concept of batch in BatchNorm for better performance, aiming to guide researchers in its effective use."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. The Authors show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "This study introduces the Vision Transformer ViT archetecture. The paper shows that a pure transformer applied directly to sequences of image patches can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM) for self-supervised visual representation in vision transformers ViT. Five different learning objectives were proposed, which degraded the input image in various ways similar to masking. Design principles are proposed for  token-based pre-training of vision transformers. The most effective strategy combined preserving the original image style with spatial misalignment and masking. This approach outperformed traditional MIM on downstream recognition tasks without increasing computational demands."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Incorporating diverse image styles into CNN training data reduces texture bias, enhances shape recognition, and improves resilience against common image corruptions. This is typically explained as decreasing model reliance on high frequency texture information. This paper challenges this explaniation through large scale testing with natural images, edge information, and stylization, finding no direct link between shape bias and robustness. The enhanced corruption robustness is instead attributed to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper investigates how self-supervised pretraining methods learn part-aware representations. The authors describe contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting that these methods predispose encoders to recognize object parts. Through empirical comparison, they find that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper introduces the AVA dataset, which features 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. The authors highlight the challenges in action recognition and introduce a novel localization approach that surpasses existing benchmarks but shows modest performance on AVA (15.6% mAP), indicating the need for advanced video understanding methods."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "This study challenges the traditional understanding of how Convolutional Neural Networks (CNNs) recognize objects, revealing that they are more biased towards recognizing textures rather than learning complex shapes. The authors found that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, training a ResNet-50 on a stylized version of ImageNet, designed to emphasize shape, aligned the network's performance more closely with human behavior. This shape-based training matched human performance in controlled experiments and enhanced object detection and robustness against image distortions, highlighting the benefits of shape-based representations in visual recognition systems."
    },
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "This paper introduces the Kinetics Human Action Video dataset. The authors note that the limited number of videos in existing datasets like UCF-101 and HMDB-51 hinders the ability to effectively evaluate architectures due to similar performance across these small benchmarks. Kinetics contains 400 classes with over 400 video clips per class from challenging YouTube videos. The paper analyzes the impact of this larger dataset on the performance of existing architectures as well as the benefits of pre-training models on Kinetics. The authors introduce the Two-Stream Inflated 3D ConvNet (I3D), an extension of 2D ConvNets to 3D for improved video feature extraction. When pre-trained on Kinetics, this I3D model sets new benchmarks for action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    },
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "This paper introduces SlowFast networks, a new convolutional architecture for video recognition tasks. These networks have two pathways: a Slow pathway operating at low frame rates to capture spatial semantics, and a Fast pathway working at high frame rates with reduced channel capacity to efficiently capture motion details. The SlowFast models achieve top accuracy on major video benchmarks like Kinetics, Charades, and AVA, significantly improving performance for action classification and detection tasks."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article provides an extensive review of vision transformer techniques applied specifically to human action recognition. These action transformers are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also examines optimizing spatio-temporal attention in transformers for longer sequences and different learning strategies like self-supervised and zero-shot learning with their respective loss functions. Additionally, it assesses progress in benchmark evaluation scores and discusses challenges and future directions in this research area."
    },
    {
      "title": "Are Convolutional Neural Networks or Transformers more like human vision?",
      "url": "/research/2021/07/01/Are-Convolutional-Neural-Networks-or-Transformers-more-like-human-vision.html",
      "date": "July 1, 2021",
      "summary": "This study investigates the performance of Convolutional Neural Networks (CNNs) and attention-based Vision Transformers (ViTs) in computer vision tasks. While CNNs excel in accuracy, ViTs offer a different approach with weaker inductive biases. By analyzing error patterns, we find that ViTs exhibit consistency with human errors, suggesting potential for more human-like vision models and insights into human object recognition."
    },
    {
      "title": "Deep Residual Learning for Image Recognition",
      "url": "/research/2015/12/10/Deep-Residual-Learning-for-Image-Recognition.html",
      "date": "December 10, 2015",
      "summary": "We introduce a residual learning framework to train deeper neural networks effectively. By reformulating layers to learn residual functions relative to inputs, our approach facilitates optimization and achieves higher accuracy with increased depth. Evaluations on ImageNet demonstrate the effectiveness of residual networks up to 152 layers deep, outperforming previous architectures. Our method achieves 3.57% error on the ImageNet test set, winning 1st place in the ILSVRC 2015 classification task. Additionally, our deep representations lead to a 28% improvement on the COCO object detection dataset. In the ILSVRC & COCO 2015 competitions, our approach secured 1st place in ImageNet detection, localization, COCO detection, and segmentation tasks."
    },
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    }
  ],
  "pixel": [
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. The authors find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. "
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. MAE involves masking random patches of an input image and reconstructing the missing pixels using an asymmetric encoder-decoder architecture and a lightweight decoder. Masking a significant portion of the input image creates a challenging yet informative self-supervisory task. These innovations allow for efficient training of large models, tripling training speed and enhancing accuracy. A vanilla ViT-Huge model reaches top accuracy (87.8%) on ImageNet-1K data among similar methods. The model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating promising scaling potential."
    },
    {
      "title": "On the Relationship Between Self-Attention And Convolutional Layers",
      "url": "/research/2020/01/10/On-the-Relationship-Between-Self-Attention-And-Convolutional-Layers.html",
      "date": "January 10, 2020",
      "summary": "Recent studies have shown that attention mechanisms can rival or even outperform convolutional layers in vision tasks, challenging their dominance. Ramachandran et al. (2019) demonstrated that attention could entirely replace convolution, achieving top results. This work investigates whether attention layers function like convolutional layers, finding that multi-head self-attention layers, with enough heads, match the expressiveness of convolutional layers. Numerical experiments confirm that self-attention layers learn to focus on pixel-grid patterns akin to convolutional layers, supporting our findings."
    }
  ],
  "shot": [
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. The authors find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. "
    },
    {
      "title": "CLIP: Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns from raw text descriptions of images, moving beyond training on fixed object categories. By pre-training on 400 million (image, text) pairs using caption-image matching, the method achieves state-of-the-art image representations from scratch. This allows for zero-shot transfer to various downstream tasks using natural language, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse datasets, showing competitive results against fully supervised baselines without task-specific training, and matching the accuracy of ResNet-50 on ImageNet zero-shot without using its training examples."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article provides an extensive review of vision transformer techniques applied specifically to human action recognition. These action transformers are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also examines optimizing spatio-temporal attention in transformers for longer sequences and different learning strategies like self-supervised and zero-shot learning with their respective loss functions. Additionally, it assesses progress in benchmark evaluation scores and discusses challenges and future directions in this research area."
    }
  ],
  "advantage": [
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. The authors find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. "
    },
    {
      "title": "Self-Paced Curriculum Learning",
      "url": "/research/2015/01/01/Self-Paced-Curriculum-Learning.html",
      "date": "January 1, 2015",
      "summary": "This paper introduces self-paced curriculum learning (SPCL), a unified framework combining curriculum learning (CL) and self-paced learning (SPL). SPCL leverages prior knowledge and ongoing learning progress through an optimization problem. It mimics collaborative instructor-student learning, exhibiting empirical advantages in two tasks."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/2020/06/17/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "June 17, 2020",
      "summary": "Implicit neural representations, parameterized by continuous, differentiable neural networks, offer numerous advantages over conventional representations. However, existing architectures struggle with fine detail and fail to capture spatial and temporal derivatives, crucial for many physical signals described by partial differential equations. We introduce sinusoidal representation networks (SIRENs) utilizing periodic activation functions, ideal for representing complex signals and their derivatives. Analyzing SIREN activation statistics informs a principled initialization scheme. We demonstrate SIRENs' efficacy in representing various signals and solving boundary value problems, including Eikonal, Poisson, Helmholtz, and wave equations. Additionally, we integrate SIRENs with hypernetworks to learn priors over the SIREN function space. Visit the project website for detailed demonstrations."
    },
    {
      "title": "MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts",
      "url": "/research/2024/02/26/MoE-Mamba-Efficient-Selective-State-Space-Models-with-Mixture-of-Experts.html",
      "date": "February 26, 2024",
      "summary": "State Space Models (SSMs) are emerging as strong competitors in sequential modeling, challenging Transformers. Integrating Mixture of Experts (MoE) has enhanced Transformer-based models, including cutting-edge open models. We suggest combining SSMs with MoE to further unlock their scaling potential. Our model, MoE-Mamba, based on the SSM model Mamba, exceeds the performance of both Mamba and traditional Transformer-MoE models. Notably, MoE-Mamba achieves Mamba's performance with 2.35 times fewer training steps, maintaining Mamba's inference advantages over Transformers."
    }
  ],
  "stem": [
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. The authors find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. "
    }
  ],
  "neuron": [
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activitivations can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases."
    },
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    },
    {
      "title": "Exponentially Faster Language Modeling",
      "url": "/research/2023/11/21/Exponentially-Faster-Language-Modeling.html",
      "date": "November 21, 2023",
      "summary": "UltraFastBERT, a BERT variant, operates with just 0.3% of its neurons—selectively using 12 out of 4095 neurons per layer—for inference, matching the performance of similar models. It replaces conventional feedforward networks with fast feedforward networks (FFFs) to achieve this efficiency. Although fully efficient conditional neural execution isn't yet practical, we offer a high-level CPU code that achieves a 78x speedup and a PyTorch implementation with a 40x speedup over standard batched feedforward inference."
    }
  ],
  "activitivation": [
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activitivations can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases."
    }
  ],
  "feed": [
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activitivations can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results. The method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. A convolutional neural network (CNN) was trained on over a million color images, operating as a feed-forward pass at test time. The algorithm's effectiveness was validated through a colorization Turing test, where it deceived participants in 32% of trials. The paper also illustrates how colorization can serve as an effective pretraining task for self-supervised feature learning, achieving state-of-the-art results on several benchmarks."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of Vision Transformer (ViT) variants against common corruptions. The benchmarking reveals that overlapping patch embedding and convolutional feed-forward networks (FFNs) significantly enhance ViT robustness. The study also scrutinizes the effectiveness of CNN-based data augmentation strategies when applied to ViTs, finding that adversarial noise training is effective, while fourier-domain augmentation falls short. A new conditional method for generating dynamic augmentation parameters based on input images is proposed, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "This paper explores Gated Linear Units (GLUs), which involve the component-wise product of two linear projections, with one undergoing a sigmoid function. The authors investigate GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers. They find that certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    }
  ],
  "forward": [
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activitivations can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results. The method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. A convolutional neural network (CNN) was trained on over a million color images, operating as a feed-forward pass at test time. The algorithm's effectiveness was validated through a colorization Turing test, where it deceived participants in 32% of trials. The paper also illustrates how colorization can serve as an effective pretraining task for self-supervised feature learning, achieving state-of-the-art results on several benchmarks."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of Vision Transformer (ViT) variants against common corruptions. The benchmarking reveals that overlapping patch embedding and convolutional feed-forward networks (FFNs) significantly enhance ViT robustness. The study also scrutinizes the effectiveness of CNN-based data augmentation strategies when applied to ViTs, finding that adversarial noise training is effective, while fourier-domain augmentation falls short. A new conditional method for generating dynamic augmentation parameters based on input images is proposed, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "This paper introduces a self-supervised vision model pretraining framework inspired by energy-based models (EBMs), leveraging symmetric mappings in deep networks without auxiliary components. The framework models energy estimation and data restoration through the network's forward and backward passes, assigning low energy to unlabeled dataset samples and using gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting various pretext tasks. Experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "This paper explores Gated Linear Units (GLUs), which involve the component-wise product of two linear projections, with one undergoing a sigmoid function. The authors investigate GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers. They find that certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    },
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    }
  ],
  "vary": [
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activitivations can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases."
    },
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "The paper investigates the use of synthetic data generated by graphics simulators for pre-training computer vision models. The authors find that model performance on downstream tasks varies with different simulation parameters used to generate the synthetic data. They introduce Task2Sim, a model that maps the requirements of a downstream task to the optimal simulation parameters for generating synthetic pre-training data."
    },
    {
      "title": "From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning",
      "url": "/research/2023/03/16/From-MNIST-to-ImageNet-and-Back:-Benchmarking-Continual-Curriculum-Learning.html",
      "date": "March 16, 2023",
      "summary": "Continual learning (CL) is a promising trend in machine learning, aiming to develop robust models and strategies for dynamic environments by incorporating new knowledge while retaining past knowledge. CL research is fragmented, with various protocols, tasks, datasets, and metrics, often not reflecting real-world complexity and tailored to specific strategies. This work addresses this gap by introducing two novel CL benchmarks with heterogeneous tasks from six image datasets, varying in complexity and quality. These benchmarks fairly evaluate state-of-the-art CL strategies in scenarios closer to real-world conditions. The benchmarks present tasks in both increasing and decreasing complexity, assessing models' ability to exploit task structure. The work emphasizes a rigorous, reproducible evaluation protocol for measuring generalization and memory retention in models. Experimental results show that popular CL strategies perform poorly on these benchmarks, exhibit high forgetting, and struggle with curriculum task ordering. These findings underscore the need for rigorous comparisons and the development of new CL strategies for complex scenarios."
    },
    {
      "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum",
      "url": "/research/2023/09/30/When-to-Learn-What:-Model-Adaptive-Data-Augmentation-Curriculum.html",
      "date": "September 30, 2023",
      "summary": "Data augmentation (DA) improves neural network generalization by enforcing invariances and symmetries to predefined transformations applied to input data. However, fixed augmentation policies affect samples differently at various training stages, and existing approaches cannot adapt policies to individual samples and the training model. This paper proposes Model-Adaptive Data Augmentation (MADAug) which trains an augmentation policy network to determine when to learn what. Unlike previous work, MADAug selects augmentation operators for each input image with a model-adaptive policy that varies between training stages, creating an optimized data augmentation curriculum. The policy is trained using a bi-level optimization scheme to minimize validation-set loss of a model trained with policy-produced augmentations. Extensive evaluations on multiple image classification tasks and network architectures show that MADAug outperforms or matches existing DA approaches, enhances fairness by improving all classes, particularly the difficult ones, and performs better when transferred to fine-grained datasets. Additionally, the auto-optimized policy in MADAug gradually increases perturbations, forming an easy-to-hard curriculum."
    }
  ],
  "rnn": [
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activitivations can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases."
    },
    {
      "title": "Root Mean Square Layer Normalization",
      "url": "/research/2019/10/16/Root-Mean-Square-Layer-Normalization.html",
      "date": "October 16, 2019",
      "summary": "Layer normalization (LayerNorm) enhances deep neural network stability and convergence by re-centering and re-scaling inputs and weight matrices. However, its computational overhead slows networks, particularly RNNs. We introduce RMSNorm, which replaces re-centering with root mean square (RMS) regularization. RMSNorm maintains re-scaling invariance and adapts learning rates implicitly, while being computationally simpler than LayerNorm. We also propose partial RMSNorm (pRMSNorm), estimating RMS from a subset of inputs. Empirical results across various tasks and architectures demonstrate that RMSNorm achieves comparable performance to LayerNorm while reducing running time by 7%∼64%."
    }
  ],
  "phase": [
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activitivations can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases."
    },
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "This paper introduces a curriculum learning framework for image-caption pretraining, inspired by children's language learning from cognitive science, to address the challenges of aligning multiple concepts from captions to objects in images. The method starts with simple image-caption pairs and gradually increases complexity by adding more concepts, leveraging knowledge from each phase for subsequent learning. This approach outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios."
    },
    {
      "title": "Automated Learning Rate Scheduler for Large-batch",
      "url": "/research/2021/07/13/Automated-Learning-Rate-Scheduler-for-Large-batch.html",
      "date": "July 13, 2021",
      "summary": "Large-batch training is crucial for deep learning with large datasets and models, but requires specific learning rate (LR) schedules for optimal performance, especially under limited training epochs. This work introduces an automated LR scheduling algorithm for large-batch neural network training within a fixed epoch budget, consisting of adaptive warmup and predefined decay phases. The LR is dynamically adjusted based on training loss monitored through Gaussian process smoothing, facilitating low computational overhead. When integrated with adaptive stochastic optimizers like AdamP and LAMB, this scheduler eliminates the need for extensive hyperparameter tuning and achieves competitive or superior results on various image classification tasks across different batch sizes and architectures."
    },
    {
      "title": "Optimal learning rate schedules in high-dimensional non-convex optimization problems",
      "url": "/research/2022/02/09/Optimal-learning-rate-schedules-in-high-dimensional-non-convex-optimization-problems.html",
      "date": "February 9, 2022",
      "summary": "This paper explores the role of learning rate schedules in high-dimensional and non-convex optimization problems, focusing on Langevin optimization with a decaying learning rate. Analyzing models with Gaussian random functions on N-dimensional spheres, the study reveals that to accelerate optimization without getting trapped in saddles, a decay rate β < 1 is optimal, contrary to convex settings where β = 1 is preferred. Introducing a signal recovery component, the dynamics involve an exploration phase navigating through rough landscape parts and a convergence phase entering convex basins. It's found optimal to maintain a large learning rate during exploration to swiftly exit non-convex regions, then transition to β = 1 for rapid convergence to the solution. These findings are validated in a neural network regression task."
    },
    {
      "title": "A Phase Transition between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention",
      "url": "/research/2024/02/06/A-Phase-Transition-between-Positional-and-Semantic-Learning-in-a-Solvable-Model-of-Dot-Product-Attention.html",
      "date": "February 6, 2024",
      "summary": "We explore the learning process of a dot-product attention layer, which learns both positional and semantic attention matrices, enabling tokens to attend based on position or meaning. Through experiments on an algorithmic task, we demonstrate that this architecture can use either mechanism for solving the task. Theoretically, we examine a non-linear self-attention layer with special query and key matrices, offering a closed-form solution for its non-convex loss landscape in high-dimensional data, which reveals a phase transition from positional to semantic mechanisms as sample complexity increases. We also show that the dot-product attention layer surpasses a linear positional baseline through the semantic mechanism with adequate data."
    },
    {
      "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
      "url": "/research/2023/11/30/Dichotomy-of-Early-and-Late-Phase-Implicit-Biases-Can-Provably-Induce-Grokking.html",
      "date": "November 30, 2023",
      "summary": "Power et al. (2022) discovered that neural networks initially memorize arithmetic tasks, achieving perfect training accuracy but poor test accuracy, then suddenly achieve perfect test accuracy after extended training. This study explains this grokking phenomenon with the theory that early and late phase implicit biases cause this shift. Training homogeneous neural networks with large initialization and small weight decay on classification and regression tasks results in a prolonged period where the network acts like a kernel predictor, followed by a sudden shift to min-norm/max-margin predictors, significantly improving test accuracy."
    },
    {
      "title": "Neural Networks Learn Statistics of Increasing Complexity",
      "url": "/research/2024/02/13/Neural-Networks-Learn-Statistics-of-Increasing-Complexity.html",
      "date": "February 13, 2024",
      "summary": "The distributional simplicity bias (DSB) theory suggests neural networks first learn basic patterns in data before understanding more complex correlations. We provide new evidence supporting DSB, showing networks initially excel with data matching training set's simple statistics but this ability diminishes later. Extending DSB to discrete domains, we demonstrate an equivalence between n-gram frequencies and vector moments, also observing this bias in large language models (LLMs). Additionally, by adjusting low-level statistics of images to resemble another class, we reveal that networks in early training phases misclassify these edited images as if they belonged to the target class."
    }
  ],
  "achieving": [
    {
      "title": "Learning RelU Networks To High Uniform Accuracy Is Intractable",
      "url": "/research/2023/02/28/Learning-Relu-Networks-To-High-Uniform-Accuracy-Is-Intractable.html",
      "date": "February 28, 2023",
      "summary": "Statistical learning theory provides guidelines on the number of training samples needed for achieving desired accuracy in learning problems. However, this is not always adequate, particularly in security-sensitive areas or computational sciences, where uniform accuracy across all inputs is necessary. This paper quantifies the training samples required for uniform accuracy in learning problems involving ReLU neural networks, revealing that the number of samples needed exponentially increases with the network's depth and input dimension."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of Vision Transformer (ViT) variants against common corruptions. The benchmarking reveals that overlapping patch embedding and convolutional feed-forward networks (FFNs) significantly enhance ViT robustness. The study also scrutinizes the effectiveness of CNN-based data augmentation strategies when applied to ViTs, finding that adversarial noise training is effective, while fourier-domain augmentation falls short. A new conditional method for generating dynamic augmentation parameters based on input images is proposed, achieving state-of-the-art robustness against common corruptions."
    }
  ],
  "desire": [
    {
      "title": "Learning RelU Networks To High Uniform Accuracy Is Intractable",
      "url": "/research/2023/02/28/Learning-Relu-Networks-To-High-Uniform-Accuracy-Is-Intractable.html",
      "date": "February 28, 2023",
      "summary": "Statistical learning theory provides guidelines on the number of training samples needed for achieving desired accuracy in learning problems. However, this is not always adequate, particularly in security-sensitive areas or computational sciences, where uniform accuracy across all inputs is necessary. This paper quantifies the training samples required for uniform accuracy in learning problems involving ReLU neural networks, revealing that the number of samples needed exponentially increases with the network's depth and input dimension."
    },
    {
      "title": "Suppressing Pink Elephants with Direct Principle Feedback",
      "url": "/research/2024/02/13/Suppressing-Pink-Elephants-with-Direct-Principle-Feedback.html",
      "date": "February 13, 2024",
      "summary": "Existing methods for controlling language models focus on training desired behaviors, but often lack the flexibility needed for diverse applications. The authors address this with the Pink Elephant Problem, demonstrating the need for language models to adapt to different contexts by avoiding certain topics (the Pink Elephant) in favor of others (Grey Elephant). The authors introduce Direct Principle Feedback (DPF), an adaptation of Constitutional AI that improves control by directly applying critiques and revisions without ranking responses. Our study shows that a 13B LLaMA 2 model fine-tuned with DPF on a synthetic dataset outperforms existing models and matches GPT-4 in managing the Pink Elephant Problem."
    }
  ],
  "science": [
    {
      "title": "Learning RelU Networks To High Uniform Accuracy Is Intractable",
      "url": "/research/2023/02/28/Learning-Relu-Networks-To-High-Uniform-Accuracy-Is-Intractable.html",
      "date": "February 28, 2023",
      "summary": "Statistical learning theory provides guidelines on the number of training samples needed for achieving desired accuracy in learning problems. However, this is not always adequate, particularly in security-sensitive areas or computational sciences, where uniform accuracy across all inputs is necessary. This paper quantifies the training samples required for uniform accuracy in learning problems involving ReLU neural networks, revealing that the number of samples needed exponentially increases with the network's depth and input dimension."
    },
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "This paper introduces a curriculum learning framework for image-caption pretraining, inspired by children's language learning from cognitive science, to address the challenges of aligning multiple concepts from captions to objects in images. The method starts with simple image-caption pairs and gradually increases complexity by adding more concepts, leveraging knowledge from each phase for subsequent learning. This approach outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios."
    }
  ],
  "quantify": [
    {
      "title": "Learning RelU Networks To High Uniform Accuracy Is Intractable",
      "url": "/research/2023/02/28/Learning-Relu-Networks-To-High-Uniform-Accuracy-Is-Intractable.html",
      "date": "February 28, 2023",
      "summary": "Statistical learning theory provides guidelines on the number of training samples needed for achieving desired accuracy in learning problems. However, this is not always adequate, particularly in security-sensitive areas or computational sciences, where uniform accuracy across all inputs is necessary. This paper quantifies the training samples required for uniform accuracy in learning problems involving ReLU neural networks, revealing that the number of samples needed exponentially increases with the network's depth and input dimension."
    }
  ],
  "building": [
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/ON-INTERACTION-BETWEEN-AUGMENTATIONS-AND-CORRUPTIONS-IN-NATURAL-CORRUPTION-ROBUSTNESS.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, the correlation between data augmentations and test-time corruptions remains unclear. This paper introduces Minimal Sample Distance, and demonstrates a strong correlation between augmentation-corruption similarity and performance. Authors argue that training with augmentations that are perceptually similar to corruptions enhances test error."
    }
  ],
  "invariance": [
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/ON-INTERACTION-BETWEEN-AUGMENTATIONS-AND-CORRUPTIONS-IN-NATURAL-CORRUPTION-ROBUSTNESS.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, the correlation between data augmentations and test-time corruptions remains unclear. This paper introduces Minimal Sample Distance, and demonstrates a strong correlation between augmentation-corruption similarity and performance. Authors argue that training with augmentations that are perceptually similar to corruptions enhances test error."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. The Authors show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements."
    },
    {
      "title": "Root Mean Square Layer Normalization",
      "url": "/research/2019/10/16/Root-Mean-Square-Layer-Normalization.html",
      "date": "October 16, 2019",
      "summary": "Layer normalization (LayerNorm) enhances deep neural network stability and convergence by re-centering and re-scaling inputs and weight matrices. However, its computational overhead slows networks, particularly RNNs. We introduce RMSNorm, which replaces re-centering with root mean square (RMS) regularization. RMSNorm maintains re-scaling invariance and adapts learning rates implicitly, while being computationally simpler than LayerNorm. We also propose partial RMSNorm (pRMSNorm), estimating RMS from a subset of inputs. Empirical results across various tasks and architectures demonstrate that RMSNorm achieves comparable performance to LayerNorm while reducing running time by 7%∼64%."
    },
    {
      "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum",
      "url": "/research/2023/09/30/When-to-Learn-What:-Model-Adaptive-Data-Augmentation-Curriculum.html",
      "date": "September 30, 2023",
      "summary": "Data augmentation (DA) improves neural network generalization by enforcing invariances and symmetries to predefined transformations applied to input data. However, fixed augmentation policies affect samples differently at various training stages, and existing approaches cannot adapt policies to individual samples and the training model. This paper proposes Model-Adaptive Data Augmentation (MADAug) which trains an augmentation policy network to determine when to learn what. Unlike previous work, MADAug selects augmentation operators for each input image with a model-adaptive policy that varies between training stages, creating an optimized data augmentation curriculum. The policy is trained using a bi-level optimization scheme to minimize validation-set loss of a model trained with policy-produced augmentations. Extensive evaluations on multiple image classification tasks and network architectures show that MADAug outperforms or matches existing DA approaches, enhances fairness by improving all classes, particularly the difficult ones, and performs better when transferred to fine-grained datasets. Additionally, the auto-optimized policy in MADAug gradually increases perturbations, forming an easy-to-hard curriculum."
    }
  ],
  "warp": [
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/ON-INTERACTION-BETWEEN-AUGMENTATIONS-AND-CORRUPTIONS-IN-NATURAL-CORRUPTION-ROBUSTNESS.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, the correlation between data augmentations and test-time corruptions remains unclear. This paper introduces Minimal Sample Distance, and demonstrates a strong correlation between augmentation-corruption similarity and performance. Authors argue that training with augmentations that are perceptually similar to corruptions enhances test error."
    }
  ],
  "color": [
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/ON-INTERACTION-BETWEEN-AUGMENTATIONS-AND-CORRUPTIONS-IN-NATURAL-CORRUPTION-ROBUSTNESS.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, the correlation between data augmentations and test-time corruptions remains unclear. This paper introduces Minimal Sample Distance, and demonstrates a strong correlation between augmentation-corruption similarity and performance. Authors argue that training with augmentations that are perceptually similar to corruptions enhances test error."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results. The method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. A convolutional neural network (CNN) was trained on over a million color images, operating as a feed-forward pass at test time. The algorithm's effectiveness was validated through a colorization Turing test, where it deceived participants in 32% of trials. The paper also illustrates how colorization can serve as an effective pretraining task for self-supervised feature learning, achieving state-of-the-art results on several benchmarks."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "This paper introduces Mask-Reconstruct Augmentation (MRA), an image augmentation technique that leverages self-supervised masked autoencoders to generate distorted inputs and address the issue of overfitting in deep neural networks. Inspired by the success of masked image modeling in self-supervised learning, MRA uses nonlinear transformations for regularization, as opposed to current hand-crafted linear techniques like scale, flip, and color jitter. Extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks."
    },
    {
      "title": "Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding",
      "url": "/research/2021/11/16/Improved-Robustness-of-Vision-Transformer-via-PreLayerNorm-in-Patch-Embedding.html",
      "date": "November 16, 2021",
      "summary": "Recent advancements in vision transformers (ViTs) have shown superior performance across diverse visual tasks, surpassing convolutional neural networks (CNNs). Given ViT's distinct architecture, understanding its behavior and reliability is imperative. This paper investigates ViT's robustness by comparing it with CNNs under various image corruptions relevant to real-world vision tasks. While ViT generally exhibits comparable or improved robustness over CNNs, it consistently underperforms in contrast enhancement tasks. Analysis suggests that positional embedding in ViT's patch embedding may malfunction with color scale changes. We propose PreLayerNorm, a modified patch embedding structure, to address this issue and ensure scale-invariant behavior in ViT. ViT with PreLayerNorm demonstrates enhanced robustness across various corruptions, particularly in contrast-varying environments."
    }
  ],
  "correlation": [
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/ON-INTERACTION-BETWEEN-AUGMENTATIONS-AND-CORRUPTIONS-IN-NATURAL-CORRUPTION-ROBUSTNESS.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, the correlation between data augmentations and test-time corruptions remains unclear. This paper introduces Minimal Sample Distance, and demonstrates a strong correlation between augmentation-corruption similarity and performance. Authors argue that training with augmentations that are perceptually similar to corruptions enhances test error."
    },
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/research/2020/08/03/Are-Labels-Necessary-for-Neural-Architecture-Search.html",
      "date": "August 3, 2020",
      "summary": "The paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning the necessity of human-annotated labels for discovering effective neural architectures. Two experimental setups sample-based and search-based were investigated. The sample-based approach evaluated 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. The search-based experiments employed DARTS, a recognized NAS algorithm, with various unsupervised objectives. The architectures identified without labels performed competitively compared to those found with labels. The findings suggested that image statistics alone could be sufficient for identifying efficient neural architectures, potentially eliminating the need for human-annotated labels."
    },
    {
      "title": "Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks",
      "url": "/research/2017/10/23/Cosine-Normalization-Using-Cosine-Similarity-Instead-of-Dot-Product-in-Neural-Networks.html",
      "date": "October 23, 2017",
      "summary": "In traditional multi-layer neural networks, the dot product between the output and weight vectors of preceding layers serves as input to the activation function, resulting in unbounded outputs and increased variance. This variance can lead to poor generalization and hinder training by exacerbating internal covariate shift. To address this, we propose cosine normalization, which replaces the dot product with cosine similarity or centered cosine similarity (Pearson Correlation Coefficient). We evaluate cosine normalization against batch, weight, and layer normalization in fully-connected and convolutional neural networks across various datasets. Our experiments demonstrate that cosine normalization outperforms other normalization techniques."
    },
    {
      "title": "Neural Networks Learn Statistics of Increasing Complexity",
      "url": "/research/2024/02/13/Neural-Networks-Learn-Statistics-of-Increasing-Complexity.html",
      "date": "February 13, 2024",
      "summary": "The distributional simplicity bias (DSB) theory suggests neural networks first learn basic patterns in data before understanding more complex correlations. We provide new evidence supporting DSB, showing networks initially excel with data matching training set's simple statistics but this ability diminishes later. Extending DSB to discrete domains, we demonstrate an equivalence between n-gram frequencies and vector moments, also observing this bias in large language models (LLMs). Additionally, by adjusting low-level statistics of images to resemble another class, we reveal that networks in early training phases misclassify these edited images as if they belonged to the target class."
    }
  ],
  "distance": [
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/ON-INTERACTION-BETWEEN-AUGMENTATIONS-AND-CORRUPTIONS-IN-NATURAL-CORRUPTION-ROBUSTNESS.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, the correlation between data augmentations and test-time corruptions remains unclear. This paper introduces Minimal Sample Distance, and demonstrates a strong correlation between augmentation-corruption similarity and performance. Authors argue that training with augmentations that are perceptually similar to corruptions enhances test error."
    },
    {
      "title": "The Platonic Representation Hypothesis",
      "url": "/research/2024/05/13/The-Platonic-Representation-Hypothesis.html",
      "date": "May 13, 2024",
      "summary": "This paper argues that AI model representations, especially in deep networks, are converging. It surveys examples in the literature showing how neural network representations align over time and across domains. It demonstrates convergence across data modalities, noting that as vision and language models grow, they measure distances between data points similarly. The authors hypothesize that this convergence leads to a shared statistical model of reality, termed the platonic representation and discuss possible selective pressures toward it. The paper also addresses the implications, limitations, and counterexamples to this trend."
    }
  ],
  "similarity": [
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/ON-INTERACTION-BETWEEN-AUGMENTATIONS-AND-CORRUPTIONS-IN-NATURAL-CORRUPTION-ROBUSTNESS.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, the correlation between data augmentations and test-time corruptions remains unclear. This paper introduces Minimal Sample Distance, and demonstrates a strong correlation between augmentation-corruption similarity and performance. Authors argue that training with augmentations that are perceptually similar to corruptions enhances test error."
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, the authors demonstrate the close relationship between these two families. This analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized."
    },
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "This paper introduces a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. This decouples batch size from the loss, and allows the authors to increase the batch size and also improves performance even with smaller batches. Pushing the batch size to one million showed diminishing returns, with 32k being a good middle ground."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. The Authors show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "Is Cosine-Similarity of Embeddings Really About Similarity?",
      "url": "/research/2024/03/08/Is-Cosine-Similarity-of-Embeddings-Really-About-Similarity.html",
      "date": "March 8, 2024",
      "summary": "Cosine similarity, often used to gauge semantic similarity between high-dimensional objects by comparing low-dimensional feature embeddings, can yield variable results compared to unnormalized dot products. We investigate this phenomenon by analyzing embeddings from regularized linear models, revealing that cosine similarity can produce arbitrary and even meaningless similarities. This applies not only to linear models but also to deep models due to the implicit effects of various regularizations. Consequently, we advise against relying solely on cosine similarity and suggest exploring alternative approaches."
    },
    {
      "title": "Searching for Activation Functions",
      "url": "/research/2017/10/27/Searching-for-Activation-Functions.html",
      "date": "October 27, 2017",
      "summary": "This study explores the impact of activation functions on deep network training and performance. While Rectified Linear Unit (ReLU) is widely used, alternatives have not consistently outperformed it. We propose using automatic search techniques to discover new activation functions. Through exhaustive and reinforcement learning-based searches, we identify novel functions. Empirical evaluation shows that our best discovered function, Swish (f(x) = x · sigmoid(βx)), performs better than ReLU on deeper models across challenging datasets. Replacing ReLUs with Swish units improves classification accuracy on ImageNet, for example, by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. Swish's simplicity and similarity to ReLU facilitate its adoption in neural networks."
    },
    {
      "title": "Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks",
      "url": "/research/2017/10/23/Cosine-Normalization-Using-Cosine-Similarity-Instead-of-Dot-Product-in-Neural-Networks.html",
      "date": "October 23, 2017",
      "summary": "In traditional multi-layer neural networks, the dot product between the output and weight vectors of preceding layers serves as input to the activation function, resulting in unbounded outputs and increased variance. This variance can lead to poor generalization and hinder training by exacerbating internal covariate shift. To address this, we propose cosine normalization, which replaces the dot product with cosine similarity or centered cosine similarity (Pearson Correlation Coefficient). We evaluate cosine normalization against batch, weight, and layer normalization in fully-connected and convolutional neural networks across various datasets. Our experiments demonstrate that cosine normalization outperforms other normalization techniques."
    }
  ],
  "duality": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, the authors demonstrate the close relationship between these two families. This analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized."
    }
  ],
  "covariance": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, the authors demonstrate the close relationship between these two families. This analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "This paper introduces a novel approach to Radiance Field methods for novel-view synthesis that addresses the challenges of high visual quality, costly training, and real-time rendering of unbounded, complete scenes at 1080p resolution. The authors propose three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. The method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    },
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "algebraically": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, the authors demonstrate the close relationship between these two families. This analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized."
    }
  ],
  "condition": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, the authors demonstrate the close relationship between these two families. This analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized."
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "This study introduces a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. The method involves creating spaces for network design that parameterize multiple network populations. A low-dimensional, efficient design space called RegNet is developed, based on modeling network widths and depths through a quantized linear function. The analysis of the RegNet space challenges existing design practices, offering simpler and faster networks effective across various computational budgets. RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs under similar conditions."
    },
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "This paper introduces a Fourier neural network (FNN) that aligns with Fourier decomposition by utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design allows for easy integration with more complex networks for data processing tasks. The authors demonstrate the FNN's efficacy on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits of the FNN include solution validity beyond the training scope, model interpretability, and ease of use."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "This paper demonstrates that artificial neural networks (ANNs) are equivalent to Gaussian processes at initialization in the infinite-width limit and introduces the Neural Tangent Kernel (NTK), which describes ANNs' behavior during training. The NTK stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. The authors prove the positive-definiteness of the limiting NTK under certain conditions and show that the network function follows a linear differential equation during training for least-squares regression. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    },
    {
      "title": "Eigencurve: Optimal Learning Rate Schedule For Sgd on Quadratic Objectives With Skewed Hessian Spectrums",
      "url": "/research/2022/06/14/Eigencurve-Optimal-Learning-Rate-Schedule-For-Sgd-on-Quadratic-Objectives-With-Skewed-Hessian-Spectrums.html",
      "date": "June 14, 2022",
      "summary": "Learning rate schedulers are essential in deep neural network training, but there's a gap between practical usage and theoretical understanding. This paper introduces Eigencurve, the first set of learning rate schedules achieving minimax optimal convergence rates for SGD on quadratic objectives with skewed eigenvalue distributions of the Hessian matrix. This condition is common in practice. Experimental results on CIFAR-10 image classification tasks demonstrate Eigencurve's superiority over step decay, particularly with fewer epochs. The theory inspires practical schedulers approximating Eigencurve, resembling cosine decay for some problems while outperforming it in others."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "url": "/research/2014/02/19/Exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks.html",
      "date": "February 19, 2014",
      "summary": "This study aims to enhance our comprehension of deep learning dynamics by analyzing deep linear neural networks. Despite their linear input-output mapping, these networks display nonlinear gradient descent dynamics, resulting in phenomena like prolonged plateaus and swift transitions to improved solutions. Through analytical exploration, we reveal that as network depth increases indefinitely, learning speed can remain finite under specific initial conditions. Unsupervised pretraining, under certain data conditions, can discover these favorable initial conditions, unlike random Gaussian initializations. We introduce a novel class of random orthogonal initial conditions that, akin to unsupervised pretraining, enable depth-independent learning. These conditions also ensure gradient propagation in deep nonlinear networks, particularly when operating at the edge of chaos."
    },
    {
      "title": "Fine-tuning with Very Large Dropout",
      "url": "/research/2024/03/01/Fine-tuning-with-Very-Large-Dropout.html",
      "date": "March 1, 2024",
      "summary": "This study challenges the notion that machine learning practices assume training and testing data have the same distribution. It explores the effectiveness of high dropout rates, as opposed to ensemble techniques, in developing rich data representations suitable for multiple distribution scenarios. These representations surpass those achieved by traditional in-distribution performance regularization and the implicit sparsity induced by common stochastic gradient methods. While training deep networks from scratch with high dropout rates is impractical, fine-tuning pre-trained models under these conditions is feasible and yields better out-of-distribution performance than ensembles and model averaging techniques like model soups. This finding is significant due to the growing relevance of fine-tuning with large pre-trained models, offering insights into the nature of rich representations and the linear characteristics of fine-tuning large networks with small datasets."
    },
    {
      "title": "From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning",
      "url": "/research/2023/03/16/From-MNIST-to-ImageNet-and-Back:-Benchmarking-Continual-Curriculum-Learning.html",
      "date": "March 16, 2023",
      "summary": "Continual learning (CL) is a promising trend in machine learning, aiming to develop robust models and strategies for dynamic environments by incorporating new knowledge while retaining past knowledge. CL research is fragmented, with various protocols, tasks, datasets, and metrics, often not reflecting real-world complexity and tailored to specific strategies. This work addresses this gap by introducing two novel CL benchmarks with heterogeneous tasks from six image datasets, varying in complexity and quality. These benchmarks fairly evaluate state-of-the-art CL strategies in scenarios closer to real-world conditions. The benchmarks present tasks in both increasing and decreasing complexity, assessing models' ability to exploit task structure. The work emphasizes a rigorous, reproducible evaluation protocol for measuring generalization and memory retention in models. Experimental results show that popular CL strategies perform poorly on these benchmarks, exhibit high forgetting, and struggle with curriculum task ordering. These findings underscore the need for rigorous comparisons and the development of new CL strategies for complex scenarios."
    },
    {
      "title": "Learning and Leveraging World Models in Visual Representation Learning",
      "url": "/research/2024/03/01/Learning-and-Leveraging-World-Models-in-Visual-Representation-Learning.html",
      "date": "March 1, 2024",
      "summary": "The Joint-Embedding Predictive Architecture (JEPA) is a self-supervised method previously used for predicting missing input parts. This study expands JEPA to predict a wider range of corruptions by introducing Image World Models (IWM), which learn the effects of global photometric transformations in latent space. Key to effective IWM learning are conditioning, prediction difficulty, and model capacity. The study demonstrates that IWM's predictive world model, when fine-tuned, can tackle diverse tasks and either matches or outperforms existing self-supervised techniques. Additionally, it enables control over the abstraction level of learned representations, achieving either invariant or equivariant representations, similar to contrastive methods or masked image modeling, respectively."
    },
    {
      "title": "The loss landscape of deep linear neural networks: a second-order analysis",
      "url": "/research/2022/03/11/The-loss-landscape-of-deep-linear-neural-networks-a-second-order-analysis.html",
      "date": "March 11, 2022",
      "summary": "This study investigates the optimization landscape of deep linear neural networks with square loss, focusing on the role of non-strict saddle points in algorithm dynamics, which has been previously understudied. the authors conduct a comprehensive second-order analysis, identifying global minimizers, strict, and non-strict saddle points among all critical points, alongside their critical values. Our findings, based on conditions related to the ranks of partial matrix products, contribute insights into global convergence and implicit regularization observed in optimizing linear neural networks. Additionally, the authors offer an explicit parameterization of global minimizers and identify extensive sets of strict and non-strict saddle points."
    }
  ],
  "relationship": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, the authors demonstrate the close relationship between these two families. This analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized."
    },
    {
      "title": "On the Maximum Hessian Eigenvalue and Generalization",
      "url": "/research/2023/05/23/On-the-Maximum-Hessian-Eigenvalue-and-Generalization.html",
      "date": "May 23, 2023",
      "summary": "This study investigates the relationship between training interventions and the generalization of deep networks. While previous research suggests that flatter solutions generalize better than sharper ones, particularly measured by λmax, the largest eigenvalue of the Hessian of the loss, this paper challenges this notion. Through experiments, we demonstrate that larger learning rates reduce λmax for all batch sizes but do not consistently improve generalization. Additionally, scaling batch size and learning rate simultaneously can change λmax without affecting generalization. Sharpness-Aware Minimization (SAM) produces smaller λmax but does not consistently enhance generalization, especially with larger batch sizes. Excessive dropout probabilities can degrade generalization, despite promoting smaller λmax. Batch normalization, while not consistently reducing λmax, still improves generalization. These findings question λmax's role in explaining generalization in neural networks, highlighting the limits of its explanatory power."
    },
    {
      "title": "On the Relationship Between Self-Attention And Convolutional Layers",
      "url": "/research/2020/01/10/On-the-Relationship-Between-Self-Attention-And-Convolutional-Layers.html",
      "date": "January 10, 2020",
      "summary": "Recent studies have shown that attention mechanisms can rival or even outperform convolutional layers in vision tasks, challenging their dominance. Ramachandran et al. (2019) demonstrated that attention could entirely replace convolution, achieving top results. This work investigates whether attention layers function like convolutional layers, finding that multi-head self-attention layers, with enough heads, match the expressiveness of convolutional layers. Numerical experiments confirm that self-attention layers learn to focus on pixel-grid patterns akin to convolutional layers, supporting our findings."
    }
  ],
  "family": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, the authors demonstrate the close relationship between these two families. This analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized."
    },
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "This paper introduces QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU while maintaining 16-bit task performance. QLORA utilizes 4-bit quantized language models and Low Rank Adapters (LoRA) and incorporates several memory-saving innovations without compromising performance. The authors' best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. The authors applied QLORA to finetune over 1,000 models and analyzed performance across various datasets, model types, and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Their findings suggest that GPT-4 evaluations are a viable substitute for human assessments and question the reliability of current chatbot benchmarks. The authors make their models and 4-bit training code public."
    },
    {
      "title": "Chronos: Learning the Language of Time Series",
      "url": "/research/2024/03/12/Chronos-Learning-the-Language-of-Time-Series.html",
      "date": "March 12, 2024",
      "summary": "Chronos is a framework that enhances pretrained probabilistic time series models by tokenizing time series data for training with transformer-based architectures, notably the T5 family. Pretrained on a mix of public and synthetic datasets created via Gaussian processes, Chronos models outshine others in 42 benchmark datasets by showing superior performance on familiar datasets and competitive or better zero-shot capabilities on new datasets. This illustrates Chronos's ability to generalize across different domains, offering a simpler approach to forecasting tasks."
    }
  ],
  "includes": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, the authors demonstrate the close relationship between these two families. This analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized."
    }
  ],
  "vicreg": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, the authors demonstrate the close relationship between these two families. This analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized."
    }
  ],
  "hyperparameter": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, the authors demonstrate the close relationship between these two families. This analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized."
    },
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrives incrementally, it's common to build a sequence of models that incorporate progressively more data. It has been shown that simply continuing training of a model on new data often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Data augmentation improves generalization, and robustness against image corruptions. However, the use of complex augmentation pipelines typically involves tuning of an enormous number of hyper parameters. RandAugment addresses this by reducing the search space. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "Automated Learning Rate Scheduler for Large-batch",
      "url": "/research/2021/07/13/Automated-Learning-Rate-Scheduler-for-Large-batch.html",
      "date": "July 13, 2021",
      "summary": "Large-batch training is crucial for deep learning with large datasets and models, but requires specific learning rate (LR) schedules for optimal performance, especially under limited training epochs. This work introduces an automated LR scheduling algorithm for large-batch neural network training within a fixed epoch budget, consisting of adaptive warmup and predefined decay phases. The LR is dynamically adjusted based on training loss monitored through Gaussian process smoothing, facilitating low computational overhead. When integrated with adaptive stochastic optimizers like AdamP and LAMB, this scheduler eliminates the need for extensive hyperparameter tuning and achieves competitive or superior results on various image classification tasks across different batch sizes and architectures."
    },
    {
      "title": "On Layer Normalization in the Transformer Architecture",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architecture.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is widely used in natural language processing. Training it typically requires a carefully designed learning rate warm-up stage, crucial for performance but slowing optimization and requiring more hyperparameter tuning. We provide theoretical insights into the necessity of the warm-up stage, demonstrating that the location of layer normalization affects gradient behavior. For Post-LN Transformers, with layer normalization between residual blocks, large gradients near the output layer at initialization make training unstable without warm-up. Conversely, for Pre-LN Transformers, with layer normalization inside residual blocks, gradients are well-behaved, suggesting the removal of the warm-up stage. Experimental results show that Pre-LN Transformers without warm-up achieve comparable performance with less training time and hyperparameter tuning."
    }
  ],
  "assumption": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, the authors demonstrate the close relationship between these two families. This analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized."
    },
    {
      "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
      "url": "/research/2021/07/24/The-Many-Faces-of-Robustness-A-Critical-Analysis-of-Out-of-Distribution-Generalization.html",
      "date": "July 24, 2021",
      "summary": "We present four novel real-world distribution shift datasets encompassing changes in image style, blurriness, location, camera settings, and more. Evaluating existing methods for enhancing out-of-distribution robustness, we discover that employing larger models and artificial data augmentations can enhance robustness against real-world distribution shifts, contradicting prior claims. Our findings demonstrate that improvements in artificial robustness benchmarks can indeed transfer to real-world distribution shifts, contrary to prior assumptions. Additionally, we introduce a novel data augmentation technique that surpasses models pretrained with significantly more labeled data, emphasizing its efficacy in addressing real-world distribution shifts. While some methods consistently mitigate texture and local image statistics shifts, they fail to address other shifts like geographic changes. Our results underscore the necessity for future research to examine multiple distribution shifts concurrently, as no single method consistently improves robustness across all evaluated scenarios."
    }
  ],
  "perturb": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrives incrementally, it's common to build a sequence of models that incorporate progressively more data. It has been shown that simply continuing training of a model on new data often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    },
    {
      "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
      "url": "/research/2022/03/19/Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training.html",
      "date": "March 19, 2022",
      "summary": "Sharpness-Aware Minimization (SAM) enhances generalization by optimizing a neighborhood-based perturbed loss, but it doesn't always favor flat minima due to both sharp and flat minima having low perturbed losses. We introduce a new measure, the surrogate gap, reflecting the dominant Hessian eigenvalue at small neighborhood radii, which is simple to compute and can be minimized during training. We propose the Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), an advancement over SAM with minimal additional computational cost. GSAM employs a dual-step approach: first, minimizing the perturbed loss similar to SAM, and second, reducing the surrogate gap without affecting the perturbed loss to target regions with low loss and sharpness, thus achieving superior generalization. GSAM is theoretically robust, showing better convergence and empirical generalization improvements, notably a +3.2% gain over SAM and +5.4% over AdamW in ImageNet accuracy for ViT-B/32."
    }
  ],
  "starting": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrives incrementally, it's common to build a sequence of models that incorporate progressively more data. It has been shown that simply continuing training of a model on new data often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    }
  ],
  "machine": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrives incrementally, it's common to build a sequence of models that incorporate progressively more data. It has been shown that simply continuing training of a model on new data often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    },
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    },
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "Fine-tuning with Very Large Dropout",
      "url": "/research/2024/03/01/Fine-tuning-with-Very-Large-Dropout.html",
      "date": "March 1, 2024",
      "summary": "This study challenges the notion that machine learning practices assume training and testing data have the same distribution. It explores the effectiveness of high dropout rates, as opposed to ensemble techniques, in developing rich data representations suitable for multiple distribution scenarios. These representations surpass those achieved by traditional in-distribution performance regularization and the implicit sparsity induced by common stochastic gradient methods. While training deep networks from scratch with high dropout rates is impractical, fine-tuning pre-trained models under these conditions is feasible and yields better out-of-distribution performance than ensembles and model averaging techniques like model soups. This finding is significant due to the growing relevance of fine-tuning with large pre-trained models, offering insights into the nature of rich representations and the linear characteristics of fine-tuning large networks with small datasets."
    },
    {
      "title": "From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning",
      "url": "/research/2023/03/16/From-MNIST-to-ImageNet-and-Back:-Benchmarking-Continual-Curriculum-Learning.html",
      "date": "March 16, 2023",
      "summary": "Continual learning (CL) is a promising trend in machine learning, aiming to develop robust models and strategies for dynamic environments by incorporating new knowledge while retaining past knowledge. CL research is fragmented, with various protocols, tasks, datasets, and metrics, often not reflecting real-world complexity and tailored to specific strategies. This work addresses this gap by introducing two novel CL benchmarks with heterogeneous tasks from six image datasets, varying in complexity and quality. These benchmarks fairly evaluate state-of-the-art CL strategies in scenarios closer to real-world conditions. The benchmarks present tasks in both increasing and decreasing complexity, assessing models' ability to exploit task structure. The work emphasizes a rigorous, reproducible evaluation protocol for measuring generalization and memory retention in models. Experimental results show that popular CL strategies perform poorly on these benchmarks, exhibit high forgetting, and struggle with curriculum task ordering. These findings underscore the need for rigorous comparisons and the development of new CL strategies for complex scenarios."
    },
    {
      "title": "HYPO: Hyperspherical Out-of-Distribution Generalization",
      "url": "/research/2024/03/19/HYPO-Hyperspherical-Out-of-Distribution-Generalization.html",
      "date": "March 19, 2024",
      "summary": "We propose HYPO, a novel framework for out-of-distribution (OOD) generalization in machine learning, which learns domain-invariant features in a hyperspherical space. Our method focuses on aligning features of the same class across different domains close to their class prototypes and separating different class prototypes maximally. We offer theoretical justifications for its improvement on OOD generalization and show through experiments on OOD benchmarks that HYPO outperforms existing baselines with superior performance."
    }
  ],
  "arrive": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrives incrementally, it's common to build a sequence of models that incorporate progressively more data. It has been shown that simply continuing training of a model on new data often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    }
  ],
  "continue": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrives incrementally, it's common to build a sequence of models that incorporate progressively more data. It has been shown that simply continuing training of a model on new data often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    }
  ],
  "discrepancy": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrives incrementally, it's common to build a sequence of models that incorporate progressively more data. It has been shown that simply continuing training of a model on new data often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    }
  ],
  "persist": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrives incrementally, it's common to build a sequence of models that incorporate progressively more data. It has been shown that simply continuing training of a model on new data often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "Understanding Deep Learning Requires Re- Thinking Generalization",
      "url": "/research/2017/02/26/Understanding-Deep-Learning-Requires-Re--Thinking-Generalization.html",
      "date": "February 26, 2017",
      "summary": "Extensive experiments reveal that traditional explanations for the strong generalization performance of large neural networks fall short. Despite conventional wisdom attributing success to model properties or regularization techniques, our findings show that state-of-the-art convolutional networks for image classification can easily overfit random labels. This phenomenon persists regardless of explicit regularization or substituting true images with random noise. Theoretical analysis suggests that shallow neural networks achieve perfect finite sample expressivity when parameters outnumber data points, as commonly observed. We interpret these results in contrast to traditional models."
    }
  ],
  "expense": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrives incrementally, it's common to build a sequence of models that incorporate progressively more data. It has been shown that simply continuing training of a model on new data often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    }
  ],
  "save": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrives incrementally, it's common to build a sequence of models that incorporate progressively more data. It has been shown that simply continuing training of a model on new data often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "A comprehensive review of Binary Neural Network",
      "url": "/research/2023/05/11/A-comprehensive-review-of-Binary-Neural-Network.html",
      "date": "May 11, 2023",
      "summary": "Deep learning (DL) is widely used in intelligent systems and real-life applications, but its deployment on computationally limited and energy-constrained devices requires efficient technologies like Binary Neural Networks (BNN). BNNs save significant storage, computation cost, and energy, making them ideal for small devices, despite the trade-offs in memory and performance. This article offers a comprehensive review of BNN developments, focusing on 1-bit activations and weights. It covers the evolution of BNN, from early models to advanced algorithms and design aspects, and addresses BNN optimization, deployment, computing architectures, and diverse applications. It also outlines potential research directions in BNN technology."
    }
  ],
  "phenomenon": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrives incrementally, it's common to build a sequence of models that incorporate progressively more data. It has been shown that simply continuing training of a model on new data often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper introduces the concept of grokking, a sudden onset of perfect generalisation long after overfitting, on small, algorithmically generated datasets. The study also finds that generalization on smaller datasets requires more optimization. The authors suggest that such datasets are ideal for investigating the puzzling phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    },
    {
      "title": "Is Cosine-Similarity of Embeddings Really About Similarity?",
      "url": "/research/2024/03/08/Is-Cosine-Similarity-of-Embeddings-Really-About-Similarity.html",
      "date": "March 8, 2024",
      "summary": "Cosine similarity, often used to gauge semantic similarity between high-dimensional objects by comparing low-dimensional feature embeddings, can yield variable results compared to unnormalized dot products. We investigate this phenomenon by analyzing embeddings from regularized linear models, revealing that cosine similarity can produce arbitrary and even meaningless similarities. This applies not only to linear models but also to deep models due to the implicit effects of various regularizations. Consequently, we advise against relying solely on cosine similarity and suggest exploring alternative approaches."
    },
    {
      "title": "Understanding Deep Learning Requires Re- Thinking Generalization",
      "url": "/research/2017/02/26/Understanding-Deep-Learning-Requires-Re--Thinking-Generalization.html",
      "date": "February 26, 2017",
      "summary": "Extensive experiments reveal that traditional explanations for the strong generalization performance of large neural networks fall short. Despite conventional wisdom attributing success to model properties or regularization techniques, our findings show that state-of-the-art convolutional networks for image classification can easily overfit random labels. This phenomenon persists regardless of explicit regularization or substituting true images with random noise. Theoretical analysis suggests that shallow neural networks achieve perfect finite sample expressivity when parameters outnumber data points, as commonly observed. We interpret these results in contrast to traditional models."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "url": "/research/2014/02/19/Exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks.html",
      "date": "February 19, 2014",
      "summary": "This study aims to enhance our comprehension of deep learning dynamics by analyzing deep linear neural networks. Despite their linear input-output mapping, these networks display nonlinear gradient descent dynamics, resulting in phenomena like prolonged plateaus and swift transitions to improved solutions. Through analytical exploration, we reveal that as network depth increases indefinitely, learning speed can remain finite under specific initial conditions. Unsupervised pretraining, under certain data conditions, can discover these favorable initial conditions, unlike random Gaussian initializations. We introduce a novel class of random orthogonal initial conditions that, akin to unsupervised pretraining, enable depth-independent learning. These conditions also ensure gradient propagation in deep nonlinear networks, particularly when operating at the edge of chaos."
    },
    {
      "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
      "url": "/research/2023/11/30/Dichotomy-of-Early-and-Late-Phase-Implicit-Biases-Can-Provably-Induce-Grokking.html",
      "date": "November 30, 2023",
      "summary": "Power et al. (2022) discovered that neural networks initially memorize arithmetic tasks, achieving perfect training accuracy but poor test accuracy, then suddenly achieve perfect test accuracy after extended training. This study explains this grokking phenomenon with the theory that early and late phase implicit biases cause this shift. Training homogeneous neural networks with large initialization and small weight decay on classification and regression tasks results in a prolonged period where the network acts like a kernel predictor, followed by a sudden shift to min-norm/max-margin predictors, significantly improving test accuracy."
    }
  ],
  "introduces": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrives incrementally, it's common to build a sequence of models that incorporate progressively more data. It has been shown that simply continuing training of a model on new data often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. This encoder serves as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The DMAE ViT-Base model achieves comparable or superior certified accuracy with fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on ImageNet. The model also shows high transferability to CIFAR-10, indicating its broad applicability."
    }
  ],
  "utility": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrives incrementally, it's common to build a sequence of models that incorporate progressively more data. It has been shown that simply continuing training of a model on new data often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "imaging": [
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets."
    }
  ],
  "tool": [
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets."
    },
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "This paper introduces tools for training monocular depth estimation models with multiple datasets despite incompatible annotations. Their approach uses a robust training objective, multi-objective learning for data integration, and pretraining encoders on auxiliary tasks. Tested across five datasets, including 3D films as a novel data source, their methods achieve superior zero-shot cross-dataset generalization, outperforming existing benchmarks through principled multi-dataset training."
    },
    {
      "title": "Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks",
      "url": "/research/2020/06/11/Batch-Normalization-Provably-Avoids-Rank-Collapse-for-Randomly-Initialised-Deep-Networks.html",
      "date": "June 11, 2020",
      "summary": "We investigate the challenge of training randomly initialized deep neural networks due to spectral instabilities in products of random matrices. Batch normalization emerges as an effective solution to prevent rank collapse in both linear and ReLU networks. Leveraging tools from Markov chain theory, we establish a lower rank bound for deep linear networks. Empirical findings show that this rank robustness extends to ReLU networks. Our experiments on real-world datasets underscore the significance of rank stability in training modern deep neural architectures."
    }
  ],
  "sensing": [
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets."
    }
  ],
  "organize": [
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets."
    }
  ],
  "sonn": [
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets."
    }
  ],
  "filter": [
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces an iterative self-training framework for sim-to-real 6D object pose estimation to enable cost-effective robotic bin-picking. The authors create a photo-realistic simulator to synthesize virtual data for training an initial pose estimation network (teacher model). This teacher predicts poses on unlabeled real data, and an adaptive selection scheme filters reliable predictions to generate pseudo-labels for training a student model on real data. Iteratively refining the teacher with the student improves pseudo-label quality. Evaluated on public benchmarks and a new dataset, their method shows 11.49% and 22.62% ADD(-S) improvements and a 19.54% increase in bin-picking success, demonstrating the effectiveness of this iterative sim-to-real approach."
    },
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    },
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    }
  ],
  "connection": [
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "The paper proposes three normalization-centric modifications to improve Transformer training: PRENORM which introduces pre-norm residual connections and smaller initializations, enabling warmup-free, validation-based training with large learning rates; SCALENORM which suggests L2 normalization with a single scale parameter for faster training and better performance; and FIXNORM which reaffirms the efficacy of normalizing word embeddings to a fixed length."
    },
    {
      "title": "SAM as an Optimal Relaxation of Bayes",
      "url": "/research/2023/12/10/SAM-as-an-Optimal-Relaxation-of-Bayes.html",
      "date": "December 10, 2023",
      "summary": "We introduce SAM, a method enhancing generalization in deep learning, as a relaxation of the Bayes objective. SAM replaces the expected negative-loss with an optimal convex lower bound derived using the Fenchel biconjugate. This connection enables an Adam-like extension of SAM, offering automatic uncertainty estimates and potential accuracy improvements. Bridging adversarial and Bayesian methods, our work paves the way for robustness enhancement."
    },
    {
      "title": "DeepNet: Scaling Transformers to 1,000 Layers",
      "url": "/research/2022/03/01/DeepNet-Scaling-Transformers-to-1,000-Layers.html",
      "date": "March 1, 2022",
      "summary": "This paper introduces DEEPNORM, a novel method to stabilize extremely deep Transformers through a new normalization function and a theoretically derived initialization for the residual connection. DEEPNORM offers a stable and efficient training regime, blending the benefits of Post-LN's performance with Pre-LN's stability, allowing for the scaling of Transformers up to 1,000 layers without difficulty. This represents a significant advancement over previous models, with a 200-layer, 3.2B parameter model outperforming a 48-layer, 12B parameter state-of-the-art model by 5 BLEU points on a multilingual benchmark with 7,482 translation directions, suggesting a promising direction for scaling."
    },
    {
      "title": "Do Vision Transformers See Like Convolutional Neural Networks?",
      "url": "/research/2022/03/03/Do-Vision-Transformers-See-Like-Convolutional-Neural-Networks.html",
      "date": "March 3, 2022",
      "summary": "Convolutional Neural Networks (CNNs) have been prominent in processing visual data, but recent studies show Vision Transformers (ViTs) can match or outperform CNNs in image classification. This work examines how ViTs solve classification tasks, discovering significant differences from CNNs, including uniformity in ViT representations across layers due to self-attention and residual connections that enhance feature propagation. ViTs also maintain spatial information effectively, influenced by classification methods. The study further explores how dataset scale affects ViT features and their transferability, linking these findings to new architectures like MLP-Mixer."
    }
  ],
  "parameters": [
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets."
    }
  ],
  "equivalent": [
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets."
    }
  ],
  "limitation": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, this paper introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, the authors enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. This simple method surpasses the current state-of-the-art in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study looks at self-supervised pre-training methods on small scale or lightweight Vision Transformers (ViTs). Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods."
    },
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "This paper introduces FineAction, a large-scale, fine-grained video dataset designed to address the limitations of current temporal action localization (TAL) benchmarks, which rely on coarse action classes and lead to model overfitting and ambiguous annotations. FineAction contains 103K instances across 106 action categories in 17K videos, offering a diverse and densely annotated dataset with co-occurring actions that pose new challenges for TAL. The authors evaluate popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and propose a baseline method achieving a 13.17% mAP. FineAction aims to advance TAL research and is accessible online."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces sinusoidal representation networks (SIRENs), which utilize periodic activation functions like Sin to effectively capture complex natural signals and their derivatives, addressing the limitations of neural networks parameterized for continuous, differentiable signal representations. The authors' analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. SIRENs are also applied in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations. The authors extend SIRENs' use with hypernetworks to learn priors for SIREN functions."
    },
    {
      "title": "Normalized Attention Without Probability Cage",
      "url": "/research/2020/05/19/Normalized-Attention-Without-Probability-Cage.html",
      "date": "May 19, 2020",
      "summary": "Softmax-attention architectures, especially popularized by Transformers, have seen significant advancements in various tasks. However, the geometric implications of softmax-attention remain underexplored. In this study, we demonstrate limitations arising from constraining attention weights to the probability simplex and its impact on the convex hull of value vectors. We reveal sequence length-dependent biases in Transformers towards token isolation at initialization and compare them with max- and sum-pooling, which are strong but often overlooked baselines. To address these issues, we propose a novel approach of replacing softmax with normalization in self-attention, resulting in a robust and widely applicable architecture. Our findings are supported by empirical results from over 25,000 trained models, and all results and implementations are publicly available."
    },
    {
      "title": "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization",
      "url": "/research/2023/07/04/Gradient-Norm-Aware-Minimization-Seeks-First-Order-Flatness-and-Improves-Generalization.html",
      "date": "July 4, 2023",
      "summary": "Recent advancements have highlighted the effectiveness of flat minima in enhancing generalization, particularly through Sharpness-Aware Minimization (SAM). However, existing definitions of flatness, such as zeroth-order flatness, have limitations in discerning between minima with low and high generalization errors. To address this, we propose first-order flatness, which considers maximal gradient norm within a perturbation radius. We introduce Gradient norm Aware Minimization (GAM) as a novel training approach to achieve uniformly small curvature across all directions. Experimental results demonstrate GAM's ability to enhance generalization compared to standard optimizers like SGD and AdamW across various datasets and networks. Moreover, GAM facilitates SAM in identifying flatter minima, leading to improved generalization."
    },
    {
      "title": "Symbolic Discovery of Optimization Algorithms",
      "url": "/research/2023/05/08/Symbolic-Discovery-of-Optimization-Algorithms.html",
      "date": "May 8, 2023",
      "summary": "We propose a method for algorithm discovery via program search, focusing on optimizing deep neural network training. Our approach, Lion (EvoLved Sign Momentum), is memory-efficient and achieves comparable or superior performance to widely-used optimizers such as Adam and Adafactor across various tasks. Specifically, Lion enhances accuracy on tasks like image classification and vision-language contrastive learning, while reducing training compute. Notably, Lion exhibits improved performance with larger batch sizes and requires smaller learning rates compared to Adam. Despite its effectiveness, Lion has limitations, which we analyze, and provide insights into its deployment and performance. Our implementation is publicly available and has been successfully utilized in Google's search ads CTR model."
    },
    {
      "title": "The Platonic Representation Hypothesis",
      "url": "/research/2024/05/13/The-Platonic-Representation-Hypothesis.html",
      "date": "May 13, 2024",
      "summary": "This paper argues that AI model representations, especially in deep networks, are converging. It surveys examples in the literature showing how neural network representations align over time and across domains. It demonstrates convergence across data modalities, noting that as vision and language models grow, they measure distances between data points similarly. The authors hypothesize that this convergence leads to a shared statistical model of reality, termed the platonic representation and discuss possible selective pressures toward it. The paper also addresses the implications, limitations, and counterexamples to this trend."
    },
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "randconv": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, this paper introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, the authors enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. This simple method surpasses the current state-of-the-art in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    }
  ],
  "kernel": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, this paper introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, the authors enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. This simple method surpasses the current state-of-the-art in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "This paper demonstrates that artificial neural networks (ANNs) are equivalent to Gaussian processes at initialization in the infinite-width limit and introduces the Neural Tangent Kernel (NTK), which describes ANNs' behavior during training. The NTK stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. The authors prove the positive-definiteness of the limiting NTK under certain conditions and show that the network function follows a linear differential equation during training for least-squares regression. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    },
    {
      "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
      "url": "/research/2023/11/30/Dichotomy-of-Early-and-Late-Phase-Implicit-Biases-Can-Provably-Induce-Grokking.html",
      "date": "November 30, 2023",
      "summary": "Power et al. (2022) discovered that neural networks initially memorize arithmetic tasks, achieving perfect training accuracy but poor test accuracy, then suddenly achieve perfect test accuracy after extended training. This study explains this grokking phenomenon with the theory that early and late phase implicit biases cause this shift. Training homogeneous neural networks with large initialization and small weight decay on classification and regression tasks results in a prolonged period where the network acts like a kernel predictor, followed by a sudden shift to min-norm/max-margin predictors, significantly improving test accuracy."
    }
  ],
  "integrity": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, this paper introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, the authors enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. This simple method surpasses the current state-of-the-art in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    }
  ],
  "diversity": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, this paper introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, the authors enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. This simple method surpasses the current state-of-the-art in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results. The method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. A convolutional neural network (CNN) was trained on over a million color images, operating as a feed-forward pass at test time. The algorithm's effectiveness was validated through a colorization Turing test, where it deceived participants in 32% of trials. The paper also illustrates how colorization can serve as an effective pretraining task for self-supervised feature learning, achieving state-of-the-art results on several benchmarks."
    }
  ],
  "affine": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, this paper introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, the authors enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. This simple method surpasses the current state-of-the-art in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    }
  ],
  "contrast": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, this paper introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, the authors enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. This simple method surpasses the current state-of-the-art in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "This paper introduces Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperformed CLIP models on various downstream tasks. The speedup from this method enables experimentation with larger model sizes, more data, or longer training periods."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "Understanding Deep Learning Requires Re- Thinking Generalization",
      "url": "/research/2017/02/26/Understanding-Deep-Learning-Requires-Re--Thinking-Generalization.html",
      "date": "February 26, 2017",
      "summary": "Extensive experiments reveal that traditional explanations for the strong generalization performance of large neural networks fall short. Despite conventional wisdom attributing success to model properties or regularization techniques, our findings show that state-of-the-art convolutional networks for image classification can easily overfit random labels. This phenomenon persists regardless of explicit regularization or substituting true images with random noise. Theoretical analysis suggests that shallow neural networks achieve perfect finite sample expressivity when parameters outnumber data points, as commonly observed. We interpret these results in contrast to traditional models."
    },
    {
      "title": "Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding",
      "url": "/research/2021/11/16/Improved-Robustness-of-Vision-Transformer-via-PreLayerNorm-in-Patch-Embedding.html",
      "date": "November 16, 2021",
      "summary": "Recent advancements in vision transformers (ViTs) have shown superior performance across diverse visual tasks, surpassing convolutional neural networks (CNNs). Given ViT's distinct architecture, understanding its behavior and reliability is imperative. This paper investigates ViT's robustness by comparing it with CNNs under various image corruptions relevant to real-world vision tasks. While ViT generally exhibits comparable or improved robustness over CNNs, it consistently underperforms in contrast enhancement tasks. Analysis suggests that positional embedding in ViT's patch embedding may malfunction with color scale changes. We propose PreLayerNorm, a modified patch embedding structure, to address this issue and ensure scale-invariant behavior in ViT. ViT with PreLayerNorm demonstrates enhanced robustness across various corruptions, particularly in contrast-varying environments."
    },
    {
      "title": "Language Modeling with Gated Convolutional Networks",
      "url": "/research/2017/09/08/Language-Modeling-with-Gated-Convolutional-Networks.html",
      "date": "September 8, 2017",
      "summary": "This paper introduces a novel language modeling approach using stacked convolutions that enable efficient parallel processing, in contrast to the traditionally used recurrent neural networks known for handling unbounded context. The proposed method features a simplified gating mechanism that outperforms previous models and demonstrates superior performance on the WikiText-103 benchmark, showing its capability to manage long-term dependencies. It also delivers competitive results on the Google Billion Words benchmark and significantly reduces sentence scoring latency compared to recurrent models. This marks the first instance of a non-recurrent model achieving comparable success to strong recurrent models on major language tasks."
    }
  ],
  "diversification": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, this paper introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, the authors enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. This simple method surpasses the current state-of-the-art in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    }
  ],
  "generator": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, this paper introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, the authors enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. This simple method surpasses the current state-of-the-art in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    }
  ],
  "randaugment": [
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Data augmentation improves generalization, and robustness against image corruptions. However, the use of complex augmentation pipelines typically involves tuning of an enormous number of hyper parameters. RandAugment addresses this by reducing the search space. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets."
    }
  ],
  "search": [
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Data augmentation improves generalization, and robustness against image corruptions. However, the use of complex augmentation pipelines typically involves tuning of an enormous number of hyper parameters. RandAugment addresses this by reducing the search space. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets."
    },
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/research/2020/08/03/Are-Labels-Necessary-for-Neural-Architecture-Search.html",
      "date": "August 3, 2020",
      "summary": "The paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning the necessity of human-annotated labels for discovering effective neural architectures. Two experimental setups sample-based and search-based were investigated. The sample-based approach evaluated 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. The search-based experiments employed DARTS, a recognized NAS algorithm, with various unsupervised objectives. The architectures identified without labels performed competitively compared to those found with labels. The findings suggested that image statistics alone could be sufficient for identifying efficient neural architectures, potentially eliminating the need for human-annotated labels."
    },
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "This paper introduces an algorithm that identifies simple and highly generalizable neural networks by searching for extensive flat minima regions in the error function, where the error rate is relatively stable. Such flat minima are associated with lower overfitting risks based on minimum description length (MDL) principles. Despite requiring second-order derivative calculations, the algorithm has a complexity level comparable to backpropagation. When tested on feedforward and recurrent networks, as well as stock market prediction tasks, this algorithm outperformed traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    },
    {
      "title": "Searching for Activation Functions",
      "url": "/research/2017/10/27/Searching-for-Activation-Functions.html",
      "date": "October 27, 2017",
      "summary": "This study explores the impact of activation functions on deep network training and performance. While Rectified Linear Unit (ReLU) is widely used, alternatives have not consistently outperformed it. We propose using automatic search techniques to discover new activation functions. Through exhaustive and reinforcement learning-based searches, we identify novel functions. Empirical evaluation shows that our best discovered function, Swish (f(x) = x · sigmoid(βx)), performs better than ReLU on deeper models across challenging datasets. Replacing ReLUs with Swish units improves classification accuracy on ImageNet, for example, by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. Swish's simplicity and similarity to ReLU facilitate its adoption in neural networks."
    },
    {
      "title": "Symbolic Discovery of Optimization Algorithms",
      "url": "/research/2023/05/08/Symbolic-Discovery-of-Optimization-Algorithms.html",
      "date": "May 8, 2023",
      "summary": "We propose a method for algorithm discovery via program search, focusing on optimizing deep neural network training. Our approach, Lion (EvoLved Sign Momentum), is memory-efficient and achieves comparable or superior performance to widely-used optimizers such as Adam and Adafactor across various tasks. Specifically, Lion enhances accuracy on tasks like image classification and vision-language contrastive learning, while reducing training compute. Notably, Lion exhibits improved performance with larger batch sizes and requires smaller learning rates compared to Adam. Despite its effectiveness, Lion has limitations, which we analyze, and provide insights into its deployment and performance. Our implementation is publicly available and has been successfully utilized in Google's search ads CTR model."
    }
  ],
  "svhn": [
    {
      "title": "SVHN: Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View House Numbers."
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of Vision Transformers (ViTs) without relying on large-scale pre-training datasets like ImageNet and JFT or requiring modifications to the architecture or loss functions. The authors show that this approach improves ViT performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, while maintaining ViT's attention to relevant regions and robustness, despite ViT's inherent lack of inductive biases and typical dependence on large-scale pre-training."
    }
  ],
  "read": [
    {
      "title": "SVHN: Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View House Numbers."
    }
  ],
  "digit": [
    {
      "title": "SVHN: Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View House Numbers."
    }
  ],
  "character": [
    {
      "title": "SVHN: Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View House Numbers."
    }
  ],
  "document": [
    {
      "title": "SVHN: Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View House Numbers."
    }
  ],
  "recognizing": [
    {
      "title": "SVHN: Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View House Numbers."
    }
  ],
  "scene": [
    {
      "title": "SVHN: Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View House Numbers."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "This paper introduces a novel approach to Radiance Field methods for novel-view synthesis that addresses the challenges of high visual quality, costly training, and real-time rendering of unbounded, complete scenes at 1080p resolution. The authors propose three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. The method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "photograph": [
    {
      "title": "SVHN: Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View House Numbers."
    }
  ],
  "street": [
    {
      "title": "SVHN: Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View House Numbers."
    }
  ],
  "photo": [
    {
      "title": "SVHN: Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View House Numbers."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results. The method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. A convolutional neural network (CNN) was trained on over a million color images, operating as a feed-forward pass at test time. The algorithm's effectiveness was validated through a colorization Turing test, where it deceived participants in 32% of trials. The paper also illustrates how colorization can serve as an effective pretraining task for self-supervised feature learning, achieving state-of-the-art results on several benchmarks."
    }
  ],
  "house": [
    {
      "title": "SVHN: Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View House Numbers."
    }
  ],
  "rethink": [
    {
      "title": "Rethinking Batch in BatchNorm",
      "url": "/research/2021/05/17/Rethinking-Batch-in-BatchNorm.html",
      "date": "May 17, 2021",
      "summary": "BatchNorm, essential in convolutional neural networks, behaves uniquely due to its batch-based operation, leading to performance issues in visual recognition tasks. This paper identifies these issues and suggests reevaluating the concept of batch in BatchNorm for better performance, aiming to guide researchers in its effective use."
    }
  ],
  "batch normalization": [
    {
      "title": "Rethinking Batch in BatchNorm",
      "url": "/research/2021/05/17/Rethinking-Batch-in-BatchNorm.html",
      "date": "May 17, 2021",
      "summary": "BatchNorm, essential in convolutional neural networks, behaves uniquely due to its batch-based operation, leading to performance issues in visual recognition tasks. This paper identifies these issues and suggests reevaluating the concept of batch in BatchNorm for better performance, aiming to guide researchers in its effective use."
    }
  ],
  "behave": [
    {
      "title": "Rethinking Batch in BatchNorm",
      "url": "/research/2021/05/17/Rethinking-Batch-in-BatchNorm.html",
      "date": "May 17, 2021",
      "summary": "BatchNorm, essential in convolutional neural networks, behaves uniquely due to its batch-based operation, leading to performance issues in visual recognition tasks. This paper identifies these issues and suggests reevaluating the concept of batch in BatchNorm for better performance, aiming to guide researchers in its effective use."
    }
  ],
  "operation": [
    {
      "title": "Rethinking Batch in BatchNorm",
      "url": "/research/2021/05/17/Rethinking-Batch-in-BatchNorm.html",
      "date": "May 17, 2021",
      "summary": "BatchNorm, essential in convolutional neural networks, behaves uniquely due to its batch-based operation, leading to performance issues in visual recognition tasks. This paper identifies these issues and suggests reevaluating the concept of batch in BatchNorm for better performance, aiming to guide researchers in its effective use."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    }
  ],
  "suggests": [
    {
      "title": "Rethinking Batch in BatchNorm",
      "url": "/research/2021/05/17/Rethinking-Batch-in-BatchNorm.html",
      "date": "May 17, 2021",
      "summary": "BatchNorm, essential in convolutional neural networks, behaves uniquely due to its batch-based operation, leading to performance issues in visual recognition tasks. This paper identifies these issues and suggests reevaluating the concept of batch in BatchNorm for better performance, aiming to guide researchers in its effective use."
    }
  ],
  "reevaluate": [
    {
      "title": "Rethinking Batch in BatchNorm",
      "url": "/research/2021/05/17/Rethinking-Batch-in-BatchNorm.html",
      "date": "May 17, 2021",
      "summary": "BatchNorm, essential in convolutional neural networks, behaves uniquely due to its batch-based operation, leading to performance issues in visual recognition tasks. This paper identifies these issues and suggests reevaluating the concept of batch in BatchNorm for better performance, aiming to guide researchers in its effective use."
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey provides a comprehensive review of transformer-based visual segmentation, a crucial field for various applications. The authors cover the evolution from convolutional methods to vision transformers, offering a unified framework to simplify understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas such as 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. The authors re-evaluate these methods on established datasets, outline current challenges, and suggest future research directions."
    }
  ],
  "researcher": [
    {
      "title": "Rethinking Batch in BatchNorm",
      "url": "/research/2021/05/17/Rethinking-Batch-in-BatchNorm.html",
      "date": "May 17, 2021",
      "summary": "BatchNorm, essential in convolutional neural networks, behaves uniquely due to its batch-based operation, leading to performance issues in visual recognition tasks. This paper identifies these issues and suggests reevaluating the concept of batch in BatchNorm for better performance, aiming to guide researchers in its effective use."
    }
  ],
  "drophead": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "This paper introduces DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. Authors also propose a dropout rate scheduler to optimize training."
    }
  ],
  "tailor": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "This paper introduces DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. Authors also propose a dropout rate scheduler to optimize training."
    },
    {
      "title": "Sharpness-Aware Training for Free",
      "url": "/research/2023/03/02/Sharpness-Aware-Training-for-Free.html",
      "date": "March 2, 2023",
      "summary": "Modern deep neural networks (DNNs) excel in performance but often suffer from over-parameterization, leading to increased generalization error without tailored training strategies. Sharpness-Aware Minimization (SAM) has proven effective in reducing generalization error by minimizing sharpness in the loss landscape. However, SAM incurs a significant computational overhead. This paper introduces Sharpness-Aware Training for Free (SAF), which mitigates sharpness at nearly zero additional computational cost. SAF achieves this by preventing sudden drops in loss within sharp local minima during weight updates. A novel trajectory loss, based on KL-divergence between current and past DNN outputs, replaces SAM's sharpness measure, guiding convergence towards flat minima for enhanced generalization. Empirical results demonstrate SAF's effectiveness on ImageNet with comparable computational efficiency to the base optimizer."
    },
    {
      "title": "From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning",
      "url": "/research/2023/03/16/From-MNIST-to-ImageNet-and-Back:-Benchmarking-Continual-Curriculum-Learning.html",
      "date": "March 16, 2023",
      "summary": "Continual learning (CL) is a promising trend in machine learning, aiming to develop robust models and strategies for dynamic environments by incorporating new knowledge while retaining past knowledge. CL research is fragmented, with various protocols, tasks, datasets, and metrics, often not reflecting real-world complexity and tailored to specific strategies. This work addresses this gap by introducing two novel CL benchmarks with heterogeneous tasks from six image datasets, varying in complexity and quality. These benchmarks fairly evaluate state-of-the-art CL strategies in scenarios closer to real-world conditions. The benchmarks present tasks in both increasing and decreasing complexity, assessing models' ability to exploit task structure. The work emphasizes a rigorous, reproducible evaluation protocol for measuring generalization and memory retention in models. Experimental results show that popular CL strategies perform poorly on these benchmarks, exhibit high forgetting, and struggle with curriculum task ordering. These findings underscore the need for rigorous comparisons and the development of new CL strategies for complex scenarios."
    }
  ],
  "multihead": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "This paper introduces DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. Authors also propose a dropout rate scheduler to optimize training."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "On the Relationship Between Self-Attention And Convolutional Layers",
      "url": "/research/2020/01/10/On-the-Relationship-Between-Self-Attention-And-Convolutional-Layers.html",
      "date": "January 10, 2020",
      "summary": "Recent studies have shown that attention mechanisms can rival or even outperform convolutional layers in vision tasks, challenging their dominance. Ramachandran et al. (2019) demonstrated that attention could entirely replace convolution, achieving top results. This work investigates whether attention layers function like convolutional layers, finding that multi-head self-attention layers, with enough heads, match the expressiveness of convolutional layers. Numerical experiments confirm that self-attention layers learn to focus on pixel-grid patterns akin to convolutional layers, supporting our findings."
    }
  ],
  "head": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "This paper introduces DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. Authors also propose a dropout rate scheduler to optimize training."
    },
    {
      "title": "On the Relationship Between Self-Attention And Convolutional Layers",
      "url": "/research/2020/01/10/On-the-Relationship-Between-Self-Attention-And-Convolutional-Layers.html",
      "date": "January 10, 2020",
      "summary": "Recent studies have shown that attention mechanisms can rival or even outperform convolutional layers in vision tasks, challenging their dominance. Ramachandran et al. (2019) demonstrated that attention could entirely replace convolution, achieving top results. This work investigates whether attention layers function like convolutional layers, finding that multi-head self-attention layers, with enough heads, match the expressiveness of convolutional layers. Numerical experiments confirm that self-attention layers learn to focus on pixel-grid patterns akin to convolutional layers, supporting our findings."
    }
  ],
  "avoid": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "This paper introduces DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. Authors also propose a dropout rate scheduler to optimize training."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets for formula-driven supervised learning (FDSL). The authors identify the optimal FDSL parameters and maximize synthetic image variety, which is crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. The authors demonstrate FDSL's potential for continuous improvement and its ability to avoid issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "This paper proposes Hard Patches Mining (HPM), a novel framework for masked image modeling (MIM) pre-training that goes beyond simply solving given problems. HPM aims for the model to generate more challenging tasks for itself, using reconstruction loss as a metric for task difficulty. It incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "The paper introduces a system that trains deep neural networks for object detection using only synthetic images generated with domain randomization. This involves randomizing non-realistic simulator parameters like lighting and object textures, allowing the network to identify essential object features. Their findings show that networks can achieve impressive performance using just synthetic data, and further improve with fine-tuning on real data. This suggests the potential of using cost-effective synthetic data for training, avoiding the need to acquire vast amounts of real-world data or create detailed synthetic environments. They validate their method on car bounding box detection using the KITTI dataset."
    },
    {
      "title": "Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks",
      "url": "/research/2020/06/11/Batch-Normalization-Provably-Avoids-Rank-Collapse-for-Randomly-Initialised-Deep-Networks.html",
      "date": "June 11, 2020",
      "summary": "We investigate the challenge of training randomly initialized deep neural networks due to spectral instabilities in products of random matrices. Batch normalization emerges as an effective solution to prevent rank collapse in both linear and ReLU networks. Leveraging tools from Markov chain theory, we establish a lower rank bound for deep linear networks. Empirical findings show that this rank robustness extends to ReLU networks. Our experiments on real-world datasets underscore the significance of rank stability in training modern deep neural architectures."
    },
    {
      "title": "Suppressing Pink Elephants with Direct Principle Feedback",
      "url": "/research/2024/02/13/Suppressing-Pink-Elephants-with-Direct-Principle-Feedback.html",
      "date": "February 13, 2024",
      "summary": "Existing methods for controlling language models focus on training desired behaviors, but often lack the flexibility needed for diverse applications. The authors address this with the Pink Elephant Problem, demonstrating the need for language models to adapt to different contexts by avoiding certain topics (the Pink Elephant) in favor of others (Grey Elephant). The authors introduce Direct Principle Feedback (DPF), an adaptation of Constitutional AI that improves control by directly applying critiques and revisions without ranking responses. Our study shows that a 13B LLaMA 2 model fine-tuned with DPF on a synthetic dataset outperforms existing models and matches GPT-4 in managing the Pink Elephant Problem."
    }
  ],
  "dominance": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "This paper introduces DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. Authors also propose a dropout rate scheduler to optimize training."
    },
    {
      "title": "On the Relationship Between Self-Attention And Convolutional Layers",
      "url": "/research/2020/01/10/On-the-Relationship-Between-Self-Attention-And-Convolutional-Layers.html",
      "date": "January 10, 2020",
      "summary": "Recent studies have shown that attention mechanisms can rival or even outperform convolutional layers in vision tasks, challenging their dominance. Ramachandran et al. (2019) demonstrated that attention could entirely replace convolution, achieving top results. This work investigates whether attention layers function like convolutional layers, finding that multi-head self-attention layers, with enough heads, match the expressiveness of convolutional layers. Numerical experiments confirm that self-attention layers learn to focus on pixel-grid patterns akin to convolutional layers, supporting our findings."
    },
    {
      "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
      "url": "/research/2024/01/24/Better-Call-GPT,-Comparing-Large-Language-Models-Against-Lawyers.html",
      "date": "January 24, 2024",
      "summary": "This paper compares Large Language Models (LLMs) with traditional legal contract reviewers—Junior Lawyers and Legal Process Outsourcers (LPOs). It evaluates whether LLMs outperform humans in accuracy, speed, and cost-efficiency during contract review. Empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, revealing that advanced models match or exceed human accuracy in identifying legal issues. LLMs complete reviews in seconds, vastly outpacing the hours required by humans. Cost-wise, LLMs achieve a 99.97 percent reduction compared to traditional methods. These findings indicate a transformative shift in legal practice, with LLMs enhancing the accessibility and efficiency of legal services. The research suggests that LLM dominance in legal contract review challenges the status quo, necessitating a reimagined future for legal workflows."
    }
  ],
  "scheduler": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "This paper introduces DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. Authors also propose a dropout rate scheduler to optimize training."
    },
    {
      "title": "Automated Learning Rate Scheduler for Large-batch",
      "url": "/research/2021/07/13/Automated-Learning-Rate-Scheduler-for-Large-batch.html",
      "date": "July 13, 2021",
      "summary": "Large-batch training is crucial for deep learning with large datasets and models, but requires specific learning rate (LR) schedules for optimal performance, especially under limited training epochs. This work introduces an automated LR scheduling algorithm for large-batch neural network training within a fixed epoch budget, consisting of adaptive warmup and predefined decay phases. The LR is dynamically adjusted based on training loss monitored through Gaussian process smoothing, facilitating low computational overhead. When integrated with adaptive stochastic optimizers like AdamP and LAMB, this scheduler eliminates the need for extensive hyperparameter tuning and achieves competitive or superior results on various image classification tasks across different batch sizes and architectures."
    },
    {
      "title": "Eigencurve: Optimal Learning Rate Schedule For Sgd on Quadratic Objectives With Skewed Hessian Spectrums",
      "url": "/research/2022/06/14/Eigencurve-Optimal-Learning-Rate-Schedule-For-Sgd-on-Quadratic-Objectives-With-Skewed-Hessian-Spectrums.html",
      "date": "June 14, 2022",
      "summary": "Learning rate schedulers are essential in deep neural network training, but there's a gap between practical usage and theoretical understanding. This paper introduces Eigencurve, the first set of learning rate schedules achieving minimax optimal convergence rates for SGD on quadratic objectives with skewed eigenvalue distributions of the Hessian matrix. This condition is common in practice. Experimental results on CIFAR-10 image classification tasks demonstrate Eigencurve's superiority over step decay, particularly with fewer epochs. The theory inspires practical schedulers approximating Eigencurve, resembling cosine decay for some problems while outperforming it in others."
    },
    {
      "title": "How to decay your learning rate",
      "url": "/research/2021/05/23/How-to-decay-your-learning-rate.html",
      "date": "May 23, 2021",
      "summary": "Empirical findings suggest that typical fine-tuned learning rate schedules decay the learning rate following weight norm fluctuations. This led to the development of ABEL, an automatic scheduler that adjusts the learning rate based on weight norm changes. ABEL performs comparably to tuned schedules but demonstrates greater robustness to parameter variations. Extensive experiments across various domains reveal that when the weight norm remains stable, simplified schedules yield equivalent performance to complex ones, resembling a constant learning rate with decay towards the end of training."
    }
  ],
  "sigmoid": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "This paper introduces a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. This decouples batch size from the loss, and allows the authors to increase the batch size and also improves performance even with smaller batches. Pushing the batch size to one million showed diminishing returns, with 32k being a good middle ground."
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "This paper introduces Xavier Glorot init for Sigmoid like activation functions. The study investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial."
    },
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "This paper explores Gated Linear Units (GLUs), which involve the component-wise product of two linear projections, with one undergoing a sigmoid function. The authors investigate GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers. They find that certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    }
  ],
  "pairwise": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "This paper introduces a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. This decouples batch size from the loss, and allows the authors to increase the batch size and also improves performance even with smaller batches. Pushing the batch size to one million showed diminishing returns, with 32k being a good middle ground."
    }
  ],
  "siglip": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "This paper introduces a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. This decouples batch size from the loss, and allows the authors to increase the batch size and also improves performance even with smaller batches. Pushing the batch size to one million showed diminishing returns, with 32k being a good middle ground."
    }
  ],
  "outperforms": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "This paper introduces a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. This decouples batch size from the loss, and allows the authors to increase the batch size and also improves performance even with smaller batches. Pushing the batch size to one million showed diminishing returns, with 32k being a good middle ground."
    },
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "Non-negative Contrastive Learning",
      "url": "/research/2024/03/19/Non-negative-Contrastive-Learning.html",
      "date": "March 19, 2024",
      "summary": "Deep representations offer promising performance for downstream tasks but lack interpretability, posing a significant challenge. In this paper, we introduce Non-negative Contrastive Learning (NCL), a refinement of Non-negative Matrix Factorization (NMF) aimed at generating interpretable features. NCL enforces non-negativity constraints, akin to NMF, resulting in sparse and disentangled representations, unlike standard contrastive learning (CL). We establish theoretical guarantees on NCL's identifiability and downstream generalization. Empirically, NCL outperforms CL in feature disentanglement, selection, and downstream classification tasks. Moreover, NCL can be extended to other learning scenarios, benefiting supervised learning."
    }
  ],
  "push": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "This paper introduces a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. This decouples batch size from the loss, and allows the authors to increase the batch size and also improves performance even with smaller batches. Pushing the batch size to one million showed diminishing returns, with 32k being a good middle ground."
    }
  ],
  "diminish": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "This paper introduces a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. This decouples batch size from the loss, and allows the authors to increase the batch size and also improves performance even with smaller batches. Pushing the batch size to one million showed diminishing returns, with 32k being a good middle ground."
    },
    {
      "title": "Neural Networks Learn Statistics of Increasing Complexity",
      "url": "/research/2024/02/13/Neural-Networks-Learn-Statistics-of-Increasing-Complexity.html",
      "date": "February 13, 2024",
      "summary": "The distributional simplicity bias (DSB) theory suggests neural networks first learn basic patterns in data before understanding more complex correlations. We provide new evidence supporting DSB, showing networks initially excel with data matching training set's simple statistics but this ability diminishes later. Extending DSB to discrete domains, we demonstrate an equivalence between n-gram frequencies and vector moments, also observing this bias in large language models (LLMs). Additionally, by adjusting low-level statistics of images to resemble another class, we reveal that networks in early training phases misclassify these edited images as if they belonged to the target class."
    }
  ],
  "return": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "This paper introduces a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. This decouples batch size from the loss, and allows the authors to increase the batch size and also improves performance even with smaller batches. Pushing the batch size to one million showed diminishing returns, with 32k being a good middle ground."
    }
  ],
  "ground": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "This paper introduces a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. This decouples batch size from the loss, and allows the authors to increase the batch size and also improves performance even with smaller batches. Pushing the batch size to one million showed diminishing returns, with 32k being a good middle ground."
    },
    {
      "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
      "url": "/research/2024/01/24/Better-Call-GPT,-Comparing-Large-Language-Models-Against-Lawyers.html",
      "date": "January 24, 2024",
      "summary": "This paper compares Large Language Models (LLMs) with traditional legal contract reviewers—Junior Lawyers and Legal Process Outsourcers (LPOs). It evaluates whether LLMs outperform humans in accuracy, speed, and cost-efficiency during contract review. Empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, revealing that advanced models match or exceed human accuracy in identifying legal issues. LLMs complete reviews in seconds, vastly outpacing the hours required by humans. Cost-wise, LLMs achieve a 99.97 percent reduction compared to traditional methods. These findings indicate a transformative shift in legal practice, with LLMs enhancing the accessibility and efficiency of legal services. The research suggests that LLM dominance in legal contract review challenges the status quo, necessitating a reimagined future for legal workflows."
    }
  ],
  "occlusion": [
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. The Authors show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements."
    }
  ],
  "mim": [
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. The Authors show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements."
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, enhances vision tasks using Vision transformers by masking and reconstructing parts of an image. The compatibility of MIM with CNNs and its operational principle are unclear. this study reveals that MIM improves generalized feature extraction through middle-order patch interactions and introduces Architecture-Agnostic Masked Image Modeling (A2MIM), compatible with both Transformers and CNNs. Extensive testing demonstrates A2MIM's ability to enhance representation learning and transferability to various tasks without specialized modifications."
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM) for self-supervised visual representation in vision transformers ViT. Five different learning objectives were proposed, which degraded the input image in various ways similar to masking. Design principles are proposed for  token-based pre-training of vision transformers. The most effective strategy combined preserving the original image style with spatial misalignment and masking. This approach outperformed traditional MIM on downstream recognition tasks without increasing computational demands."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. In this paper Masked Image Residual Learning (MIRL), a self-supervised learning framework, was introduced to alleviate the degradation issue and enable effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Extensive testing shows that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "This paper proposes Hard Patches Mining (HPM), a novel framework for masked image modeling (MIM) pre-training that goes beyond simply solving given problems. HPM aims for the model to generate more challenging tasks for itself, using reconstruction loss as a metric for task difficulty. It incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "working": [
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. The Authors show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements."
    },
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "This paper introduces SlowFast networks, a new convolutional architecture for video recognition tasks. These networks have two pathways: a Slow pathway operating at low frame rates to capture spatial semantics, and a Fast pathway working at high frame rates with reduced channel capacity to efficiently capture motion details. The SlowFast models achieve top accuracy on major video benchmarks like Kinetics, Charades, and AVA, significantly improving performance for action classification and detection tasks."
    }
  ],
  "unclear": [
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. The Authors show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements."
    },
    {
      "title": "How Does Sharpness-Aware Minimization Minimize Sharpness?",
      "url": "/research/2023/01/05/How-Does-Sharpness-Aware-Minimization-Minimize-Sharpness.html",
      "date": "January 5, 2023",
      "summary": "Sharpness-Aware Minimization (SAM) enhances deep neural networks' generalization across various settings. Although SAM aims to penalize model sharpness using a computationally efficient approach, its exact operational mechanism and the sharpness notion it regularizes remain unclear, partly due to differing sharpness concepts used in its theoretical framework and empirical validation. This study identifies the precise sharpness concept SAM regulates and explains its mechanism. It reveals that the combined effect of SAM's two-step approximations, despite being individually misleading, correctly improves generalization when using full-batch gradients. Moreover, we demonstrate that the stochastic SAM version indeed regularizes a third sharpness notion, aligning more closely with practical performance. This effectiveness is attributed to the gradient's alignment with the Hessian's top eigenvector under SAM."
    }
  ],
  "perspective": [
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. The Authors show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) uses synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. Given this perspective, a one-instance fractal database (OFDB) is developed where only a single image per category is present. Despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M comparable or superior results are obtaine to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning."
    },
    {
      "title": "Model Generalization: A Sharpness Aware Optimization Perspective",
      "url": "/research/2022/08/14/Model-Generalization-A-Sharpness-Aware-Optimization-Perspective.html",
      "date": "August 14, 2022",
      "summary": "This study investigates the effectiveness of Sharpness-Aware Minimization (SAM) and adaptive Sharpness-Aware Minimization (ASAM) in enhancing model generalization. Through three experiments, we assess their impact from a sharpness-aware perspective. Results demonstrate that optimization techniques based on sharpness awareness can bolster model generalization. Furthermore, ASAM exhibits potential for enhancing generalization performance on un-normalized data, though additional research is required for validation."
    },
    {
      "title": "Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes",
      "url": "/research/2017/11/28/Towards-Understanding-Generalization-of-Deep-Learning-Perspective-of-Loss-Landscapes.html",
      "date": "November 28, 2017",
      "summary": "This study examines why deep learning models, despite having more parameters than training samples, generalize well. It highlights the importance of the loss function's landscape, showing that areas leading to better generalization (good minima) are more prevalent than those leading to poor outcomes. This predominance facilitates the convergence of optimization methods to these beneficial minima. The research provides theoretical backing by analyzing 2-layer neural networks, noting that solutions with better generalization have a smaller Hessian matrix norm. For deeper networks, extensive numerical data supports these conclusions."
    }
  ],
  "differ": [
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. The Authors show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "This study challenges the traditional understanding of how Convolutional Neural Networks (CNNs) recognize objects, revealing that they are more biased towards recognizing textures rather than learning complex shapes. The authors found that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, training a ResNet-50 on a stylized version of ImageNet, designed to emphasize shape, aligned the network's performance more closely with human behavior. This shape-based training matched human performance in controlled experiments and enhanced object detection and robustness against image distortions, highlighting the benefits of shape-based representations in visual recognition systems."
    }
  ],
  "measurement": [
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. The Authors show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements."
    }
  ],
  "difficulty": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "This paper introduces Xavier Glorot init for Sigmoid like activation functions. The study investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. A ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training. Synthetic images generated by FDSL circumvent issues associated with real images, offering a promising avenue for pre-training general models. Investigations suggest that simple object contours can match fractal-based dataset performance and increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "This paper proposes Hard Patches Mining (HPM), a novel framework for masked image modeling (MIM) pre-training that goes beyond simply solving given problems. HPM aims for the model to generate more challenging tasks for itself, using reconstruction loss as a metric for task difficulty. It incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "Curriculum Learning by Dynamic Instance Hardness",
      "url": "/research/2020/12/06/Curriculum-Learning-by-Dynamic-Instance-Hardness.html",
      "date": "December 6, 2020",
      "summary": "This paper improves curriculum learning via introduces Dynamic Instance Hardness (DIH), a method that measures a sample's learning difficulty over time. DIH provides a stable indicator of learning progress by tracking the exponential moving average of a sample's hardness. DIHCL enhances learning efficiency and model accuracy without extra computational costs by leveraging data from the training process itself. Tested on 11 datasets, DIHCL surpasses traditional training methods and recent curriculum learning techniques in both efficiency and effectiveness."
    },
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "This paper introduces a novel approach called mixed-order self-paced curriculum learning (Mo-SCL) to address the challenges faced by self-paced curriculum learning (SCL) in medical image analysis tasks, such as universal lesion detection. These challenges include inaccurate difficulty estimation and the under-utilization of hard samples. Mo-SCL combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches. Through theoretical analysis and experiments on the DeepLesion dataset, the authors demonstrate that Mo-SCL enhances lesion detection accuracy in state-of-the-art methods without requiring additional network modifications."
    },
    {
      "title": "Taming the Waves: Sine as Activation Function in Deep Neural Networks",
      "url": "/research/2016/11/04/Taming-the-Waves-Sine-as-Activation-Function-in-Deep-Neural-Networks.html",
      "date": "November 4, 2016",
      "summary": "This paper explores the challenges in training deep neural networks that use sinusoidal activation functions. The authors explain that the difficulty arises from the emergence of numerous shallow local minima in the loss landscape. The study reveals that successful learning in typical classification tasks occurs when the network effectively ignores the periodic cycles of the sinusoidal functions. However, for certain non-trivial tasks, networks with sinusoidal activations can outperform those using traditional monotonic activation functions."
    },
    {
      "title": "DeepNet: Scaling Transformers to 1,000 Layers",
      "url": "/research/2022/03/01/DeepNet-Scaling-Transformers-to-1,000-Layers.html",
      "date": "March 1, 2022",
      "summary": "This paper introduces DEEPNORM, a novel method to stabilize extremely deep Transformers through a new normalization function and a theoretically derived initialization for the residual connection. DEEPNORM offers a stable and efficient training regime, blending the benefits of Post-LN's performance with Pre-LN's stability, allowing for the scaling of Transformers up to 1,000 layers without difficulty. This represents a significant advancement over previous models, with a 200-layer, 3.2B parameter model outperforming a 48-layer, 12B parameter state-of-the-art model by 5 BLEU points on a multilingual benchmark with 7,482 translation directions, suggesting a promising direction for scaling."
    },
    {
      "title": "Learning and Leveraging World Models in Visual Representation Learning",
      "url": "/research/2024/03/01/Learning-and-Leveraging-World-Models-in-Visual-Representation-Learning.html",
      "date": "March 1, 2024",
      "summary": "The Joint-Embedding Predictive Architecture (JEPA) is a self-supervised method previously used for predicting missing input parts. This study expands JEPA to predict a wider range of corruptions by introducing Image World Models (IWM), which learn the effects of global photometric transformations in latent space. Key to effective IWM learning are conditioning, prediction difficulty, and model capacity. The study demonstrates that IWM's predictive world model, when fine-tuned, can tackle diverse tasks and either matches or outperforms existing self-supervised techniques. Additionally, it enables control over the abstraction level of learned representations, achieving either invariant or equivariant representations, similar to contrastive methods or masked image modeling, respectively."
    }
  ],
  "xavier": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "This paper introduces Xavier Glorot init for Sigmoid like activation functions. The study investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial."
    }
  ],
  "glorot": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "This paper introduces Xavier Glorot init for Sigmoid like activation functions. The study investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial."
    }
  ],
  "saturation": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "This paper introduces Xavier Glorot init for Sigmoid like activation functions. The study investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial."
    }
  ],
  "hide": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "This paper introduces Xavier Glorot init for Sigmoid like activation functions. The study investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "This paper introduces Online Normalization, a novel method for normalizing neural network hidden activations that offers a batch-independent alternative with comparable accuracy to Batch Normalization. Online Normalization addresses Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. The method is applicable to recurrent, fully connected networks, and those with high activation memory requirements. The authors demonstrate its effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    }
  ],
  "discover": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "This paper introduces Xavier Glorot init for Sigmoid like activation functions. The study investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial."
    },
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "This paper introduces an algorithm that identifies simple and highly generalizable neural networks by searching for extensive flat minima regions in the error function, where the error rate is relatively stable. Such flat minima are associated with lower overfitting risks based on minimum description length (MDL) principles. Despite requiring second-order derivative calculations, the algorithm has a complexity level comparable to backpropagation. When tested on feedforward and recurrent networks, as well as stock market prediction tasks, this algorithm outperformed traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    },
    {
      "title": "Searching for Activation Functions",
      "url": "/research/2017/10/27/Searching-for-Activation-Functions.html",
      "date": "October 27, 2017",
      "summary": "This study explores the impact of activation functions on deep network training and performance. While Rectified Linear Unit (ReLU) is widely used, alternatives have not consistently outperformed it. We propose using automatic search techniques to discover new activation functions. Through exhaustive and reinforcement learning-based searches, we identify novel functions. Empirical evaluation shows that our best discovered function, Swish (f(x) = x · sigmoid(βx)), performs better than ReLU on deeper models across challenging datasets. Replacing ReLUs with Swish units improves classification accuracy on ImageNet, for example, by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. Swish's simplicity and similarity to ReLU facilitate its adoption in neural networks."
    },
    {
      "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
      "url": "/research/2021/07/24/The-Many-Faces-of-Robustness-A-Critical-Analysis-of-Out-of-Distribution-Generalization.html",
      "date": "July 24, 2021",
      "summary": "We present four novel real-world distribution shift datasets encompassing changes in image style, blurriness, location, camera settings, and more. Evaluating existing methods for enhancing out-of-distribution robustness, we discover that employing larger models and artificial data augmentations can enhance robustness against real-world distribution shifts, contradicting prior claims. Our findings demonstrate that improvements in artificial robustness benchmarks can indeed transfer to real-world distribution shifts, contrary to prior assumptions. Additionally, we introduce a novel data augmentation technique that surpasses models pretrained with significantly more labeled data, emphasizing its efficacy in addressing real-world distribution shifts. While some methods consistently mitigate texture and local image statistics shifts, they fail to address other shifts like geographic changes. Our results underscore the necessity for future research to examine multiple distribution shifts concurrently, as no single method consistently improves robustness across all evaluated scenarios."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "url": "/research/2014/02/19/Exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks.html",
      "date": "February 19, 2014",
      "summary": "This study aims to enhance our comprehension of deep learning dynamics by analyzing deep linear neural networks. Despite their linear input-output mapping, these networks display nonlinear gradient descent dynamics, resulting in phenomena like prolonged plateaus and swift transitions to improved solutions. Through analytical exploration, we reveal that as network depth increases indefinitely, learning speed can remain finite under specific initial conditions. Unsupervised pretraining, under certain data conditions, can discover these favorable initial conditions, unlike random Gaussian initializations. We introduce a novel class of random orthogonal initial conditions that, akin to unsupervised pretraining, enable depth-independent learning. These conditions also ensure gradient propagation in deep nonlinear networks, particularly when operating at the edge of chaos."
    },
    {
      "title": "Do Vision Transformers See Like Convolutional Neural Networks?",
      "url": "/research/2022/03/03/Do-Vision-Transformers-See-Like-Convolutional-Neural-Networks.html",
      "date": "March 3, 2022",
      "summary": "Convolutional Neural Networks (CNNs) have been prominent in processing visual data, but recent studies show Vision Transformers (ViTs) can match or outperform CNNs in image classification. This work examines how ViTs solve classification tasks, discovering significant differences from CNNs, including uniformity in ViT representations across layers due to self-attention and residual connections that enhance feature propagation. ViTs also maintain spatial information effectively, influenced by classification methods. The study further explores how dataset scale affects ViT features and their transferability, linking these findings to new architectures like MLP-Mixer."
    },
    {
      "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
      "url": "/research/2023/11/30/Dichotomy-of-Early-and-Late-Phase-Implicit-Biases-Can-Provably-Induce-Grokking.html",
      "date": "November 30, 2023",
      "summary": "Power et al. (2022) discovered that neural networks initially memorize arithmetic tasks, achieving perfect training accuracy but poor test accuracy, then suddenly achieve perfect test accuracy after extended training. This study explains this grokking phenomenon with the theory that early and late phase implicit biases cause this shift. Training homogeneous neural networks with large initialization and small weight decay on classification and regression tasks results in a prolonged period where the network acts like a kernel predictor, followed by a sudden shift to min-norm/max-margin predictors, significantly improving test accuracy."
    }
  ],
  "desaturate": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "This paper introduces Xavier Glorot init for Sigmoid like activation functions. The study investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial."
    }
  ],
  "explains": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "This paper introduces Xavier Glorot init for Sigmoid like activation functions. The study investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial."
    }
  ],
  "plateaus": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "This paper introduces Xavier Glorot init for Sigmoid like activation functions. The study investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "url": "/research/2014/02/19/Exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks.html",
      "date": "February 19, 2014",
      "summary": "This study aims to enhance our comprehension of deep learning dynamics by analyzing deep linear neural networks. Despite their linear input-output mapping, these networks display nonlinear gradient descent dynamics, resulting in phenomena like prolonged plateaus and swift transitions to improved solutions. Through analytical exploration, we reveal that as network depth increases indefinitely, learning speed can remain finite under specific initial conditions. Unsupervised pretraining, under certain data conditions, can discover these favorable initial conditions, unlike random Gaussian initializations. We introduce a novel class of random orthogonal initial conditions that, akin to unsupervised pretraining, enable depth-independent learning. These conditions also ensure gradient propagation in deep nonlinear networks, particularly when operating at the edge of chaos."
    }
  ],
  "linearity": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "This paper introduces Xavier Glorot init for Sigmoid like activation functions. The study investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial."
    }
  ],
  "saturate": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "This paper introduces Xavier Glorot init for Sigmoid like activation functions. The study investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial."
    }
  ],
  "what": [
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum",
      "url": "/research/2023/09/30/When-to-Learn-What:-Model-Adaptive-Data-Augmentation-Curriculum.html",
      "date": "September 30, 2023",
      "summary": "Data augmentation (DA) improves neural network generalization by enforcing invariances and symmetries to predefined transformations applied to input data. However, fixed augmentation policies affect samples differently at various training stages, and existing approaches cannot adapt policies to individual samples and the training model. This paper proposes Model-Adaptive Data Augmentation (MADAug) which trains an augmentation policy network to determine when to learn what. Unlike previous work, MADAug selects augmentation operators for each input image with a model-adaptive policy that varies between training stages, creating an optimized data augmentation curriculum. The policy is trained using a bi-level optimization scheme to minimize validation-set loss of a model trained with policy-produced augmentations. Extensive evaluations on multiple image classification tasks and network architectures show that MADAug outperforms or matches existing DA approaches, enhances fairness by improving all classes, particularly the difficult ones, and performs better when transferred to fine-grained datasets. Additionally, the auto-optimized policy in MADAug gradually increases perturbations, forming an easy-to-hard curriculum."
    }
  ],
  "cl": [
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "Self-Paced Curriculum Learning",
      "url": "/research/2015/01/01/Self-Paced-Curriculum-Learning.html",
      "date": "January 1, 2015",
      "summary": "This paper introduces self-paced curriculum learning (SPCL), a unified framework combining curriculum learning (CL) and self-paced learning (SPL). SPCL leverages prior knowledge and ongoing learning progress through an optimization problem. It mimics collaborative instructor-student learning, exhibiting empirical advantages in two tasks."
    },
    {
      "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
      "url": "/research/2024/02/28/CL-MAE:-Curriculum-Learned-Masked-Autoencoders.html",
      "date": "February 28, 2024",
      "summary": "This paper proposes a curriculum learning approach that updates the masking strategy of Masked Auto Encoders (MAE) to progressively increase the complexity of the self-supervised reconstruction task. To achieve this, a novel learnable masking module is introduced, capable of generating masks of varying complexities, and integrated into masked autoencoders (MAE). This module is jointly trained with the MAE, adjusting its behavior from partner (optimizing the same reconstruction loss) to adversary (optimizing the opposite loss), with a smooth transition regulated by a factor multiplied with the reconstruction loss. This training procedure creates an easy-to-hard curriculum. The Curriculum-Learned Masked Autoencoder (CL-MAE) is trained on ImageNet and demonstrates superior representation learning compared to MAE. Empirical results on five downstream tasks confirm that curriculum learning can successfully self-supervise masked autoencoders."
    },
    {
      "title": "From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning",
      "url": "/research/2023/03/16/From-MNIST-to-ImageNet-and-Back:-Benchmarking-Continual-Curriculum-Learning.html",
      "date": "March 16, 2023",
      "summary": "Continual learning (CL) is a promising trend in machine learning, aiming to develop robust models and strategies for dynamic environments by incorporating new knowledge while retaining past knowledge. CL research is fragmented, with various protocols, tasks, datasets, and metrics, often not reflecting real-world complexity and tailored to specific strategies. This work addresses this gap by introducing two novel CL benchmarks with heterogeneous tasks from six image datasets, varying in complexity and quality. These benchmarks fairly evaluate state-of-the-art CL strategies in scenarios closer to real-world conditions. The benchmarks present tasks in both increasing and decreasing complexity, assessing models' ability to exploit task structure. The work emphasizes a rigorous, reproducible evaluation protocol for measuring generalization and memory retention in models. Experimental results show that popular CL strategies perform poorly on these benchmarks, exhibit high forgetting, and struggle with curriculum task ordering. These findings underscore the need for rigorous comparisons and the development of new CL strategies for complex scenarios."
    },
    {
      "title": "Non-negative Contrastive Learning",
      "url": "/research/2024/03/19/Non-negative-Contrastive-Learning.html",
      "date": "March 19, 2024",
      "summary": "Deep representations offer promising performance for downstream tasks but lack interpretability, posing a significant challenge. In this paper, we introduce Non-negative Contrastive Learning (NCL), a refinement of Non-negative Matrix Factorization (NMF) aimed at generating interpretable features. NCL enforces non-negativity constraints, akin to NMF, resulting in sparse and disentangled representations, unlike standard contrastive learning (CL). We establish theoretical guarantees on NCL's identifiability and downstream generalization. Empirically, NCL outperforms CL in feature disentanglement, selection, and downstream classification tasks. Moreover, NCL can be extended to other learning scenarios, benefiting supervised learning."
    }
  ],
  "downstream": [
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study looks at self-supervised pre-training methods on small scale or lightweight Vision Transformers (ViTs). Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods."
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM) for self-supervised visual representation in vision transformers ViT. Five different learning objectives were proposed, which degraded the input image in various ways similar to masking. Design principles are proposed for  token-based pre-training of vision transformers. The most effective strategy combined preserving the original image style with spatial misalignment and masking. This approach outperformed traditional MIM on downstream recognition tasks without increasing computational demands."
    },
    {
      "title": "CLIP: Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns from raw text descriptions of images, moving beyond training on fixed object categories. By pre-training on 400 million (image, text) pairs using caption-image matching, the method achieves state-of-the-art image representations from scratch. This allows for zero-shot transfer to various downstream tasks using natural language, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse datasets, showing competitive results against fully supervised baselines without task-specific training, and matching the accuracy of ResNet-50 on ImageNet zero-shot without using its training examples."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. MAE involves masking random patches of an input image and reconstructing the missing pixels using an asymmetric encoder-decoder architecture and a lightweight decoder. Masking a significant portion of the input image creates a challenging yet informative self-supervisory task. These innovations allow for efficient training of large models, tripling training speed and enhancing accuracy. A vanilla ViT-Huge model reaches top accuracy (87.8%) on ImageNet-1K data among similar methods. The model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating promising scaling potential."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) uses synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. Given this perspective, a one-instance fractal database (OFDB) is developed where only a single image per category is present. Despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M comparable or superior results are obtaine to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "This paper introduces Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperformed CLIP models on various downstream tasks. The speedup from this method enables experimentation with larger model sizes, more data, or longer training periods."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "This paper investigates the geometric and statistical properties of representations across layers in large transformers used for self-supervised learning on various data types. The authors observe common evolution patterns, with data manifolds expanding initially, contracting at intermediate layers, and the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. They find that semantic information peaks after initial expansion, a trend consistent across models and datasets. The study proposes an unsupervised method to identify layers richest in semantic content, suggesting that those at a relative ID minimum are optimal for downstream tasks."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces TinyViT, a series of compact and efficient vision transformers (ViTs) designed for devices with limited resources. The authors employ a fast distillation framework to transfer knowledge from large pretrained models to smaller ones during pretraining, using sparsified logits from teacher models to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. With increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters, and demonstrates strong transferability across various downstream tasks."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This paper is effectively a follow-up to DinoV2. The study addresses the issue of artifacts in feature maps of Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. The authors introduce a straightforward strategy of adding extra tokens to the input sequence of the Vision Transformer, which eliminates these artifacts for both supervised and self-supervised models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks, facilitates object discovery with larger models, and results in smoother feature and attention maps for downstream visual processing tasks."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "The paper investigates the use of synthetic data generated by graphics simulators for pre-training computer vision models. The authors find that model performance on downstream tasks varies with different simulation parameters used to generate the synthetic data. They introduce Task2Sim, a model that maps the requirements of a downstream task to the optimal simulation parameters for generating synthetic pre-training data."
    },
    {
      "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
      "url": "/research/2024/02/28/CL-MAE:-Curriculum-Learned-Masked-Autoencoders.html",
      "date": "February 28, 2024",
      "summary": "This paper proposes a curriculum learning approach that updates the masking strategy of Masked Auto Encoders (MAE) to progressively increase the complexity of the self-supervised reconstruction task. To achieve this, a novel learnable masking module is introduced, capable of generating masks of varying complexities, and integrated into masked autoencoders (MAE). This module is jointly trained with the MAE, adjusting its behavior from partner (optimizing the same reconstruction loss) to adversary (optimizing the opposite loss), with a smooth transition regulated by a factor multiplied with the reconstruction loss. This training procedure creates an easy-to-hard curriculum. The Curriculum-Learned Masked Autoencoder (CL-MAE) is trained on ImageNet and demonstrates superior representation learning compared to MAE. Empirical results on five downstream tasks confirm that curriculum learning can successfully self-supervise masked autoencoders."
    },
    {
      "title": "Non-negative Contrastive Learning",
      "url": "/research/2024/03/19/Non-negative-Contrastive-Learning.html",
      "date": "March 19, 2024",
      "summary": "Deep representations offer promising performance for downstream tasks but lack interpretability, posing a significant challenge. In this paper, we introduce Non-negative Contrastive Learning (NCL), a refinement of Non-negative Matrix Factorization (NMF) aimed at generating interpretable features. NCL enforces non-negativity constraints, akin to NMF, resulting in sparse and disentangled representations, unlike standard contrastive learning (CL). We establish theoretical guarantees on NCL's identifiability and downstream generalization. Empirically, NCL outperforms CL in feature disentanglement, selection, and downstream classification tasks. Moreover, NCL can be extended to other learning scenarios, benefiting supervised learning."
    }
  ],
  "pattern": [
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "This paper investigates the geometric and statistical properties of representations across layers in large transformers used for self-supervised learning on various data types. The authors observe common evolution patterns, with data manifolds expanding initially, contracting at intermediate layers, and the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. They find that semantic information peaks after initial expansion, a trend consistent across models and datasets. The study proposes an unsupervised method to identify layers richest in semantic content, suggesting that those at a relative ID minimum are optimal for downstream tasks."
    },
    {
      "title": "Are Convolutional Neural Networks or Transformers more like human vision?",
      "url": "/research/2021/07/01/Are-Convolutional-Neural-Networks-or-Transformers-more-like-human-vision.html",
      "date": "July 1, 2021",
      "summary": "This study investigates the performance of Convolutional Neural Networks (CNNs) and attention-based Vision Transformers (ViTs) in computer vision tasks. While CNNs excel in accuracy, ViTs offer a different approach with weaker inductive biases. By analyzing error patterns, we find that ViTs exhibit consistency with human errors, suggesting potential for more human-like vision models and insights into human object recognition."
    },
    {
      "title": "Neural Networks Learn Statistics of Increasing Complexity",
      "url": "/research/2024/02/13/Neural-Networks-Learn-Statistics-of-Increasing-Complexity.html",
      "date": "February 13, 2024",
      "summary": "The distributional simplicity bias (DSB) theory suggests neural networks first learn basic patterns in data before understanding more complex correlations. We provide new evidence supporting DSB, showing networks initially excel with data matching training set's simple statistics but this ability diminishes later. Extending DSB to discrete domains, we demonstrate an equivalence between n-gram frequencies and vector moments, also observing this bias in large language models (LLMs). Additionally, by adjusting low-level statistics of images to resemble another class, we reveal that networks in early training phases misclassify these edited images as if they belonged to the target class."
    }
  ],
  "orient": [
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets for formula-driven supervised learning (FDSL). The authors identify the optimal FDSL parameters and maximize synthetic image variety, which is crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. The authors demonstrate FDSL's potential for continuous improvement and its ability to avoid issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "separation": [
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    }
  ],
  "frequency": [
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Incorporating diverse image styles into CNN training data reduces texture bias, enhances shape recognition, and improves resilience against common image corruptions. This is typically explained as decreasing model reliance on high frequency texture information. This paper challenges this explaniation through large scale testing with natural images, edge information, and stylization, finding no direct link between shape bias and robustness. The enhanced corruption robustness is instead attributed to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Neural Networks Learn Statistics of Increasing Complexity",
      "url": "/research/2024/02/13/Neural-Networks-Learn-Statistics-of-Increasing-Complexity.html",
      "date": "February 13, 2024",
      "summary": "The distributional simplicity bias (DSB) theory suggests neural networks first learn basic patterns in data before understanding more complex correlations. We provide new evidence supporting DSB, showing networks initially excel with data matching training set's simple statistics but this ability diminishes later. Extending DSB to discrete domains, we demonstrate an equivalence between n-gram frequencies and vector moments, also observing this bias in large language models (LLMs). Additionally, by adjusting low-level statistics of images to resemble another class, we reveal that networks in early training phases misclassify these edited images as if they belonged to the target class."
    }
  ],
  "harmonize": [
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    }
  ],
  "pace": [
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This study suggest that label smoothing not only boosts generalization but also enhances model calibration. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. Label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "Self-Paced Curriculum Learning",
      "url": "/research/2015/01/01/Self-Paced-Curriculum-Learning.html",
      "date": "January 1, 2015",
      "summary": "This paper introduces self-paced curriculum learning (SPCL), a unified framework combining curriculum learning (CL) and self-paced learning (SPL). SPCL leverages prior knowledge and ongoing learning progress through an optimization problem. It mimics collaborative instructor-student learning, exhibiting empirical advantages in two tasks."
    },
    {
      "title": "Self-Paced Absolute Learning Progress as a Regularized Approach to Curriculum Learning",
      "url": "/research/2023/06/09/Self-Paced-Absolute-Learning-Progress-as-a-Regularized-Approach-to-Curriculum-Learning.html",
      "date": "June 9, 2023",
      "summary": "Reinforcement Learning's usability is limited by its high computation times. Curriculum Reinforcement Learning accelerates learning by ordering tasks from simple to hard. Curricula based on Absolute Learning Progress (ALP) have shown success in various environments but waste computation on redundant tasks. This issue is addressed by introducing Self-Paced Absolute Learning Progress (SPALP), a regularization method inspired by Self-Paced Learning. Evaluated in three environments, SPALP achieves performance comparable to ALP in all cases and reaches it faster in two. Further improvements in SPALP's efficiency and performance are also discussed."
    }
  ],
  "multiclass": [
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This study suggest that label smoothing not only boosts generalization but also enhances model calibration. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. Label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    }
  ],
  "clustering": [
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This study suggest that label smoothing not only boosts generalization but also enhances model calibration. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. Label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    }
  ],
  "penultimate": [
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This study suggest that label smoothing not only boosts generalization but also enhances model calibration. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. Label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    }
  ],
  "resemblance": [
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This study suggest that label smoothing not only boosts generalization but also enhances model calibration. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. Label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    }
  ],
  "box": [
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets transformers as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "The paper introduces a system that trains deep neural networks for object detection using only synthetic images generated with domain randomization. This involves randomizing non-realistic simulator parameters like lighting and object textures, allowing the network to identify essential object features. Their findings show that networks can achieve impressive performance using just synthetic data, and further improve with fine-tuning on real data. This suggests the potential of using cost-effective synthetic data for training, avoiding the need to acquire vast amounts of real-world data or create detailed synthetic environments. They validate their method on car bounding box detection using the KITTI dataset."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    },
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    }
  ],
  "compress": [
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets transformers as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet."
    }
  ],
  "gaussian": [
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets transformers as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. This encoder serves as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The DMAE ViT-Base model achieves comparable or superior certified accuracy with fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on ImageNet. The model also shows high transferability to CIFAR-10, indicating its broad applicability."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "This paper introduces a novel approach to Radiance Field methods for novel-view synthesis that addresses the challenges of high visual quality, costly training, and real-time rendering of unbounded, complete scenes at 1080p resolution. The authors propose three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. The method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    },
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "This paper introduces the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. The authors' empirical evaluation across computer vision, natural language processing, and speech tasks demonstrates that GELU offers performance improvements over ReLU and ELU activations."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "This paper demonstrates that artificial neural networks (ANNs) are equivalent to Gaussian processes at initialization in the infinite-width limit and introduces the Neural Tangent Kernel (NTK), which describes ANNs' behavior during training. The NTK stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. The authors prove the positive-definiteness of the limiting NTK under certain conditions and show that the network function follows a linear differential equation during training for least-squares regression. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    },
    {
      "title": "Automated Learning Rate Scheduler for Large-batch",
      "url": "/research/2021/07/13/Automated-Learning-Rate-Scheduler-for-Large-batch.html",
      "date": "July 13, 2021",
      "summary": "Large-batch training is crucial for deep learning with large datasets and models, but requires specific learning rate (LR) schedules for optimal performance, especially under limited training epochs. This work introduces an automated LR scheduling algorithm for large-batch neural network training within a fixed epoch budget, consisting of adaptive warmup and predefined decay phases. The LR is dynamically adjusted based on training loss monitored through Gaussian process smoothing, facilitating low computational overhead. When integrated with adaptive stochastic optimizers like AdamP and LAMB, this scheduler eliminates the need for extensive hyperparameter tuning and achieves competitive or superior results on various image classification tasks across different batch sizes and architectures."
    },
    {
      "title": "Optimal learning rate schedules in high-dimensional non-convex optimization problems",
      "url": "/research/2022/02/09/Optimal-learning-rate-schedules-in-high-dimensional-non-convex-optimization-problems.html",
      "date": "February 9, 2022",
      "summary": "This paper explores the role of learning rate schedules in high-dimensional and non-convex optimization problems, focusing on Langevin optimization with a decaying learning rate. Analyzing models with Gaussian random functions on N-dimensional spheres, the study reveals that to accelerate optimization without getting trapped in saddles, a decay rate β < 1 is optimal, contrary to convex settings where β = 1 is preferred. Introducing a signal recovery component, the dynamics involve an exploration phase navigating through rough landscape parts and a convergence phase entering convex basins. It's found optimal to maintain a large learning rate during exploration to swiftly exit non-convex regions, then transition to β = 1 for rapid convergence to the solution. These findings are validated in a neural network regression task."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "url": "/research/2014/02/19/Exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks.html",
      "date": "February 19, 2014",
      "summary": "This study aims to enhance our comprehension of deep learning dynamics by analyzing deep linear neural networks. Despite their linear input-output mapping, these networks display nonlinear gradient descent dynamics, resulting in phenomena like prolonged plateaus and swift transitions to improved solutions. Through analytical exploration, we reveal that as network depth increases indefinitely, learning speed can remain finite under specific initial conditions. Unsupervised pretraining, under certain data conditions, can discover these favorable initial conditions, unlike random Gaussian initializations. We introduce a novel class of random orthogonal initial conditions that, akin to unsupervised pretraining, enable depth-independent learning. These conditions also ensure gradient propagation in deep nonlinear networks, particularly when operating at the edge of chaos."
    },
    {
      "title": "Chronos: Learning the Language of Time Series",
      "url": "/research/2024/03/12/Chronos-Learning-the-Language-of-Time-Series.html",
      "date": "March 12, 2024",
      "summary": "Chronos is a framework that enhances pretrained probabilistic time series models by tokenizing time series data for training with transformer-based architectures, notably the T5 family. Pretrained on a mix of public and synthetic datasets created via Gaussian processes, Chronos models outshine others in 42 benchmark datasets by showing superior performance on familiar datasets and competitive or better zero-shot capabilities on new datasets. This illustrates Chronos's ability to generalize across different domains, offering a simpler approach to forecasting tasks."
    }
  ],
  "optimizer": [
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets transformers as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet."
    },
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "Automated Learning Rate Scheduler for Large-batch",
      "url": "/research/2021/07/13/Automated-Learning-Rate-Scheduler-for-Large-batch.html",
      "date": "July 13, 2021",
      "summary": "Large-batch training is crucial for deep learning with large datasets and models, but requires specific learning rate (LR) schedules for optimal performance, especially under limited training epochs. This work introduces an automated LR scheduling algorithm for large-batch neural network training within a fixed epoch budget, consisting of adaptive warmup and predefined decay phases. The LR is dynamically adjusted based on training loss monitored through Gaussian process smoothing, facilitating low computational overhead. When integrated with adaptive stochastic optimizers like AdamP and LAMB, this scheduler eliminates the need for extensive hyperparameter tuning and achieves competitive or superior results on various image classification tasks across different batch sizes and architectures."
    },
    {
      "title": "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization",
      "url": "/research/2023/07/04/Gradient-Norm-Aware-Minimization-Seeks-First-Order-Flatness-and-Improves-Generalization.html",
      "date": "July 4, 2023",
      "summary": "Recent advancements have highlighted the effectiveness of flat minima in enhancing generalization, particularly through Sharpness-Aware Minimization (SAM). However, existing definitions of flatness, such as zeroth-order flatness, have limitations in discerning between minima with low and high generalization errors. To address this, we propose first-order flatness, which considers maximal gradient norm within a perturbation radius. We introduce Gradient norm Aware Minimization (GAM) as a novel training approach to achieve uniformly small curvature across all directions. Experimental results demonstrate GAM's ability to enhance generalization compared to standard optimizers like SGD and AdamW across various datasets and networks. Moreover, GAM facilitates SAM in identifying flatter minima, leading to improved generalization."
    },
    {
      "title": "Penalizing Gradient Norm for Efficiently Improving Generalization in Deep Learning",
      "url": "/research/2022/06/26/Penalizing-Gradient-Norm-for-Efficiently-Improving-Generalization-in-Deep-Learning.html",
      "date": "June 26, 2022",
      "summary": "This paper proposes a method to enhance the generalization of deep neural networks (DNNs) by penalizing the gradient norm of the loss function during optimization. By constraining the gradient norm, the optimizers tend to find flat minima, improving generalization. We efficiently implement this method using first-order approximation within the gradient descent framework. Experimental results demonstrate improved generalization across various models and datasets. Additionally, we show that a recent method, sharpness-aware minimization, is a special case of our approach, with our method achieving new state-of-the-art performance on tested tasks."
    },
    {
      "title": "Sharpness-Aware Training for Free",
      "url": "/research/2023/03/02/Sharpness-Aware-Training-for-Free.html",
      "date": "March 2, 2023",
      "summary": "Modern deep neural networks (DNNs) excel in performance but often suffer from over-parameterization, leading to increased generalization error without tailored training strategies. Sharpness-Aware Minimization (SAM) has proven effective in reducing generalization error by minimizing sharpness in the loss landscape. However, SAM incurs a significant computational overhead. This paper introduces Sharpness-Aware Training for Free (SAF), which mitigates sharpness at nearly zero additional computational cost. SAF achieves this by preventing sudden drops in loss within sharp local minima during weight updates. A novel trajectory loss, based on KL-divergence between current and past DNN outputs, replaces SAM's sharpness measure, guiding convergence towards flat minima for enhanced generalization. Empirical results demonstrate SAF's effectiveness on ImageNet with comparable computational efficiency to the base optimizer."
    },
    {
      "title": "Symbolic Discovery of Optimization Algorithms",
      "url": "/research/2023/05/08/Symbolic-Discovery-of-Optimization-Algorithms.html",
      "date": "May 8, 2023",
      "summary": "We propose a method for algorithm discovery via program search, focusing on optimizing deep neural network training. Our approach, Lion (EvoLved Sign Momentum), is memory-efficient and achieves comparable or superior performance to widely-used optimizers such as Adam and Adafactor across various tasks. Specifically, Lion enhances accuracy on tasks like image classification and vision-language contrastive learning, while reducing training compute. Notably, Lion exhibits improved performance with larger batch sizes and requires smaller learning rates compared to Adam. Despite its effectiveness, Lion has limitations, which we analyze, and provide insights into its deployment and performance. Our implementation is publicly available and has been successfully utilized in Google's search ads CTR model."
    },
    {
      "title": "AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks",
      "url": "/research/2023/03/01/AdaSAM-Boosting-Sharpness-Aware-Minimization-with-Adaptive-Learning-Rate-and-Momentum-for-Training-Deep-Neural-Networks.html",
      "date": "March 1, 2023",
      "summary": "The Sharpness Aware Minimization (SAM) optimizer, known for enhancing the generalization of deep neural networks by introducing extra perturbation steps, is further developed into AdaSAM by integrating adaptive learning rates and momentum acceleration. Although AdaSAM has been empirically applied to large-scale networks, a theoretical understanding of its performance, considering the complexity of its components, was lacking. This study presents a theoretical analysis of AdaSAM's convergence in stochastic non-convex settings, demonstrating a convergence rate of O(1/√bT) that scales linearly with mini-batch size. By introducing a delayed second-order momentum term, the study successfully decouples and analyzes the intertwined effects of stochastic gradients, adaptive learning rates, and perturbations. This is the first work to offer a detailed convergence rate for SAM with adaptive mechanisms. Experimental results on various NLP tasks indicate AdaSAM's superior performance over SGD, AMSGrad, and SAM optimizers."
    },
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    },
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    }
  ],
  "alternate": [
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets transformers as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet."
    }
  ],
  "sparsify": [
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets transformers as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet."
    }
  ],
  "transformerlike": [
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets transformers as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet."
    }
  ],
  "compres": [
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets transformers as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet."
    }
  ],
  "look": [
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study looks at self-supervised pre-training methods on small scale or lightweight Vision Transformers (ViTs). Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods."
    }
  ],
  "sota": [
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study looks at self-supervised pre-training methods on small scale or lightweight Vision Transformers (ViTs). Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods."
    }
  ],
  "map": [
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study looks at self-supervised pre-training methods on small scale or lightweight Vision Transformers (ViTs). Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This paper is effectively a follow-up to DinoV2. The study addresses the issue of artifacts in feature maps of Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. The authors introduce a straightforward strategy of adding extra tokens to the input sequence of the Vision Transformer, which eliminates these artifacts for both supervised and self-supervised models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks, facilitates object discovery with larger models, and results in smoother feature and attention maps for downstream visual processing tasks."
    },
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper introduces the AVA dataset, which features 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. The authors highlight the challenges in action recognition and introduce a novel localization approach that surpasses existing benchmarks but shows modest performance on AVA (15.6% mAP), indicating the need for advanced video understanding methods."
    },
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "This paper introduces FineAction, a large-scale, fine-grained video dataset designed to address the limitations of current temporal action localization (TAL) benchmarks, which rely on coarse action classes and lead to model overfitting and ambiguous annotations. FineAction contains 103K instances across 106 action categories in 17K videos, offering a diverse and densely annotated dataset with co-occurring actions that pose new challenges for TAL. The authors evaluate popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and propose a baseline method achieving a 13.17% mAP. FineAction aims to advance TAL research and is accessible online."
    },
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "The paper investigates the use of synthetic data generated by graphics simulators for pre-training computer vision models. The authors find that model performance on downstream tasks varies with different simulation parameters used to generate the synthetic data. They introduce Task2Sim, a model that maps the requirements of a downstream task to the optimal simulation parameters for generating synthetic pre-training data."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "url": "/research/2014/02/19/Exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks.html",
      "date": "February 19, 2014",
      "summary": "This study aims to enhance our comprehension of deep learning dynamics by analyzing deep linear neural networks. Despite their linear input-output mapping, these networks display nonlinear gradient descent dynamics, resulting in phenomena like prolonged plateaus and swift transitions to improved solutions. Through analytical exploration, we reveal that as network depth increases indefinitely, learning speed can remain finite under specific initial conditions. Unsupervised pretraining, under certain data conditions, can discover these favorable initial conditions, unlike random Gaussian initializations. We introduce a novel class of random orthogonal initial conditions that, akin to unsupervised pretraining, enable depth-independent learning. These conditions also ensure gradient propagation in deep nonlinear networks, particularly when operating at the edge of chaos."
    }
  ],
  "enhancing": [
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study looks at self-supervised pre-training methods on small scale or lightweight Vision Transformers (ViTs). Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods."
    }
  ],
  "maebase": [
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study looks at self-supervised pre-training methods on small scale or lightweight Vision Transformers (ViTs). Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods."
    }
  ],
  "explores": [
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "This paper explores Gated Linear Units (GLUs), which involve the component-wise product of two linear projections, with one undergoing a sigmoid function. The authors investigate GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers. They find that certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    },
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "defense": [
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "rotnet": [
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    }
  ],
  "radius": [
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. This encoder serves as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The DMAE ViT-Base model achieves comparable or superior certified accuracy with fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on ImageNet. The model also shows high transferability to CIFAR-10, indicating its broad applicability."
    },
    {
      "title": "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization",
      "url": "/research/2023/07/04/Gradient-Norm-Aware-Minimization-Seeks-First-Order-Flatness-and-Improves-Generalization.html",
      "date": "July 4, 2023",
      "summary": "Recent advancements have highlighted the effectiveness of flat minima in enhancing generalization, particularly through Sharpness-Aware Minimization (SAM). However, existing definitions of flatness, such as zeroth-order flatness, have limitations in discerning between minima with low and high generalization errors. To address this, we propose first-order flatness, which considers maximal gradient norm within a perturbation radius. We introduce Gradient norm Aware Minimization (GAM) as a novel training approach to achieve uniformly small curvature across all directions. Experimental results demonstrate GAM's ability to enhance generalization compared to standard optimizers like SGD and AdamW across various datasets and networks. Moreover, GAM facilitates SAM in identifying flatter minima, leading to improved generalization."
    },
    {
      "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
      "url": "/research/2022/03/19/Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training.html",
      "date": "March 19, 2022",
      "summary": "Sharpness-Aware Minimization (SAM) enhances generalization by optimizing a neighborhood-based perturbed loss, but it doesn't always favor flat minima due to both sharp and flat minima having low perturbed losses. We introduce a new measure, the surrogate gap, reflecting the dominant Hessian eigenvalue at small neighborhood radii, which is simple to compute and can be minimized during training. We propose the Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), an advancement over SAM with minimal additional computational cost. GSAM employs a dual-step approach: first, minimizing the perturbed loss similar to SAM, and second, reducing the surrogate gap without affecting the perturbed loss to target regions with low loss and sharpness, thus achieving superior generalization. GSAM is theoretically robust, showing better convergence and empirical generalization improvements, notably a +3.2% gain over SAM and +5.4% over AdamW in ImageNet accuracy for ViT-B/32."
    }
  ],
  "instability": [
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/research/2021/08/16/An-Empirical-Study-of-Training-Self-Supervised-Vision-Transformers.html",
      "date": "August 16, 2021",
      "summary": "This study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks."
    },
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    },
    {
      "title": "Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks",
      "url": "/research/2020/06/11/Batch-Normalization-Provably-Avoids-Rank-Collapse-for-Randomly-Initialised-Deep-Networks.html",
      "date": "June 11, 2020",
      "summary": "We investigate the challenge of training randomly initialized deep neural networks due to spectral instabilities in products of random matrices. Batch normalization emerges as an effective solution to prevent rank collapse in both linear and ReLU networks. Leveraging tools from Markov chain theory, we establish a lower rank bound for deep linear networks. Empirical findings show that this rank robustness extends to ReLU networks. Our experiments on real-world datasets underscore the significance of rank stability in training modern deep neural architectures."
    }
  ],
  "undermine": [
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/research/2021/08/16/An-Empirical-Study-of-Training-Self-Supervised-Vision-Transformers.html",
      "date": "August 16, 2021",
      "summary": "This study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks."
    }
  ],
  "outcome": [
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/research/2021/08/16/An-Empirical-Study-of-Training-Self-Supervised-Vision-Transformers.html",
      "date": "August 16, 2021",
      "summary": "This study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks."
    },
    {
      "title": "Don't Decay the Learning Rate, Increase the Batch Size",
      "url": "/research/2017/11/01/Don't-Decay-the-Learning-Rate,-Increase-the-Batch-Size.html",
      "date": "November 1, 2017",
      "summary": "This study demonstrates that increasing the batch size during training achieves similar learning outcomes as the common practice of decaying the learning rate, applicable to stochastic gradient descent and its variants, including with momentum and Adam optimization. This approach not only matches test accuracies within the same number of epochs but also enhances parallelism and reduces training time due to fewer parameter updates. Efficiency can be further improved by adjusting the learning rate and batch size proportionally, and although increasing the momentum coefficient and scaling the batch size accordingly may slightly lower test accuracy, it enables the use of large batch training without needing to tune hyper-parameters. Using these methods, ResNet-50 was trained on ImageNet to a 76.1% validation accuracy in less than 30 minutes."
    },
    {
      "title": "Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes",
      "url": "/research/2017/11/28/Towards-Understanding-Generalization-of-Deep-Learning-Perspective-of-Loss-Landscapes.html",
      "date": "November 28, 2017",
      "summary": "This study examines why deep learning models, despite having more parameters than training samples, generalize well. It highlights the importance of the loss function's landscape, showing that areas leading to better generalization (good minima) are more prevalent than those leading to poor outcomes. This predominance facilitates the convergence of optimization methods to these beneficial minima. The research provides theoretical backing by analyzing 2-layer neural networks, noting that solutions with better generalization have a smaller Hessian matrix norm. For deeper networks, extensive numerical data supports these conclusions."
    }
  ],
  "ablation": [
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/research/2021/08/16/An-Empirical-Study-of-Training-Self-Supervised-Vision-Transformers.html",
      "date": "August 16, 2021",
      "summary": "This study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "This paper introduces FixMatch, a streamlined semi-supervised learning (SSL) algorithm. FixMatch generates pseudo-labels using a model's predictions on weakly-augmented unlabeled images, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness, and the code is publicly available."
    },
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "The paper introduces TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method that applies a single augmentation to each image. Despite its minimal complexity and cost, TrivialAugment outperforms existing methods across various image classification scenarios, as validated through extensive experiments against state-of-the-art methods and multiple ablation studies involving different augmentation spaces and methods. The work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Noting a stagnation in automatic augmentation research, the authors conclude by proposing best practices for future advancements in the field."
    }
  ],
  "moco": [
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/research/2021/08/16/An-Empirical-Study-of-Training-Self-Supervised-Vision-Transformers.html",
      "date": "August 16, 2021",
      "summary": "This study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks."
    }
  ],
  "v3": [
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/research/2021/08/16/An-Empirical-Study-of-Training-Self-Supervised-Vision-Transformers.html",
      "date": "August 16, 2021",
      "summary": "This study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks."
    }
  ],
  "worth": [
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "This study introduces the Vision Transformer ViT archetecture. The paper shows that a pure transformer applied directly to sequences of image patches can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    }
  ],
  "16x16": [
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "This study introduces the Vision Transformer ViT archetecture. The paper shows that a pure transformer applied directly to sequences of image patches can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    }
  ],
  "words": [
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "This study introduces the Vision Transformer ViT archetecture. The paper shows that a pure transformer applied directly to sequences of image patches can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    }
  ],
  "vtab": [
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "This study introduces the Vision Transformer ViT archetecture. The paper shows that a pure transformer applied directly to sequences of image patches can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    }
  ],
  "reconstruction": [
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, enhances vision tasks using Vision transformers by masking and reconstructing parts of an image. The compatibility of MIM with CNNs and its operational principle are unclear. this study reveals that MIM improves generalized feature extraction through middle-order patch interactions and introduces Architecture-Agnostic Masked Image Modeling (A2MIM), compatible with both Transformers and CNNs. Extensive testing demonstrates A2MIM's ability to enhance representation learning and transferability to various tasks without specialized modifications."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. MAE involves masking random patches of an input image and reconstructing the missing pixels using an asymmetric encoder-decoder architecture and a lightweight decoder. Masking a significant portion of the input image creates a challenging yet informative self-supervisory task. These innovations allow for efficient training of large models, tripling training speed and enhancing accuracy. A vanilla ViT-Huge model reaches top accuracy (87.8%) on ImageNet-1K data among similar methods. The model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating promising scaling potential."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. This encoder serves as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The DMAE ViT-Base model achieves comparable or superior certified accuracy with fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on ImageNet. The model also shows high transferability to CIFAR-10, indicating its broad applicability."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "This paper proposes Hard Patches Mining (HPM), a novel framework for masked image modeling (MIM) pre-training that goes beyond simply solving given problems. HPM aims for the model to generate more challenging tasks for itself, using reconstruction loss as a metric for task difficulty. It incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "This paper introduces Mask-Reconstruct Augmentation (MRA), an image augmentation technique that leverages self-supervised masked autoencoders to generate distorted inputs and address the issue of overfitting in deep neural networks. Inspired by the success of masked image modeling in self-supervised learning, MRA uses nonlinear transformations for regularization, as opposed to current hand-crafted linear techniques like scale, flip, and color jitter. Extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks."
    }
  ],
  "part": [
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, enhances vision tasks using Vision transformers by masking and reconstructing parts of an image. The compatibility of MIM with CNNs and its operational principle are unclear. this study reveals that MIM improves generalized feature extraction through middle-order patch interactions and introduces Architecture-Agnostic Masked Image Modeling (A2MIM), compatible with both Transformers and CNNs. Extensive testing demonstrates A2MIM's ability to enhance representation learning and transferability to various tasks without specialized modifications."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper investigates how self-supervised pretraining methods learn part-aware representations. The authors describe contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting that these methods predispose encoders to recognize object parts. Through empirical comparison, they find that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Optimal learning rate schedules in high-dimensional non-convex optimization problems",
      "url": "/research/2022/02/09/Optimal-learning-rate-schedules-in-high-dimensional-non-convex-optimization-problems.html",
      "date": "February 9, 2022",
      "summary": "This paper explores the role of learning rate schedules in high-dimensional and non-convex optimization problems, focusing on Langevin optimization with a decaying learning rate. Analyzing models with Gaussian random functions on N-dimensional spheres, the study reveals that to accelerate optimization without getting trapped in saddles, a decay rate β < 1 is optimal, contrary to convex settings where β = 1 is preferred. Introducing a signal recovery component, the dynamics involve an exploration phase navigating through rough landscape parts and a convergence phase entering convex basins. It's found optimal to maintain a large learning rate during exploration to swiftly exit non-convex regions, then transition to β = 1 for rapid convergence to the solution. These findings are validated in a neural network regression task."
    },
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    },
    {
      "title": "Learning and Leveraging World Models in Visual Representation Learning",
      "url": "/research/2024/03/01/Learning-and-Leveraging-World-Models-in-Visual-Representation-Learning.html",
      "date": "March 1, 2024",
      "summary": "The Joint-Embedding Predictive Architecture (JEPA) is a self-supervised method previously used for predicting missing input parts. This study expands JEPA to predict a wider range of corruptions by introducing Image World Models (IWM), which learn the effects of global photometric transformations in latent space. Key to effective IWM learning are conditioning, prediction difficulty, and model capacity. The study demonstrates that IWM's predictive world model, when fine-tuned, can tackle diverse tasks and either matches or outperforms existing self-supervised techniques. Additionally, it enables control over the abstraction level of learned representations, achieving either invariant or equivariant representations, similar to contrastive methods or masked image modeling, respectively."
    }
  ],
  "compatibility": [
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, enhances vision tasks using Vision transformers by masking and reconstructing parts of an image. The compatibility of MIM with CNNs and its operational principle are unclear. this study reveals that MIM improves generalized feature extraction through middle-order patch interactions and introduces Architecture-Agnostic Masked Image Modeling (A2MIM), compatible with both Transformers and CNNs. Extensive testing demonstrates A2MIM's ability to enhance representation learning and transferability to various tasks without specialized modifications."
    },
    {
      "title": "VideoMamba: State Space Model for Efficient Video Understanding",
      "url": "/research/2024/03/12/VideoMamba-State-Space-Model-for-Efficient-Video-Understanding.html",
      "date": "March 12, 2024",
      "summary": "This work introduces VideoMamba, an innovative approach that addresses local redundancy and global dependencies in video understanding by adapting the Mamba framework to video analysis. VideoMamba outperforms existing 3D convolutional neural networks and video transformers through its linear-complexity operator, which facilitates efficient long-term modeling for high-resolution, long-duration videos. Its effectiveness is demonstrated across four main areas: scalability in the visual domain without needing extensive dataset pretraining, thanks to a novel self-distillation technique; the ability to recognize short-term actions with fine-grained motion differences; superior performance in long-term video understanding compared to traditional feature-based models; and robust compatibility with multiple modalities, enhancing multi-modal video analysis. VideoMamba establishes a new standard for comprehensive and efficient video understanding."
    }
  ],
  "a2mim": [
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, enhances vision tasks using Vision transformers by masking and reconstructing parts of an image. The compatibility of MIM with CNNs and its operational principle are unclear. this study reveals that MIM improves generalized feature extraction through middle-order patch interactions and introduces Architecture-Agnostic Masked Image Modeling (A2MIM), compatible with both Transformers and CNNs. Extensive testing demonstrates A2MIM's ability to enhance representation learning and transferability to various tasks without specialized modifications."
    }
  ],
  "unna": [
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/research/2020/08/03/Are-Labels-Necessary-for-Neural-Architecture-Search.html",
      "date": "August 3, 2020",
      "summary": "The paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning the necessity of human-annotated labels for discovering effective neural architectures. Two experimental setups sample-based and search-based were investigated. The sample-based approach evaluated 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. The search-based experiments employed DARTS, a recognized NAS algorithm, with various unsupervised objectives. The architectures identified without labels performed competitively compared to those found with labels. The findings suggested that image statistics alone could be sufficient for identifying efficient neural architectures, potentially eliminating the need for human-annotated labels."
    }
  ],
  "discovering": [
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/research/2020/08/03/Are-Labels-Necessary-for-Neural-Architecture-Search.html",
      "date": "August 3, 2020",
      "summary": "The paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning the necessity of human-annotated labels for discovering effective neural architectures. Two experimental setups sample-based and search-based were investigated. The sample-based approach evaluated 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. The search-based experiments employed DARTS, a recognized NAS algorithm, with various unsupervised objectives. The architectures identified without labels performed competitively compared to those found with labels. The findings suggested that image statistics alone could be sufficient for identifying efficient neural architectures, potentially eliminating the need for human-annotated labels."
    }
  ],
  "ranking": [
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/research/2020/08/03/Are-Labels-Necessary-for-Neural-Architecture-Search.html",
      "date": "August 3, 2020",
      "summary": "The paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning the necessity of human-annotated labels for discovering effective neural architectures. Two experimental setups sample-based and search-based were investigated. The sample-based approach evaluated 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. The search-based experiments employed DARTS, a recognized NAS algorithm, with various unsupervised objectives. The architectures identified without labels performed competitively compared to those found with labels. The findings suggested that image statistics alone could be sufficient for identifying efficient neural architectures, potentially eliminating the need for human-annotated labels."
    }
  ],
  "dart": [
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/research/2020/08/03/Are-Labels-Necessary-for-Neural-Architecture-Search.html",
      "date": "August 3, 2020",
      "summary": "The paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning the necessity of human-annotated labels for discovering effective neural architectures. Two experimental setups sample-based and search-based were investigated. The sample-based approach evaluated 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. The search-based experiments employed DARTS, a recognized NAS algorithm, with various unsupervised objectives. The architectures identified without labels performed competitively compared to those found with labels. The findings suggested that image statistics alone could be sufficient for identifying efficient neural architectures, potentially eliminating the need for human-annotated labels."
    }
  ],
  "recognize": [
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/research/2020/08/03/Are-Labels-Necessary-for-Neural-Architecture-Search.html",
      "date": "August 3, 2020",
      "summary": "The paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning the necessity of human-annotated labels for discovering effective neural architectures. Two experimental setups sample-based and search-based were investigated. The sample-based approach evaluated 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. The search-based experiments employed DARTS, a recognized NAS algorithm, with various unsupervised objectives. The architectures identified without labels performed competitively compared to those found with labels. The findings suggested that image statistics alone could be sufficient for identifying efficient neural architectures, potentially eliminating the need for human-annotated labels."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper investigates how self-supervised pretraining methods learn part-aware representations. The authors describe contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting that these methods predispose encoders to recognize object parts. Through empirical comparison, they find that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "This study challenges the traditional understanding of how Convolutional Neural Networks (CNNs) recognize objects, revealing that they are more biased towards recognizing textures rather than learning complex shapes. The authors found that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, training a ResNet-50 on a stylized version of ImageNet, designed to emphasize shape, aligned the network's performance more closely with human behavior. This shape-based training matched human performance in controlled experiments and enhanced object detection and robustness against image distortions, highlighting the benefits of shape-based representations in visual recognition systems."
    },
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    },
    {
      "title": "VideoMamba: State Space Model for Efficient Video Understanding",
      "url": "/research/2024/03/12/VideoMamba-State-Space-Model-for-Efficient-Video-Understanding.html",
      "date": "March 12, 2024",
      "summary": "This work introduces VideoMamba, an innovative approach that addresses local redundancy and global dependencies in video understanding by adapting the Mamba framework to video analysis. VideoMamba outperforms existing 3D convolutional neural networks and video transformers through its linear-complexity operator, which facilitates efficient long-term modeling for high-resolution, long-duration videos. Its effectiveness is demonstrated across four main areas: scalability in the visual domain without needing extensive dataset pretraining, thanks to a novel self-distillation technique; the ability to recognize short-term actions with fine-grained motion differences; superior performance in long-term video understanding compared to traditional feature-based models; and robust compatibility with multiple modalities, enhancing multi-modal video analysis. VideoMamba establishes a new standard for comprehensive and efficient video understanding."
    }
  ],
  "nas": [
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/research/2020/08/03/Are-Labels-Necessary-for-Neural-Architecture-Search.html",
      "date": "August 3, 2020",
      "summary": "The paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning the necessity of human-annotated labels for discovering effective neural architectures. Two experimental setups sample-based and search-based were investigated. The sample-based approach evaluated 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. The search-based experiments employed DARTS, a recognized NAS algorithm, with various unsupervised objectives. The architectures identified without labels performed competitively compared to those found with labels. The findings suggested that image statistics alone could be sufficient for identifying efficient neural architectures, potentially eliminating the need for human-annotated labels."
    }
  ],
  "eliminate": [
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/research/2020/08/03/Are-Labels-Necessary-for-Neural-Architecture-Search.html",
      "date": "August 3, 2020",
      "summary": "The paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning the necessity of human-annotated labels for discovering effective neural architectures. Two experimental setups sample-based and search-based were investigated. The sample-based approach evaluated 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. The search-based experiments employed DARTS, a recognized NAS algorithm, with various unsupervised objectives. The architectures identified without labels performed competitively compared to those found with labels. The findings suggested that image statistics alone could be sufficient for identifying efficient neural architectures, potentially eliminating the need for human-annotated labels."
    },
    {
      "title": "CLIP: Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns from raw text descriptions of images, moving beyond training on fixed object categories. By pre-training on 400 million (image, text) pairs using caption-image matching, the method achieves state-of-the-art image representations from scratch. This allows for zero-shot transfer to various downstream tasks using natural language, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse datasets, showing competitive results against fully supervised baselines without task-specific training, and matching the accuracy of ResNet-50 on ImageNet zero-shot without using its training examples."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This paper is effectively a follow-up to DinoV2. The study addresses the issue of artifacts in feature maps of Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. The authors introduce a straightforward strategy of adding extra tokens to the input sequence of the Vision Transformer, which eliminates these artifacts for both supervised and self-supervised models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks, facilitates object discovery with larger models, and results in smoother feature and attention maps for downstream visual processing tasks."
    },
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    },
    {
      "title": "Automated Learning Rate Scheduler for Large-batch",
      "url": "/research/2021/07/13/Automated-Learning-Rate-Scheduler-for-Large-batch.html",
      "date": "July 13, 2021",
      "summary": "Large-batch training is crucial for deep learning with large datasets and models, but requires specific learning rate (LR) schedules for optimal performance, especially under limited training epochs. This work introduces an automated LR scheduling algorithm for large-batch neural network training within a fixed epoch budget, consisting of adaptive warmup and predefined decay phases. The LR is dynamically adjusted based on training loss monitored through Gaussian process smoothing, facilitating low computational overhead. When integrated with adaptive stochastic optimizers like AdamP and LAMB, this scheduler eliminates the need for extensive hyperparameter tuning and achieves competitive or superior results on various image classification tasks across different batch sizes and architectures."
    }
  ],
  "demystify": [
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM) for self-supervised visual representation in vision transformers ViT. Five different learning objectives were proposed, which degraded the input image in various ways similar to masking. Design principles are proposed for  token-based pre-training of vision transformers. The most effective strategy combined preserving the original image style with spatial misalignment and masking. This approach outperformed traditional MIM on downstream recognition tasks without increasing computational demands."
    }
  ],
  "degrade": [
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM) for self-supervised visual representation in vision transformers ViT. Five different learning objectives were proposed, which degraded the input image in various ways similar to masking. Design principles are proposed for  token-based pre-training of vision transformers. The most effective strategy combined preserving the original image style with spatial misalignment and masking. This approach outperformed traditional MIM on downstream recognition tasks without increasing computational demands."
    },
    {
      "title": "On the Maximum Hessian Eigenvalue and Generalization",
      "url": "/research/2023/05/23/On-the-Maximum-Hessian-Eigenvalue-and-Generalization.html",
      "date": "May 23, 2023",
      "summary": "This study investigates the relationship between training interventions and the generalization of deep networks. While previous research suggests that flatter solutions generalize better than sharper ones, particularly measured by λmax, the largest eigenvalue of the Hessian of the loss, this paper challenges this notion. Through experiments, we demonstrate that larger learning rates reduce λmax for all batch sizes but do not consistently improve generalization. Additionally, scaling batch size and learning rate simultaneously can change λmax without affecting generalization. Sharpness-Aware Minimization (SAM) produces smaller λmax but does not consistently enhance generalization, especially with larger batch sizes. Excessive dropout probabilities can degrade generalization, despite promoting smaller λmax. Batch normalization, while not consistently reducing λmax, still improves generalization. These findings question λmax's role in explaining generalization in neural networks, highlighting the limits of its explanatory power."
    }
  ],
  "misalignment": [
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM) for self-supervised visual representation in vision transformers ViT. Five different learning objectives were proposed, which degraded the input image in various ways similar to masking. Design principles are proposed for  token-based pre-training of vision transformers. The most effective strategy combined preserving the original image style with spatial misalignment and masking. This approach outperformed traditional MIM on downstream recognition tasks without increasing computational demands."
    }
  ],
  "colorization": [
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results. The method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. A convolutional neural network (CNN) was trained on over a million color images, operating as a feed-forward pass at test time. The algorithm's effectiveness was validated through a colorization Turing test, where it deceived participants in 32% of trials. The paper also illustrates how colorization can serve as an effective pretraining task for self-supervised feature learning, achieving state-of-the-art results on several benchmarks."
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores self-supervision through automatic colorization as an alternative to ImageNet pretraining. Self-supervised training achieved state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. This paper highlights the significance of loss formulation, training specifics, and network architecture when pretraining through colorization. It also revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and feature adaptability upon fine-tuning. The findings suggest that colorization offers a better supervisory signal comparable to various types of ImageNet pretraining."
    }
  ],
  "colorize": [
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results. The method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. A convolutional neural network (CNN) was trained on over a million color images, operating as a feed-forward pass at test time. The algorithm's effectiveness was validated through a colorization Turing test, where it deceived participants in 32% of trials. The paper also illustrates how colorization can serve as an effective pretraining task for self-supervised feature learning, achieving state-of-the-art results on several benchmarks."
    }
  ],
  "grayscale": [
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results. The method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. A convolutional neural network (CNN) was trained on over a million color images, operating as a feed-forward pass at test time. The algorithm's effectiveness was validated through a colorization Turing test, where it deceived participants in 32% of trials. The paper also illustrates how colorization can serve as an effective pretraining task for self-supervised feature learning, achieving state-of-the-art results on several benchmarks."
    }
  ],
  "frame": [
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results. The method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. A convolutional neural network (CNN) was trained on over a million color images, operating as a feed-forward pass at test time. The algorithm's effectiveness was validated through a colorization Turing test, where it deceived participants in 32% of trials. The paper also illustrates how colorization can serve as an effective pretraining task for self-supervised feature learning, achieving state-of-the-art results on several benchmarks."
    },
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "This paper introduces SlowFast networks, a new convolutional architecture for video recognition tasks. These networks have two pathways: a Slow pathway operating at low frame rates to capture spatial semantics, and a Fast pathway working at high frame rates with reduced channel capacity to efficiently capture motion details. The SlowFast models achieve top accuracy on major video benchmarks like Kinetics, Charades, and AVA, significantly improving performance for action classification and detection tasks."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article provides an extensive review of vision transformer techniques applied specifically to human action recognition. These action transformers are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also examines optimizing spatio-temporal attention in transformers for longer sequences and different learning strategies like self-supervised and zero-shot learning with their respective loss functions. Additionally, it assesses progress in benchmark evaluation scores and discusses challenges and future directions in this research area."
    }
  ],
  "rebalance": [
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results. The method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. A convolutional neural network (CNN) was trained on over a million color images, operating as a feed-forward pass at test time. The algorithm's effectiveness was validated through a colorization Turing test, where it deceived participants in 32% of trials. The paper also illustrates how colorization can serve as an effective pretraining task for self-supervised feature learning, achieving state-of-the-art results on several benchmarks."
    }
  ],
  "pass": [
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results. The method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. A convolutional neural network (CNN) was trained on over a million color images, operating as a feed-forward pass at test time. The algorithm's effectiveness was validated through a colorization Turing test, where it deceived participants in 32% of trials. The paper also illustrates how colorization can serve as an effective pretraining task for self-supervised feature learning, achieving state-of-the-art results on several benchmarks."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    }
  ],
  "turing": [
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results. The method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. A convolutional neural network (CNN) was trained on over a million color images, operating as a feed-forward pass at test time. The algorithm's effectiveness was validated through a colorization Turing test, where it deceived participants in 32% of trials. The paper also illustrates how colorization can serve as an effective pretraining task for self-supervised feature learning, achieving state-of-the-art results on several benchmarks."
    }
  ],
  "deceive": [
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results. The method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. A convolutional neural network (CNN) was trained on over a million color images, operating as a feed-forward pass at test time. The algorithm's effectiveness was validated through a colorization Turing test, where it deceived participants in 32% of trials. The paper also illustrates how colorization can serve as an effective pretraining task for self-supervised feature learning, achieving state-of-the-art results on several benchmarks."
    }
  ],
  "participant": [
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results. The method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. A convolutional neural network (CNN) was trained on over a million color images, operating as a feed-forward pass at test time. The algorithm's effectiveness was validated through a colorization Turing test, where it deceived participants in 32% of trials. The paper also illustrates how colorization can serve as an effective pretraining task for self-supervised feature learning, achieving state-of-the-art results on several benchmarks."
    }
  ],
  "illustrate": [
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results. The method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. A convolutional neural network (CNN) was trained on over a million color images, operating as a feed-forward pass at test time. The algorithm's effectiveness was validated through a colorization Turing test, where it deceived participants in 32% of trials. The paper also illustrates how colorization can serve as an effective pretraining task for self-supervised feature learning, achieving state-of-the-art results on several benchmarks."
    },
    {
      "title": "Chronos: Learning the Language of Time Series",
      "url": "/research/2024/03/12/Chronos-Learning-the-Language-of-Time-Series.html",
      "date": "March 12, 2024",
      "summary": "Chronos is a framework that enhances pretrained probabilistic time series models by tokenizing time series data for training with transformer-based architectures, notably the T5 family. Pretrained on a mix of public and synthetic datasets created via Gaussian processes, Chronos models outshine others in 42 benchmark datasets by showing superior performance on familiar datasets and competitive or better zero-shot capabilities on new datasets. This illustrates Chronos's ability to generalize across different domains, offering a simpler approach to forecasting tasks."
    }
  ],
  "serve": [
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results. The method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. A convolutional neural network (CNN) was trained on over a million color images, operating as a feed-forward pass at test time. The algorithm's effectiveness was validated through a colorization Turing test, where it deceived participants in 32% of trials. The paper also illustrates how colorization can serve as an effective pretraining task for self-supervised feature learning, achieving state-of-the-art results on several benchmarks."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. This encoder serves as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The DMAE ViT-Base model achieves comparable or superior certified accuracy with fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on ImageNet. The model also shows high transferability to CIFAR-10, indicating its broad applicability."
    },
    {
      "title": "Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks",
      "url": "/research/2017/10/23/Cosine-Normalization-Using-Cosine-Similarity-Instead-of-Dot-Product-in-Neural-Networks.html",
      "date": "October 23, 2017",
      "summary": "In traditional multi-layer neural networks, the dot product between the output and weight vectors of preceding layers serves as input to the activation function, resulting in unbounded outputs and increased variance. This variance can lead to poor generalization and hinder training by exacerbating internal covariate shift. To address this, we propose cosine normalization, which replaces the dot product with cosine similarity or centered cosine similarity (Pearson Correlation Coefficient). We evaluate cosine normalization against batch, weight, and layer normalization in fully-connected and convolutional neural networks across various datasets. Our experiments demonstrate that cosine normalization outperforms other normalization techniques."
    },
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    },
    {
      "title": "Neural Network Diffusion",
      "url": "/research/2024/02/20/Neural-Network-Diffusion.html",
      "date": "February 20, 2024",
      "summary": "Diffusion models, known for their success in image and video generation, can also generate high-performing neural network parameters, as demonstrated in this study. By employing a simple combination of an autoencoder and a standard latent diffusion model, our method involves extracting latent representations of network parameters, which are then synthesized from random noise by the diffusion model. These new representations, processed through the autoencoder's decoder, serve as fresh network parameters. Tested across various architectures and datasets, this diffusion approach consistently produces models with comparable or superior performance to traditionally trained networks at minimal extra cost. Importantly, the generated models show distinct performance differences from the trained ones, suggesting further exploration into the versatile applications of diffusion models."
    }
  ],
  "proxy": [
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores self-supervision through automatic colorization as an alternative to ImageNet pretraining. Self-supervised training achieved state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. This paper highlights the significance of loss formulation, training specifics, and network architecture when pretraining through colorization. It also revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and feature adaptability upon fine-tuning. The findings suggest that colorization offers a better supervisory signal comparable to various types of ImageNet pretraining."
    }
  ],
  "voc": [
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores self-supervision through automatic colorization as an alternative to ImageNet pretraining. Self-supervised training achieved state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. This paper highlights the significance of loss formulation, training specifics, and network architecture when pretraining through colorization. It also revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and feature adaptability upon fine-tuning. The findings suggest that colorization offers a better supervisory signal comparable to various types of ImageNet pretraining."
    },
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "This paper introdices LOST, a method for unsupervised object localization in images. LOST utilizes activation features from a self-supervised pre-trained vision transformer. Unlike other methods, LOST works on individual images without relying on external object proposals or image collection exploration. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, demonstrating its effectiveness in unsupervised object discovery."
    }
  ],
  "formulation": [
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores self-supervision through automatic colorization as an alternative to ImageNet pretraining. Self-supervised training achieved state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. This paper highlights the significance of loss formulation, training specifics, and network architecture when pretraining through colorization. It also revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and feature adaptability upon fine-tuning. The findings suggest that colorization offers a better supervisory signal comparable to various types of ImageNet pretraining."
    }
  ],
  "specific": [
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores self-supervision through automatic colorization as an alternative to ImageNet pretraining. Self-supervised training achieved state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. This paper highlights the significance of loss formulation, training specifics, and network architecture when pretraining through colorization. It also revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and feature adaptability upon fine-tuning. The findings suggest that colorization offers a better supervisory signal comparable to various types of ImageNet pretraining."
    }
  ],
  "revisit": [
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores self-supervision through automatic colorization as an alternative to ImageNet pretraining. Self-supervised training achieved state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. This paper highlights the significance of loss formulation, training specifics, and network architecture when pretraining through colorization. It also revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and feature adaptability upon fine-tuning. The findings suggest that colorization offers a better supervisory signal comparable to various types of ImageNet pretraining."
    }
  ],
  "volume": [
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores self-supervision through automatic colorization as an alternative to ImageNet pretraining. Self-supervised training achieved state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. This paper highlights the significance of loss formulation, training specifics, and network architecture when pretraining through colorization. It also revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and feature adaptability upon fine-tuning. The findings suggest that colorization offers a better supervisory signal comparable to various types of ImageNet pretraining."
    }
  ],
  "quantity": [
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores self-supervision through automatic colorization as an alternative to ImageNet pretraining. Self-supervised training achieved state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. This paper highlights the significance of loss formulation, training specifics, and network architecture when pretraining through colorization. It also revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and feature adaptability upon fine-tuning. The findings suggest that colorization offers a better supervisory signal comparable to various types of ImageNet pretraining."
    },
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "The authors introduce VideoMAE, a data-efficient self-supervised video pre-training (SSVP) approach that utilizes video masked autoencoders with a novel high-ratio video tube masking technique inspired by ImageMAE. Key findings include the effectiveness of high masking ratios (90-95%) due to video's temporal redundancy, strong performance on small datasets (~3k-4k videos) without extra data highlighting the importance of high-level structure learning, and data quality being more crucial than quantity with domain shift being significant. Notably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone without extra data."
    }
  ],
  "adaptability": [
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores self-supervision through automatic colorization as an alternative to ImageNet pretraining. Self-supervised training achieved state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. This paper highlights the significance of loss formulation, training specifics, and network architecture when pretraining through colorization. It also revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and feature adaptability upon fine-tuning. The findings suggest that colorization offers a better supervisory signal comparable to various types of ImageNet pretraining."
    }
  ],
  "overlap": [
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of Vision Transformer (ViT) variants against common corruptions. The benchmarking reveals that overlapping patch embedding and convolutional feed-forward networks (FFNs) significantly enhance ViT robustness. The study also scrutinizes the effectiveness of CNN-based data augmentation strategies when applied to ViTs, finding that adversarial noise training is effective, while fourier-domain augmentation falls short. A new conditional method for generating dynamic augmentation parameters based on input images is proposed, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    }
  ],
  "embed": [
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of Vision Transformer (ViT) variants against common corruptions. The benchmarking reveals that overlapping patch embedding and convolutional feed-forward networks (FFNs) significantly enhance ViT robustness. The study also scrutinizes the effectiveness of CNN-based data augmentation strategies when applied to ViTs, finding that adversarial noise training is effective, while fourier-domain augmentation falls short. A new conditional method for generating dynamic augmentation parameters based on input images is proposed, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding",
      "url": "/research/2021/11/16/Improved-Robustness-of-Vision-Transformer-via-PreLayerNorm-in-Patch-Embedding.html",
      "date": "November 16, 2021",
      "summary": "Recent advancements in vision transformers (ViTs) have shown superior performance across diverse visual tasks, surpassing convolutional neural networks (CNNs). Given ViT's distinct architecture, understanding its behavior and reliability is imperative. This paper investigates ViT's robustness by comparing it with CNNs under various image corruptions relevant to real-world vision tasks. While ViT generally exhibits comparable or improved robustness over CNNs, it consistently underperforms in contrast enhancement tasks. Analysis suggests that positional embedding in ViT's patch embedding may malfunction with color scale changes. We propose PreLayerNorm, a modified patch embedding structure, to address this issue and ensure scale-invariant behavior in ViT. ViT with PreLayerNorm demonstrates enhanced robustness across various corruptions, particularly in contrast-varying environments."
    }
  ],
  "ffn": [
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of Vision Transformer (ViT) variants against common corruptions. The benchmarking reveals that overlapping patch embedding and convolutional feed-forward networks (FFNs) significantly enhance ViT robustness. The study also scrutinizes the effectiveness of CNN-based data augmentation strategies when applied to ViTs, finding that adversarial noise training is effective, while fourier-domain augmentation falls short. A new conditional method for generating dynamic augmentation parameters based on input images is proposed, achieving state-of-the-art robustness against common corruptions."
    }
  ],
  "scrutinize": [
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of Vision Transformer (ViT) variants against common corruptions. The benchmarking reveals that overlapping patch embedding and convolutional feed-forward networks (FFNs) significantly enhance ViT robustness. The study also scrutinizes the effectiveness of CNN-based data augmentation strategies when applied to ViTs, finding that adversarial noise training is effective, while fourier-domain augmentation falls short. A new conditional method for generating dynamic augmentation parameters based on input images is proposed, achieving state-of-the-art robustness against common corruptions."
    }
  ],
  "fourier": [
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of Vision Transformer (ViT) variants against common corruptions. The benchmarking reveals that overlapping patch embedding and convolutional feed-forward networks (FFNs) significantly enhance ViT robustness. The study also scrutinizes the effectiveness of CNN-based data augmentation strategies when applied to ViTs, finding that adversarial noise training is effective, while fourier-domain augmentation falls short. A new conditional method for generating dynamic augmentation parameters based on input images is proposed, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "This paper introduces a Fourier neural network (FNN) that aligns with Fourier decomposition by utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design allows for easy integration with more complex networks for data processing tasks. The authors demonstrate the FNN's efficacy on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits of the FNN include solution validity beyond the training scope, model interpretability, and ease of use."
    },
    {
      "title": "FNet: Mixing Tokens with Fourier Transforms",
      "url": "/research/2022/05/26/FNet-Mixing-Tokens-with-Fourier-Transforms.html",
      "date": "May 26, 2022",
      "summary": "We demonstrate that Transformer encoder architectures can be accelerated with minimal impact on accuracy by substituting self-attention sublayers with simple linear transformations. Remarkably, using a standard Fourier Transform instead of the self-attention sublayer in a Transformer encoder achieves 92-97% of BERT models' accuracy on the GLUE benchmark, while training 80% faster on GPUs and 70% faster on TPUs for standard 512 input lengths. Our FNet model significantly outperforms in speed at longer input lengths, matching the accuracy of the most accurate models on the Long Range Arena benchmark and surpassing the fastest models in speed across all sequence lengths on GPUs and shorter lengths on TPUs. Additionally, FNet is lightweight in memory use and exceptionally efficient in smaller sizes, outperforming Transformer models under the same speed and accuracy constraints."
    }
  ],
  "fall": [
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of Vision Transformer (ViT) variants against common corruptions. The benchmarking reveals that overlapping patch embedding and convolutional feed-forward networks (FFNs) significantly enhance ViT robustness. The study also scrutinizes the effectiveness of CNN-based data augmentation strategies when applied to ViTs, finding that adversarial noise training is effective, while fourier-domain augmentation falls short. A new conditional method for generating dynamic augmentation parameters based on input images is proposed, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Understanding Deep Learning Requires Re- Thinking Generalization",
      "url": "/research/2017/02/26/Understanding-Deep-Learning-Requires-Re--Thinking-Generalization.html",
      "date": "February 26, 2017",
      "summary": "Extensive experiments reveal that traditional explanations for the strong generalization performance of large neural networks fall short. Despite conventional wisdom attributing success to model properties or regularization techniques, our findings show that state-of-the-art convolutional networks for image classification can easily overfit random labels. This phenomenon persists regardless of explicit regularization or substituting true images with random noise. Theoretical analysis suggests that shallow neural networks achieve perfect finite sample expressivity when parameters outnumber data points, as commonly observed. We interpret these results in contrast to traditional models."
    }
  ],
  "parameterize": [
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "This study introduces a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. The method involves creating spaces for network design that parameterize multiple network populations. A low-dimensional, efficient design space called RegNet is developed, based on modeling network widths and depths through a quantized linear function. The analysis of the RegNet space challenges existing design practices, offering simpler and faster networks effective across various computational budgets. RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs under similar conditions."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces sinusoidal representation networks (SIRENs), which utilize periodic activation functions like Sin to effectively capture complex natural signals and their derivatives, addressing the limitations of neural networks parameterized for continuous, differentiable signal representations. The authors' analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. SIRENs are also applied in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations. The authors extend SIRENs' use with hypernetworks to learn priors for SIREN functions."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/2020/06/17/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "June 17, 2020",
      "summary": "Implicit neural representations, parameterized by continuous, differentiable neural networks, offer numerous advantages over conventional representations. However, existing architectures struggle with fine detail and fail to capture spatial and temporal derivatives, crucial for many physical signals described by partial differential equations. We introduce sinusoidal representation networks (SIRENs) utilizing periodic activation functions, ideal for representing complex signals and their derivatives. Analyzing SIREN activation statistics informs a principled initialization scheme. We demonstrate SIRENs' efficacy in representing various signals and solving boundary value problems, including Eikonal, Poisson, Helmholtz, and wave equations. Additionally, we integrate SIRENs with hypernetworks to learn priors over the SIREN function space. Visit the project website for detailed demonstrations."
    }
  ],
  "population": [
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "This study introduces a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. The method involves creating spaces for network design that parameterize multiple network populations. A low-dimensional, efficient design space called RegNet is developed, based on modeling network widths and depths through a quantized linear function. The analysis of the RegNet space challenges existing design practices, offering simpler and faster networks effective across various computational budgets. RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs under similar conditions."
    },
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "The study presents a novel approach for detection identification of Holstein-Friesian cattle. This technique offers a completely hands-off solution for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training when new cattle are introduced. The system utilizes convolutional neural networks and deep metric learning, achieving high accuracy with a 93.8% success rate in identifying cattle unseen during training, using only half of the population."
    }
  ],
  "regnet": [
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "This study introduces a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. The method involves creating spaces for network design that parameterize multiple network populations. A low-dimensional, efficient design space called RegNet is developed, based on modeling network widths and depths through a quantized linear function. The analysis of the RegNet space challenges existing design practices, offering simpler and faster networks effective across various computational budgets. RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs under similar conditions."
    }
  ],
  "width": [
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "This study introduces a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. The method involves creating spaces for network design that parameterize multiple network populations. A low-dimensional, efficient design space called RegNet is developed, based on modeling network widths and depths through a quantized linear function. The analysis of the RegNet space challenges existing design practices, offering simpler and faster networks effective across various computational budgets. RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs under similar conditions."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "This paper demonstrates that artificial neural networks (ANNs) are equivalent to Gaussian processes at initialization in the infinite-width limit and introduces the Neural Tangent Kernel (NTK), which describes ANNs' behavior during training. The NTK stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. The authors prove the positive-definiteness of the limiting NTK under certain conditions and show that the network function follows a linear differential equation during training for least-squares regression. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    }
  ],
  "efficientnet": [
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "This study introduces a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. The method involves creating spaces for network design that parameterize multiple network populations. A low-dimensional, efficient design space called RegNet is developed, based on modeling network widths and depths through a quantized linear function. The analysis of the RegNet space challenges existing design practices, offering simpler and faster networks effective across various computational budgets. RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs under similar conditions."
    },
    {
      "title": "ImageNet-D: A new challenging robustness dataset inspired by domain adaptation",
      "url": "/research/2023/05/05/ImageNet-D-A-new-challenging-robustness-dataset-inspired-by-domain-adaptation.html",
      "date": "May 5, 2023",
      "summary": "We introduce ImageNet-D, a novel dataset designed to evaluate the robustness of ImageNet-trained models across various domains. With six distinct domains including Real, Painting, Clipart, Sketch, Infograph, and Quick-draw, ImageNet-D challenges even state-of-the-art models, revealing interpretable errors. For instance, our leading EfficientNet-L2 model exhibits a significant performance decrease, dropping from 11.6% on clean ImageNet to 29.2% on the Real domain."
    }
  ],
  "gpu": [
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "This study introduces a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. The method involves creating spaces for network design that parameterize multiple network populations. A low-dimensional, efficient design space called RegNet is developed, based on modeling network widths and depths through a quantized linear function. The analysis of the RegNet space challenges existing design practices, offering simpler and faster networks effective across various computational budgets. RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs under similar conditions."
    },
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "This paper introduces QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU while maintaining 16-bit task performance. QLORA utilizes 4-bit quantized language models and Low Rank Adapters (LoRA) and incorporates several memory-saving innovations without compromising performance. The authors' best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. The authors applied QLORA to finetune over 1,000 models and analyzed performance across various datasets, model types, and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Their findings suggest that GPT-4 evaluations are a viable substitute for human assessments and question the reliability of current chatbot benchmarks. The authors make their models and 4-bit training code public."
    }
  ],
  "reliance": [
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Incorporating diverse image styles into CNN training data reduces texture bias, enhances shape recognition, and improves resilience against common image corruptions. This is typically explained as decreasing model reliance on high frequency texture information. This paper challenges this explaniation through large scale testing with natural images, edge information, and stylization, finding no direct link between shape bias and robustness. The enhanced corruption robustness is instead attributed to style variation in data augmentation, with increased shape bias being an indirect effect."
    }
  ],
  "explaniation": [
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Incorporating diverse image styles into CNN training data reduces texture bias, enhances shape recognition, and improves resilience against common image corruptions. This is typically explained as decreasing model reliance on high frequency texture information. This paper challenges this explaniation through large scale testing with natural images, edge information, and stylization, finding no direct link between shape bias and robustness. The enhanced corruption robustness is instead attributed to style variation in data augmentation, with increased shape bias being an indirect effect."
    }
  ],
  "edge": [
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Incorporating diverse image styles into CNN training data reduces texture bias, enhances shape recognition, and improves resilience against common image corruptions. This is typically explained as decreasing model reliance on high frequency texture information. This paper challenges this explaniation through large scale testing with natural images, edge information, and stylization, finding no direct link between shape bias and robustness. The enhanced corruption robustness is instead attributed to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "url": "/research/2014/02/19/Exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks.html",
      "date": "February 19, 2014",
      "summary": "This study aims to enhance our comprehension of deep learning dynamics by analyzing deep linear neural networks. Despite their linear input-output mapping, these networks display nonlinear gradient descent dynamics, resulting in phenomena like prolonged plateaus and swift transitions to improved solutions. Through analytical exploration, we reveal that as network depth increases indefinitely, learning speed can remain finite under specific initial conditions. Unsupervised pretraining, under certain data conditions, can discover these favorable initial conditions, unlike random Gaussian initializations. We introduce a novel class of random orthogonal initial conditions that, akin to unsupervised pretraining, enable depth-independent learning. These conditions also ensure gradient propagation in deep nonlinear networks, particularly when operating at the edge of chaos."
    },
    {
      "title": "MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts",
      "url": "/research/2024/02/26/MoE-Mamba-Efficient-Selective-State-Space-Models-with-Mixture-of-Experts.html",
      "date": "February 26, 2024",
      "summary": "State Space Models (SSMs) are emerging as strong competitors in sequential modeling, challenging Transformers. Integrating Mixture of Experts (MoE) has enhanced Transformer-based models, including cutting-edge open models. We suggest combining SSMs with MoE to further unlock their scaling potential. Our model, MoE-Mamba, based on the SSM model Mamba, exceeds the performance of both Mamba and traditional Transformer-MoE models. Notably, MoE-Mamba achieves Mamba's performance with 2.35 times fewer training steps, maintaining Mamba's inference advantages over Transformers."
    },
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "link": [
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Incorporating diverse image styles into CNN training data reduces texture bias, enhances shape recognition, and improves resilience against common image corruptions. This is typically explained as decreasing model reliance on high frequency texture information. This paper challenges this explaniation through large scale testing with natural images, edge information, and stylization, finding no direct link between shape bias and robustness. The enhanced corruption robustness is instead attributed to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    },
    {
      "title": "Do Vision Transformers See Like Convolutional Neural Networks?",
      "url": "/research/2022/03/03/Do-Vision-Transformers-See-Like-Convolutional-Neural-Networks.html",
      "date": "March 3, 2022",
      "summary": "Convolutional Neural Networks (CNNs) have been prominent in processing visual data, but recent studies show Vision Transformers (ViTs) can match or outperform CNNs in image classification. This work examines how ViTs solve classification tasks, discovering significant differences from CNNs, including uniformity in ViT representations across layers due to self-attention and residual connections that enhance feature propagation. ViTs also maintain spatial information effectively, influenced by classification methods. The study further explores how dataset scale affects ViT features and their transferability, linking these findings to new architectures like MLP-Mixer."
    }
  ],
  "bia": [
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Incorporating diverse image styles into CNN training data reduces texture bias, enhances shape recognition, and improves resilience against common image corruptions. This is typically explained as decreasing model reliance on high frequency texture information. This paper challenges this explaniation through large scale testing with natural images, edge information, and stylization, finding no direct link between shape bias and robustness. The enhanced corruption robustness is instead attributed to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    }
  ],
  "attribute": [
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Incorporating diverse image styles into CNN training data reduces texture bias, enhances shape recognition, and improves resilience against common image corruptions. This is typically explained as decreasing model reliance on high frequency texture information. This paper challenges this explaniation through large scale testing with natural images, edge information, and stylization, finding no direct link between shape bias and robustness. The enhanced corruption robustness is instead attributed to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "Understanding Deep Learning Requires Re- Thinking Generalization",
      "url": "/research/2017/02/26/Understanding-Deep-Learning-Requires-Re--Thinking-Generalization.html",
      "date": "February 26, 2017",
      "summary": "Extensive experiments reveal that traditional explanations for the strong generalization performance of large neural networks fall short. Despite conventional wisdom attributing success to model properties or regularization techniques, our findings show that state-of-the-art convolutional networks for image classification can easily overfit random labels. This phenomenon persists regardless of explicit regularization or substituting true images with random noise. Theoretical analysis suggests that shallow neural networks achieve perfect finite sample expressivity when parameters outnumber data points, as commonly observed. We interpret these results in contrast to traditional models."
    },
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    },
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    },
    {
      "title": "How Does Sharpness-Aware Minimization Minimize Sharpness?",
      "url": "/research/2023/01/05/How-Does-Sharpness-Aware-Minimization-Minimize-Sharpness.html",
      "date": "January 5, 2023",
      "summary": "Sharpness-Aware Minimization (SAM) enhances deep neural networks' generalization across various settings. Although SAM aims to penalize model sharpness using a computationally efficient approach, its exact operational mechanism and the sharpness notion it regularizes remain unclear, partly due to differing sharpness concepts used in its theoretical framework and empirical validation. This study identifies the precise sharpness concept SAM regulates and explains its mechanism. It reveals that the combined effect of SAM's two-step approximations, despite being individually misleading, correctly improves generalization when using full-batch gradients. Moreover, we demonstrate that the stochastic SAM version indeed regularizes a third sharpness notion, aligning more closely with practical performance. This effectiveness is attributed to the gradient's alignment with the Hessian's top eigenvector under SAM."
    }
  ],
  "clip": [
    {
      "title": "CLIP: Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns from raw text descriptions of images, moving beyond training on fixed object categories. By pre-training on 400 million (image, text) pairs using caption-image matching, the method achieves state-of-the-art image representations from scratch. This allows for zero-shot transfer to various downstream tasks using natural language, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse datasets, showing competitive results against fully supervised baselines without task-specific training, and matching the accuracy of ResNet-50 on ImageNet zero-shot without using its training examples."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "This paper introduces Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperformed CLIP models on various downstream tasks. The speedup from this method enables experimentation with larger model sizes, more data, or longer training periods."
    },
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper introduces the AVA dataset, which features 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. The authors highlight the challenges in action recognition and introduce a novel localization approach that surpasses existing benchmarks but shows modest performance on AVA (15.6% mAP), indicating the need for advanced video understanding methods."
    },
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "This paper introduces the Kinetics Human Action Video dataset. The authors note that the limited number of videos in existing datasets like UCF-101 and HMDB-51 hinders the ability to effectively evaluate architectures due to similar performance across these small benchmarks. Kinetics contains 400 classes with over 400 video clips per class from challenging YouTube videos. The paper analyzes the impact of this larger dataset on the performance of existing architectures as well as the benefits of pre-training models on Kinetics. The authors introduce the Two-Stream Inflated 3D ConvNet (I3D), an extension of 2D ConvNets to 3D for improved video feature extraction. When pre-trained on Kinetics, this I3D model sets new benchmarks for action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    }
  ],
  "text": [
    {
      "title": "CLIP: Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns from raw text descriptions of images, moving beyond training on fixed object categories. By pre-training on 400 million (image, text) pairs using caption-image matching, the method achieves state-of-the-art image representations from scratch. This allows for zero-shot transfer to various downstream tasks using natural language, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse datasets, showing competitive results against fully supervised baselines without task-specific training, and matching the accuracy of ResNet-50 on ImageNet zero-shot without using its training examples."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "This paper introduces Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperformed CLIP models on various downstream tasks. The speedup from this method enables experimentation with larger model sizes, more data, or longer training periods."
    },
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model consists of two parallel transformer encoders (text prompt and image) and a shared mask decoder. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. "
    },
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "This paper introduces DiffBIR, a framework that utilizes pretrained text-to-image diffusion models for blind image restoration. The two-stage pipeline involves pretraining a restoration module on various degradations to enhance real-world applicability, followed by the use of latent diffusion models for realistic restoration. The authors introduce a novel injective modulation sub-network, LAControlNet, for fine-tuning and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, a controllable module allows users to adjust quality and fidelity during the denoising process. Extensive tests demonstrate DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets."
    },
    {
      "title": "Stable Diffusion 3: Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "This paper focuses on improving diffusion models for high-dimensional data like images. The authors enhance noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Their large-scale study shows superior performance over established diffusion methods in high-resolution text-to-image synthesis. A new transformer-based architecture is introduced for better text comprehension, typography, and human preference in text-to-image generation. The findings demonstrate predictable scaling trends, with lower validation loss tied to improved synthesis. The largest models outperform state-of-the-art approaches."
    },
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    },
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    }
  ],
  "description": [
    {
      "title": "CLIP: Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns from raw text descriptions of images, moving beyond training on fixed object categories. By pre-training on 400 million (image, text) pairs using caption-image matching, the method achieves state-of-the-art image representations from scratch. This allows for zero-shot transfer to various downstream tasks using natural language, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse datasets, showing competitive results against fully supervised baselines without task-specific training, and matching the accuracy of ResNet-50 on ImageNet zero-shot without using its training examples."
    },
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "This paper introduces an algorithm that identifies simple and highly generalizable neural networks by searching for extensive flat minima regions in the error function, where the error rate is relatively stable. Such flat minima are associated with lower overfitting risks based on minimum description length (MDL) principles. Despite requiring second-order derivative calculations, the algorithm has a complexity level comparable to backpropagation. When tested on feedforward and recurrent networks, as well as stock market prediction tasks, this algorithm outperformed traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    }
  ],
  "caption": [
    {
      "title": "CLIP: Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns from raw text descriptions of images, moving beyond training on fixed object categories. By pre-training on 400 million (image, text) pairs using caption-image matching, the method achieves state-of-the-art image representations from scratch. This allows for zero-shot transfer to various downstream tasks using natural language, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse datasets, showing competitive results against fully supervised baselines without task-specific training, and matching the accuracy of ResNet-50 on ImageNet zero-shot without using its training examples."
    },
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "This paper introduces a curriculum learning framework for image-caption pretraining, inspired by children's language learning from cognitive science, to address the challenges of aligning multiple concepts from captions to objects in images. The method starts with simple image-caption pairs and gradually increases complexity by adding more concepts, leveraging knowledge from each phase for subsequent learning. This approach outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios."
    }
  ],
  "scratch": [
    {
      "title": "CLIP: Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns from raw text descriptions of images, moving beyond training on fixed object categories. By pre-training on 400 million (image, text) pairs using caption-image matching, the method achieves state-of-the-art image representations from scratch. This allows for zero-shot transfer to various downstream tasks using natural language, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse datasets, showing competitive results against fully supervised baselines without task-specific training, and matching the accuracy of ResNet-50 on ImageNet zero-shot without using its training examples."
    },
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "This paper introduces a curriculum learning framework for image-caption pretraining, inspired by children's language learning from cognitive science, to address the challenges of aligning multiple concepts from captions to objects in images. The method starts with simple image-caption pairs and gradually increases complexity by adding more concepts, leveraging knowledge from each phase for subsequent learning. This approach outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios."
    },
    {
      "title": "Fine-tuning with Very Large Dropout",
      "url": "/research/2024/03/01/Fine-tuning-with-Very-Large-Dropout.html",
      "date": "March 1, 2024",
      "summary": "This study challenges the notion that machine learning practices assume training and testing data have the same distribution. It explores the effectiveness of high dropout rates, as opposed to ensemble techniques, in developing rich data representations suitable for multiple distribution scenarios. These representations surpass those achieved by traditional in-distribution performance regularization and the implicit sparsity induced by common stochastic gradient methods. While training deep networks from scratch with high dropout rates is impractical, fine-tuning pre-trained models under these conditions is feasible and yields better out-of-distribution performance than ensembles and model averaging techniques like model soups. This finding is significant due to the growing relevance of fine-tuning with large pre-trained models, offering insights into the nature of rich representations and the linear characteristics of fine-tuning large networks with small datasets."
    }
  ],
  "baseline": [
    {
      "title": "CLIP: Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns from raw text descriptions of images, moving beyond training on fixed object categories. By pre-training on 400 million (image, text) pairs using caption-image matching, the method achieves state-of-the-art image representations from scratch. This allows for zero-shot transfer to various downstream tasks using natural language, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse datasets, showing competitive results against fully supervised baselines without task-specific training, and matching the accuracy of ResNet-50 on ImageNet zero-shot without using its training examples."
    },
    {
      "title": "Better plain ViT baselines for ImageNet-1k",
      "url": "/research/2022/05/02/Better-plain-ViT-baselines-for-ImageNet-1k.html",
      "date": "May 2, 2022",
      "summary": "This paper challenges the common belief that the Vision Transformer (ViT) model requires sophisticated regularization techniques to excel on ImageNet-1k scale data. The authors present minor modifications to the original ViT vanilla training setting that significantly improve the performance of plain ViT models. They find that standard data augmentation is sufficient, with 90 epochs of training surpassing 76% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reaching 80% in less than one day."
    },
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "This paper introduces FineAction, a large-scale, fine-grained video dataset designed to address the limitations of current temporal action localization (TAL) benchmarks, which rely on coarse action classes and lead to model overfitting and ambiguous annotations. FineAction contains 103K instances across 106 action categories in 17K videos, offering a diverse and densely annotated dataset with co-occurring actions that pose new challenges for TAL. The authors evaluate popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and propose a baseline method achieving a 13.17% mAP. FineAction aims to advance TAL research and is accessible online."
    },
    {
      "title": "Normalized Attention Without Probability Cage",
      "url": "/research/2020/05/19/Normalized-Attention-Without-Probability-Cage.html",
      "date": "May 19, 2020",
      "summary": "Softmax-attention architectures, especially popularized by Transformers, have seen significant advancements in various tasks. However, the geometric implications of softmax-attention remain underexplored. In this study, we demonstrate limitations arising from constraining attention weights to the probability simplex and its impact on the convex hull of value vectors. We reveal sequence length-dependent biases in Transformers towards token isolation at initialization and compare them with max- and sum-pooling, which are strong but often overlooked baselines. To address these issues, we propose a novel approach of replacing softmax with normalization in self-attention, resulting in a robust and widely applicable architecture. Our findings are supported by empirical results from over 25,000 trained models, and all results and implementations are publicly available."
    },
    {
      "title": "A Phase Transition between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention",
      "url": "/research/2024/02/06/A-Phase-Transition-between-Positional-and-Semantic-Learning-in-a-Solvable-Model-of-Dot-Product-Attention.html",
      "date": "February 6, 2024",
      "summary": "We explore the learning process of a dot-product attention layer, which learns both positional and semantic attention matrices, enabling tokens to attend based on position or meaning. Through experiments on an algorithmic task, we demonstrate that this architecture can use either mechanism for solving the task. Theoretically, we examine a non-linear self-attention layer with special query and key matrices, offering a closed-form solution for its non-convex loss landscape in high-dimensional data, which reveals a phase transition from positional to semantic mechanisms as sample complexity increases. We also show that the dot-product attention layer surpasses a linear positional baseline through the semantic mechanism with adequate data."
    },
    {
      "title": "HYPO: Hyperspherical Out-of-Distribution Generalization",
      "url": "/research/2024/03/19/HYPO-Hyperspherical-Out-of-Distribution-Generalization.html",
      "date": "March 19, 2024",
      "summary": "We propose HYPO, a novel framework for out-of-distribution (OOD) generalization in machine learning, which learns domain-invariant features in a hyperspherical space. Our method focuses on aligning features of the same class across different domains close to their class prototypes and separating different class prototypes maximally. We offer theoretical justifications for its improvement on OOD generalization and show through experiments on OOD benchmarks that HYPO outperforms existing baselines with superior performance."
    }
  ],
  "using": [
    {
      "title": "CLIP: Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns from raw text descriptions of images, moving beyond training on fixed object categories. By pre-training on 400 million (image, text) pairs using caption-image matching, the method achieves state-of-the-art image representations from scratch. This allows for zero-shot transfer to various downstream tasks using natural language, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse datasets, showing competitive results against fully supervised baselines without task-specific training, and matching the accuracy of ResNet-50 on ImageNet zero-shot without using its training examples."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "This paper introduces FixMatch, a streamlined semi-supervised learning (SSL) algorithm. FixMatch generates pseudo-labels using a model's predictions on weakly-augmented unlabeled images, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness, and the code is publicly available."
    },
    {
      "title": "Language Modeling with Gated Convolutional Networks",
      "url": "/research/2017/09/08/Language-Modeling-with-Gated-Convolutional-Networks.html",
      "date": "September 8, 2017",
      "summary": "This paper introduces a novel language modeling approach using stacked convolutions that enable efficient parallel processing, in contrast to the traditionally used recurrent neural networks known for handling unbounded context. The proposed method features a simplified gating mechanism that outperforms previous models and demonstrates superior performance on the WikiText-103 benchmark, showing its capability to manage long-term dependencies. It also delivers competitive results on the Google Billion Words benchmark and significantly reduces sentence scoring latency compared to recurrent models. This marks the first instance of a non-recurrent model achieving comparable success to strong recurrent models on major language tasks."
    },
    {
      "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum",
      "url": "/research/2023/09/30/When-to-Learn-What:-Model-Adaptive-Data-Augmentation-Curriculum.html",
      "date": "September 30, 2023",
      "summary": "Data augmentation (DA) improves neural network generalization by enforcing invariances and symmetries to predefined transformations applied to input data. However, fixed augmentation policies affect samples differently at various training stages, and existing approaches cannot adapt policies to individual samples and the training model. This paper proposes Model-Adaptive Data Augmentation (MADAug) which trains an augmentation policy network to determine when to learn what. Unlike previous work, MADAug selects augmentation operators for each input image with a model-adaptive policy that varies between training stages, creating an optimized data augmentation curriculum. The policy is trained using a bi-level optimization scheme to minimize validation-set loss of a model trained with policy-produced augmentations. Extensive evaluations on multiple image classification tasks and network architectures show that MADAug outperforms or matches existing DA approaches, enhances fairness by improving all classes, particularly the difficult ones, and performs better when transferred to fine-grained datasets. Additionally, the auto-optimized policy in MADAug gradually increases perturbations, forming an easy-to-hard curriculum."
    }
  ],
  "localize": [
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "This paper introdices LOST, a method for unsupervised object localization in images. LOST utilizes activation features from a self-supervised pre-trained vision transformer. Unlike other methods, LOST works on individual images without relying on external object proposals or image collection exploration. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, demonstrating its effectiveness in unsupervised object discovery."
    },
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper introduces the AVA dataset, which features 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. The authors highlight the challenges in action recognition and introduce a novel localization approach that surpasses existing benchmarks but shows modest performance on AVA (15.6% mAP), indicating the need for advanced video understanding methods."
    }
  ],
  "localization": [
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "This paper introdices LOST, a method for unsupervised object localization in images. LOST utilizes activation features from a self-supervised pre-trained vision transformer. Unlike other methods, LOST works on individual images without relying on external object proposals or image collection exploration. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, demonstrating its effectiveness in unsupervised object discovery."
    },
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper introduces the AVA dataset, which features 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. The authors highlight the challenges in action recognition and introduce a novel localization approach that surpasses existing benchmarks but shows modest performance on AVA (15.6% mAP), indicating the need for advanced video understanding methods."
    },
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "This paper introduces FineAction, a large-scale, fine-grained video dataset designed to address the limitations of current temporal action localization (TAL) benchmarks, which rely on coarse action classes and lead to model overfitting and ambiguous annotations. FineAction contains 103K instances across 106 action categories in 17K videos, offering a diverse and densely annotated dataset with co-occurring actions that pose new challenges for TAL. The authors evaluate popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and propose a baseline method achieving a 13.17% mAP. FineAction aims to advance TAL research and is accessible online."
    },
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "The study presents a novel approach for detection identification of Holstein-Friesian cattle. This technique offers a completely hands-off solution for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training when new cattle are introduced. The system utilizes convolutional neural networks and deep metric learning, achieving high accuracy with a 93.8% success rate in identifying cattle unseen during training, using only half of the population."
    },
    {
      "title": "Deep Residual Learning for Image Recognition",
      "url": "/research/2015/12/10/Deep-Residual-Learning-for-Image-Recognition.html",
      "date": "December 10, 2015",
      "summary": "We introduce a residual learning framework to train deeper neural networks effectively. By reformulating layers to learn residual functions relative to inputs, our approach facilitates optimization and achieves higher accuracy with increased depth. Evaluations on ImageNet demonstrate the effectiveness of residual networks up to 152 layers deep, outperforming previous architectures. Our method achieves 3.57% error on the ImageNet test set, winning 1st place in the ILSVRC 2015 classification task. Additionally, our deep representations lead to a 28% improvement on the COCO object detection dataset. In the ILSVRC & COCO 2015 competitions, our approach secured 1st place in ImageNet detection, localization, COCO detection, and segmentation tasks."
    }
  ],
  "collection": [
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "This paper introdices LOST, a method for unsupervised object localization in images. LOST utilizes activation features from a self-supervised pre-trained vision transformer. Unlike other methods, LOST works on individual images without relying on external object proposals or image collection exploration. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, demonstrating its effectiveness in unsupervised object discovery."
    }
  ],
  "corloc": [
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "This paper introdices LOST, a method for unsupervised object localization in images. LOST utilizes activation features from a self-supervised pre-trained vision transformer. Unlike other methods, LOST works on individual images without relying on external object proposals or image collection exploration. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, demonstrating its effectiveness in unsupervised object discovery."
    }
  ],
  "pascal": [
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "This paper introdices LOST, a method for unsupervised object localization in images. LOST utilizes activation features from a self-supervised pre-trained vision transformer. Unlike other methods, LOST works on individual images without relying on external object proposals or image collection exploration. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, demonstrating its effectiveness in unsupervised object discovery."
    }
  ],
  "detector": [
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "This paper introdices LOST, a method for unsupervised object localization in images. LOST utilizes activation features from a self-supervised pre-trained vision transformer. Unlike other methods, LOST works on individual images without relying on external object proposals or image collection exploration. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, demonstrating its effectiveness in unsupervised object discovery."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    }
  ],
  "presents": [
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. MAE involves masking random patches of an input image and reconstructing the missing pixels using an asymmetric encoder-decoder architecture and a lightweight decoder. Masking a significant portion of the input image creates a challenging yet informative self-supervisory task. These innovations allow for efficient training of large models, tripling training speed and enhancing accuracy. A vanilla ViT-Huge model reaches top accuracy (87.8%) on ImageNet-1K data among similar methods. The model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating promising scaling potential."
    }
  ],
  "portion": [
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. MAE involves masking random patches of an input image and reconstructing the missing pixels using an asymmetric encoder-decoder architecture and a lightweight decoder. Masking a significant portion of the input image creates a challenging yet informative self-supervisory task. These innovations allow for efficient training of large models, tripling training speed and enhancing accuracy. A vanilla ViT-Huge model reaches top accuracy (87.8%) on ImageNet-1K data among similar methods. The model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating promising scaling potential."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "This paper introduces Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperformed CLIP models on various downstream tasks. The speedup from this method enables experimentation with larger model sizes, more data, or longer training periods."
    }
  ],
  "triple": [
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. MAE involves masking random patches of an input image and reconstructing the missing pixels using an asymmetric encoder-decoder architecture and a lightweight decoder. Masking a significant portion of the input image creates a challenging yet informative self-supervisory task. These innovations allow for efficient training of large models, tripling training speed and enhancing accuracy. A vanilla ViT-Huge model reaches top accuracy (87.8%) on ImageNet-1K data among similar methods. The model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating promising scaling potential."
    }
  ],
  "degradation": [
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. In this paper Masked Image Residual Learning (MIRL), a self-supervised learning framework, was introduced to alleviate the degradation issue and enable effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Extensive testing shows that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth."
    },
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "This paper introduces DiffBIR, a framework that utilizes pretrained text-to-image diffusion models for blind image restoration. The two-stage pipeline involves pretraining a restoration module on various degradations to enhance real-world applicability, followed by the use of latent diffusion models for realistic restoration. The authors introduce a novel injective modulation sub-network, LAControlNet, for fine-tuning and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, a controllable module allows users to adjust quality and fidelity during the denoising process. Extensive tests demonstrate DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets."
    }
  ],
  "mirl": [
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. In this paper Masked Image Residual Learning (MIRL), a self-supervised learning framework, was introduced to alleviate the degradation issue and enable effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Extensive testing shows that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth."
    }
  ],
  "alleviate": [
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. In this paper Masked Image Residual Learning (MIRL), a self-supervised learning framework, was introduced to alleviate the degradation issue and enable effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Extensive testing shows that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth."
    }
  ],
  "redefine": [
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. In this paper Masked Image Residual Learning (MIRL), a self-supervised learning framework, was introduced to alleviate the degradation issue and enable effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Extensive testing shows that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth."
    }
  ],
  "recover": [
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. In this paper Masked Image Residual Learning (MIRL), a self-supervised learning framework, was introduced to alleviate the degradation issue and enable effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Extensive testing shows that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth."
    }
  ],
  "formula": [
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) uses synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. Given this perspective, a one-instance fractal database (OFDB) is developed where only a single image per category is present. Despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M comparable or superior results are obtaine to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. A ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training. Synthetic images generated by FDSL circumvent issues associated with real images, offering a promising avenue for pre-training general models. Investigations suggest that simple object contours can match fractal-based dataset performance and increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets for formula-driven supervised learning (FDSL). The authors identify the optimal FDSL parameters and maximize synthetic image variety, which is crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. The authors demonstrate FDSL's potential for continuous improvement and its ability to avoid issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "fdsl": [
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) uses synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. Given this perspective, a one-instance fractal database (OFDB) is developed where only a single image per category is present. Despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M comparable or superior results are obtaine to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. A ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training. Synthetic images generated by FDSL circumvent issues associated with real images, offering a promising avenue for pre-training general models. Investigations suggest that simple object contours can match fractal-based dataset performance and increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets for formula-driven supervised learning (FDSL). The authors identify the optimal FDSL parameters and maximize synthetic image variety, which is crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. The authors demonstrate FDSL's potential for continuous improvement and its ability to avoid issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "fractal": [
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) uses synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. Given this perspective, a one-instance fractal database (OFDB) is developed where only a single image per category is present. Despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M comparable or superior results are obtaine to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning."
    }
  ],
  "generating": [
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) uses synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. Given this perspective, a one-instance fractal database (OFDB) is developed where only a single image per category is present. Despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M comparable or superior results are obtaine to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning."
    },
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "The paper investigates the use of synthetic data generated by graphics simulators for pre-training computer vision models. The authors find that model performance on downstream tasks varies with different simulation parameters used to generate the synthetic data. They introduce Task2Sim, a model that maps the requirements of a downstream task to the optimal simulation parameters for generating synthetic pre-training data."
    },
    {
      "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
      "url": "/research/2024/02/28/CL-MAE:-Curriculum-Learned-Masked-Autoencoders.html",
      "date": "February 28, 2024",
      "summary": "This paper proposes a curriculum learning approach that updates the masking strategy of Masked Auto Encoders (MAE) to progressively increase the complexity of the self-supervised reconstruction task. To achieve this, a novel learnable masking module is introduced, capable of generating masks of varying complexities, and integrated into masked autoencoders (MAE). This module is jointly trained with the MAE, adjusting its behavior from partner (optimizing the same reconstruction loss) to adversary (optimizing the opposite loss), with a smooth transition regulated by a factor multiplied with the reconstruction loss. This training procedure creates an easy-to-hard curriculum. The Curriculum-Learned Masked Autoencoder (CL-MAE) is trained on ImageNet and demonstrates superior representation learning compared to MAE. Empirical results on five downstream tasks confirm that curriculum learning can successfully self-supervise masked autoencoders."
    }
  ],
  "act": [
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) uses synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. Given this perspective, a one-instance fractal database (OFDB) is developed where only a single image per category is present. Despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M comparable or superior results are obtaine to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
      "url": "/research/2023/11/30/Dichotomy-of-Early-and-Late-Phase-Implicit-Biases-Can-Provably-Induce-Grokking.html",
      "date": "November 30, 2023",
      "summary": "Power et al. (2022) discovered that neural networks initially memorize arithmetic tasks, achieving perfect training accuracy but poor test accuracy, then suddenly achieve perfect test accuracy after extended training. This study explains this grokking phenomenon with the theory that early and late phase implicit biases cause this shift. Training homogeneous neural networks with large initialization and small weight decay on classification and regression tasks results in a prolonged period where the network acts like a kernel predictor, followed by a sudden shift to min-norm/max-margin predictors, significantly improving test accuracy."
    }
  ],
  "database": [
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) uses synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. Given this perspective, a one-instance fractal database (OFDB) is developed where only a single image per category is present. Despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M comparable or superior results are obtaine to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning."
    }
  ],
  "ofdb": [
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) uses synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. Given this perspective, a one-instance fractal database (OFDB) is developed where only a single image per category is present. Despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M comparable or superior results are obtaine to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning."
    }
  ],
  "ofdbs": [
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) uses synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. Given this perspective, a one-instance fractal database (OFDB) is developed where only a single image per category is present. Despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M comparable or superior results are obtaine to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning."
    }
  ],
  "obtaine": [
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) uses synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. Given this perspective, a one-instance fractal database (OFDB) is developed where only a single image per category is present. Despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M comparable or superior results are obtaine to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning."
    }
  ],
  "replacing": [
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. A ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training. Synthetic images generated by FDSL circumvent issues associated with real images, offering a promising avenue for pre-training general models. Investigations suggest that simple object contours can match fractal-based dataset performance and increasing pre-training task difficulty enhances fine-tuning accuracy."
    }
  ],
  "contour": [
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. A ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training. Synthetic images generated by FDSL circumvent issues associated with real images, offering a promising avenue for pre-training general models. Investigations suggest that simple object contours can match fractal-based dataset performance and increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets for formula-driven supervised learning (FDSL). The authors identify the optimal FDSL parameters and maximize synthetic image variety, which is crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. The authors demonstrate FDSL's potential for continuous improvement and its ability to avoid issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "circumvent": [
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. A ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training. Synthetic images generated by FDSL circumvent issues associated with real images, offering a promising avenue for pre-training general models. Investigations suggest that simple object contours can match fractal-based dataset performance and increasing pre-training task difficulty enhances fine-tuning accuracy."
    }
  ],
  "associate": [
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. A ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training. Synthetic images generated by FDSL circumvent issues associated with real images, offering a promising avenue for pre-training general models. Investigations suggest that simple object contours can match fractal-based dataset performance and increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "This paper introduces an algorithm that identifies simple and highly generalizable neural networks by searching for extensive flat minima regions in the error function, where the error rate is relatively stable. Such flat minima are associated with lower overfitting risks based on minimum description length (MDL) principles. Despite requiring second-order derivative calculations, the algorithm has a complexity level comparable to backpropagation. When tested on feedforward and recurrent networks, as well as stock market prediction tasks, this algorithm outperformed traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    }
  ],
  "avenue": [
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. A ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training. Synthetic images generated by FDSL circumvent issues associated with real images, offering a promising avenue for pre-training general models. Investigations suggest that simple object contours can match fractal-based dataset performance and increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Scaling Vision Transformers to 22 Billion Parameters",
      "url": "/research/2023/02/10/Scaling-Vision-Transformers-to-22-Billion-Parameters.html",
      "date": "February 10, 2023",
      "summary": "We introduce a method for efficiently training a 22B-parameter Vision Transformer (ViT-22B) and conduct various experiments to assess its performance. Compared to existing models, ViT-22B shows improved scalability and benefits such as enhanced fairness-performance tradeoff, alignment with human visual perception, and increased robustness. Our findings suggest promising avenues for achieving large-scale language model-like capabilities in vision tasks."
    }
  ],
  "flip": [
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "This paper introduces Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperformed CLIP models on various downstream tasks. The speedup from this method enables experimentation with larger model sizes, more data, or longer training periods."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "This paper introduces Mask-Reconstruct Augmentation (MRA), an image augmentation technique that leverages self-supervised masked autoencoders to generate distorted inputs and address the issue of overfitting in deep neural networks. Inspired by the success of masked image modeling in self-supervised learning, MRA uses nonlinear transformations for regularization, as opposed to current hand-crafted linear techniques like scale, flip, and color jitter. Extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks."
    }
  ],
  "amount": [
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "This paper introduces Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperformed CLIP models on various downstream tasks. The speedup from this method enables experimentation with larger model sizes, more data, or longer training periods."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "This paper explores the interactions between training data amount, model regularization or data augmentation (AugReg), model size, and compute budget for Vision Transformers (ViT) in various vision tasks. The authors find that using more compute and AugReg can achieve the same performance as training with significantly more data. They demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    },
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This paper investigates using high-quality rendering software and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle. This synthetic dataset is used to augment limited real training data for image classification and semantic segmentation tasks. While models trained solely on synthetic images showed low accuracy on real validation data, including even small amounts of real data significantly improved performance. Augmenting real training data with synthetic images outperformed using only real images. Furthermore, pretraining models on the synthetic dataset before transfer learning significantly reduced training costs by allowing most of the training to be completed upfront using the synthetic data."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "The paper introduces a system that trains deep neural networks for object detection using only synthetic images generated with domain randomization. This involves randomizing non-realistic simulator parameters like lighting and object textures, allowing the network to identify essential object features. Their findings show that networks can achieve impressive performance using just synthetic data, and further improve with fine-tuning on real data. This suggests the potential of using cost-effective synthetic data for training, avoiding the need to acquire vast amounts of real-world data or create detailed synthetic environments. They validate their method on car bounding box detection using the KITTI dataset."
    }
  ],
  "usage": [
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "This paper introduces Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperformed CLIP models on various downstream tasks. The speedup from this method enables experimentation with larger model sizes, more data, or longer training periods."
    },
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/research/2022/10/10/Turbo-Training-with-Token-Dropout.html",
      "date": "October 10, 2022",
      "summary": "This paper introduces Turbo training, an efficient method for training Transformers on video-related tasks. The authors present a simple yet versatile training paradigm applicable to multiple video tasks and demonstrate its effectiveness across action classification, video-language representation learning, and long-video activity classification. Turbo training achieves competitive performance with up to 4× speed-up and reduced memory usage, enabling long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous resource-intensive methods.."
    },
    {
      "title": "Eigencurve: Optimal Learning Rate Schedule For Sgd on Quadratic Objectives With Skewed Hessian Spectrums",
      "url": "/research/2022/06/14/Eigencurve-Optimal-Learning-Rate-Schedule-For-Sgd-on-Quadratic-Objectives-With-Skewed-Hessian-Spectrums.html",
      "date": "June 14, 2022",
      "summary": "Learning rate schedulers are essential in deep neural network training, but there's a gap between practical usage and theoretical understanding. This paper introduces Eigencurve, the first set of learning rate schedules achieving minimax optimal convergence rates for SGD on quadratic objectives with skewed eigenvalue distributions of the Hessian matrix. This condition is common in practice. Experimental results on CIFAR-10 image classification tasks demonstrate Eigencurve's superiority over step decay, particularly with fewer epochs. The theory inspires practical schedulers approximating Eigencurve, resembling cosine decay for some problems while outperforming it in others."
    }
  ],
  "speedup": [
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "This paper introduces Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperformed CLIP models on various downstream tasks. The speedup from this method enables experimentation with larger model sizes, more data, or longer training periods."
    },
    {
      "title": "Exponentially Faster Language Modeling",
      "url": "/research/2023/11/21/Exponentially-Faster-Language-Modeling.html",
      "date": "November 21, 2023",
      "summary": "UltraFastBERT, a BERT variant, operates with just 0.3% of its neurons—selectively using 12 out of 4095 neurons per layer—for inference, matching the performance of similar models. It replaces conventional feedforward networks with fast feedforward networks (FFFs) to achieve this efficiency. Although fully efficient conditional neural execution isn't yet practical, we offer a high-level CPU code that achieves a 78x speedup and a PyTorch implementation with a 40x speedup over standard batched feedforward inference."
    }
  ],
  "period": [
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "This paper introduces Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperformed CLIP models on various downstream tasks. The speedup from this method enables experimentation with larger model sizes, more data, or longer training periods."
    },
    {
      "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
      "url": "/research/2023/11/30/Dichotomy-of-Early-and-Late-Phase-Implicit-Biases-Can-Provably-Induce-Grokking.html",
      "date": "November 30, 2023",
      "summary": "Power et al. (2022) discovered that neural networks initially memorize arithmetic tasks, achieving perfect training accuracy but poor test accuracy, then suddenly achieve perfect test accuracy after extended training. This study explains this grokking phenomenon with the theory that early and late phase implicit biases cause this shift. Training homogeneous neural networks with large initialization and small weight decay on classification and regression tasks results in a prolonged period where the network acts like a kernel predictor, followed by a sudden shift to min-norm/max-margin predictors, significantly improving test accuracy."
    }
  ],
  "anything": [
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model consists of two parallel transformer encoders (text prompt and image) and a shared mask decoder. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. "
    }
  ],
  "sa": [
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model consists of two parallel transformer encoders (text prompt and image) and a shared mask decoder. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. "
    }
  ],
  "date": [
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model consists of two parallel transformer encoders (text prompt and image) and a shared mask decoder. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. "
    }
  ],
  "privacy": [
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model consists of two parallel transformer encoders (text prompt and image) and a shared mask decoder. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. "
    }
  ],
  "respecting": [
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model consists of two parallel transformer encoders (text prompt and image) and a shared mask decoder. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. "
    }
  ],
  "consist": [
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model consists of two parallel transformer encoders (text prompt and image) and a shared mask decoder. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. "
    },
    {
      "title": "Automated Learning Rate Scheduler for Large-batch",
      "url": "/research/2021/07/13/Automated-Learning-Rate-Scheduler-for-Large-batch.html",
      "date": "July 13, 2021",
      "summary": "Large-batch training is crucial for deep learning with large datasets and models, but requires specific learning rate (LR) schedules for optimal performance, especially under limited training epochs. This work introduces an automated LR scheduling algorithm for large-batch neural network training within a fixed epoch budget, consisting of adaptive warmup and predefined decay phases. The LR is dynamically adjusted based on training loss monitored through Gaussian process smoothing, facilitating low computational overhead. When integrated with adaptive stochastic optimizers like AdamP and LAMB, this scheduler eliminates the need for extensive hyperparameter tuning and achieves competitive or superior results on various image classification tasks across different batch sizes and architectures."
    }
  ],
  "cut": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    }
  ],
  "treat": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    }
  ],
  "token": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This paper is effectively a follow-up to DinoV2. The study addresses the issue of artifacts in feature maps of Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. The authors introduce a straightforward strategy of adding extra tokens to the input sequence of the Vision Transformer, which eliminates these artifacts for both supervised and self-supervised models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks, facilitates object discovery with larger models, and results in smoother feature and attention maps for downstream visual processing tasks."
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel normalization technique for self-supervised transformer training that normalizes the [CLS] token and regular tokens separately. The authors find that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks compared to previous models like masked autoencoders (MAE) that use a single normalization layer for all tokens."
    },
    {
      "title": "Training data-efficient image transformers & distillation through attention",
      "url": "/research/2021/01/15/Training-data-efficient-image-transformers-&-distillation-through-attention.html",
      "date": "January 15, 2021",
      "summary": "This paper introduces DeiT, an efficient method for training vision transformers. The authors present a unique teacher-student strategy for transformers, leveraging a distillation token for efficient learning from a convolutional network teacher. This method achieves performance on par with convolutional networks, with up to 85.2% accuracy on ImageNet, and demonstrates effective transferability to other tasks."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    },
    {
      "title": "FNet: Mixing Tokens with Fourier Transforms",
      "url": "/research/2022/05/26/FNet-Mixing-Tokens-with-Fourier-Transforms.html",
      "date": "May 26, 2022",
      "summary": "We demonstrate that Transformer encoder architectures can be accelerated with minimal impact on accuracy by substituting self-attention sublayers with simple linear transformations. Remarkably, using a standard Fourier Transform instead of the self-attention sublayer in a Transformer encoder achieves 92-97% of BERT models' accuracy on the GLUE benchmark, while training 80% faster on GPUs and 70% faster on TPUs for standard 512 input lengths. Our FNet model significantly outperforms in speed at longer input lengths, matching the accuracy of the most accurate models on the Long Range Arena benchmark and surpassing the fastest models in speed across all sequence lengths on GPUs and shorter lengths on TPUs. Additionally, FNet is lightweight in memory use and exceptionally efficient in smaller sizes, outperforming Transformer models under the same speed and accuracy constraints."
    },
    {
      "title": "A Phase Transition between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention",
      "url": "/research/2024/02/06/A-Phase-Transition-between-Positional-and-Semantic-Learning-in-a-Solvable-Model-of-Dot-Product-Attention.html",
      "date": "February 6, 2024",
      "summary": "We explore the learning process of a dot-product attention layer, which learns both positional and semantic attention matrices, enabling tokens to attend based on position or meaning. Through experiments on an algorithmic task, we demonstrate that this architecture can use either mechanism for solving the task. Theoretically, we examine a non-linear self-attention layer with special query and key matrices, offering a closed-form solution for its non-convex loss landscape in high-dimensional data, which reveals a phase transition from positional to semantic mechanisms as sample complexity increases. We also show that the dot-product attention layer surpasses a linear positional baseline through the semantic mechanism with adequate data."
    }
  ],
  "node": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    },
    {
      "title": "Large Batch Training of Convolutional Networks",
      "url": "/research/2017/09/13/Large-Batch-Training-of-Convolutional-Networks.html",
      "date": "September 13, 2017",
      "summary": "To expedite the training of large convolutional networks, computational units are added and trained with data-parallel synchronous Stochastic Gradient Descent (SGD) across units, increasing batch size with node count. However, larger batch sizes can reduce model accuracy. The existing method of large batch training—linear learning rate scaling with warm-up—is not universally effective and may cause training divergence. To address these challenges, we introduce a novel training algorithm, Layer-wise Adaptive Rate Scaling (LARS), enabling us to train Alexnet with a batch size of 8K and Resnet-50 with a batch size of 32K, without compromising accuracy."
    }
  ],
  "graph": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel normalization technique for self-supervised transformer training that normalizes the [CLS] token and regular tokens separately. The authors find that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks compared to previous models like masked autoencoders (MAE) that use a single normalization layer for all tokens."
    },
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    }
  ],
  "represent": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "This paper introduces a novel approach to Radiance Field methods for novel-view synthesis that addresses the challenges of high visual quality, costly training, and real-time rendering of unbounded, complete scenes at 1080p resolution. The authors propose three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. The method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/2020/06/17/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "June 17, 2020",
      "summary": "Implicit neural representations, parameterized by continuous, differentiable neural networks, offer numerous advantages over conventional representations. However, existing architectures struggle with fine detail and fail to capture spatial and temporal derivatives, crucial for many physical signals described by partial differential equations. We introduce sinusoidal representation networks (SIRENs) utilizing periodic activation functions, ideal for representing complex signals and their derivatives. Analyzing SIREN activation statistics informs a principled initialization scheme. We demonstrate SIRENs' efficacy in representing various signals and solving boundary value problems, including Eikonal, Poisson, Helmholtz, and wave equations. Additionally, we integrate SIRENs with hypernetworks to learn priors over the SIREN function space. Visit the project website for detailed demonstrations."
    },
    {
      "title": "DeepNet: Scaling Transformers to 1,000 Layers",
      "url": "/research/2022/03/01/DeepNet-Scaling-Transformers-to-1,000-Layers.html",
      "date": "March 1, 2022",
      "summary": "This paper introduces DEEPNORM, a novel method to stabilize extremely deep Transformers through a new normalization function and a theoretically derived initialization for the residual connection. DEEPNORM offers a stable and efficient training regime, blending the benefits of Post-LN's performance with Pre-LN's stability, allowing for the scaling of Transformers up to 1,000 layers without difficulty. This represents a significant advancement over previous models, with a 200-layer, 3.2B parameter model outperforming a 48-layer, 12B parameter state-of-the-art model by 5 BLEU points on a multilingual benchmark with 7,482 translation directions, suggesting a promising direction for scaling."
    }
  ],
  "eigendecomposition": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    }
  ],
  "foreground": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    }
  ],
  "eigenvector": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    },
    {
      "title": "How Does Sharpness-Aware Minimization Minimize Sharpness?",
      "url": "/research/2023/01/05/How-Does-Sharpness-Aware-Minimization-Minimize-Sharpness.html",
      "date": "January 5, 2023",
      "summary": "Sharpness-Aware Minimization (SAM) enhances deep neural networks' generalization across various settings. Although SAM aims to penalize model sharpness using a computationally efficient approach, its exact operational mechanism and the sharpness notion it regularizes remain unclear, partly due to differing sharpness concepts used in its theoretical framework and empirical validation. This study identifies the precise sharpness concept SAM regulates and explains its mechanism. It reveals that the combined effect of SAM's two-step approximations, despite being individually misleading, correctly improves generalization when using full-batch gradients. Moreover, we demonstrate that the stochastic SAM version indeed regularizes a third sharpness notion, aligning more closely with practical performance. This effectiveness is attributed to the gradient's alignment with the Hessian's top eigenvector under SAM."
    }
  ],
  "voc07": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    }
  ],
  "voc12": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    }
  ],
  "coco20k": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    }
  ],
  "cad": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    },
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This paper investigates using high-quality rendering software and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle. This synthetic dataset is used to augment limited real training data for image classification and semantic segmentation tasks. While models trained solely on synthetic images showed low accuracy on real validation data, including even small amounts of real data significantly improved performance. Augmenting real training data with synthetic images outperformed using only real images. Furthermore, pretraining models on the synthetic dataset before transfer learning significantly reduced training costs by allowing most of the training to be completed upfront using the synthetic data."
    }
  ],
  "saliency": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    }
  ],
  "cub": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This paper presents a graph-based method for object discovery in images using self-supervised transformer features, specifically those trained with self-distillation loss (DINO). The authors treat visual tokens as nodes within a weighted graph, with edges representing similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, they segment foreground objects based on the second smallest eigenvector. The method outperforms the state-of-the-art LOST on VOC07, VOC12, and COCO20K datasets, and incorporating a second stage class-agnostic detector (CAD) further improves performance. The approach also extends to unsupervised saliency detection and achieves competitive results in weakly supervised object detection on CUB and ImageNet."
    }
  ],
  "geometry": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "This paper investigates the geometric and statistical properties of representations across layers in large transformers used for self-supervised learning on various data types. The authors observe common evolution patterns, with data manifolds expanding initially, contracting at intermediate layers, and the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. They find that semantic information peaks after initial expansion, a trend consistent across models and datasets. The study proposes an unsupervised method to identify layers richest in semantic content, suggesting that those at a relative ID minimum are optimal for downstream tasks."
    },
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    }
  ],
  "hidden": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "This paper investigates the geometric and statistical properties of representations across layers in large transformers used for self-supervised learning on various data types. The authors observe common evolution patterns, with data manifolds expanding initially, contracting at intermediate layers, and the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. They find that semantic information peaks after initial expansion, a trend consistent across models and datasets. The study proposes an unsupervised method to identify layers richest in semantic content, suggesting that those at a relative ID minimum are optimal for downstream tasks."
    }
  ],
  "evolution": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "This paper investigates the geometric and statistical properties of representations across layers in large transformers used for self-supervised learning on various data types. The authors observe common evolution patterns, with data manifolds expanding initially, contracting at intermediate layers, and the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. They find that semantic information peaks after initial expansion, a trend consistent across models and datasets. The study proposes an unsupervised method to identify layers richest in semantic content, suggesting that those at a relative ID minimum are optimal for downstream tasks."
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey provides a comprehensive review of transformer-based visual segmentation, a crucial field for various applications. The authors cover the evolution from convolutional methods to vision transformers, offering a unified framework to simplify understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas such as 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. The authors re-evaluate these methods on established datasets, outline current challenges, and suggest future research directions."
    },
    {
      "title": "A comprehensive review of Binary Neural Network",
      "url": "/research/2023/05/11/A-comprehensive-review-of-Binary-Neural-Network.html",
      "date": "May 11, 2023",
      "summary": "Deep learning (DL) is widely used in intelligent systems and real-life applications, but its deployment on computationally limited and energy-constrained devices requires efficient technologies like Binary Neural Networks (BNN). BNNs save significant storage, computation cost, and energy, making them ideal for small devices, despite the trade-offs in memory and performance. This article offers a comprehensive review of BNN developments, focusing on 1-bit activations and weights. It covers the evolution of BNN, from early models to advanced algorithms and design aspects, and addresses BNN optimization, deployment, computing architectures, and diverse applications. It also outlines potential research directions in BNN technology."
    }
  ],
  "manifold": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "This paper investigates the geometric and statistical properties of representations across layers in large transformers used for self-supervised learning on various data types. The authors observe common evolution patterns, with data manifolds expanding initially, contracting at intermediate layers, and the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. They find that semantic information peaks after initial expansion, a trend consistent across models and datasets. The study proposes an unsupervised method to identify layers richest in semantic content, suggesting that those at a relative ID minimum are optimal for downstream tasks."
    }
  ],
  "contract": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "This paper investigates the geometric and statistical properties of representations across layers in large transformers used for self-supervised learning on various data types. The authors observe common evolution patterns, with data manifolds expanding initially, contracting at intermediate layers, and the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. They find that semantic information peaks after initial expansion, a trend consistent across models and datasets. The study proposes an unsupervised method to identify layers richest in semantic content, suggesting that those at a relative ID minimum are optimal for downstream tasks."
    },
    {
      "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
      "url": "/research/2024/01/24/Better-Call-GPT,-Comparing-Large-Language-Models-Against-Lawyers.html",
      "date": "January 24, 2024",
      "summary": "This paper compares Large Language Models (LLMs) with traditional legal contract reviewers—Junior Lawyers and Legal Process Outsourcers (LPOs). It evaluates whether LLMs outperform humans in accuracy, speed, and cost-efficiency during contract review. Empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, revealing that advanced models match or exceed human accuracy in identifying legal issues. LLMs complete reviews in seconds, vastly outpacing the hours required by humans. Cost-wise, LLMs achieve a 99.97 percent reduction compared to traditional methods. These findings indicate a transformative shift in legal practice, with LLMs enhancing the accessibility and efficiency of legal services. The research suggests that LLM dominance in legal contract review challenges the status quo, necessitating a reimagined future for legal workflows."
    }
  ],
  "id": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "This paper investigates the geometric and statistical properties of representations across layers in large transformers used for self-supervised learning on various data types. The authors observe common evolution patterns, with data manifolds expanding initially, contracting at intermediate layers, and the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. They find that semantic information peaks after initial expansion, a trend consistent across models and datasets. The study proposes an unsupervised method to identify layers richest in semantic content, suggesting that those at a relative ID minimum are optimal for downstream tasks."
    }
  ],
  "stabilize": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "This paper investigates the geometric and statistical properties of representations across layers in large transformers used for self-supervised learning on various data types. The authors observe common evolution patterns, with data manifolds expanding initially, contracting at intermediate layers, and the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. They find that semantic information peaks after initial expansion, a trend consistent across models and datasets. The study proposes an unsupervised method to identify layers richest in semantic content, suggesting that those at a relative ID minimum are optimal for downstream tasks."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "This paper demonstrates that artificial neural networks (ANNs) are equivalent to Gaussian processes at initialization in the infinite-width limit and introduces the Neural Tangent Kernel (NTK), which describes ANNs' behavior during training. The NTK stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. The authors prove the positive-definiteness of the limiting NTK under certain conditions and show that the network function follows a linear differential equation during training for least-squares regression. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    },
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "This paper investigates the importance of the warm-up stage in training transformers, and the impact of layer normalization placement. Using mean field theory, the authors demonstrate that the original Post-LN Transformer's design results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    },
    {
      "title": "DeepNet: Scaling Transformers to 1,000 Layers",
      "url": "/research/2022/03/01/DeepNet-Scaling-Transformers-to-1,000-Layers.html",
      "date": "March 1, 2022",
      "summary": "This paper introduces DEEPNORM, a novel method to stabilize extremely deep Transformers through a new normalization function and a theoretically derived initialization for the residual connection. DEEPNORM offers a stable and efficient training regime, blending the benefits of Post-LN's performance with Pre-LN's stability, allowing for the scaling of Transformers up to 1,000 layers without difficulty. This represents a significant advancement over previous models, with a 200-layer, 3.2B parameter model outperforming a 48-layer, 12B parameter state-of-the-art model by 5 BLEU points on a multilingual benchmark with 7,482 translation directions, suggesting a promising direction for scaling."
    }
  ],
  "peak": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "This paper investigates the geometric and statistical properties of representations across layers in large transformers used for self-supervised learning on various data types. The authors observe common evolution patterns, with data manifolds expanding initially, contracting at intermediate layers, and the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. They find that semantic information peaks after initial expansion, a trend consistent across models and datasets. The study proposes an unsupervised method to identify layers richest in semantic content, suggesting that those at a relative ID minimum are optimal for downstream tasks."
    }
  ],
  "expansion": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "This paper investigates the geometric and statistical properties of representations across layers in large transformers used for self-supervised learning on various data types. The authors observe common evolution patterns, with data manifolds expanding initially, contracting at intermediate layers, and the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. They find that semantic information peaks after initial expansion, a trend consistent across models and datasets. The study proposes an unsupervised method to identify layers richest in semantic content, suggesting that those at a relative ID minimum are optimal for downstream tasks."
    },
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "This paper introduces a Fourier neural network (FNN) that aligns with Fourier decomposition by utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design allows for easy integration with more complex networks for data processing tasks. The authors demonstrate the FNN's efficacy on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits of the FNN include solution validity beyond the training scope, model interpretability, and ease of use."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    }
  ],
  "trend": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "This paper investigates the geometric and statistical properties of representations across layers in large transformers used for self-supervised learning on various data types. The authors observe common evolution patterns, with data manifolds expanding initially, contracting at intermediate layers, and the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. They find that semantic information peaks after initial expansion, a trend consistent across models and datasets. The study proposes an unsupervised method to identify layers richest in semantic content, suggesting that those at a relative ID minimum are optimal for downstream tasks."
    },
    {
      "title": "Stable Diffusion 3: Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "This paper focuses on improving diffusion models for high-dimensional data like images. The authors enhance noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Their large-scale study shows superior performance over established diffusion methods in high-resolution text-to-image synthesis. A new transformer-based architecture is introduced for better text comprehension, typography, and human preference in text-to-image generation. The findings demonstrate predictable scaling trends, with lower validation loss tied to improved synthesis. The largest models outperform state-of-the-art approaches."
    },
    {
      "title": "From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning",
      "url": "/research/2023/03/16/From-MNIST-to-ImageNet-and-Back:-Benchmarking-Continual-Curriculum-Learning.html",
      "date": "March 16, 2023",
      "summary": "Continual learning (CL) is a promising trend in machine learning, aiming to develop robust models and strategies for dynamic environments by incorporating new knowledge while retaining past knowledge. CL research is fragmented, with various protocols, tasks, datasets, and metrics, often not reflecting real-world complexity and tailored to specific strategies. This work addresses this gap by introducing two novel CL benchmarks with heterogeneous tasks from six image datasets, varying in complexity and quality. These benchmarks fairly evaluate state-of-the-art CL strategies in scenarios closer to real-world conditions. The benchmarks present tasks in both increasing and decreasing complexity, assessing models' ability to exploit task structure. The work emphasizes a rigorous, reproducible evaluation protocol for measuring generalization and memory retention in models. Experimental results show that popular CL strategies perform poorly on these benchmarks, exhibit high forgetting, and struggle with curriculum task ordering. These findings underscore the need for rigorous comparisons and the development of new CL strategies for complex scenarios."
    },
    {
      "title": "The Platonic Representation Hypothesis",
      "url": "/research/2024/05/13/The-Platonic-Representation-Hypothesis.html",
      "date": "May 13, 2024",
      "summary": "This paper argues that AI model representations, especially in deep networks, are converging. It surveys examples in the literature showing how neural network representations align over time and across domains. It demonstrates convergence across data modalities, noting that as vision and language models grow, they measure distances between data points similarly. The authors hypothesize that this convergence leads to a shared statistical model of reality, termed the platonic representation and discuss possible selective pressures toward it. The paper also addresses the implications, limitations, and counterexamples to this trend."
    }
  ],
  "tinyvit": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces TinyViT, a series of compact and efficient vision transformers (ViTs) designed for devices with limited resources. The authors employ a fast distillation framework to transfer knowledge from large pretrained models to smaller ones during pretraining, using sparsified logits from teacher models to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. With increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters, and demonstrates strong transferability across various downstream tasks."
    }
  ],
  "series": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces TinyViT, a series of compact and efficient vision transformers (ViTs) designed for devices with limited resources. The authors employ a fast distillation framework to transfer knowledge from large pretrained models to smaller ones during pretraining, using sparsified logits from teacher models to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. With increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters, and demonstrates strong transferability across various downstream tasks."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "This paper introduces a Fourier neural network (FNN) that aligns with Fourier decomposition by utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design allows for easy integration with more complex networks for data processing tasks. The authors demonstrate the FNN's efficacy on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits of the FNN include solution validity beyond the training scope, model interpretability, and ease of use."
    },
    {
      "title": "Chronos: Learning the Language of Time Series",
      "url": "/research/2024/03/12/Chronos-Learning-the-Language-of-Time-Series.html",
      "date": "March 12, 2024",
      "summary": "Chronos is a framework that enhances pretrained probabilistic time series models by tokenizing time series data for training with transformer-based architectures, notably the T5 family. Pretrained on a mix of public and synthetic datasets created via Gaussian processes, Chronos models outshine others in 42 benchmark datasets by showing superior performance on familiar datasets and competitive or better zero-shot capabilities on new datasets. This illustrates Chronos's ability to generalize across different domains, offering a simpler approach to forecasting tasks."
    }
  ],
  "compact": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces TinyViT, a series of compact and efficient vision transformers (ViTs) designed for devices with limited resources. The authors employ a fast distillation framework to transfer knowledge from large pretrained models to smaller ones during pretraining, using sparsified logits from teacher models to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. With increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters, and demonstrates strong transferability across various downstream tasks."
    }
  ],
  "device": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces TinyViT, a series of compact and efficient vision transformers (ViTs) designed for devices with limited resources. The authors employ a fast distillation framework to transfer knowledge from large pretrained models to smaller ones during pretraining, using sparsified logits from teacher models to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. With increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters, and demonstrates strong transferability across various downstream tasks."
    },
    {
      "title": "A comprehensive review of Binary Neural Network",
      "url": "/research/2023/05/11/A-comprehensive-review-of-Binary-Neural-Network.html",
      "date": "May 11, 2023",
      "summary": "Deep learning (DL) is widely used in intelligent systems and real-life applications, but its deployment on computationally limited and energy-constrained devices requires efficient technologies like Binary Neural Networks (BNN). BNNs save significant storage, computation cost, and energy, making them ideal for small devices, despite the trade-offs in memory and performance. This article offers a comprehensive review of BNN developments, focusing on 1-bit activations and weights. It covers the evolution of BNN, from early models to advanced algorithms and design aspects, and addresses BNN optimization, deployment, computing architectures, and diverse applications. It also outlines potential research directions in BNN technology."
    }
  ],
  "authors": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces TinyViT, a series of compact and efficient vision transformers (ViTs) designed for devices with limited resources. The authors employ a fast distillation framework to transfer knowledge from large pretrained models to smaller ones during pretraining, using sparsified logits from teacher models to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. With increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters, and demonstrates strong transferability across various downstream tasks."
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey provides a comprehensive review of transformer-based visual segmentation, a crucial field for various applications. The authors cover the evolution from convolutional methods to vision transformers, offering a unified framework to simplify understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas such as 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. The authors re-evaluate these methods on established datasets, outline current challenges, and suggest future research directions."
    },
    {
      "title": "SinLU: Sinu-Sigmoidal Linear Uni",
      "url": "/research/2021/12/02/SinLU-Sinu-Sigmoidal-Linear-Uni.html",
      "date": "December 2, 2021",
      "summary": "This paper introduces a non-linear activation function called the Sinu-sigmoidal Linear Unit (SinLU), formulated as SinLU(x) = (x + a sin bx) · σ(x). It incorporates a sine wave for added functionality over traditional linear units, with two trainable parameters controlling the sinusoidal participation. The authors compare SinLU's performance to ReLU, GELU, and SiLU across various domains, models, and standard datasets. The results demonstrate SinLU's robustness and superior performance due to the incorporation of the trainable sine wave parameters, facilitating easy training and fast convergence."
    }
  ],
  "logit": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces TinyViT, a series of compact and efficient vision transformers (ViTs) designed for devices with limited resources. The authors employ a fast distillation framework to transfer knowledge from large pretrained models to smaller ones during pretraining, using sparsified logits from teacher models to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. With increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters, and demonstrates strong transferability across various downstream tasks."
    }
  ],
  "counterpart": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces TinyViT, a series of compact and efficient vision transformers (ViTs) designed for devices with limited resources. The authors employ a fast distillation framework to transfer knowledge from large pretrained models to smaller ones during pretraining, using sparsified logits from teacher models to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. With increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters, and demonstrates strong transferability across various downstream tasks."
    }
  ],
  "comparable": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces TinyViT, a series of compact and efficient vision transformers (ViTs) designed for devices with limited resources. The authors employ a fast distillation framework to transfer knowledge from large pretrained models to smaller ones during pretraining, using sparsified logits from teacher models to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. With increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters, and demonstrates strong transferability across various downstream tasks."
    }
  ],
  "swin": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces TinyViT, a series of compact and efficient vision transformers (ViTs) designed for devices with limited resources. The authors employ a fast distillation framework to transfer knowledge from large pretrained models to smaller ones during pretraining, using sparsified logits from teacher models to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. With increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters, and demonstrates strong transferability across various downstream tasks."
    },
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "The paper focuses on evaluating the performance of recent advancements in image transformers, such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, on the Tiny ImageNet dataset for transfer learning tasks. While these models are typically trained on large datasets like ImageNet-21k and then fine-tuned on ImageNet-1k, their assessments often overlook the Tiny ImageNet benchmark. The study updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. This underscores the importance of evaluating these models on the Tiny ImageNet dataset for transfer learning performance."
    },
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    }
  ],
  "l": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces TinyViT, a series of compact and efficient vision transformers (ViTs) designed for devices with limited resources. The authors employ a fast distillation framework to transfer knowledge from large pretrained models to smaller ones during pretraining, using sparsified logits from teacher models to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. With increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters, and demonstrates strong transferability across various downstream tasks."
    },
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    }
  ],
  "cover": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey provides a comprehensive review of transformer-based visual segmentation, a crucial field for various applications. The authors cover the evolution from convolutional methods to vision transformers, offering a unified framework to simplify understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas such as 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. The authors re-evaluate these methods on established datasets, outline current challenges, and suggest future research directions."
    },
    {
      "title": "A comprehensive review of Binary Neural Network",
      "url": "/research/2023/05/11/A-comprehensive-review-of-Binary-Neural-Network.html",
      "date": "May 11, 2023",
      "summary": "Deep learning (DL) is widely used in intelligent systems and real-life applications, but its deployment on computationally limited and energy-constrained devices requires efficient technologies like Binary Neural Networks (BNN). BNNs save significant storage, computation cost, and energy, making them ideal for small devices, despite the trade-offs in memory and performance. This article offers a comprehensive review of BNN developments, focusing on 1-bit activations and weights. It covers the evolution of BNN, from early models to advanced algorithms and design aspects, and addresses BNN optimization, deployment, computing architectures, and diverse applications. It also outlines potential research directions in BNN technology."
    }
  ],
  "cloud": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey provides a comprehensive review of transformer-based visual segmentation, a crucial field for various applications. The authors cover the evolution from convolutional methods to vision transformers, offering a unified framework to simplify understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas such as 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. The authors re-evaluate these methods on established datasets, outline current challenges, and suggest future research directions."
    }
  ],
  "foundation": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey provides a comprehensive review of transformer-based visual segmentation, a crucial field for various applications. The authors cover the evolution from convolutional methods to vision transformers, offering a unified framework to simplify understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas such as 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. The authors re-evaluate these methods on established datasets, outline current challenges, and suggest future research directions."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    }
  ],
  "outline": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey provides a comprehensive review of transformer-based visual segmentation, a crucial field for various applications. The authors cover the evolution from convolutional methods to vision transformers, offering a unified framework to simplify understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas such as 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. The authors re-evaluate these methods on established datasets, outline current challenges, and suggest future research directions."
    },
    {
      "title": "A comprehensive review of Binary Neural Network",
      "url": "/research/2023/05/11/A-comprehensive-review-of-Binary-Neural-Network.html",
      "date": "May 11, 2023",
      "summary": "Deep learning (DL) is widely used in intelligent systems and real-life applications, but its deployment on computationally limited and energy-constrained devices requires efficient technologies like Binary Neural Networks (BNN). BNNs save significant storage, computation cost, and energy, making them ideal for small devices, despite the trade-offs in memory and performance. This article offers a comprehensive review of BNN developments, focusing on 1-bit activations and weights. It covers the evolution of BNN, from early models to advanced algorithms and design aspects, and addresses BNN optimization, deployment, computing architectures, and diverse applications. It also outlines potential research directions in BNN technology."
    }
  ],
  "turbo": [
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/research/2022/10/10/Turbo-Training-with-Token-Dropout.html",
      "date": "October 10, 2022",
      "summary": "This paper introduces Turbo training, an efficient method for training Transformers on video-related tasks. The authors present a simple yet versatile training paradigm applicable to multiple video tasks and demonstrate its effectiveness across action classification, video-language representation learning, and long-video activity classification. Turbo training achieves competitive performance with up to 4× speed-up and reduced memory usage, enabling long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous resource-intensive methods.."
    }
  ],
  "video": [
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/research/2022/10/10/Turbo-Training-with-Token-Dropout.html",
      "date": "October 10, 2022",
      "summary": "This paper introduces Turbo training, an efficient method for training Transformers on video-related tasks. The authors present a simple yet versatile training paradigm applicable to multiple video tasks and demonstrate its effectiveness across action classification, video-language representation learning, and long-video activity classification. Turbo training achieves competitive performance with up to 4× speed-up and reduced memory usage, enabling long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous resource-intensive methods.."
    },
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper introduces the AVA dataset, which features 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. The authors highlight the challenges in action recognition and introduce a novel localization approach that surpasses existing benchmarks but shows modest performance on AVA (15.6% mAP), indicating the need for advanced video understanding methods."
    },
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "This paper introduces FineAction, a large-scale, fine-grained video dataset designed to address the limitations of current temporal action localization (TAL) benchmarks, which rely on coarse action classes and lead to model overfitting and ambiguous annotations. FineAction contains 103K instances across 106 action categories in 17K videos, offering a diverse and densely annotated dataset with co-occurring actions that pose new challenges for TAL. The authors evaluate popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and propose a baseline method achieving a 13.17% mAP. FineAction aims to advance TAL research and is accessible online."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces sinusoidal representation networks (SIRENs), which utilize periodic activation functions like Sin to effectively capture complex natural signals and their derivatives, addressing the limitations of neural networks parameterized for continuous, differentiable signal representations. The authors' analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. SIRENs are also applied in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations. The authors extend SIRENs' use with hypernetworks to learn priors for SIREN functions."
    },
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "This paper introduces the Kinetics Human Action Video dataset. The authors note that the limited number of videos in existing datasets like UCF-101 and HMDB-51 hinders the ability to effectively evaluate architectures due to similar performance across these small benchmarks. Kinetics contains 400 classes with over 400 video clips per class from challenging YouTube videos. The paper analyzes the impact of this larger dataset on the performance of existing architectures as well as the benefits of pre-training models on Kinetics. The authors introduce the Two-Stream Inflated 3D ConvNet (I3D), an extension of 2D ConvNets to 3D for improved video feature extraction. When pre-trained on Kinetics, this I3D model sets new benchmarks for action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    },
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "This paper introduces SlowFast networks, a new convolutional architecture for video recognition tasks. These networks have two pathways: a Slow pathway operating at low frame rates to capture spatial semantics, and a Fast pathway working at high frame rates with reduced channel capacity to efficiently capture motion details. The SlowFast models achieve top accuracy on major video benchmarks like Kinetics, Charades, and AVA, significantly improving performance for action classification and detection tasks."
    },
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "The authors introduce VideoMAE, a data-efficient self-supervised video pre-training (SSVP) approach that utilizes video masked autoencoders with a novel high-ratio video tube masking technique inspired by ImageMAE. Key findings include the effectiveness of high masking ratios (90-95%) due to video's temporal redundancy, strong performance on small datasets (~3k-4k videos) without extra data highlighting the importance of high-level structure learning, and data quality being more crucial than quantity with domain shift being significant. Notably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone without extra data."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    },
    {
      "title": "Neural Network Diffusion",
      "url": "/research/2024/02/20/Neural-Network-Diffusion.html",
      "date": "February 20, 2024",
      "summary": "Diffusion models, known for their success in image and video generation, can also generate high-performing neural network parameters, as demonstrated in this study. By employing a simple combination of an autoencoder and a standard latent diffusion model, our method involves extracting latent representations of network parameters, which are then synthesized from random noise by the diffusion model. These new representations, processed through the autoencoder's decoder, serve as fresh network parameters. Tested across various architectures and datasets, this diffusion approach consistently produces models with comparable or superior performance to traditionally trained networks at minimal extra cost. Importantly, the generated models show distinct performance differences from the trained ones, suggesting further exploration into the versatile applications of diffusion models."
    },
    {
      "title": "V-Jepa: Latent Video Prediction For Visual Representation Learning",
      "url": "/research/2023/02/15/V-Jepa-Latent-Video-Prediction-For-Visual-Representation-Learning.html",
      "date": "February 15, 2023",
      "summary": "This paper introduces V-JEPA, a self-supervised video learning method that predicts masked spatio-temporal regions in latent space, effectively applying the masked-modelling principle of large language models to video. This approach generates visual features useful across various image and video tasks without needing model adjustment, achieving significant improvements on Kinetics-400 (82.1%) and Something-Something-v2 (71.2%) benchmarks, outperforming prior video models. V-JEPA also excels in motion understanding tasks, surpassing leading image models like DINOv2 and OpenCLIP, and achieves 77.9% on ImageNet classification with video training alone, setting a new standard for video models."
    },
    {
      "title": "VideoMamba: State Space Model for Efficient Video Understanding",
      "url": "/research/2024/03/12/VideoMamba-State-Space-Model-for-Efficient-Video-Understanding.html",
      "date": "March 12, 2024",
      "summary": "This work introduces VideoMamba, an innovative approach that addresses local redundancy and global dependencies in video understanding by adapting the Mamba framework to video analysis. VideoMamba outperforms existing 3D convolutional neural networks and video transformers through its linear-complexity operator, which facilitates efficient long-term modeling for high-resolution, long-duration videos. Its effectiveness is demonstrated across four main areas: scalability in the visual domain without needing extensive dataset pretraining, thanks to a novel self-distillation technique; the ability to recognize short-term actions with fine-grained motion differences; superior performance in long-term video understanding compared to traditional feature-based models; and robust compatibility with multiple modalities, enhancing multi-modal video analysis. VideoMamba establishes a new standard for comprehensive and efficient video understanding."
    }
  ],
  "relate": [
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/research/2022/10/10/Turbo-Training-with-Token-Dropout.html",
      "date": "October 10, 2022",
      "summary": "This paper introduces Turbo training, an efficient method for training Transformers on video-related tasks. The authors present a simple yet versatile training paradigm applicable to multiple video tasks and demonstrate its effectiveness across action classification, video-language representation learning, and long-video activity classification. Turbo training achieves competitive performance with up to 4× speed-up and reduced memory usage, enabling long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous resource-intensive methods.."
    },
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    },
    {
      "title": "The loss landscape of deep linear neural networks: a second-order analysis",
      "url": "/research/2022/03/11/The-loss-landscape-of-deep-linear-neural-networks-a-second-order-analysis.html",
      "date": "March 11, 2022",
      "summary": "This study investigates the optimization landscape of deep linear neural networks with square loss, focusing on the role of non-strict saddle points in algorithm dynamics, which has been previously understudied. the authors conduct a comprehensive second-order analysis, identifying global minimizers, strict, and non-strict saddle points among all critical points, alongside their critical values. Our findings, based on conditions related to the ranks of partial matrix products, contribute insights into global convergence and implicit regularization observed in optimizing linear neural networks. Additionally, the authors offer an explicit parameterization of global minimizers and identify extensive sets of strict and non-strict saddle points."
    }
  ],
  "action": [
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/research/2022/10/10/Turbo-Training-with-Token-Dropout.html",
      "date": "October 10, 2022",
      "summary": "This paper introduces Turbo training, an efficient method for training Transformers on video-related tasks. The authors present a simple yet versatile training paradigm applicable to multiple video tasks and demonstrate its effectiveness across action classification, video-language representation learning, and long-video activity classification. Turbo training achieves competitive performance with up to 4× speed-up and reduced memory usage, enabling long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous resource-intensive methods.."
    },
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper introduces the AVA dataset, which features 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. The authors highlight the challenges in action recognition and introduce a novel localization approach that surpasses existing benchmarks but shows modest performance on AVA (15.6% mAP), indicating the need for advanced video understanding methods."
    },
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "This paper introduces FineAction, a large-scale, fine-grained video dataset designed to address the limitations of current temporal action localization (TAL) benchmarks, which rely on coarse action classes and lead to model overfitting and ambiguous annotations. FineAction contains 103K instances across 106 action categories in 17K videos, offering a diverse and densely annotated dataset with co-occurring actions that pose new challenges for TAL. The authors evaluate popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and propose a baseline method achieving a 13.17% mAP. FineAction aims to advance TAL research and is accessible online."
    },
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "This paper introduces the Kinetics Human Action Video dataset. The authors note that the limited number of videos in existing datasets like UCF-101 and HMDB-51 hinders the ability to effectively evaluate architectures due to similar performance across these small benchmarks. Kinetics contains 400 classes with over 400 video clips per class from challenging YouTube videos. The paper analyzes the impact of this larger dataset on the performance of existing architectures as well as the benefits of pre-training models on Kinetics. The authors introduce the Two-Stream Inflated 3D ConvNet (I3D), an extension of 2D ConvNets to 3D for improved video feature extraction. When pre-trained on Kinetics, this I3D model sets new benchmarks for action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    },
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions, SiLU and dSiLU. It challenges the need for experience replay and separate target networks in deep reinforcement learning. By using on-policy learning with eligibility traces and softmax action selection, it achieves state-of-the-art results in stochastic SZ-Tetris and small-board Tetris with TD(λ) learning and shallow dSiLU agents. It also outperforms DQN in Atari 2600 with a deep Sarsa(λ) agent using SiLU/dSiLU, suggesting an alternative to traditional DQN approaches."
    },
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "This paper introduces SlowFast networks, a new convolutional architecture for video recognition tasks. These networks have two pathways: a Slow pathway operating at low frame rates to capture spatial semantics, and a Fast pathway working at high frame rates with reduced channel capacity to efficiently capture motion details. The SlowFast models achieve top accuracy on major video benchmarks like Kinetics, Charades, and AVA, significantly improving performance for action classification and detection tasks."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article provides an extensive review of vision transformer techniques applied specifically to human action recognition. These action transformers are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also examines optimizing spatio-temporal attention in transformers for longer sequences and different learning strategies like self-supervised and zero-shot learning with their respective loss functions. Additionally, it assesses progress in benchmark evaluation scores and discusses challenges and future directions in this research area."
    },
    {
      "title": "VideoMamba: State Space Model for Efficient Video Understanding",
      "url": "/research/2024/03/12/VideoMamba-State-Space-Model-for-Efficient-Video-Understanding.html",
      "date": "March 12, 2024",
      "summary": "This work introduces VideoMamba, an innovative approach that addresses local redundancy and global dependencies in video understanding by adapting the Mamba framework to video analysis. VideoMamba outperforms existing 3D convolutional neural networks and video transformers through its linear-complexity operator, which facilitates efficient long-term modeling for high-resolution, long-duration videos. Its effectiveness is demonstrated across four main areas: scalability in the visual domain without needing extensive dataset pretraining, thanks to a novel self-distillation technique; the ability to recognize short-term actions with fine-grained motion differences; superior performance in long-term video understanding compared to traditional feature-based models; and robust compatibility with multiple modalities, enhancing multi-modal video analysis. VideoMamba establishes a new standard for comprehensive and efficient video understanding."
    }
  ],
  "activity": [
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/research/2022/10/10/Turbo-Training-with-Token-Dropout.html",
      "date": "October 10, 2022",
      "summary": "This paper introduces Turbo training, an efficient method for training Transformers on video-related tasks. The authors present a simple yet versatile training paradigm applicable to multiple video tasks and demonstrate its effectiveness across action classification, video-language representation learning, and long-video activity classification. Turbo training achieves competitive performance with up to 4× speed-up and reduced memory usage, enabling long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous resource-intensive methods.."
    }
  ],
  "inferring": [
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper investigates how self-supervised pretraining methods learn part-aware representations. The authors describe contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting that these methods predispose encoders to recognize object parts. Through empirical comparison, they find that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    }
  ],
  "predispose": [
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper investigates how self-supervised pretraining methods learn part-aware representations. The authors describe contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting that these methods predispose encoders to recognize object parts. Through empirical comparison, they find that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    }
  ],
  "uap": [
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/research/2021/08/31/Universal-Adversarial-Robustness-of-Texture-and-Shape-biased-Models.html",
      "date": "August 31, 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    }
  ],
  "retain": [
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/research/2021/08/31/Universal-Adversarial-Robustness-of-Texture-and-Shape-biased-Models.html",
      "date": "August 31, 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    },
    {
      "title": "From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning",
      "url": "/research/2023/03/16/From-MNIST-to-ImageNet-and-Back:-Benchmarking-Continual-Curriculum-Learning.html",
      "date": "March 16, 2023",
      "summary": "Continual learning (CL) is a promising trend in machine learning, aiming to develop robust models and strategies for dynamic environments by incorporating new knowledge while retaining past knowledge. CL research is fragmented, with various protocols, tasks, datasets, and metrics, often not reflecting real-world complexity and tailored to specific strategies. This work addresses this gap by introducing two novel CL benchmarks with heterogeneous tasks from six image datasets, varying in complexity and quality. These benchmarks fairly evaluate state-of-the-art CL strategies in scenarios closer to real-world conditions. The benchmarks present tasks in both increasing and decreasing complexity, assessing models' ability to exploit task structure. The work emphasizes a rigorous, reproducible evaluation protocol for measuring generalization and memory retention in models. Experimental results show that popular CL strategies perform poorly on these benchmarks, exhibit high forgetting, and struggle with curriculum task ordering. These findings underscore the need for rigorous comparisons and the development of new CL strategies for complex scenarios."
    }
  ],
  "register": [
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This paper is effectively a follow-up to DinoV2. The study addresses the issue of artifacts in feature maps of Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. The authors introduce a straightforward strategy of adding extra tokens to the input sequence of the Vision Transformer, which eliminates these artifacts for both supervised and self-supervised models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks, facilitates object discovery with larger models, and results in smoother feature and attention maps for downstream visual processing tasks."
    }
  ],
  "follow": [
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This paper is effectively a follow-up to DinoV2. The study addresses the issue of artifacts in feature maps of Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. The authors introduce a straightforward strategy of adding extra tokens to the input sequence of the Vision Transformer, which eliminates these artifacts for both supervised and self-supervised models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks, facilitates object discovery with larger models, and results in smoother feature and attention maps for downstream visual processing tasks."
    },
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "This paper introduces DiffBIR, a framework that utilizes pretrained text-to-image diffusion models for blind image restoration. The two-stage pipeline involves pretraining a restoration module on various degradations to enhance real-world applicability, followed by the use of latent diffusion models for realistic restoration. The authors introduce a novel injective modulation sub-network, LAControlNet, for fine-tuning and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, a controllable module allows users to adjust quality and fidelity during the denoising process. Extensive tests demonstrate DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "This paper demonstrates that artificial neural networks (ANNs) are equivalent to Gaussian processes at initialization in the infinite-width limit and introduces the Neural Tangent Kernel (NTK), which describes ANNs' behavior during training. The NTK stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. The authors prove the positive-definiteness of the limiting NTK under certain conditions and show that the network function follows a linear differential equation during training for least-squares regression. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    },
    {
      "title": "How to decay your learning rate",
      "url": "/research/2021/05/23/How-to-decay-your-learning-rate.html",
      "date": "May 23, 2021",
      "summary": "Empirical findings suggest that typical fine-tuned learning rate schedules decay the learning rate following weight norm fluctuations. This led to the development of ABEL, an automatic scheduler that adjusts the learning rate based on weight norm changes. ABEL performs comparably to tuned schedules but demonstrates greater robustness to parameter variations. Extensive experiments across various domains reveal that when the weight norm remains stable, simplified schedules yield equivalent performance to complex ones, resembling a constant learning rate with decay towards the end of training."
    },
    {
      "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
      "url": "/research/2023/11/30/Dichotomy-of-Early-and-Late-Phase-Implicit-Biases-Can-Provably-Induce-Grokking.html",
      "date": "November 30, 2023",
      "summary": "Power et al. (2022) discovered that neural networks initially memorize arithmetic tasks, achieving perfect training accuracy but poor test accuracy, then suddenly achieve perfect test accuracy after extended training. This study explains this grokking phenomenon with the theory that early and late phase implicit biases cause this shift. Training homogeneous neural networks with large initialization and small weight decay on classification and regression tasks results in a prolonged period where the network acts like a kernel predictor, followed by a sudden shift to min-norm/max-margin predictors, significantly improving test accuracy."
    }
  ],
  "artifact": [
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This paper is effectively a follow-up to DinoV2. The study addresses the issue of artifacts in feature maps of Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. The authors introduce a straightforward strategy of adding extra tokens to the input sequence of the Vision Transformer, which eliminates these artifacts for both supervised and self-supervised models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks, facilitates object discovery with larger models, and results in smoother feature and attention maps for downstream visual processing tasks."
    }
  ],
  "appear": [
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This paper is effectively a follow-up to DinoV2. The study addresses the issue of artifacts in feature maps of Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. The authors introduce a straightforward strategy of adding extra tokens to the input sequence of the Vision Transformer, which eliminates these artifacts for both supervised and self-supervised models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks, facilitates object discovery with larger models, and results in smoother feature and attention maps for downstream visual processing tasks."
    }
  ],
  "resolve": [
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This paper is effectively a follow-up to DinoV2. The study addresses the issue of artifacts in feature maps of Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. The authors introduce a straightforward strategy of adding extra tokens to the input sequence of the Vision Transformer, which eliminates these artifacts for both supervised and self-supervised models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks, facilitates object discovery with larger models, and results in smoother feature and attention maps for downstream visual processing tasks."
    }
  ],
  "atom": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets for formula-driven supervised learning (FDSL). The authors identify the optimal FDSL parameters and maximize synthetic image variety, which is crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. The authors demonstrate FDSL's potential for continuous improvement and its ability to avoid issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "wave": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets for formula-driven supervised learning (FDSL). The authors identify the optimal FDSL parameters and maximize synthetic image variety, which is crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. The authors demonstrate FDSL's potential for continuous improvement and its ability to avoid issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces sinusoidal representation networks (SIRENs), which utilize periodic activation functions like Sin to effectively capture complex natural signals and their derivatives, addressing the limitations of neural networks parameterized for continuous, differentiable signal representations. The authors' analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. SIRENs are also applied in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations. The authors extend SIRENs' use with hypernetworks to learn priors for SIREN functions."
    },
    {
      "title": "SinLU: Sinu-Sigmoidal Linear Uni",
      "url": "/research/2021/12/02/SinLU-Sinu-Sigmoidal-Linear-Uni.html",
      "date": "December 2, 2021",
      "summary": "This paper introduces a non-linear activation function called the Sinu-sigmoidal Linear Unit (SinLU), formulated as SinLU(x) = (x + a sin bx) · σ(x). It incorporates a sine wave for added functionality over traditional linear units, with two trainable parameters controlling the sinusoidal participation. The authors compare SinLU's performance to ReLU, GELU, and SiLU across various domains, models, and standard datasets. The results demonstrate SinLU's robustness and superior performance due to the incorporation of the trainable sine wave parameters, facilitating easy training and fast convergence."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/2020/06/17/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "June 17, 2020",
      "summary": "Implicit neural representations, parameterized by continuous, differentiable neural networks, offer numerous advantages over conventional representations. However, existing architectures struggle with fine detail and fail to capture spatial and temporal derivatives, crucial for many physical signals described by partial differential equations. We introduce sinusoidal representation networks (SIRENs) utilizing periodic activation functions, ideal for representing complex signals and their derivatives. Analyzing SIREN activation statistics informs a principled initialization scheme. We demonstrate SIRENs' efficacy in representing various signals and solving boundary value problems, including Eikonal, Poisson, Helmholtz, and wave equations. Additionally, we integrate SIRENs with hypernetworks to learn priors over the SIREN function space. Visit the project website for detailed demonstrations."
    }
  ],
  "harmonic": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets for formula-driven supervised learning (FDSL). The authors identify the optimal FDSL parameters and maximize synthetic image variety, which is crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. The authors demonstrate FDSL's potential for continuous improvement and its ability to avoid issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "variety": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets for formula-driven supervised learning (FDSL). The authors identify the optimal FDSL parameters and maximize synthetic image variety, which is crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. The authors demonstrate FDSL's potential for continuous improvement and its ability to avoid issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "visualatom": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets for formula-driven supervised learning (FDSL). The authors identify the optimal FDSL parameters and maximize synthetic image variety, which is crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. The authors demonstrate FDSL's potential for continuous improvement and its ability to avoid issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "near": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets for formula-driven supervised learning (FDSL). The authors identify the optimal FDSL parameters and maximize synthetic image variety, which is crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. The authors demonstrate FDSL's potential for continuous improvement and its ability to avoid issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "jft": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets for formula-driven supervised learning (FDSL). The authors identify the optimal FDSL parameters and maximize synthetic image variety, which is crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. The authors demonstrate FDSL's potential for continuous improvement and its ability to avoid issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "This paper explores the interactions between training data amount, model regularization or data augmentation (AugReg), model size, and compute budget for Vision Transformers (ViT) in various vision tasks. The authors find that using more compute and AugReg can achieve the same performance as training with significantly more data. They demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of Vision Transformers (ViTs) without relying on large-scale pre-training datasets like ImageNet and JFT or requiring modifications to the architecture or loss functions. The authors show that this approach improves ViT performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, while maintaining ViT's attention to relevant regions and robustness, despite ViT's inherent lack of inductive biases and typical dependence on large-scale pre-training."
    }
  ],
  "m": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets for formula-driven supervised learning (FDSL). The authors identify the optimal FDSL parameters and maximize synthetic image variety, which is crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. The authors demonstrate FDSL's potential for continuous improvement and its ability to avoid issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "This paper explores the interactions between training data amount, model regularization or data augmentation (AugReg), model size, and compute budget for Vision Transformers (ViT) in various vision tasks. The authors find that using more compute and AugReg can achieve the same performance as training with significantly more data. They demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    },
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper introduces the AVA dataset, which features 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. The authors highlight the challenges in action recognition and introduce a novel localization approach that surpasses existing benchmarks but shows modest performance on AVA (15.6% mAP), indicating the need for advanced video understanding methods."
    }
  ],
  "privacycopyright": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets for formula-driven supervised learning (FDSL). The authors identify the optimal FDSL parameters and maximize synthetic image variety, which is crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. The authors demonstrate FDSL's potential for continuous improvement and its ability to avoid issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "concern": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets for formula-driven supervised learning (FDSL). The authors identify the optimal FDSL parameters and maximize synthetic image variety, which is crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. The authors demonstrate FDSL's potential for continuous improvement and its ability to avoid issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    },
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    }
  ],
  "costserror": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets for formula-driven supervised learning (FDSL). The authors identify the optimal FDSL parameters and maximize synthetic image variety, which is crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. The authors demonstrate FDSL's potential for continuous improvement and its ability to avoid issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "biase": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets for formula-driven supervised learning (FDSL). The authors identify the optimal FDSL parameters and maximize synthetic image variety, which is crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. The authors demonstrate FDSL's potential for continuous improvement and its ability to avoid issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of Vision Transformers (ViTs) without relying on large-scale pre-training datasets like ImageNet and JFT or requiring modifications to the architecture or loss functions. The authors show that this approach improves ViT performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, while maintaining ViT's attention to relevant regions and robustness, despite ViT's inherent lack of inductive biases and typical dependence on large-scale pre-training."
    },
    {
      "title": "Are Convolutional Neural Networks or Transformers more like human vision?",
      "url": "/research/2021/07/01/Are-Convolutional-Neural-Networks-or-Transformers-more-like-human-vision.html",
      "date": "July 1, 2021",
      "summary": "This study investigates the performance of Convolutional Neural Networks (CNNs) and attention-based Vision Transformers (ViTs) in computer vision tasks. While CNNs excel in accuracy, ViTs offer a different approach with weaker inductive biases. By analyzing error patterns, we find that ViTs exhibit consistency with human errors, suggesting potential for more human-like vision models and insights into human object recognition."
    },
    {
      "title": "Normalized Attention Without Probability Cage",
      "url": "/research/2020/05/19/Normalized-Attention-Without-Probability-Cage.html",
      "date": "May 19, 2020",
      "summary": "Softmax-attention architectures, especially popularized by Transformers, have seen significant advancements in various tasks. However, the geometric implications of softmax-attention remain underexplored. In this study, we demonstrate limitations arising from constraining attention weights to the probability simplex and its impact on the convex hull of value vectors. We reveal sequence length-dependent biases in Transformers towards token isolation at initialization and compare them with max- and sum-pooling, which are strong but often overlooked baselines. To address these issues, we propose a novel approach of replacing softmax with normalization in self-attention, resulting in a robust and widely applicable architecture. Our findings are supported by empirical results from over 25,000 trained models, and all results and implementations are publicly available."
    },
    {
      "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
      "url": "/research/2023/11/30/Dichotomy-of-Early-and-Late-Phase-Implicit-Biases-Can-Provably-Induce-Grokking.html",
      "date": "November 30, 2023",
      "summary": "Power et al. (2022) discovered that neural networks initially memorize arithmetic tasks, achieving perfect training accuracy but poor test accuracy, then suddenly achieve perfect test accuracy after extended training. This study explains this grokking phenomenon with the theory that early and late phase implicit biases cause this shift. Training homogeneous neural networks with large initialization and small weight decay on classification and regression tasks results in a prolonged period where the network acts like a kernel predictor, followed by a sudden shift to min-norm/max-margin predictors, significantly improving test accuracy."
    }
  ],
  "free": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates how to achieve state-of-the-art certified adversarial robustness to ℓ2-norm bounded perturbations using only off-the-shelf pretrained models. The authors instantiate the denoised smoothing approach by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This method certifies 71% accuracy on ImageNet under adversarial perturbations constrained to an ℓ2 norm of ε = 0.5, improving upon the prior certified state-of-the-art by 14 percentage points and denoised smoothing by 30 percentage points, without requiring any fine-tuning or retraining of model parameters."
    },
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "The paper introduces TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method that applies a single augmentation to each image. Despite its minimal complexity and cost, TrivialAugment outperforms existing methods across various image classification scenarios, as validated through extensive experiments against state-of-the-art methods and multiple ablation studies involving different augmentation spaces and methods. The work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Noting a stagnation in automatic augmentation research, the authors conclude by proposing best practices for future advancements in the field."
    }
  ],
  "certify": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates how to achieve state-of-the-art certified adversarial robustness to ℓ2-norm bounded perturbations using only off-the-shelf pretrained models. The authors instantiate the denoised smoothing approach by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This method certifies 71% accuracy on ImageNet under adversarial perturbations constrained to an ℓ2 norm of ε = 0.5, improving upon the prior certified state-of-the-art by 14 percentage points and denoised smoothing by 30 percentage points, without requiring any fine-tuning or retraining of model parameters."
    }
  ],
  "2norm": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates how to achieve state-of-the-art certified adversarial robustness to ℓ2-norm bounded perturbations using only off-the-shelf pretrained models. The authors instantiate the denoised smoothing approach by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This method certifies 71% accuracy on ImageNet under adversarial perturbations constrained to an ℓ2 norm of ε = 0.5, improving upon the prior certified state-of-the-art by 14 percentage points and denoised smoothing by 30 percentage points, without requiring any fine-tuning or retraining of model parameters."
    }
  ],
  "bind": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates how to achieve state-of-the-art certified adversarial robustness to ℓ2-norm bounded perturbations using only off-the-shelf pretrained models. The authors instantiate the denoised smoothing approach by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This method certifies 71% accuracy on ImageNet under adversarial perturbations constrained to an ℓ2 norm of ε = 0.5, improving upon the prior certified state-of-the-art by 14 percentage points and denoised smoothing by 30 percentage points, without requiring any fine-tuning or retraining of model parameters."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "The paper introduces a system that trains deep neural networks for object detection using only synthetic images generated with domain randomization. This involves randomizing non-realistic simulator parameters like lighting and object textures, allowing the network to identify essential object features. Their findings show that networks can achieve impressive performance using just synthetic data, and further improve with fine-tuning on real data. This suggests the potential of using cost-effective synthetic data for training, avoiding the need to acquire vast amounts of real-world data or create detailed synthetic environments. They validate their method on car bounding box detection using the KITTI dataset."
    },
    {
      "title": "Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks",
      "url": "/research/2020/06/11/Batch-Normalization-Provably-Avoids-Rank-Collapse-for-Randomly-Initialised-Deep-Networks.html",
      "date": "June 11, 2020",
      "summary": "We investigate the challenge of training randomly initialized deep neural networks due to spectral instabilities in products of random matrices. Batch normalization emerges as an effective solution to prevent rank collapse in both linear and ReLU networks. Leveraging tools from Markov chain theory, we establish a lower rank bound for deep linear networks. Empirical findings show that this rank robustness extends to ReLU networks. Our experiments on real-world datasets underscore the significance of rank stability in training modern deep neural architectures."
    },
    {
      "title": "ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Network",
      "url": "/research/2021/06/29/ASAM-Adaptive-Sharpness-Aware-Minimization-for-Scale-Invariant-Learning-of-Deep-Neural-Network.html",
      "date": "June 29, 2021",
      "summary": "Recent advances in learning algorithms have highlighted the importance of the sharpness of the loss surface as a key indicator of the generalization gap, achieving state-of-the-art results. However, traditional sharpness measures, defined within a fixed region, suffer from sensitivity to parameter scaling, weakening their predictive value for the generalization gap. This paper introduces a scale-invariant measure, adaptive sharpness, along with a new generalization bound. We present a new learning method, adaptive sharpness-aware minimization (ASAM), which leverages this bound. Our experiments across various benchmark datasets demonstrate that ASAM significantly enhances model generalization."
    },
    {
      "title": "Atomically Accurate De Novo Design of Single-domain Antibodies",
      "url": "/research/2024/03/18/Atomically-Accurate-De-Novo-Design-of-Single-domain-Antibodies.html",
      "date": "March 18, 2024",
      "summary": "Despite the critical role antibodies play in medicine, current methods for designing new antibodies targeting specific epitopes are time-consuming. Here, we showcase the effectiveness of a refined RFdiffusion network in designing novel antibody variable heavy chains (VHH’s) to target user-specified epitopes. Through experiments, we validate the binding capability of these designed VHH's to four disease-relevant epitopes. Furthermore, the cryo-EM structure analysis reveals that a designed VHH bound to influenza hemagglutinin closely matches the design model, affirming the accuracy of our approach."
    }
  ],
  "shelf": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates how to achieve state-of-the-art certified adversarial robustness to ℓ2-norm bounded perturbations using only off-the-shelf pretrained models. The authors instantiate the denoised smoothing approach by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This method certifies 71% accuracy on ImageNet under adversarial perturbations constrained to an ℓ2 norm of ε = 0.5, improving upon the prior certified state-of-the-art by 14 percentage points and denoised smoothing by 30 percentage points, without requiring any fine-tuning or retraining of model parameters."
    }
  ],
  "instantiate": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates how to achieve state-of-the-art certified adversarial robustness to ℓ2-norm bounded perturbations using only off-the-shelf pretrained models. The authors instantiate the denoised smoothing approach by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This method certifies 71% accuracy on ImageNet under adversarial perturbations constrained to an ℓ2 norm of ε = 0.5, improving upon the prior certified state-of-the-art by 14 percentage points and denoised smoothing by 30 percentage points, without requiring any fine-tuning or retraining of model parameters."
    }
  ],
  "denoise": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates how to achieve state-of-the-art certified adversarial robustness to ℓ2-norm bounded perturbations using only off-the-shelf pretrained models. The authors instantiate the denoised smoothing approach by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This method certifies 71% accuracy on ImageNet under adversarial perturbations constrained to an ℓ2 norm of ε = 0.5, improving upon the prior certified state-of-the-art by 14 percentage points and denoised smoothing by 30 percentage points, without requiring any fine-tuning or retraining of model parameters."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. This encoder serves as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The DMAE ViT-Base model achieves comparable or superior certified accuracy with fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on ImageNet. The model also shows high transferability to CIFAR-10, indicating its broad applicability."
    }
  ],
  "denoising": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates how to achieve state-of-the-art certified adversarial robustness to ℓ2-norm bounded perturbations using only off-the-shelf pretrained models. The authors instantiate the denoised smoothing approach by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This method certifies 71% accuracy on ImageNet under adversarial perturbations constrained to an ℓ2 norm of ε = 0.5, improving upon the prior certified state-of-the-art by 14 percentage points and denoised smoothing by 30 percentage points, without requiring any fine-tuning or retraining of model parameters."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. This encoder serves as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The DMAE ViT-Base model achieves comparable or superior certified accuracy with fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on ImageNet. The model also shows high transferability to CIFAR-10, indicating its broad applicability."
    }
  ],
  "diffusion": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates how to achieve state-of-the-art certified adversarial robustness to ℓ2-norm bounded perturbations using only off-the-shelf pretrained models. The authors instantiate the denoised smoothing approach by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This method certifies 71% accuracy on ImageNet under adversarial perturbations constrained to an ℓ2 norm of ε = 0.5, improving upon the prior certified state-of-the-art by 14 percentage points and denoised smoothing by 30 percentage points, without requiring any fine-tuning or retraining of model parameters."
    },
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "This paper introduces DiffBIR, a framework that utilizes pretrained text-to-image diffusion models for blind image restoration. The two-stage pipeline involves pretraining a restoration module on various degradations to enhance real-world applicability, followed by the use of latent diffusion models for realistic restoration. The authors introduce a novel injective modulation sub-network, LAControlNet, for fine-tuning and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, a controllable module allows users to adjust quality and fidelity during the denoising process. Extensive tests demonstrate DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets."
    },
    {
      "title": "Stable Diffusion 3: Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "This paper focuses on improving diffusion models for high-dimensional data like images. The authors enhance noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Their large-scale study shows superior performance over established diffusion methods in high-resolution text-to-image synthesis. A new transformer-based architecture is introduced for better text comprehension, typography, and human preference in text-to-image generation. The findings demonstrate predictable scaling trends, with lower validation loss tied to improved synthesis. The largest models outperform state-of-the-art approaches."
    },
    {
      "title": "Neural Network Diffusion",
      "url": "/research/2024/02/20/Neural-Network-Diffusion.html",
      "date": "February 20, 2024",
      "summary": "Diffusion models, known for their success in image and video generation, can also generate high-performing neural network parameters, as demonstrated in this study. By employing a simple combination of an autoencoder and a standard latent diffusion model, our method involves extracting latent representations of network parameters, which are then synthesized from random noise by the diffusion model. These new representations, processed through the autoencoder's decoder, serve as fresh network parameters. Tested across various architectures and datasets, this diffusion approach consistently produces models with comparable or superior performance to traditionally trained networks at minimal extra cost. Importantly, the generated models show distinct performance differences from the trained ones, suggesting further exploration into the versatile applications of diffusion models."
    }
  ],
  "e": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates how to achieve state-of-the-art certified adversarial robustness to ℓ2-norm bounded perturbations using only off-the-shelf pretrained models. The authors instantiate the denoised smoothing approach by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This method certifies 71% accuracy on ImageNet under adversarial perturbations constrained to an ℓ2 norm of ε = 0.5, improving upon the prior certified state-of-the-art by 14 percentage points and denoised smoothing by 30 percentage points, without requiring any fine-tuning or retraining of model parameters."
    }
  ],
  "percentage": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates how to achieve state-of-the-art certified adversarial robustness to ℓ2-norm bounded perturbations using only off-the-shelf pretrained models. The authors instantiate the denoised smoothing approach by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This method certifies 71% accuracy on ImageNet under adversarial perturbations constrained to an ℓ2 norm of ε = 0.5, improving upon the prior certified state-of-the-art by 14 percentage points and denoised smoothing by 30 percentage points, without requiring any fine-tuning or retraining of model parameters."
    }
  ],
  "retraining": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates how to achieve state-of-the-art certified adversarial robustness to ℓ2-norm bounded perturbations using only off-the-shelf pretrained models. The authors instantiate the denoised smoothing approach by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This method certifies 71% accuracy on ImageNet under adversarial perturbations constrained to an ℓ2 norm of ε = 0.5, improving upon the prior certified state-of-the-art by 14 percentage points and denoised smoothing by 30 percentage points, without requiring any fine-tuning or retraining of model parameters."
    },
    {
      "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
      "url": "/research/2024/03/13/Simple-and-Scalable-Strategies-to-Continually-Pre-train-Large-Language-Models.html",
      "date": "March 13, 2024",
      "summary": "Large language models (LLMs) often require re-training on new data, consuming extensive computational resources. We present an efficient method that combines learning rate re-warming, re-decaying, and data replay, effectively maintaining performance without full re-training. This approach works well across different data distributions, including minor shifts (English→English) and significant shifts (English→German), tested up to 405M and 10B parameter models. Our findings suggest that continual learning strategies can update LLMs with minimal computational cost, rivaling traditional re-training methods. Additionally, we propose alternatives to the cosine learning rate schedule to reduce forgetting, offering more flexibility in learning without a fixed token budget."
    }
  ],
  "dmae": [
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. This encoder serves as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The DMAE ViT-Base model achieves comparable or superior certified accuracy with fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on ImageNet. The model also shows high transferability to CIFAR-10, indicating its broad applicability."
    }
  ],
  "semantic": [
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. This encoder serves as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The DMAE ViT-Base model achieves comparable or superior certified accuracy with fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on ImageNet. The model also shows high transferability to CIFAR-10, indicating its broad applicability."
    },
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "This paper introduces SlowFast networks, a new convolutional architecture for video recognition tasks. These networks have two pathways: a Slow pathway operating at low frame rates to capture spatial semantics, and a Fast pathway working at high frame rates with reduced channel capacity to efficiently capture motion details. The SlowFast models achieve top accuracy on major video benchmarks like Kinetics, Charades, and AVA, significantly improving performance for action classification and detection tasks."
    }
  ],
  "applicability": [
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. This encoder serves as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The DMAE ViT-Base model achieves comparable or superior certified accuracy with fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on ImageNet. The model also shows high transferability to CIFAR-10, indicating its broad applicability."
    },
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "This paper introduces DiffBIR, a framework that utilizes pretrained text-to-image diffusion models for blind image restoration. The two-stage pipeline involves pretraining a restoration module on various degradations to enhance real-world applicability, followed by the use of latent diffusion models for realistic restoration. The authors introduce a novel injective modulation sub-network, LAControlNet, for fine-tuning and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, a controllable module allows users to adjust quality and fidelity during the denoising process. Extensive tests demonstrate DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets."
    }
  ],
  "consistency": [
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "This paper introduces FixMatch, a streamlined semi-supervised learning (SSL) algorithm. FixMatch generates pseudo-labels using a model's predictions on weakly-augmented unlabeled images, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness, and the code is publicly available."
    },
    {
      "title": "Are Convolutional Neural Networks or Transformers more like human vision?",
      "url": "/research/2021/07/01/Are-Convolutional-Neural-Networks-or-Transformers-more-like-human-vision.html",
      "date": "July 1, 2021",
      "summary": "This study investigates the performance of Convolutional Neural Networks (CNNs) and attention-based Vision Transformers (ViTs) in computer vision tasks. While CNNs excel in accuracy, ViTs offer a different approach with weaker inductive biases. By analyzing error patterns, we find that ViTs exhibit consistency with human errors, suggesting potential for more human-like vision models and insights into human object recognition."
    }
  ],
  "emae": [
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    }
  ],
  "subject": [
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    }
  ],
  "ensure": [
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding",
      "url": "/research/2021/11/16/Improved-Robustness-of-Vision-Transformer-via-PreLayerNorm-in-Patch-Embedding.html",
      "date": "November 16, 2021",
      "summary": "Recent advancements in vision transformers (ViTs) have shown superior performance across diverse visual tasks, surpassing convolutional neural networks (CNNs). Given ViT's distinct architecture, understanding its behavior and reliability is imperative. This paper investigates ViT's robustness by comparing it with CNNs under various image corruptions relevant to real-world vision tasks. While ViT generally exhibits comparable or improved robustness over CNNs, it consistently underperforms in contrast enhancement tasks. Analysis suggests that positional embedding in ViT's patch embedding may malfunction with color scale changes. We propose PreLayerNorm, a modified patch embedding structure, to address this issue and ensure scale-invariant behavior in ViT. ViT with PreLayerNorm demonstrates enhanced robustness across various corruptions, particularly in contrast-varying environments."
    },
    {
      "title": "Learning Rate Perturbation: A Generic Plugin of Learning Rate schedule towards Flatter Local Minima",
      "url": "/research/2022/08/25/Learning-Rate-Perturbation-A-Generic-Plugin-of-Learning-Rate-schedule-towards-Flatter-Local-Minima.html",
      "date": "August 25, 2022",
      "summary": "Learning rate is crucial in neural network training, yet existing schedules lack theoretical backing, often leading to suboptimal choices made through trial and error. To address this, we propose LEAP, a plugin enhancing various learning rate schedules by introducing perturbations. This simple yet effective strategy favors flat minima, ensuring better generalization. Extensive experiments demonstrate LEAP's ability to improve performance across diverse datasets and learning rate schedules, including constant ones."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "url": "/research/2014/02/19/Exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks.html",
      "date": "February 19, 2014",
      "summary": "This study aims to enhance our comprehension of deep learning dynamics by analyzing deep linear neural networks. Despite their linear input-output mapping, these networks display nonlinear gradient descent dynamics, resulting in phenomena like prolonged plateaus and swift transitions to improved solutions. Through analytical exploration, we reveal that as network depth increases indefinitely, learning speed can remain finite under specific initial conditions. Unsupervised pretraining, under certain data conditions, can discover these favorable initial conditions, unlike random Gaussian initializations. We introduce a novel class of random orthogonal initial conditions that, akin to unsupervised pretraining, enable depth-independent learning. These conditions also ensure gradient propagation in deep nonlinear networks, particularly when operating at the edge of chaos."
    }
  ],
  "epoch": [
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "This paper introduces a self-supervised vision model pretraining framework inspired by energy-based models (EBMs), leveraging symmetric mappings in deep networks without auxiliary components. The framework models energy estimation and data restoration through the network's forward and backward passes, assigning low energy to unlabeled dataset samples and using gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting various pretext tasks. Experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "Better plain ViT baselines for ImageNet-1k",
      "url": "/research/2022/05/02/Better-plain-ViT-baselines-for-ImageNet-1k.html",
      "date": "May 2, 2022",
      "summary": "This paper challenges the common belief that the Vision Transformer (ViT) model requires sophisticated regularization techniques to excel on ImageNet-1k scale data. The authors present minor modifications to the original ViT vanilla training setting that significantly improve the performance of plain ViT models. They find that standard data augmentation is sufficient, with 90 epochs of training surpassing 76% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reaching 80% in less than one day."
    },
    {
      "title": "Automated Learning Rate Scheduler for Large-batch",
      "url": "/research/2021/07/13/Automated-Learning-Rate-Scheduler-for-Large-batch.html",
      "date": "July 13, 2021",
      "summary": "Large-batch training is crucial for deep learning with large datasets and models, but requires specific learning rate (LR) schedules for optimal performance, especially under limited training epochs. This work introduces an automated LR scheduling algorithm for large-batch neural network training within a fixed epoch budget, consisting of adaptive warmup and predefined decay phases. The LR is dynamically adjusted based on training loss monitored through Gaussian process smoothing, facilitating low computational overhead. When integrated with adaptive stochastic optimizers like AdamP and LAMB, this scheduler eliminates the need for extensive hyperparameter tuning and achieves competitive or superior results on various image classification tasks across different batch sizes and architectures."
    },
    {
      "title": "Don't Decay the Learning Rate, Increase the Batch Size",
      "url": "/research/2017/11/01/Don't-Decay-the-Learning-Rate,-Increase-the-Batch-Size.html",
      "date": "November 1, 2017",
      "summary": "This study demonstrates that increasing the batch size during training achieves similar learning outcomes as the common practice of decaying the learning rate, applicable to stochastic gradient descent and its variants, including with momentum and Adam optimization. This approach not only matches test accuracies within the same number of epochs but also enhances parallelism and reduces training time due to fewer parameter updates. Efficiency can be further improved by adjusting the learning rate and batch size proportionally, and although increasing the momentum coefficient and scaling the batch size accordingly may slightly lower test accuracy, it enables the use of large batch training without needing to tune hyper-parameters. Using these methods, ResNet-50 was trained on ImageNet to a 76.1% validation accuracy in less than 30 minutes."
    },
    {
      "title": "Eigencurve: Optimal Learning Rate Schedule For Sgd on Quadratic Objectives With Skewed Hessian Spectrums",
      "url": "/research/2022/06/14/Eigencurve-Optimal-Learning-Rate-Schedule-For-Sgd-on-Quadratic-Objectives-With-Skewed-Hessian-Spectrums.html",
      "date": "June 14, 2022",
      "summary": "Learning rate schedulers are essential in deep neural network training, but there's a gap between practical usage and theoretical understanding. This paper introduces Eigencurve, the first set of learning rate schedules achieving minimax optimal convergence rates for SGD on quadratic objectives with skewed eigenvalue distributions of the Hessian matrix. This condition is common in practice. Experimental results on CIFAR-10 image classification tasks demonstrate Eigencurve's superiority over step decay, particularly with fewer epochs. The theory inspires practical schedulers approximating Eigencurve, resembling cosine decay for some problems while outperforming it in others."
    }
  ],
  "tier": [
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "This paper introduces efficient masked autoencoders with self-consistency (EMAE) to enhance pre-training efficiency and prediction consistency for masked image modeling (MIM). EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module ensures consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. EMAE also demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    }
  ],
  "ebm": [
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "This paper introduces a self-supervised vision model pretraining framework inspired by energy-based models (EBMs), leveraging symmetric mappings in deep networks without auxiliary components. The framework models energy estimation and data restoration through the network's forward and backward passes, assigning low energy to unlabeled dataset samples and using gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting various pretext tasks. Experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "mapping": [
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "This paper introduces a self-supervised vision model pretraining framework inspired by energy-based models (EBMs), leveraging symmetric mappings in deep networks without auxiliary components. The framework models energy estimation and data restoration through the network's forward and backward passes, assigning low energy to unlabeled dataset samples and using gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting various pretext tasks. Experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    }
  ],
  "auxiliary": [
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "This paper introduces a self-supervised vision model pretraining framework inspired by energy-based models (EBMs), leveraging symmetric mappings in deep networks without auxiliary components. The framework models energy estimation and data restoration through the network's forward and backward passes, assigning low energy to unlabeled dataset samples and using gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting various pretext tasks. Experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "This paper proposes Hard Patches Mining (HPM), a novel framework for masked image modeling (MIM) pre-training that goes beyond simply solving given problems. HPM aims for the model to generate more challenging tasks for itself, using reconstruction loss as a metric for task difficulty. It incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "This paper introduces tools for training monocular depth estimation models with multiple datasets despite incompatible annotations. Their approach uses a robust training objective, multi-objective learning for data integration, and pretraining encoders on auxiliary tasks. Tested across five datasets, including 3D films as a novel data source, their methods achieve superior zero-shot cross-dataset generalization, outperforming existing benchmarks through principled multi-dataset training."
    }
  ],
  "energy": [
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "This paper introduces a self-supervised vision model pretraining framework inspired by energy-based models (EBMs), leveraging symmetric mappings in deep networks without auxiliary components. The framework models energy estimation and data restoration through the network's forward and backward passes, assigning low energy to unlabeled dataset samples and using gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting various pretext tasks. Experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "A comprehensive review of Binary Neural Network",
      "url": "/research/2023/05/11/A-comprehensive-review-of-Binary-Neural-Network.html",
      "date": "May 11, 2023",
      "summary": "Deep learning (DL) is widely used in intelligent systems and real-life applications, but its deployment on computationally limited and energy-constrained devices requires efficient technologies like Binary Neural Networks (BNN). BNNs save significant storage, computation cost, and energy, making them ideal for small devices, despite the trade-offs in memory and performance. This article offers a comprehensive review of BNN developments, focusing on 1-bit activations and weights. It covers the evolution of BNN, from early models to advanced algorithms and design aspects, and addresses BNN optimization, deployment, computing architectures, and diverse applications. It also outlines potential research directions in BNN technology."
    },
    {
      "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
      "url": "/research/2024/02/27/The-Era-of-1-bit-LLMs-All-Large-Language-Models-are-in-1.58-Bits.html",
      "date": "February 27, 2024",
      "summary": "Recent research introduces BitNet b1.58, a ternary (values of -1, 0, 1) 1-bit Large Language Model (LLM) that matches traditional full-precision LLMs in performance and perplexity but is significantly more efficient in terms of latency, memory, throughput, and energy. This work establishes a new scaling law and training approach for future high-performance, cost-effective LLMs, while also facilitating the design of specialized hardware optimized for 1-bit LLMs."
    }
  ],
  "restoration": [
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "This paper introduces a self-supervised vision model pretraining framework inspired by energy-based models (EBMs), leveraging symmetric mappings in deep networks without auxiliary components. The framework models energy estimation and data restoration through the network's forward and backward passes, assigning low energy to unlabeled dataset samples and using gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting various pretext tasks. Experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "This paper introduces DiffBIR, a framework that utilizes pretrained text-to-image diffusion models for blind image restoration. The two-stage pipeline involves pretraining a restoration module on various degradations to enhance real-world applicability, followed by the use of latent diffusion models for realistic restoration. The authors introduce a novel injective modulation sub-network, LAControlNet, for fine-tuning and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, a controllable module allows users to adjust quality and fidelity during the denoising process. Extensive tests demonstrate DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets."
    }
  ],
  "passes": [
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "This paper introduces a self-supervised vision model pretraining framework inspired by energy-based models (EBMs), leveraging symmetric mappings in deep networks without auxiliary components. The framework models energy estimation and data restoration through the network's forward and backward passes, assigning low energy to unlabeled dataset samples and using gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting various pretext tasks. Experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "assign": [
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "This paper introduces a self-supervised vision model pretraining framework inspired by energy-based models (EBMs), leveraging symmetric mappings in deep networks without auxiliary components. The framework models energy estimation and data restoration through the network's forward and backward passes, assigning low energy to unlabeled dataset samples and using gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting various pretext tasks. Experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "Weight Agnostic Neural Networks",
      "url": "/research/2019/09/05/Weight-Agnostic-Neural-Networks.html",
      "date": "September 5, 2019",
      "summary": "This study investigates the importance of neural network architectures versus weight parameters for task performance. We introduce a method to find architectures capable of performing tasks without weight training. By assigning random weights, we show that minimal architectures can achieve notable performance on various tasks, including reinforcement learning and MNIST classification."
    }
  ],
  "unlabeled": [
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "This paper introduces a self-supervised vision model pretraining framework inspired by energy-based models (EBMs), leveraging symmetric mappings in deep networks without auxiliary components. The framework models energy estimation and data restoration through the network's forward and backward passes, assigning low energy to unlabeled dataset samples and using gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting various pretext tasks. Experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "This paper introduces FixMatch, a streamlined semi-supervised learning (SSL) algorithm. FixMatch generates pseudo-labels using a model's predictions on weakly-augmented unlabeled images, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness, and the code is publicly available."
    },
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces an iterative self-training framework for sim-to-real 6D object pose estimation to enable cost-effective robotic bin-picking. The authors create a photo-realistic simulator to synthesize virtual data for training an initial pose estimation network (teacher model). This teacher predicts poses on unlabeled real data, and an adaptive selection scheme filters reliable predictions to generate pseudo-labels for training a student model on real data. Iteratively refining the teacher with the student improves pseudo-label quality. Evaluated on public benchmarks and a new dataset, their method shows 11.49% and 22.62% ADD(-S) improvements and a 19.54% increase in bin-picking success, demonstrating the effectiveness of this iterative sim-to-real approach."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    }
  ],
  "restore": [
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "This paper introduces a self-supervised vision model pretraining framework inspired by energy-based models (EBMs), leveraging symmetric mappings in deep networks without auxiliary components. The framework models energy estimation and data restoration through the network's forward and backward passes, assigning low energy to unlabeled dataset samples and using gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting various pretext tasks. Experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "pretext": [
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "This paper introduces a self-supervised vision model pretraining framework inspired by energy-based models (EBMs), leveraging symmetric mappings in deep networks without auxiliary components. The framework models energy estimation and data restoration through the network's forward and backward passes, assigning low energy to unlabeled dataset samples and using gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting various pretext tasks. Experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "role": [
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    },
    {
      "title": "On the Maximum Hessian Eigenvalue and Generalization",
      "url": "/research/2023/05/23/On-the-Maximum-Hessian-Eigenvalue-and-Generalization.html",
      "date": "May 23, 2023",
      "summary": "This study investigates the relationship between training interventions and the generalization of deep networks. While previous research suggests that flatter solutions generalize better than sharper ones, particularly measured by λmax, the largest eigenvalue of the Hessian of the loss, this paper challenges this notion. Through experiments, we demonstrate that larger learning rates reduce λmax for all batch sizes but do not consistently improve generalization. Additionally, scaling batch size and learning rate simultaneously can change λmax without affecting generalization. Sharpness-Aware Minimization (SAM) produces smaller λmax but does not consistently enhance generalization, especially with larger batch sizes. Excessive dropout probabilities can degrade generalization, despite promoting smaller λmax. Batch normalization, while not consistently reducing λmax, still improves generalization. These findings question λmax's role in explaining generalization in neural networks, highlighting the limits of its explanatory power."
    },
    {
      "title": "Optimal learning rate schedules in high-dimensional non-convex optimization problems",
      "url": "/research/2022/02/09/Optimal-learning-rate-schedules-in-high-dimensional-non-convex-optimization-problems.html",
      "date": "February 9, 2022",
      "summary": "This paper explores the role of learning rate schedules in high-dimensional and non-convex optimization problems, focusing on Langevin optimization with a decaying learning rate. Analyzing models with Gaussian random functions on N-dimensional spheres, the study reveals that to accelerate optimization without getting trapped in saddles, a decay rate β < 1 is optimal, contrary to convex settings where β = 1 is preferred. Introducing a signal recovery component, the dynamics involve an exploration phase navigating through rough landscape parts and a convergence phase entering convex basins. It's found optimal to maintain a large learning rate during exploration to swiftly exit non-convex regions, then transition to β = 1 for rapid convergence to the solution. These findings are validated in a neural network regression task."
    },
    {
      "title": "Atomically Accurate De Novo Design of Single-domain Antibodies",
      "url": "/research/2024/03/18/Atomically-Accurate-De-Novo-Design-of-Single-domain-Antibodies.html",
      "date": "March 18, 2024",
      "summary": "Despite the critical role antibodies play in medicine, current methods for designing new antibodies targeting specific epitopes are time-consuming. Here, we showcase the effectiveness of a refined RFdiffusion network in designing novel antibody variable heavy chains (VHH’s) to target user-specified epitopes. Through experiments, we validate the binding capability of these designed VHH's to four disease-relevant epitopes. Furthermore, the cryo-EM structure analysis reveals that a designed VHH bound to influenza hemagglutinin closely matches the design model, affirming the accuracy of our approach."
    },
    {
      "title": "The loss landscape of deep linear neural networks: a second-order analysis",
      "url": "/research/2022/03/11/The-loss-landscape-of-deep-linear-neural-networks-a-second-order-analysis.html",
      "date": "March 11, 2022",
      "summary": "This study investigates the optimization landscape of deep linear neural networks with square loss, focusing on the role of non-strict saddle points in algorithm dynamics, which has been previously understudied. the authors conduct a comprehensive second-order analysis, identifying global minimizers, strict, and non-strict saddle points among all critical points, alongside their critical values. Our findings, based on conditions related to the ranks of partial matrix products, contribute insights into global convergence and implicit regularization observed in optimizing linear neural networks. Additionally, the authors offer an explicit parameterization of global minimizers and identify extensive sets of strict and non-strict saddle points."
    }
  ],
  "studentteacher": [
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    }
  ],
  "ssl": [
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "This paper introduces FixMatch, a streamlined semi-supervised learning (SSL) algorithm. FixMatch generates pseudo-labels using a model's predictions on weakly-augmented unlabeled images, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness, and the code is publicly available."
    }
  ],
  "regularizer": [
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    }
  ],
  "rcmae": [
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    }
  ],
  "rc": [
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    }
  ],
  "convergence": [
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "This paper demonstrates that artificial neural networks (ANNs) are equivalent to Gaussian processes at initialization in the infinite-width limit and introduces the Neural Tangent Kernel (NTK), which describes ANNs' behavior during training. The NTK stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. The authors prove the positive-definiteness of the limiting NTK under certain conditions and show that the network function follows a linear differential equation during training for least-squares regression. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    },
    {
      "title": "SinLU: Sinu-Sigmoidal Linear Uni",
      "url": "/research/2021/12/02/SinLU-Sinu-Sigmoidal-Linear-Uni.html",
      "date": "December 2, 2021",
      "summary": "This paper introduces a non-linear activation function called the Sinu-sigmoidal Linear Unit (SinLU), formulated as SinLU(x) = (x + a sin bx) · σ(x). It incorporates a sine wave for added functionality over traditional linear units, with two trainable parameters controlling the sinusoidal participation. The authors compare SinLU's performance to ReLU, GELU, and SiLU across various domains, models, and standard datasets. The results demonstrate SinLU's robustness and superior performance due to the incorporation of the trainable sine wave parameters, facilitating easy training and fast convergence."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "Eigencurve: Optimal Learning Rate Schedule For Sgd on Quadratic Objectives With Skewed Hessian Spectrums",
      "url": "/research/2022/06/14/Eigencurve-Optimal-Learning-Rate-Schedule-For-Sgd-on-Quadratic-Objectives-With-Skewed-Hessian-Spectrums.html",
      "date": "June 14, 2022",
      "summary": "Learning rate schedulers are essential in deep neural network training, but there's a gap between practical usage and theoretical understanding. This paper introduces Eigencurve, the first set of learning rate schedules achieving minimax optimal convergence rates for SGD on quadratic objectives with skewed eigenvalue distributions of the Hessian matrix. This condition is common in practice. Experimental results on CIFAR-10 image classification tasks demonstrate Eigencurve's superiority over step decay, particularly with fewer epochs. The theory inspires practical schedulers approximating Eigencurve, resembling cosine decay for some problems while outperforming it in others."
    },
    {
      "title": "Optimal learning rate schedules in high-dimensional non-convex optimization problems",
      "url": "/research/2022/02/09/Optimal-learning-rate-schedules-in-high-dimensional-non-convex-optimization-problems.html",
      "date": "February 9, 2022",
      "summary": "This paper explores the role of learning rate schedules in high-dimensional and non-convex optimization problems, focusing on Langevin optimization with a decaying learning rate. Analyzing models with Gaussian random functions on N-dimensional spheres, the study reveals that to accelerate optimization without getting trapped in saddles, a decay rate β < 1 is optimal, contrary to convex settings where β = 1 is preferred. Introducing a signal recovery component, the dynamics involve an exploration phase navigating through rough landscape parts and a convergence phase entering convex basins. It's found optimal to maintain a large learning rate during exploration to swiftly exit non-convex regions, then transition to β = 1 for rapid convergence to the solution. These findings are validated in a neural network regression task."
    },
    {
      "title": "Root Mean Square Layer Normalization",
      "url": "/research/2019/10/16/Root-Mean-Square-Layer-Normalization.html",
      "date": "October 16, 2019",
      "summary": "Layer normalization (LayerNorm) enhances deep neural network stability and convergence by re-centering and re-scaling inputs and weight matrices. However, its computational overhead slows networks, particularly RNNs. We introduce RMSNorm, which replaces re-centering with root mean square (RMS) regularization. RMSNorm maintains re-scaling invariance and adapts learning rates implicitly, while being computationally simpler than LayerNorm. We also propose partial RMSNorm (pRMSNorm), estimating RMS from a subset of inputs. Empirical results across various tasks and architectures demonstrate that RMSNorm achieves comparable performance to LayerNorm while reducing running time by 7%∼64%."
    },
    {
      "title": "Sharpness-Aware Training for Free",
      "url": "/research/2023/03/02/Sharpness-Aware-Training-for-Free.html",
      "date": "March 2, 2023",
      "summary": "Modern deep neural networks (DNNs) excel in performance but often suffer from over-parameterization, leading to increased generalization error without tailored training strategies. Sharpness-Aware Minimization (SAM) has proven effective in reducing generalization error by minimizing sharpness in the loss landscape. However, SAM incurs a significant computational overhead. This paper introduces Sharpness-Aware Training for Free (SAF), which mitigates sharpness at nearly zero additional computational cost. SAF achieves this by preventing sudden drops in loss within sharp local minima during weight updates. A novel trajectory loss, based on KL-divergence between current and past DNN outputs, replaces SAM's sharpness measure, guiding convergence towards flat minima for enhanced generalization. Empirical results demonstrate SAF's effectiveness on ImageNet with comparable computational efficiency to the base optimizer."
    },
    {
      "title": "AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks",
      "url": "/research/2023/03/01/AdaSAM-Boosting-Sharpness-Aware-Minimization-with-Adaptive-Learning-Rate-and-Momentum-for-Training-Deep-Neural-Networks.html",
      "date": "March 1, 2023",
      "summary": "The Sharpness Aware Minimization (SAM) optimizer, known for enhancing the generalization of deep neural networks by introducing extra perturbation steps, is further developed into AdaSAM by integrating adaptive learning rates and momentum acceleration. Although AdaSAM has been empirically applied to large-scale networks, a theoretical understanding of its performance, considering the complexity of its components, was lacking. This study presents a theoretical analysis of AdaSAM's convergence in stochastic non-convex settings, demonstrating a convergence rate of O(1/√bT) that scales linearly with mini-batch size. By introducing a delayed second-order momentum term, the study successfully decouples and analyzes the intertwined effects of stochastic gradients, adaptive learning rates, and perturbations. This is the first work to offer a detailed convergence rate for SAM with adaptive mechanisms. Experimental results on various NLP tasks indicate AdaSAM's superior performance over SGD, AMSGrad, and SAM optimizers."
    },
    {
      "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
      "url": "/research/2022/03/19/Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training.html",
      "date": "March 19, 2022",
      "summary": "Sharpness-Aware Minimization (SAM) enhances generalization by optimizing a neighborhood-based perturbed loss, but it doesn't always favor flat minima due to both sharp and flat minima having low perturbed losses. We introduce a new measure, the surrogate gap, reflecting the dominant Hessian eigenvalue at small neighborhood radii, which is simple to compute and can be minimized during training. We propose the Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), an advancement over SAM with minimal additional computational cost. GSAM employs a dual-step approach: first, minimizing the perturbed loss similar to SAM, and second, reducing the surrogate gap without affecting the perturbed loss to target regions with low loss and sharpness, thus achieving superior generalization. GSAM is theoretically robust, showing better convergence and empirical generalization improvements, notably a +3.2% gain over SAM and +5.4% over AdamW in ImageNet accuracy for ViT-B/32."
    },
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    },
    {
      "title": "The loss landscape of deep linear neural networks: a second-order analysis",
      "url": "/research/2022/03/11/The-loss-landscape-of-deep-linear-neural-networks-a-second-order-analysis.html",
      "date": "March 11, 2022",
      "summary": "This study investigates the optimization landscape of deep linear neural networks with square loss, focusing on the role of non-strict saddle points in algorithm dynamics, which has been previously understudied. the authors conduct a comprehensive second-order analysis, identifying global minimizers, strict, and non-strict saddle points among all critical points, alongside their critical values. Our findings, based on conditions related to the ranks of partial matrix products, contribute insights into global convergence and implicit regularization observed in optimizing linear neural networks. Additionally, the authors offer an explicit parameterization of global minimizers and identify extensive sets of strict and non-strict saddle points."
    },
    {
      "title": "Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes",
      "url": "/research/2017/11/28/Towards-Understanding-Generalization-of-Deep-Learning-Perspective-of-Loss-Landscapes.html",
      "date": "November 28, 2017",
      "summary": "This study examines why deep learning models, despite having more parameters than training samples, generalize well. It highlights the importance of the loss function's landscape, showing that areas leading to better generalization (good minima) are more prevalent than those leading to poor outcomes. This predominance facilitates the convergence of optimization methods to these beneficial minima. The research provides theoretical backing by analyzing 2-layer neural networks, noting that solutions with better generalization have a smaller Hessian matrix norm. For deeper networks, extensive numerical data supports these conclusions."
    },
    {
      "title": "The Platonic Representation Hypothesis",
      "url": "/research/2024/05/13/The-Platonic-Representation-Hypothesis.html",
      "date": "May 13, 2024",
      "summary": "This paper argues that AI model representations, especially in deep networks, are converging. It surveys examples in the literature showing how neural network representations align over time and across domains. It demonstrates convergence across data modalities, noting that as vision and language models grow, they measure distances between data points similarly. The authors hypothesize that this convergence leads to a shared statistical model of reality, termed the platonic representation and discuss possible selective pressures toward it. The paper also addresses the implications, limitations, and counterexamples to this trend."
    }
  ],
  "requirement": [
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "This paper examines the role of the student/teacher paradigm in masked image modeling (MIM) for self-supervised learning (SSL) with Vision Transformers, particularly in the context of the Masked Auto-Encoder (MAE). Analysis of a simple linear model reveals that the teacher model acts as a conditional momentum regularizer by selectively filtering gradient directions based on feature similarity. Building on this insight, the authors introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an exponential moving average (EMA) teacher with MAE. RC-MAE demonstrates faster convergence, reduced memory requirements, greater robustness, and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation compared to the original MAE."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "This paper introduces Online Normalization, a novel method for normalizing neural network hidden activations that offers a batch-independent alternative with comparable accuracy to Batch Normalization. Online Normalization addresses Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. The method is applicable to recurrent, fully connected networks, and those with high activation memory requirements. The authors demonstrate its effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    },
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "The paper investigates the use of synthetic data generated by graphics simulators for pre-training computer vision models. The authors find that model performance on downstream tasks varies with different simulation parameters used to generate the synthetic data. They introduce Task2Sim, a model that maps the requirements of a downstream task to the optimal simulation parameters for generating synthetic pre-training data."
    }
  ],
  "mine": [
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "This paper proposes Hard Patches Mining (HPM), a novel framework for masked image modeling (MIM) pre-training that goes beyond simply solving given problems. HPM aims for the model to generate more challenging tasks for itself, using reconstruction loss as a metric for task difficulty. It incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "hpm": [
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "This paper proposes Hard Patches Mining (HPM), a novel framework for masked image modeling (MIM) pre-training that goes beyond simply solving given problems. HPM aims for the model to generate more challenging tasks for itself, using reconstruction loss as a metric for task difficulty. It incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "go": [
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "This paper proposes Hard Patches Mining (HPM), a novel framework for masked image modeling (MIM) pre-training that goes beyond simply solving given problems. HPM aims for the model to generate more challenging tasks for itself, using reconstruction loss as a metric for task difficulty. It incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "solving": [
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "This paper proposes Hard Patches Mining (HPM), a novel framework for masked image modeling (MIM) pre-training that goes beyond simply solving given problems. HPM aims for the model to generate more challenging tasks for itself, using reconstruction loss as a metric for task difficulty. It incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "predictor": [
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "This paper proposes Hard Patches Mining (HPM), a novel framework for masked image modeling (MIM) pre-training that goes beyond simply solving given problems. HPM aims for the model to generate more challenging tasks for itself, using reconstruction loss as a metric for task difficulty. It incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
      "url": "/research/2023/11/30/Dichotomy-of-Early-and-Late-Phase-Implicit-Biases-Can-Provably-Induce-Grokking.html",
      "date": "November 30, 2023",
      "summary": "Power et al. (2022) discovered that neural networks initially memorize arithmetic tasks, achieving perfect training accuracy but poor test accuracy, then suddenly achieve perfect test accuracy after extended training. This study explains this grokking phenomenon with the theory that early and late phase implicit biases cause this shift. Training homogeneous neural networks with large initialization and small weight decay on classification and regression tasks results in a prolonged period where the network acts like a kernel predictor, followed by a sudden shift to min-norm/max-margin predictors, significantly improving test accuracy."
    },
    {
      "title": "Towards a Unified Theoretical Understanding of Non-contrastive Learning via Rank Differential Mechanism",
      "url": "/research/2023/03/04/Towards-a-Unified-Theoretical-Understanding-of-Non-contrastive-Learning-via-Rank-Differential-Mechanism.html",
      "date": "March 4, 2023",
      "summary": "Recent non-contrastive learning methods like BYOL, SimSiam, SwAV, and DINO have shown that asymmetric architectural designs can achieve good self-supervised visual learning performance by aligning positive pairs alone, despite a lack of unified theoretical understanding on how these designs prevent feature collapse. This study introduces the Rank Differential Mechanism (RDM) as a unified theoretical framework for non-contrastive learning. RDM demonstrates that these asymmetric designs maintain a consistent rank difference in output features, enhancing effective dimensionality and preventing feature collapse. Unlike previous theories, RDM applies to designs both with and without a predictor, offering a comprehensive understanding of non-contrastive learning methods and guiding the development of new variants. Experiments confirm that these variants perform comparably, if not better than, existing methods on benchmark datasets."
    }
  ],
  "mechanic": [
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    }
  ],
  "msa": [
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    }
  ],
  "specificity": [
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    }
  ],
  "dependency": [
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "Language Modeling with Gated Convolutional Networks",
      "url": "/research/2017/09/08/Language-Modeling-with-Gated-Convolutional-Networks.html",
      "date": "September 8, 2017",
      "summary": "This paper introduces a novel language modeling approach using stacked convolutions that enable efficient parallel processing, in contrast to the traditionally used recurrent neural networks known for handling unbounded context. The proposed method features a simplified gating mechanism that outperforms previous models and demonstrates superior performance on the WikiText-103 benchmark, showing its capability to manage long-term dependencies. It also delivers competitive results on the Google Billion Words benchmark and significantly reduces sentence scoring latency compared to recurrent models. This marks the first instance of a non-recurrent model achieving comparable success to strong recurrent models on major language tasks."
    },
    {
      "title": "VideoMamba: State Space Model for Efficient Video Understanding",
      "url": "/research/2024/03/12/VideoMamba-State-Space-Model-for-Efficient-Video-Understanding.html",
      "date": "March 12, 2024",
      "summary": "This work introduces VideoMamba, an innovative approach that addresses local redundancy and global dependencies in video understanding by adapting the Mamba framework to video analysis. VideoMamba outperforms existing 3D convolutional neural networks and video transformers through its linear-complexity operator, which facilitates efficient long-term modeling for high-resolution, long-duration videos. Its effectiveness is demonstrated across four main areas: scalability in the visual domain without needing extensive dataset pretraining, thanks to a novel self-distillation technique; the ability to recognize short-term actions with fine-grained motion differences; superior performance in long-term video understanding compared to traditional feature-based models; and robust compatibility with multiple modalities, enhancing multi-modal video analysis. VideoMamba establishes a new standard for comprehensive and efficient video understanding."
    }
  ],
  "grapple": [
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    }
  ],
  "convex": [
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    },
    {
      "title": "Optimal learning rate schedules in high-dimensional non-convex optimization problems",
      "url": "/research/2022/02/09/Optimal-learning-rate-schedules-in-high-dimensional-non-convex-optimization-problems.html",
      "date": "February 9, 2022",
      "summary": "This paper explores the role of learning rate schedules in high-dimensional and non-convex optimization problems, focusing on Langevin optimization with a decaying learning rate. Analyzing models with Gaussian random functions on N-dimensional spheres, the study reveals that to accelerate optimization without getting trapped in saddles, a decay rate β < 1 is optimal, contrary to convex settings where β = 1 is preferred. Introducing a signal recovery component, the dynamics involve an exploration phase navigating through rough landscape parts and a convergence phase entering convex basins. It's found optimal to maintain a large learning rate during exploration to swiftly exit non-convex regions, then transition to β = 1 for rapid convergence to the solution. These findings are validated in a neural network regression task."
    },
    {
      "title": "Normalized Attention Without Probability Cage",
      "url": "/research/2020/05/19/Normalized-Attention-Without-Probability-Cage.html",
      "date": "May 19, 2020",
      "summary": "Softmax-attention architectures, especially popularized by Transformers, have seen significant advancements in various tasks. However, the geometric implications of softmax-attention remain underexplored. In this study, we demonstrate limitations arising from constraining attention weights to the probability simplex and its impact on the convex hull of value vectors. We reveal sequence length-dependent biases in Transformers towards token isolation at initialization and compare them with max- and sum-pooling, which are strong but often overlooked baselines. To address these issues, we propose a novel approach of replacing softmax with normalization in self-attention, resulting in a robust and widely applicable architecture. Our findings are supported by empirical results from over 25,000 trained models, and all results and implementations are publicly available."
    },
    {
      "title": "SAM as an Optimal Relaxation of Bayes",
      "url": "/research/2023/12/10/SAM-as-an-Optimal-Relaxation-of-Bayes.html",
      "date": "December 10, 2023",
      "summary": "We introduce SAM, a method enhancing generalization in deep learning, as a relaxation of the Bayes objective. SAM replaces the expected negative-loss with an optimal convex lower bound derived using the Fenchel biconjugate. This connection enables an Adam-like extension of SAM, offering automatic uncertainty estimates and potential accuracy improvements. Bridging adversarial and Bayesian methods, our work paves the way for robustness enhancement."
    },
    {
      "title": "AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks",
      "url": "/research/2023/03/01/AdaSAM-Boosting-Sharpness-Aware-Minimization-with-Adaptive-Learning-Rate-and-Momentum-for-Training-Deep-Neural-Networks.html",
      "date": "March 1, 2023",
      "summary": "The Sharpness Aware Minimization (SAM) optimizer, known for enhancing the generalization of deep neural networks by introducing extra perturbation steps, is further developed into AdaSAM by integrating adaptive learning rates and momentum acceleration. Although AdaSAM has been empirically applied to large-scale networks, a theoretical understanding of its performance, considering the complexity of its components, was lacking. This study presents a theoretical analysis of AdaSAM's convergence in stochastic non-convex settings, demonstrating a convergence rate of O(1/√bT) that scales linearly with mini-batch size. By introducing a delayed second-order momentum term, the study successfully decouples and analyzes the intertwined effects of stochastic gradients, adaptive learning rates, and perturbations. This is the first work to offer a detailed convergence rate for SAM with adaptive mechanisms. Experimental results on various NLP tasks indicate AdaSAM's superior performance over SGD, AMSGrad, and SAM optimizers."
    },
    {
      "title": "A Phase Transition between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention",
      "url": "/research/2024/02/06/A-Phase-Transition-between-Positional-and-Semantic-Learning-in-a-Solvable-Model-of-Dot-Product-Attention.html",
      "date": "February 6, 2024",
      "summary": "We explore the learning process of a dot-product attention layer, which learns both positional and semantic attention matrices, enabling tokens to attend based on position or meaning. Through experiments on an algorithmic task, we demonstrate that this architecture can use either mechanism for solving the task. Theoretically, we examine a non-linear self-attention layer with special query and key matrices, offering a closed-form solution for its non-convex loss landscape in high-dimensional data, which reveals a phase transition from positional to semantic mechanisms as sample complexity increases. We also show that the dot-product attention layer surpasses a linear positional baseline through the semantic mechanism with adequate data."
    }
  ],
  "layers": [
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    }
  ],
  "note": [
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "This paper introduces the Kinetics Human Action Video dataset. The authors note that the limited number of videos in existing datasets like UCF-101 and HMDB-51 hinders the ability to effectively evaluate architectures due to similar performance across these small benchmarks. Kinetics contains 400 classes with over 400 video clips per class from challenging YouTube videos. The paper analyzes the impact of this larger dataset on the performance of existing architectures as well as the benefits of pre-training models on Kinetics. The authors introduce the Two-Stream Inflated 3D ConvNet (I3D), an extension of 2D ConvNets to 3D for improved video feature extraction. When pre-trained on Kinetics, this I3D model sets new benchmarks for action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    },
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "The paper introduces TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method that applies a single augmentation to each image. Despite its minimal complexity and cost, TrivialAugment outperforms existing methods across various image classification scenarios, as validated through extensive experiments against state-of-the-art methods and multiple ablation studies involving different augmentation spaces and methods. The work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Noting a stagnation in automatic augmentation research, the authors conclude by proposing best practices for future advancements in the field."
    },
    {
      "title": "Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes",
      "url": "/research/2017/11/28/Towards-Understanding-Generalization-of-Deep-Learning-Perspective-of-Loss-Landscapes.html",
      "date": "November 28, 2017",
      "summary": "This study examines why deep learning models, despite having more parameters than training samples, generalize well. It highlights the importance of the loss function's landscape, showing that areas leading to better generalization (good minima) are more prevalent than those leading to poor outcomes. This predominance facilitates the convergence of optimization methods to these beneficial minima. The research provides theoretical backing by analyzing 2-layer neural networks, noting that solutions with better generalization have a smaller Hessian matrix norm. For deeper networks, extensive numerical data supports these conclusions."
    },
    {
      "title": "The Platonic Representation Hypothesis",
      "url": "/research/2024/05/13/The-Platonic-Representation-Hypothesis.html",
      "date": "May 13, 2024",
      "summary": "This paper argues that AI model representations, especially in deep networks, are converging. It surveys examples in the literature showing how neural network representations align over time and across domains. It demonstrates convergence across data modalities, noting that as vision and language models grow, they measure distances between data points similarly. The authors hypothesize that this convergence leads to a shared statistical model of reality, termed the platonic representation and discuss possible selective pressures toward it. The paper also addresses the implications, limitations, and counterexamples to this trend."
    }
  ],
  "nature": [
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "Fine-tuning with Very Large Dropout",
      "url": "/research/2024/03/01/Fine-tuning-with-Very-Large-Dropout.html",
      "date": "March 1, 2024",
      "summary": "This study challenges the notion that machine learning practices assume training and testing data have the same distribution. It explores the effectiveness of high dropout rates, as opposed to ensemble techniques, in developing rich data representations suitable for multiple distribution scenarios. These representations surpass those achieved by traditional in-distribution performance regularization and the implicit sparsity induced by common stochastic gradient methods. While training deep networks from scratch with high dropout rates is impractical, fine-tuning pre-trained models under these conditions is feasible and yields better out-of-distribution performance than ensembles and model averaging techniques like model soups. This finding is significant due to the growing relevance of fine-tuning with large pre-trained models, offering insights into the nature of rich representations and the linear characteristics of fine-tuning large networks with small datasets."
    }
  ],
  "multistage": [
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    }
  ],
  "alternet": [
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    }
  ],
  "substitute": [
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "This study explores the operational mechanics of multi-head self-attentions (MSAs) and Vision Transformers (ViTs). The authors find that MSAs enhance accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than managing long-range dependencies. ViTs, however, grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research contrasts MSAs and convolutional layers (Convs), noting their complementary nature as low-pass and high-pass filters, respectively. Multi-stage neural networks are found to function like a series of small models, with MSAs crucial for predictions at stage ends. The study introduces AlterNet, a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "This paper explores Gated Linear Units (GLUs), which involve the component-wise product of two linear projections, with one undergoing a sigmoid function. The authors investigate GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers. They find that certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    },
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "This paper introduces QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU while maintaining 16-bit task performance. QLORA utilizes 4-bit quantized language models and Low Rank Adapters (LoRA) and incorporates several memory-saving innovations without compromising performance. The authors' best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. The authors applied QLORA to finetune over 1,000 models and analyzed performance across various datasets, model types, and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Their findings suggest that GPT-4 evaluations are a viable substitute for human assessments and question the reliability of current chatbot benchmarks. The authors make their models and 4-bit training code public."
    },
    {
      "title": "Understanding Deep Learning Requires Re- Thinking Generalization",
      "url": "/research/2017/02/26/Understanding-Deep-Learning-Requires-Re--Thinking-Generalization.html",
      "date": "February 26, 2017",
      "summary": "Extensive experiments reveal that traditional explanations for the strong generalization performance of large neural networks fall short. Despite conventional wisdom attributing success to model properties or regularization techniques, our findings show that state-of-the-art convolutional networks for image classification can easily overfit random labels. This phenomenon persists regardless of explicit regularization or substituting true images with random noise. Theoretical analysis suggests that shallow neural networks achieve perfect finite sample expressivity when parameters outnumber data points, as commonly observed. We interpret these results in contrast to traditional models."
    },
    {
      "title": "FNet: Mixing Tokens with Fourier Transforms",
      "url": "/research/2022/05/26/FNet-Mixing-Tokens-with-Fourier-Transforms.html",
      "date": "May 26, 2022",
      "summary": "We demonstrate that Transformer encoder architectures can be accelerated with minimal impact on accuracy by substituting self-attention sublayers with simple linear transformations. Remarkably, using a standard Fourier Transform instead of the self-attention sublayer in a Transformer encoder achieves 92-97% of BERT models' accuracy on the GLUE benchmark, while training 80% faster on GPUs and 70% faster on TPUs for standard 512 input lengths. Our FNet model significantly outperforms in speed at longer input lengths, matching the accuracy of the most accurate models on the Long Range Arena benchmark and surpassing the fastest models in speed across all sequence lengths on GPUs and shorter lengths on TPUs. Additionally, FNet is lightweight in memory use and exceptionally efficient in smaller sizes, outperforming Transformer models under the same speed and accuracy constraints."
    }
  ],
  "augreg": [
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "This paper explores the interactions between training data amount, model regularization or data augmentation (AugReg), model size, and compute budget for Vision Transformers (ViT) in various vision tasks. The authors find that using more compute and AugReg can achieve the same performance as training with significantly more data. They demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    }
  ],
  "cls": [
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel normalization technique for self-supervised transformer training that normalizes the [CLS] token and regular tokens separately. The authors find that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks compared to previous models like masked autoencoders (MAE) that use a single normalization layer for all tokens."
    }
  ],
  "things": [
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "This research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. The authors validate these approaches using the ImageNet-1k dataset and confirm their findings with the ImageNet-v2 test set, evaluating transfer performance across six additional datasets."
    }
  ],
  "everyone": [
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "This research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. The authors validate these approaches using the ImageNet-1k dataset and confirm their findings with the ImageNet-v2 test set, evaluating transfer performance across six additional datasets."
    }
  ],
  "know": [
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "This research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. The authors validate these approaches using the ImageNet-1k dataset and confirm their findings with the ImageNet-v2 test set, evaluating transfer performance across six additional datasets."
    },
    {
      "title": "Language Modeling with Gated Convolutional Networks",
      "url": "/research/2017/09/08/Language-Modeling-with-Gated-Convolutional-Networks.html",
      "date": "September 8, 2017",
      "summary": "This paper introduces a novel language modeling approach using stacked convolutions that enable efficient parallel processing, in contrast to the traditionally used recurrent neural networks known for handling unbounded context. The proposed method features a simplified gating mechanism that outperforms previous models and demonstrates superior performance on the WikiText-103 benchmark, showing its capability to manage long-term dependencies. It also delivers competitive results on the Google Billion Words benchmark and significantly reduces sentence scoring latency compared to recurrent models. This marks the first instance of a non-recurrent model achieving comparable success to strong recurrent models on major language tasks."
    },
    {
      "title": "AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks",
      "url": "/research/2023/03/01/AdaSAM-Boosting-Sharpness-Aware-Minimization-with-Adaptive-Learning-Rate-and-Momentum-for-Training-Deep-Neural-Networks.html",
      "date": "March 1, 2023",
      "summary": "The Sharpness Aware Minimization (SAM) optimizer, known for enhancing the generalization of deep neural networks by introducing extra perturbation steps, is further developed into AdaSAM by integrating adaptive learning rates and momentum acceleration. Although AdaSAM has been empirically applied to large-scale networks, a theoretical understanding of its performance, considering the complexity of its components, was lacking. This study presents a theoretical analysis of AdaSAM's convergence in stochastic non-convex settings, demonstrating a convergence rate of O(1/√bT) that scales linearly with mini-batch size. By introducing a delayed second-order momentum term, the study successfully decouples and analyzes the intertwined effects of stochastic gradients, adaptive learning rates, and perturbations. This is the first work to offer a detailed convergence rate for SAM with adaptive mechanisms. Experimental results on various NLP tasks indicate AdaSAM's superior performance over SGD, AMSGrad, and SAM optimizers."
    },
    {
      "title": "Neural Network Diffusion",
      "url": "/research/2024/02/20/Neural-Network-Diffusion.html",
      "date": "February 20, 2024",
      "summary": "Diffusion models, known for their success in image and video generation, can also generate high-performing neural network parameters, as demonstrated in this study. By employing a simple combination of an autoencoder and a standard latent diffusion model, our method involves extracting latent representations of network parameters, which are then synthesized from random noise by the diffusion model. These new representations, processed through the autoencoder's decoder, serve as fresh network parameters. Tested across various architectures and datasets, this diffusion approach consistently produces models with comparable or superior performance to traditionally trained networks at minimal extra cost. Importantly, the generated models show distinct performance differences from the trained ones, suggesting further exploration into the versatile applications of diffusion models."
    }
  ],
  "parallel": [
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "This research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. The authors validate these approaches using the ImageNet-1k dataset and confirm their findings with the ImageNet-v2 test set, evaluating transfer performance across six additional datasets."
    },
    {
      "title": "Large Batch Training of Convolutional Networks",
      "url": "/research/2017/09/13/Large-Batch-Training-of-Convolutional-Networks.html",
      "date": "September 13, 2017",
      "summary": "To expedite the training of large convolutional networks, computational units are added and trained with data-parallel synchronous Stochastic Gradient Descent (SGD) across units, increasing batch size with node count. However, larger batch sizes can reduce model accuracy. The existing method of large batch training—linear learning rate scaling with warm-up—is not universally effective and may cause training divergence. To address these challenges, we introduce a novel training algorithm, Layer-wise Adaptive Rate Scaling (LARS), enabling us to train Alexnet with a batch size of 8K and Resnet-50 with a batch size of 32K, without compromising accuracy."
    }
  ],
  "extent": [
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "This research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. The authors validate these approaches using the ImageNet-1k dataset and confirm their findings with the ImageNet-v2 test set, evaluating transfer performance across six additional datasets."
    }
  ],
  "preprocess": [
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "This research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. The authors validate these approaches using the ImageNet-1k dataset and confirm their findings with the ImageNet-v2 test set, evaluating transfer performance across six additional datasets."
    }
  ],
  "bertlike": [
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "This research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. The authors validate these approaches using the ImageNet-1k dataset and confirm their findings with the ImageNet-v2 test set, evaluating transfer performance across six additional datasets."
    }
  ],
  "v2": [
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "This research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. The authors validate these approaches using the ImageNet-1k dataset and confirm their findings with the ImageNet-v2 test set, evaluating transfer performance across six additional datasets."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    },
    {
      "title": "Searching for Activation Functions",
      "url": "/research/2017/10/27/Searching-for-Activation-Functions.html",
      "date": "October 27, 2017",
      "summary": "This study explores the impact of activation functions on deep network training and performance. While Rectified Linear Unit (ReLU) is widely used, alternatives have not consistently outperformed it. We propose using automatic search techniques to discover new activation functions. Through exhaustive and reinforcement learning-based searches, we identify novel functions. Empirical evaluation shows that our best discovered function, Swish (f(x) = x · sigmoid(βx)), performs better than ReLU on deeper models across challenging datasets. Replacing ReLUs with Swish units improves classification accuracy on ImageNet, for example, by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. Swish's simplicity and similarity to ReLU facilitate its adoption in neural networks."
    },
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    },
    {
      "title": "V-Jepa: Latent Video Prediction For Visual Representation Learning",
      "url": "/research/2023/02/15/V-Jepa-Latent-Video-Prediction-For-Visual-Representation-Learning.html",
      "date": "February 15, 2023",
      "summary": "This paper introduces V-JEPA, a self-supervised video learning method that predicts masked spatio-temporal regions in latent space, effectively applying the masked-modelling principle of large language models to video. This approach generates visual features useful across various image and video tasks without needing model adjustment, achieving significant improvements on Kinetics-400 (82.1%) and Something-Something-v2 (71.2%) benchmarks, outperforming prior video models. V-JEPA also excels in motion understanding tasks, surpassing leading image models like DINOv2 and OpenCLIP, and achieves 77.9% on ImageNet classification with video training alone, setting a new standard for video models."
    }
  ],
  "par": [
    {
      "title": "Training data-efficient image transformers & distillation through attention",
      "url": "/research/2021/01/15/Training-data-efficient-image-transformers-&-distillation-through-attention.html",
      "date": "January 15, 2021",
      "summary": "This paper introduces DeiT, an efficient method for training vision transformers. The authors present a unique teacher-student strategy for transformers, leveraging a distillation token for efficient learning from a convolutional network teacher. This method achieves performance on par with convolutional networks, with up to 85.2% accuracy on ImageNet, and demonstrates effective transferability to other tasks."
    }
  ],
  "splat": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "This paper introduces a novel approach to Radiance Field methods for novel-view synthesis that addresses the challenges of high visual quality, costly training, and real-time rendering of unbounded, complete scenes at 1080p resolution. The authors propose three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. The method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "radiance": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "This paper introduces a novel approach to Radiance Field methods for novel-view synthesis that addresses the challenges of high visual quality, costly training, and real-time rendering of unbounded, complete scenes at 1080p resolution. The authors propose three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. The method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "render": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "This paper introduces a novel approach to Radiance Field methods for novel-view synthesis that addresses the challenges of high visual quality, costly training, and real-time rendering of unbounded, complete scenes at 1080p resolution. The authors propose three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. The method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "synthesis": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "This paper introduces a novel approach to Radiance Field methods for novel-view synthesis that addresses the challenges of high visual quality, costly training, and real-time rendering of unbounded, complete scenes at 1080p resolution. The authors propose three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. The method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    },
    {
      "title": "Stable Diffusion 3: Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "This paper focuses on improving diffusion models for high-dimensional data like images. The authors enhance noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Their large-scale study shows superior performance over established diffusion methods in high-resolution text-to-image synthesis. A new transformer-based architecture is introduced for better text comprehension, typography, and human preference in text-to-image generation. The findings demonstrate predictable scaling trends, with lower validation loss tied to improved synthesis. The largest models outperform state-of-the-art approaches."
    }
  ],
  "rendering": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "This paper introduces a novel approach to Radiance Field methods for novel-view synthesis that addresses the challenges of high visual quality, costly training, and real-time rendering of unbounded, complete scenes at 1080p resolution. The authors propose three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. The method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    },
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This paper investigates using high-quality rendering software and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle. This synthetic dataset is used to augment limited real training data for image classification and semantic segmentation tasks. While models trained solely on synthetic images showed low accuracy on real validation data, including even small amounts of real data significantly improved performance. Augmenting real training data with synthetic images outperformed using only real images. Furthermore, pretraining models on the synthetic dataset before transfer learning significantly reduced training costs by allowing most of the training to be completed upfront using the synthetic data."
    }
  ],
  "camera": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "This paper introduces a novel approach to Radiance Field methods for novel-view synthesis that addresses the challenges of high visual quality, costly training, and real-time rendering of unbounded, complete scenes at 1080p resolution. The authors propose three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. The method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    },
    {
      "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
      "url": "/research/2021/07/24/The-Many-Faces-of-Robustness-A-Critical-Analysis-of-Out-of-Distribution-Generalization.html",
      "date": "July 24, 2021",
      "summary": "We present four novel real-world distribution shift datasets encompassing changes in image style, blurriness, location, camera settings, and more. Evaluating existing methods for enhancing out-of-distribution robustness, we discover that employing larger models and artificial data augmentations can enhance robustness against real-world distribution shifts, contradicting prior claims. Our findings demonstrate that improvements in artificial robustness benchmarks can indeed transfer to real-world distribution shifts, contrary to prior assumptions. Additionally, we introduce a novel data augmentation technique that surpasses models pretrained with significantly more labeled data, emphasizing its efficacy in addressing real-world distribution shifts. While some methods consistently mitigate texture and local image statistics shifts, they fail to address other shifts like geographic changes. Our results underscore the necessity for future research to examine multiple distribution shifts concurrently, as no single method consistently improves robustness across all evaluated scenarios."
    }
  ],
  "fidelity": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "This paper introduces a novel approach to Radiance Field methods for novel-view synthesis that addresses the challenges of high visual quality, costly training, and real-time rendering of unbounded, complete scenes at 1080p resolution. The authors propose three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. The method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    },
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "This paper introduces DiffBIR, a framework that utilizes pretrained text-to-image diffusion models for blind image restoration. The two-stage pipeline involves pretraining a restoration module on various degradations to enhance real-world applicability, followed by the use of latent diffusion models for realistic restoration. The authors introduce a novel injective modulation sub-network, LAControlNet, for fine-tuning and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, a controllable module allows users to adjust quality and fidelity during the denoising process. Extensive tests demonstrate DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets."
    }
  ],
  "implementing": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "This paper introduces a novel approach to Radiance Field methods for novel-view synthesis that addresses the challenges of high visual quality, costly training, and real-time rendering of unbounded, complete scenes at 1080p resolution. The authors propose three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. The method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "interleave": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "This paper introduces a novel approach to Radiance Field methods for novel-view synthesis that addresses the challenges of high visual quality, costly training, and real-time rendering of unbounded, complete scenes at 1080p resolution. The authors propose three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. The method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "depiction": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "This paper introduces a novel approach to Radiance Field methods for novel-view synthesis that addresses the challenges of high visual quality, costly training, and real-time rendering of unbounded, complete scenes at 1080p resolution. The authors propose three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. The method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "splatting": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "This paper introduces a novel approach to Radiance Field methods for novel-view synthesis that addresses the challenges of high visual quality, costly training, and real-time rendering of unbounded, complete scenes at 1080p resolution. The authors propose three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. The method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "and": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "This paper introduces a novel approach to Radiance Field methods for novel-view synthesis that addresses the challenges of high visual quality, costly training, and real-time rendering of unbounded, complete scenes at 1080p resolution. The authors propose three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. The method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "fps": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "This paper introduces a novel approach to Radiance Field methods for novel-view synthesis that addresses the challenges of high visual quality, costly training, and real-time rendering of unbounded, complete scenes at 1080p resolution. The authors propose three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. The method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "augmentor": [
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "This paper introduces Mask-Reconstruct Augmentation (MRA), an image augmentation technique that leverages self-supervised masked autoencoders to generate distorted inputs and address the issue of overfitting in deep neural networks. Inspired by the success of masked image modeling in self-supervised learning, MRA uses nonlinear transformations for regularization, as opposed to current hand-crafted linear techniques like scale, flip, and color jitter. Extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks."
    }
  ],
  "mra": [
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "This paper introduces Mask-Reconstruct Augmentation (MRA), an image augmentation technique that leverages self-supervised masked autoencoders to generate distorted inputs and address the issue of overfitting in deep neural networks. Inspired by the success of masked image modeling in self-supervised learning, MRA uses nonlinear transformations for regularization, as opposed to current hand-crafted linear techniques like scale, flip, and color jitter. Extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks."
    }
  ],
  "oppose": [
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "This paper introduces Mask-Reconstruct Augmentation (MRA), an image augmentation technique that leverages self-supervised masked autoencoders to generate distorted inputs and address the issue of overfitting in deep neural networks. Inspired by the success of masked image modeling in self-supervised learning, MRA uses nonlinear transformations for regularization, as opposed to current hand-crafted linear techniques like scale, flip, and color jitter. Extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks."
    },
    {
      "title": "Fine-tuning with Very Large Dropout",
      "url": "/research/2024/03/01/Fine-tuning-with-Very-Large-Dropout.html",
      "date": "March 1, 2024",
      "summary": "This study challenges the notion that machine learning practices assume training and testing data have the same distribution. It explores the effectiveness of high dropout rates, as opposed to ensemble techniques, in developing rich data representations suitable for multiple distribution scenarios. These representations surpass those achieved by traditional in-distribution performance regularization and the implicit sparsity induced by common stochastic gradient methods. While training deep networks from scratch with high dropout rates is impractical, fine-tuning pre-trained models under these conditions is feasible and yields better out-of-distribution performance than ensembles and model averaging techniques like model soups. This finding is significant due to the growing relevance of fine-tuning with large pre-trained models, offering insights into the nature of rich representations and the linear characteristics of fine-tuning large networks with small datasets."
    }
  ],
  "jitter": [
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "This paper introduces Mask-Reconstruct Augmentation (MRA), an image augmentation technique that leverages self-supervised masked autoencoders to generate distorted inputs and address the issue of overfitting in deep neural networks. Inspired by the success of masked image modeling in self-supervised learning, MRA uses nonlinear transformations for regularization, as opposed to current hand-crafted linear techniques like scale, flip, and color jitter. Extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks."
    }
  ],
  "ava": [
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper introduces the AVA dataset, which features 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. The authors highlight the challenges in action recognition and introduce a novel localization approach that surpasses existing benchmarks but shows modest performance on AVA (15.6% mAP), indicating the need for advanced video understanding methods."
    },
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "This paper introduces SlowFast networks, a new convolutional architecture for video recognition tasks. These networks have two pathways: a Slow pathway operating at low frame rates to capture spatial semantics, and a Fast pathway working at high frame rates with reduced channel capacity to efficiently capture motion details. The SlowFast models achieve top accuracy on major video benchmarks like Kinetics, Charades, and AVA, significantly improving performance for action classification and detection tasks."
    }
  ],
  "spatio": [
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper introduces the AVA dataset, which features 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. The authors highlight the challenges in action recognition and introduce a novel localization approach that surpasses existing benchmarks but shows modest performance on AVA (15.6% mAP), indicating the need for advanced video understanding methods."
    }
  ],
  "annotation": [
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper introduces the AVA dataset, which features 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. The authors highlight the challenges in action recognition and introduce a novel localization approach that surpasses existing benchmarks but shows modest performance on AVA (15.6% mAP), indicating the need for advanced video understanding methods."
    },
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "This paper introduces FineAction, a large-scale, fine-grained video dataset designed to address the limitations of current temporal action localization (TAL) benchmarks, which rely on coarse action classes and lead to model overfitting and ambiguous annotations. FineAction contains 103K instances across 106 action categories in 17K videos, offering a diverse and densely annotated dataset with co-occurring actions that pose new challenges for TAL. The authors evaluate popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and propose a baseline method achieving a 13.17% mAP. FineAction aims to advance TAL research and is accessible online."
    },
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "This paper introduces tools for training monocular depth estimation models with multiple datasets despite incompatible annotations. Their approach uses a robust training objective, multi-objective learning for data integration, and pretraining encoders on auxiliary tasks. Tested across five datasets, including 3D films as a novel data source, their methods achieve superior zero-shot cross-dataset generalization, outperforming existing benchmarks through principled multi-dataset training."
    },
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    }
  ],
  "videos": [
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper introduces the AVA dataset, which features 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. The authors highlight the challenges in action recognition and introduce a novel localization approach that surpasses existing benchmarks but shows modest performance on AVA (15.6% mAP), indicating the need for advanced video understanding methods."
    },
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "The authors introduce VideoMAE, a data-efficient self-supervised video pre-training (SSVP) approach that utilizes video masked autoencoders with a novel high-ratio video tube masking technique inspired by ImageMAE. Key findings include the effectiveness of high masking ratios (90-95%) due to video's temporal redundancy, strong performance on small datasets (~3k-4k videos) without extra data highlighting the importance of high-level structure learning, and data quality being more crucial than quantity with domain shift being significant. Notably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone without extra data."
    }
  ],
  "continuity": [
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper introduces the AVA dataset, which features 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. The authors highlight the challenges in action recognition and introduce a novel localization approach that surpasses existing benchmarks but shows modest performance on AVA (15.6% mAP), indicating the need for advanced video understanding methods."
    }
  ],
  "person": [
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper introduces the AVA dataset, which features 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. The authors highlight the challenges in action recognition and introduce a novel localization approach that surpasses existing benchmarks but shows modest performance on AVA (15.6% mAP), indicating the need for advanced video understanding methods."
    }
  ],
  "movie": [
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper introduces the AVA dataset, which features 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. The authors highlight the challenges in action recognition and introduce a novel localization approach that surpasses existing benchmarks but shows modest performance on AVA (15.6% mAP), indicating the need for advanced video understanding methods."
    }
  ],
  "belief": [
    {
      "title": "Better plain ViT baselines for ImageNet-1k",
      "url": "/research/2022/05/02/Better-plain-ViT-baselines-for-ImageNet-1k.html",
      "date": "May 2, 2022",
      "summary": "This paper challenges the common belief that the Vision Transformer (ViT) model requires sophisticated regularization techniques to excel on ImageNet-1k scale data. The authors present minor modifications to the original ViT vanilla training setting that significantly improve the performance of plain ViT models. They find that standard data augmentation is sufficient, with 90 epochs of training surpassing 76% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reaching 80% in less than one day."
    }
  ],
  "tpuv38": [
    {
      "title": "Better plain ViT baselines for ImageNet-1k",
      "url": "/research/2022/05/02/Better-plain-ViT-baselines-for-ImageNet-1k.html",
      "date": "May 2, 2022",
      "summary": "This paper challenges the common belief that the Vision Transformer (ViT) model requires sophisticated regularization techniques to excel on ImageNet-1k scale data. The authors present minor modifications to the original ViT vanilla training setting that significantly improve the performance of plain ViT models. They find that standard data augmentation is sufficient, with 90 epochs of training surpassing 76% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reaching 80% in less than one day."
    }
  ],
  "resnet50": [
    {
      "title": "Better plain ViT baselines for ImageNet-1k",
      "url": "/research/2022/05/02/Better-plain-ViT-baselines-for-ImageNet-1k.html",
      "date": "May 2, 2022",
      "summary": "This paper challenges the common belief that the Vision Transformer (ViT) model requires sophisticated regularization techniques to excel on ImageNet-1k scale data. The authors present minor modifications to the original ViT vanilla training setting that significantly improve the performance of plain ViT models. They find that standard data augmentation is sufficient, with 90 epochs of training surpassing 76% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reaching 80% in less than one day."
    }
  ],
  "curriculum": [
    {
      "title": "Curriculum Learning by Dynamic Instance Hardness",
      "url": "/research/2020/12/06/Curriculum-Learning-by-Dynamic-Instance-Hardness.html",
      "date": "December 6, 2020",
      "summary": "This paper improves curriculum learning via introduces Dynamic Instance Hardness (DIH), a method that measures a sample's learning difficulty over time. DIH provides a stable indicator of learning progress by tracking the exponential moving average of a sample's hardness. DIHCL enhances learning efficiency and model accuracy without extra computational costs by leveraging data from the training process itself. Tested on 11 datasets, DIHCL surpasses traditional training methods and recent curriculum learning techniques in both efficiency and effectiveness."
    },
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "This paper introduces a curriculum learning framework for image-caption pretraining, inspired by children's language learning from cognitive science, to address the challenges of aligning multiple concepts from captions to objects in images. The method starts with simple image-caption pairs and gradually increases complexity by adding more concepts, leveraging knowledge from each phase for subsequent learning. This approach outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios."
    },
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "This paper introduces a novel approach called mixed-order self-paced curriculum learning (Mo-SCL) to address the challenges faced by self-paced curriculum learning (SCL) in medical image analysis tasks, such as universal lesion detection. These challenges include inaccurate difficulty estimation and the under-utilization of hard samples. Mo-SCL combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches. Through theoretical analysis and experiments on the DeepLesion dataset, the authors demonstrate that Mo-SCL enhances lesion detection accuracy in state-of-the-art methods without requiring additional network modifications."
    },
    {
      "title": "Self-Paced Curriculum Learning",
      "url": "/research/2015/01/01/Self-Paced-Curriculum-Learning.html",
      "date": "January 1, 2015",
      "summary": "This paper introduces self-paced curriculum learning (SPCL), a unified framework combining curriculum learning (CL) and self-paced learning (SPL). SPCL leverages prior knowledge and ongoing learning progress through an optimization problem. It mimics collaborative instructor-student learning, exhibiting empirical advantages in two tasks."
    },
    {
      "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
      "url": "/research/2024/02/28/CL-MAE:-Curriculum-Learned-Masked-Autoencoders.html",
      "date": "February 28, 2024",
      "summary": "This paper proposes a curriculum learning approach that updates the masking strategy of Masked Auto Encoders (MAE) to progressively increase the complexity of the self-supervised reconstruction task. To achieve this, a novel learnable masking module is introduced, capable of generating masks of varying complexities, and integrated into masked autoencoders (MAE). This module is jointly trained with the MAE, adjusting its behavior from partner (optimizing the same reconstruction loss) to adversary (optimizing the opposite loss), with a smooth transition regulated by a factor multiplied with the reconstruction loss. This training procedure creates an easy-to-hard curriculum. The Curriculum-Learned Masked Autoencoder (CL-MAE) is trained on ImageNet and demonstrates superior representation learning compared to MAE. Empirical results on five downstream tasks confirm that curriculum learning can successfully self-supervise masked autoencoders."
    },
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    },
    {
      "title": "From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning",
      "url": "/research/2023/03/16/From-MNIST-to-ImageNet-and-Back:-Benchmarking-Continual-Curriculum-Learning.html",
      "date": "March 16, 2023",
      "summary": "Continual learning (CL) is a promising trend in machine learning, aiming to develop robust models and strategies for dynamic environments by incorporating new knowledge while retaining past knowledge. CL research is fragmented, with various protocols, tasks, datasets, and metrics, often not reflecting real-world complexity and tailored to specific strategies. This work addresses this gap by introducing two novel CL benchmarks with heterogeneous tasks from six image datasets, varying in complexity and quality. These benchmarks fairly evaluate state-of-the-art CL strategies in scenarios closer to real-world conditions. The benchmarks present tasks in both increasing and decreasing complexity, assessing models' ability to exploit task structure. The work emphasizes a rigorous, reproducible evaluation protocol for measuring generalization and memory retention in models. Experimental results show that popular CL strategies perform poorly on these benchmarks, exhibit high forgetting, and struggle with curriculum task ordering. These findings underscore the need for rigorous comparisons and the development of new CL strategies for complex scenarios."
    },
    {
      "title": "Self-Paced Absolute Learning Progress as a Regularized Approach to Curriculum Learning",
      "url": "/research/2023/06/09/Self-Paced-Absolute-Learning-Progress-as-a-Regularized-Approach-to-Curriculum-Learning.html",
      "date": "June 9, 2023",
      "summary": "Reinforcement Learning's usability is limited by its high computation times. Curriculum Reinforcement Learning accelerates learning by ordering tasks from simple to hard. Curricula based on Absolute Learning Progress (ALP) have shown success in various environments but waste computation on redundant tasks. This issue is addressed by introducing Self-Paced Absolute Learning Progress (SPALP), a regularization method inspired by Self-Paced Learning. Evaluated in three environments, SPALP achieves performance comparable to ALP in all cases and reaches it faster in two. Further improvements in SPALP's efficiency and performance are also discussed."
    },
    {
      "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum",
      "url": "/research/2023/09/30/When-to-Learn-What:-Model-Adaptive-Data-Augmentation-Curriculum.html",
      "date": "September 30, 2023",
      "summary": "Data augmentation (DA) improves neural network generalization by enforcing invariances and symmetries to predefined transformations applied to input data. However, fixed augmentation policies affect samples differently at various training stages, and existing approaches cannot adapt policies to individual samples and the training model. This paper proposes Model-Adaptive Data Augmentation (MADAug) which trains an augmentation policy network to determine when to learn what. Unlike previous work, MADAug selects augmentation operators for each input image with a model-adaptive policy that varies between training stages, creating an optimized data augmentation curriculum. The policy is trained using a bi-level optimization scheme to minimize validation-set loss of a model trained with policy-produced augmentations. Extensive evaluations on multiple image classification tasks and network architectures show that MADAug outperforms or matches existing DA approaches, enhances fairness by improving all classes, particularly the difficult ones, and performs better when transferred to fine-grained datasets. Additionally, the auto-optimized policy in MADAug gradually increases perturbations, forming an easy-to-hard curriculum."
    }
  ],
  "hardness": [
    {
      "title": "Curriculum Learning by Dynamic Instance Hardness",
      "url": "/research/2020/12/06/Curriculum-Learning-by-Dynamic-Instance-Hardness.html",
      "date": "December 6, 2020",
      "summary": "This paper improves curriculum learning via introduces Dynamic Instance Hardness (DIH), a method that measures a sample's learning difficulty over time. DIH provides a stable indicator of learning progress by tracking the exponential moving average of a sample's hardness. DIHCL enhances learning efficiency and model accuracy without extra computational costs by leveraging data from the training process itself. Tested on 11 datasets, DIHCL surpasses traditional training methods and recent curriculum learning techniques in both efficiency and effectiveness."
    }
  ],
  "dih": [
    {
      "title": "Curriculum Learning by Dynamic Instance Hardness",
      "url": "/research/2020/12/06/Curriculum-Learning-by-Dynamic-Instance-Hardness.html",
      "date": "December 6, 2020",
      "summary": "This paper improves curriculum learning via introduces Dynamic Instance Hardness (DIH), a method that measures a sample's learning difficulty over time. DIH provides a stable indicator of learning progress by tracking the exponential moving average of a sample's hardness. DIHCL enhances learning efficiency and model accuracy without extra computational costs by leveraging data from the training process itself. Tested on 11 datasets, DIHCL surpasses traditional training methods and recent curriculum learning techniques in both efficiency and effectiveness."
    }
  ],
  "track": [
    {
      "title": "Curriculum Learning by Dynamic Instance Hardness",
      "url": "/research/2020/12/06/Curriculum-Learning-by-Dynamic-Instance-Hardness.html",
      "date": "December 6, 2020",
      "summary": "This paper improves curriculum learning via introduces Dynamic Instance Hardness (DIH), a method that measures a sample's learning difficulty over time. DIH provides a stable indicator of learning progress by tracking the exponential moving average of a sample's hardness. DIHCL enhances learning efficiency and model accuracy without extra computational costs by leveraging data from the training process itself. Tested on 11 datasets, DIHCL surpasses traditional training methods and recent curriculum learning techniques in both efficiency and effectiveness."
    }
  ],
  "dihcl": [
    {
      "title": "Curriculum Learning by Dynamic Instance Hardness",
      "url": "/research/2020/12/06/Curriculum-Learning-by-Dynamic-Instance-Hardness.html",
      "date": "December 6, 2020",
      "summary": "This paper improves curriculum learning via introduces Dynamic Instance Hardness (DIH), a method that measures a sample's learning difficulty over time. DIH provides a stable indicator of learning progress by tracking the exponential moving average of a sample's hardness. DIHCL enhances learning efficiency and model accuracy without extra computational costs by leveraging data from the training process itself. Tested on 11 datasets, DIHCL surpasses traditional training methods and recent curriculum learning techniques in both efficiency and effectiveness."
    }
  ],
  "enhances": [
    {
      "title": "Curriculum Learning by Dynamic Instance Hardness",
      "url": "/research/2020/12/06/Curriculum-Learning-by-Dynamic-Instance-Hardness.html",
      "date": "December 6, 2020",
      "summary": "This paper improves curriculum learning via introduces Dynamic Instance Hardness (DIH), a method that measures a sample's learning difficulty over time. DIH provides a stable indicator of learning progress by tracking the exponential moving average of a sample's hardness. DIHCL enhances learning efficiency and model accuracy without extra computational costs by leveraging data from the training process itself. Tested on 11 datasets, DIHCL surpasses traditional training methods and recent curriculum learning techniques in both efficiency and effectiveness."
    }
  ],
  "iii": [
    {
      "title": "DeiT III: Revenge of the ViT",
      "url": "/research/2023/01/18/DeiT-III-Revenge-of-the-ViT.html",
      "date": "January 18, 2023",
      "summary": "This paper explores the supervised training of Vision Transformers (ViTs) using a simplified training approach adapted from ResNet-50 that includes a novel data-augmentation method with just 3 augmentations. The study demonstrates that this method significantly improves ViTs' performance in image classification, transfer learning, and semantic segmentation over previous supervised training techniques. Moreover, it shows ViTs' performance can match newer architectures, providing a new benchmark for evaluating self-supervised methods on ViTs."
    }
  ],
  "revenge": [
    {
      "title": "DeiT III: Revenge of the ViT",
      "url": "/research/2023/01/18/DeiT-III-Revenge-of-the-ViT.html",
      "date": "January 18, 2023",
      "summary": "This paper explores the supervised training of Vision Transformers (ViTs) using a simplified training approach adapted from ResNet-50 that includes a novel data-augmentation method with just 3 augmentations. The study demonstrates that this method significantly improves ViTs' performance in image classification, transfer learning, and semantic segmentation over previous supervised training techniques. Moreover, it shows ViTs' performance can match newer architectures, providing a new benchmark for evaluating self-supervised methods on ViTs."
    }
  ],
  "diffbir": [
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "This paper introduces DiffBIR, a framework that utilizes pretrained text-to-image diffusion models for blind image restoration. The two-stage pipeline involves pretraining a restoration module on various degradations to enhance real-world applicability, followed by the use of latent diffusion models for realistic restoration. The authors introduce a novel injective modulation sub-network, LAControlNet, for fine-tuning and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, a controllable module allows users to adjust quality and fidelity during the denoising process. Extensive tests demonstrate DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets."
    }
  ],
  "world": [
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "This paper introduces DiffBIR, a framework that utilizes pretrained text-to-image diffusion models for blind image restoration. The two-stage pipeline involves pretraining a restoration module on various degradations to enhance real-world applicability, followed by the use of latent diffusion models for realistic restoration. The authors introduce a novel injective modulation sub-network, LAControlNet, for fine-tuning and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, a controllable module allows users to adjust quality and fidelity during the denoising process. Extensive tests demonstrate DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "The paper introduces a system that trains deep neural networks for object detection using only synthetic images generated with domain randomization. This involves randomizing non-realistic simulator parameters like lighting and object textures, allowing the network to identify essential object features. Their findings show that networks can achieve impressive performance using just synthetic data, and further improve with fine-tuning on real data. This suggests the potential of using cost-effective synthetic data for training, avoiding the need to acquire vast amounts of real-world data or create detailed synthetic environments. They validate their method on car bounding box detection using the KITTI dataset."
    },
    {
      "title": "Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks",
      "url": "/research/2020/06/11/Batch-Normalization-Provably-Avoids-Rank-Collapse-for-Randomly-Initialised-Deep-Networks.html",
      "date": "June 11, 2020",
      "summary": "We investigate the challenge of training randomly initialized deep neural networks due to spectral instabilities in products of random matrices. Batch normalization emerges as an effective solution to prevent rank collapse in both linear and ReLU networks. Leveraging tools from Markov chain theory, we establish a lower rank bound for deep linear networks. Empirical findings show that this rank robustness extends to ReLU networks. Our experiments on real-world datasets underscore the significance of rank stability in training modern deep neural architectures."
    },
    {
      "title": "Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding",
      "url": "/research/2021/11/16/Improved-Robustness-of-Vision-Transformer-via-PreLayerNorm-in-Patch-Embedding.html",
      "date": "November 16, 2021",
      "summary": "Recent advancements in vision transformers (ViTs) have shown superior performance across diverse visual tasks, surpassing convolutional neural networks (CNNs). Given ViT's distinct architecture, understanding its behavior and reliability is imperative. This paper investigates ViT's robustness by comparing it with CNNs under various image corruptions relevant to real-world vision tasks. While ViT generally exhibits comparable or improved robustness over CNNs, it consistently underperforms in contrast enhancement tasks. Analysis suggests that positional embedding in ViT's patch embedding may malfunction with color scale changes. We propose PreLayerNorm, a modified patch embedding structure, to address this issue and ensure scale-invariant behavior in ViT. ViT with PreLayerNorm demonstrates enhanced robustness across various corruptions, particularly in contrast-varying environments."
    },
    {
      "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
      "url": "/research/2021/07/24/The-Many-Faces-of-Robustness-A-Critical-Analysis-of-Out-of-Distribution-Generalization.html",
      "date": "July 24, 2021",
      "summary": "We present four novel real-world distribution shift datasets encompassing changes in image style, blurriness, location, camera settings, and more. Evaluating existing methods for enhancing out-of-distribution robustness, we discover that employing larger models and artificial data augmentations can enhance robustness against real-world distribution shifts, contradicting prior claims. Our findings demonstrate that improvements in artificial robustness benchmarks can indeed transfer to real-world distribution shifts, contrary to prior assumptions. Additionally, we introduce a novel data augmentation technique that surpasses models pretrained with significantly more labeled data, emphasizing its efficacy in addressing real-world distribution shifts. While some methods consistently mitigate texture and local image statistics shifts, they fail to address other shifts like geographic changes. Our results underscore the necessity for future research to examine multiple distribution shifts concurrently, as no single method consistently improves robustness across all evaluated scenarios."
    },
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    },
    {
      "title": "From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning",
      "url": "/research/2023/03/16/From-MNIST-to-ImageNet-and-Back:-Benchmarking-Continual-Curriculum-Learning.html",
      "date": "March 16, 2023",
      "summary": "Continual learning (CL) is a promising trend in machine learning, aiming to develop robust models and strategies for dynamic environments by incorporating new knowledge while retaining past knowledge. CL research is fragmented, with various protocols, tasks, datasets, and metrics, often not reflecting real-world complexity and tailored to specific strategies. This work addresses this gap by introducing two novel CL benchmarks with heterogeneous tasks from six image datasets, varying in complexity and quality. These benchmarks fairly evaluate state-of-the-art CL strategies in scenarios closer to real-world conditions. The benchmarks present tasks in both increasing and decreasing complexity, assessing models' ability to exploit task structure. The work emphasizes a rigorous, reproducible evaluation protocol for measuring generalization and memory retention in models. Experimental results show that popular CL strategies perform poorly on these benchmarks, exhibit high forgetting, and struggle with curriculum task ordering. These findings underscore the need for rigorous comparisons and the development of new CL strategies for complex scenarios."
    },
    {
      "title": "Learning and Leveraging World Models in Visual Representation Learning",
      "url": "/research/2024/03/01/Learning-and-Leveraging-World-Models-in-Visual-Representation-Learning.html",
      "date": "March 1, 2024",
      "summary": "The Joint-Embedding Predictive Architecture (JEPA) is a self-supervised method previously used for predicting missing input parts. This study expands JEPA to predict a wider range of corruptions by introducing Image World Models (IWM), which learn the effects of global photometric transformations in latent space. Key to effective IWM learning are conditioning, prediction difficulty, and model capacity. The study demonstrates that IWM's predictive world model, when fine-tuned, can tackle diverse tasks and either matches or outperforms existing self-supervised techniques. Additionally, it enables control over the abstraction level of learned representations, achieving either invariant or equivariant representations, similar to contrastive methods or masked image modeling, respectively."
    }
  ],
  "modulation": [
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "This paper introduces DiffBIR, a framework that utilizes pretrained text-to-image diffusion models for blind image restoration. The two-stage pipeline involves pretraining a restoration module on various degradations to enhance real-world applicability, followed by the use of latent diffusion models for realistic restoration. The authors introduce a novel injective modulation sub-network, LAControlNet, for fine-tuning and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, a controllable module allows users to adjust quality and fidelity during the denoising process. Extensive tests demonstrate DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets."
    }
  ],
  "subnetwork": [
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "This paper introduces DiffBIR, a framework that utilizes pretrained text-to-image diffusion models for blind image restoration. The two-stage pipeline involves pretraining a restoration module on various degradations to enhance real-world applicability, followed by the use of latent diffusion models for realistic restoration. The authors introduce a novel injective modulation sub-network, LAControlNet, for fine-tuning and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, a controllable module allows users to adjust quality and fidelity during the denoising process. Extensive tests demonstrate DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets."
    }
  ],
  "lacontrolnet": [
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "This paper introduces DiffBIR, a framework that utilizes pretrained text-to-image diffusion models for blind image restoration. The two-stage pipeline involves pretraining a restoration module on various degradations to enhance real-world applicability, followed by the use of latent diffusion models for realistic restoration. The authors introduce a novel injective modulation sub-network, LAControlNet, for fine-tuning and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, a controllable module allows users to adjust quality and fidelity during the denoising process. Extensive tests demonstrate DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets."
    }
  ],
  "user": [
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "This paper introduces DiffBIR, a framework that utilizes pretrained text-to-image diffusion models for blind image restoration. The two-stage pipeline involves pretraining a restoration module on various degradations to enhance real-world applicability, followed by the use of latent diffusion models for realistic restoration. The authors introduce a novel injective modulation sub-network, LAControlNet, for fine-tuning and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, a controllable module allows users to adjust quality and fidelity during the denoising process. Extensive tests demonstrate DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets."
    },
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "superiority": [
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "This paper introduces DiffBIR, a framework that utilizes pretrained text-to-image diffusion models for blind image restoration. The two-stage pipeline involves pretraining a restoration module on various degradations to enhance real-world applicability, followed by the use of latent diffusion models for realistic restoration. The authors introduce a novel injective modulation sub-network, LAControlNet, for fine-tuning and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, a controllable module allows users to adjust quality and fidelity during the denoising process. Extensive tests demonstrate DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets."
    },
    {
      "title": "Eigencurve: Optimal Learning Rate Schedule For Sgd on Quadratic Objectives With Skewed Hessian Spectrums",
      "url": "/research/2022/06/14/Eigencurve-Optimal-Learning-Rate-Schedule-For-Sgd-on-Quadratic-Objectives-With-Skewed-Hessian-Spectrums.html",
      "date": "June 14, 2022",
      "summary": "Learning rate schedulers are essential in deep neural network training, but there's a gap between practical usage and theoretical understanding. This paper introduces Eigencurve, the first set of learning rate schedules achieving minimax optimal convergence rates for SGD on quadratic objectives with skewed eigenvalue distributions of the Hessian matrix. This condition is common in practice. Experimental results on CIFAR-10 image classification tasks demonstrate Eigencurve's superiority over step decay, particularly with fewer epochs. The theory inspires practical schedulers approximating Eigencurve, resembling cosine decay for some problems while outperforming it in others."
    }
  ],
  "fineaction": [
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "This paper introduces FineAction, a large-scale, fine-grained video dataset designed to address the limitations of current temporal action localization (TAL) benchmarks, which rely on coarse action classes and lead to model overfitting and ambiguous annotations. FineAction contains 103K instances across 106 action categories in 17K videos, offering a diverse and densely annotated dataset with co-occurring actions that pose new challenges for TAL. The authors evaluate popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and propose a baseline method achieving a 13.17% mAP. FineAction aims to advance TAL research and is accessible online."
    }
  ],
  "tal": [
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "This paper introduces FineAction, a large-scale, fine-grained video dataset designed to address the limitations of current temporal action localization (TAL) benchmarks, which rely on coarse action classes and lead to model overfitting and ambiguous annotations. FineAction contains 103K instances across 106 action categories in 17K videos, offering a diverse and densely annotated dataset with co-occurring actions that pose new challenges for TAL. The authors evaluate popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and propose a baseline method achieving a 13.17% mAP. FineAction aims to advance TAL research and is accessible online."
    }
  ],
  "across": [
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "This paper introduces FineAction, a large-scale, fine-grained video dataset designed to address the limitations of current temporal action localization (TAL) benchmarks, which rely on coarse action classes and lead to model overfitting and ambiguous annotations. FineAction contains 103K instances across 106 action categories in 17K videos, offering a diverse and densely annotated dataset with co-occurring actions that pose new challenges for TAL. The authors evaluate popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and propose a baseline method achieving a 13.17% mAP. FineAction aims to advance TAL research and is accessible online."
    }
  ],
  "annotate": [
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "This paper introduces FineAction, a large-scale, fine-grained video dataset designed to address the limitations of current temporal action localization (TAL) benchmarks, which rely on coarse action classes and lead to model overfitting and ambiguous annotations. FineAction contains 103K instances across 106 action categories in 17K videos, offering a diverse and densely annotated dataset with co-occurring actions that pose new challenges for TAL. The authors evaluate popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and propose a baseline method achieving a 13.17% mAP. FineAction aims to advance TAL research and is accessible online."
    }
  ],
  "cooccur": [
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "This paper introduces FineAction, a large-scale, fine-grained video dataset designed to address the limitations of current temporal action localization (TAL) benchmarks, which rely on coarse action classes and lead to model overfitting and ambiguous annotations. FineAction contains 103K instances across 106 action categories in 17K videos, offering a diverse and densely annotated dataset with co-occurring actions that pose new challenges for TAL. The authors evaluate popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and propose a baseline method achieving a 13.17% mAP. FineAction aims to advance TAL research and is accessible online."
    }
  ],
  "pose": [
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "This paper introduces FineAction, a large-scale, fine-grained video dataset designed to address the limitations of current temporal action localization (TAL) benchmarks, which rely on coarse action classes and lead to model overfitting and ambiguous annotations. FineAction contains 103K instances across 106 action categories in 17K videos, offering a diverse and densely annotated dataset with co-occurring actions that pose new challenges for TAL. The authors evaluate popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and propose a baseline method achieving a 13.17% mAP. FineAction aims to advance TAL research and is accessible online."
    },
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces an iterative self-training framework for sim-to-real 6D object pose estimation to enable cost-effective robotic bin-picking. The authors create a photo-realistic simulator to synthesize virtual data for training an initial pose estimation network (teacher model). This teacher predicts poses on unlabeled real data, and an adaptive selection scheme filters reliable predictions to generate pseudo-labels for training a student model on real data. Iteratively refining the teacher with the student improves pseudo-label quality. Evaluated on public benchmarks and a new dataset, their method shows 11.49% and 22.62% ADD(-S) improvements and a 19.54% increase in bin-picking success, demonstrating the effectiveness of this iterative sim-to-real approach."
    },
    {
      "title": "Non-negative Contrastive Learning",
      "url": "/research/2024/03/19/Non-negative-Contrastive-Learning.html",
      "date": "March 19, 2024",
      "summary": "Deep representations offer promising performance for downstream tasks but lack interpretability, posing a significant challenge. In this paper, we introduce Non-negative Contrastive Learning (NCL), a refinement of Non-negative Matrix Factorization (NMF) aimed at generating interpretable features. NCL enforces non-negativity constraints, akin to NMF, resulting in sparse and disentangled representations, unlike standard contrastive learning (CL). We establish theoretical guarantees on NCL's identifiability and downstream generalization. Empirically, NCL outperforms CL in feature disentanglement, selection, and downstream classification tasks. Moreover, NCL can be extended to other learning scenarios, benefiting supervised learning."
    }
  ],
  "fixmatch": [
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "This paper introduces FixMatch, a streamlined semi-supervised learning (SSL) algorithm. FixMatch generates pseudo-labels using a model's predictions on weakly-augmented unlabeled images, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness, and the code is publicly available."
    }
  ],
  "confidence": [
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "This paper introduces FixMatch, a streamlined semi-supervised learning (SSL) algorithm. FixMatch generates pseudo-labels using a model's predictions on weakly-augmented unlabeled images, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness, and the code is publicly available."
    },
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    }
  ],
  "approximator": [
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "This paper introduces a Fourier neural network (FNN) that aligns with Fourier decomposition by utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design allows for easy integration with more complex networks for data processing tasks. The authors demonstrate the FNN's efficacy on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits of the FNN include solution validity beyond the training scope, model interpretability, and ease of use."
    }
  ],
  "equation": [
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "This paper introduces a Fourier neural network (FNN) that aligns with Fourier decomposition by utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design allows for easy integration with more complex networks for data processing tasks. The authors demonstrate the FNN's efficacy on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits of the FNN include solution validity beyond the training scope, model interpretability, and ease of use."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces sinusoidal representation networks (SIRENs), which utilize periodic activation functions like Sin to effectively capture complex natural signals and their derivatives, addressing the limitations of neural networks parameterized for continuous, differentiable signal representations. The authors' analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. SIRENs are also applied in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations. The authors extend SIRENs' use with hypernetworks to learn priors for SIREN functions."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "This paper demonstrates that artificial neural networks (ANNs) are equivalent to Gaussian processes at initialization in the infinite-width limit and introduces the Neural Tangent Kernel (NTK), which describes ANNs' behavior during training. The NTK stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. The authors prove the positive-definiteness of the limiting NTK under certain conditions and show that the network function follows a linear differential equation during training for least-squares regression. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/2020/06/17/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "June 17, 2020",
      "summary": "Implicit neural representations, parameterized by continuous, differentiable neural networks, offer numerous advantages over conventional representations. However, existing architectures struggle with fine detail and fail to capture spatial and temporal derivatives, crucial for many physical signals described by partial differential equations. We introduce sinusoidal representation networks (SIRENs) utilizing periodic activation functions, ideal for representing complex signals and their derivatives. Analyzing SIREN activation statistics informs a principled initialization scheme. We demonstrate SIRENs' efficacy in representing various signals and solving boundary value problems, including Eikonal, Poisson, Helmholtz, and wave equations. Additionally, we integrate SIRENs with hypernetworks to learn priors over the SIREN function space. Visit the project website for detailed demonstrations."
    }
  ],
  "solver": [
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "This paper introduces a Fourier neural network (FNN) that aligns with Fourier decomposition by utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design allows for easy integration with more complex networks for data processing tasks. The authors demonstrate the FNN's efficacy on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits of the FNN include solution validity beyond the training scope, model interpretability, and ease of use."
    }
  ],
  "fnn": [
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "This paper introduces a Fourier neural network (FNN) that aligns with Fourier decomposition by utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design allows for easy integration with more complex networks for data processing tasks. The authors demonstrate the FNN's efficacy on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits of the FNN include solution validity beyond the training scope, model interpretability, and ease of use."
    }
  ],
  "decomposition": [
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "This paper introduces a Fourier neural network (FNN) that aligns with Fourier decomposition by utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design allows for easy integration with more complex networks for data processing tasks. The authors demonstrate the FNN's efficacy on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits of the FNN include solution validity beyond the training scope, model interpretability, and ease of use."
    }
  ],
  "mimic": [
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "This paper introduces a Fourier neural network (FNN) that aligns with Fourier decomposition by utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design allows for easy integration with more complex networks for data processing tasks. The authors demonstrate the FNN's efficacy on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits of the FNN include solution validity beyond the training scope, model interpretability, and ease of use."
    },
    {
      "title": "Self-Paced Curriculum Learning",
      "url": "/research/2015/01/01/Self-Paced-Curriculum-Learning.html",
      "date": "January 1, 2015",
      "summary": "This paper introduces self-paced curriculum learning (SPCL), a unified framework combining curriculum learning (CL) and self-paced learning (SPL). SPCL leverages prior knowledge and ongoing learning progress through an optimization problem. It mimics collaborative instructor-student learning, exhibiting empirical advantages in two tasks."
    }
  ],
  "efficacy": [
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "This paper introduces a Fourier neural network (FNN) that aligns with Fourier decomposition by utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design allows for easy integration with more complex networks for data processing tasks. The authors demonstrate the FNN's efficacy on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits of the FNN include solution validity beyond the training scope, model interpretability, and ease of use."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "The paper proposes three normalization-centric modifications to improve Transformer training: PRENORM which introduces pre-norm residual connections and smaller initializations, enabling warmup-free, validation-based training with large learning rates; SCALENORM which suggests L2 normalization with a single scale parameter for faster training and better performance; and FIXNORM which reaffirms the efficacy of normalizing word embeddings to a fixed length."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/2020/06/17/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "June 17, 2020",
      "summary": "Implicit neural representations, parameterized by continuous, differentiable neural networks, offer numerous advantages over conventional representations. However, existing architectures struggle with fine detail and fail to capture spatial and temporal derivatives, crucial for many physical signals described by partial differential equations. We introduce sinusoidal representation networks (SIRENs) utilizing periodic activation functions, ideal for representing complex signals and their derivatives. Analyzing SIREN activation statistics informs a principled initialization scheme. We demonstrate SIRENs' efficacy in representing various signals and solving boundary value problems, including Eikonal, Poisson, Helmholtz, and wave equations. Additionally, we integrate SIRENs with hypernetworks to learn priors over the SIREN function space. Visit the project website for detailed demonstrations."
    },
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    },
    {
      "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
      "url": "/research/2021/07/24/The-Many-Faces-of-Robustness-A-Critical-Analysis-of-Out-of-Distribution-Generalization.html",
      "date": "July 24, 2021",
      "summary": "We present four novel real-world distribution shift datasets encompassing changes in image style, blurriness, location, camera settings, and more. Evaluating existing methods for enhancing out-of-distribution robustness, we discover that employing larger models and artificial data augmentations can enhance robustness against real-world distribution shifts, contradicting prior claims. Our findings demonstrate that improvements in artificial robustness benchmarks can indeed transfer to real-world distribution shifts, contrary to prior assumptions. Additionally, we introduce a novel data augmentation technique that surpasses models pretrained with significantly more labeled data, emphasizing its efficacy in addressing real-world distribution shifts. While some methods consistently mitigate texture and local image statistics shifts, they fail to address other shifts like geographic changes. Our results underscore the necessity for future research to examine multiple distribution shifts concurrently, as no single method consistently improves robustness across all evaluated scenarios."
    }
  ],
  "piecewise": [
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "This paper introduces a Fourier neural network (FNN) that aligns with Fourier decomposition by utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design allows for easy integration with more complex networks for data processing tasks. The authors demonstrate the FNN's efficacy on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits of the FNN include solution validity beyond the training scope, model interpretability, and ease of use."
    }
  ],
  "boundary": [
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "This paper introduces a Fourier neural network (FNN) that aligns with Fourier decomposition by utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design allows for easy integration with more complex networks for data processing tasks. The authors demonstrate the FNN's efficacy on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits of the FNN include solution validity beyond the training scope, model interpretability, and ease of use."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces sinusoidal representation networks (SIRENs), which utilize periodic activation functions like Sin to effectively capture complex natural signals and their derivatives, addressing the limitations of neural networks parameterized for continuous, differentiable signal representations. The authors' analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. SIRENs are also applied in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations. The authors extend SIRENs' use with hypernetworks to learn priors for SIREN functions."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/2020/06/17/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "June 17, 2020",
      "summary": "Implicit neural representations, parameterized by continuous, differentiable neural networks, offer numerous advantages over conventional representations. However, existing architectures struggle with fine detail and fail to capture spatial and temporal derivatives, crucial for many physical signals described by partial differential equations. We introduce sinusoidal representation networks (SIRENs) utilizing periodic activation functions, ideal for representing complex signals and their derivatives. Analyzing SIREN activation statistics informs a principled initialization scheme. We demonstrate SIRENs' efficacy in representing various signals and solving boundary value problems, including Eikonal, Poisson, Helmholtz, and wave equations. Additionally, we integrate SIRENs with hypernetworks to learn priors over the SIREN function space. Visit the project website for detailed demonstrations."
    }
  ],
  "validity": [
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "This paper introduces a Fourier neural network (FNN) that aligns with Fourier decomposition by utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design allows for easy integration with more complex networks for data processing tasks. The authors demonstrate the FNN's efficacy on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits of the FNN include solution validity beyond the training scope, model interpretability, and ease of use."
    }
  ],
  "scope": [
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "This paper introduces a Fourier neural network (FNN) that aligns with Fourier decomposition by utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design allows for easy integration with more complex networks for data processing tasks. The authors demonstrate the FNN's efficacy on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits of the FNN include solution validity beyond the training scope, model interpretability, and ease of use."
    }
  ],
  "interpretability": [
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "This paper introduces a Fourier neural network (FNN) that aligns with Fourier decomposition by utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design allows for easy integration with more complex networks for data processing tasks. The authors demonstrate the FNN's efficacy on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits of the FNN include solution validity beyond the training scope, model interpretability, and ease of use."
    },
    {
      "title": "Non-negative Contrastive Learning",
      "url": "/research/2024/03/19/Non-negative-Contrastive-Learning.html",
      "date": "March 19, 2024",
      "summary": "Deep representations offer promising performance for downstream tasks but lack interpretability, posing a significant challenge. In this paper, we introduce Non-negative Contrastive Learning (NCL), a refinement of Non-negative Matrix Factorization (NMF) aimed at generating interpretable features. NCL enforces non-negativity constraints, akin to NMF, resulting in sparse and disentangled representations, unlike standard contrastive learning (CL). We establish theoretical guarantees on NCL's identifiability and downstream generalization. Empirically, NCL outperforms CL in feature disentanglement, selection, and downstream classification tasks. Moreover, NCL can be extended to other learning scenarios, benefiting supervised learning."
    },
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "ease": [
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "This paper introduces a Fourier neural network (FNN) that aligns with Fourier decomposition by utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design allows for easy integration with more complex networks for data processing tasks. The authors demonstrate the FNN's efficacy on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits of the FNN include solution validity beyond the training scope, model interpretability, and ease of use."
    }
  ],
  "glu": [
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "This paper explores Gated Linear Units (GLUs), which involve the component-wise product of two linear projections, with one undergoing a sigmoid function. The authors investigate GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers. They find that certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    }
  ],
  "gate": [
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "This paper explores Gated Linear Units (GLUs), which involve the component-wise product of two linear projections, with one undergoing a sigmoid function. The authors investigate GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers. They find that certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    },
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "This paper introduces the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. The authors' empirical evaluation across computer vision, natural language processing, and speech tasks demonstrates that GELU offers performance improvements over ReLU and ELU activations."
    },
    {
      "title": "Language Modeling with Gated Convolutional Networks",
      "url": "/research/2017/09/08/Language-Modeling-with-Gated-Convolutional-Networks.html",
      "date": "September 8, 2017",
      "summary": "This paper introduces a novel language modeling approach using stacked convolutions that enable efficient parallel processing, in contrast to the traditionally used recurrent neural networks known for handling unbounded context. The proposed method features a simplified gating mechanism that outperforms previous models and demonstrates superior performance on the WikiText-103 benchmark, showing its capability to manage long-term dependencies. It also delivers competitive results on the Google Billion Words benchmark and significantly reduces sentence scoring latency compared to recurrent models. This marks the first instance of a non-recurrent model achieving comparable success to strong recurrent models on major language tasks."
    }
  ],
  "units": [
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "This paper explores Gated Linear Units (GLUs), which involve the component-wise product of two linear projections, with one undergoing a sigmoid function. The authors investigate GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers. They find that certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    },
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "This paper introduces the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. The authors' empirical evaluation across computer vision, natural language processing, and speech tasks demonstrates that GELU offers performance improvements over ReLU and ELU activations."
    }
  ],
  "product": [
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "This paper explores Gated Linear Units (GLUs), which involve the component-wise product of two linear projections, with one undergoing a sigmoid function. The authors investigate GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers. They find that certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    },
    {
      "title": "Is Cosine-Similarity of Embeddings Really About Similarity?",
      "url": "/research/2024/03/08/Is-Cosine-Similarity-of-Embeddings-Really-About-Similarity.html",
      "date": "March 8, 2024",
      "summary": "Cosine similarity, often used to gauge semantic similarity between high-dimensional objects by comparing low-dimensional feature embeddings, can yield variable results compared to unnormalized dot products. We investigate this phenomenon by analyzing embeddings from regularized linear models, revealing that cosine similarity can produce arbitrary and even meaningless similarities. This applies not only to linear models but also to deep models due to the implicit effects of various regularizations. Consequently, we advise against relying solely on cosine similarity and suggest exploring alternative approaches."
    },
    {
      "title": "Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks",
      "url": "/research/2020/06/11/Batch-Normalization-Provably-Avoids-Rank-Collapse-for-Randomly-Initialised-Deep-Networks.html",
      "date": "June 11, 2020",
      "summary": "We investigate the challenge of training randomly initialized deep neural networks due to spectral instabilities in products of random matrices. Batch normalization emerges as an effective solution to prevent rank collapse in both linear and ReLU networks. Leveraging tools from Markov chain theory, we establish a lower rank bound for deep linear networks. Empirical findings show that this rank robustness extends to ReLU networks. Our experiments on real-world datasets underscore the significance of rank stability in training modern deep neural architectures."
    },
    {
      "title": "Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks",
      "url": "/research/2017/10/23/Cosine-Normalization-Using-Cosine-Similarity-Instead-of-Dot-Product-in-Neural-Networks.html",
      "date": "October 23, 2017",
      "summary": "In traditional multi-layer neural networks, the dot product between the output and weight vectors of preceding layers serves as input to the activation function, resulting in unbounded outputs and increased variance. This variance can lead to poor generalization and hinder training by exacerbating internal covariate shift. To address this, we propose cosine normalization, which replaces the dot product with cosine similarity or centered cosine similarity (Pearson Correlation Coefficient). We evaluate cosine normalization against batch, weight, and layer normalization in fully-connected and convolutional neural networks across various datasets. Our experiments demonstrate that cosine normalization outperforms other normalization techniques."
    },
    {
      "title": "A Phase Transition between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention",
      "url": "/research/2024/02/06/A-Phase-Transition-between-Positional-and-Semantic-Learning-in-a-Solvable-Model-of-Dot-Product-Attention.html",
      "date": "February 6, 2024",
      "summary": "We explore the learning process of a dot-product attention layer, which learns both positional and semantic attention matrices, enabling tokens to attend based on position or meaning. Through experiments on an algorithmic task, we demonstrate that this architecture can use either mechanism for solving the task. Theoretically, we examine a non-linear self-attention layer with special query and key matrices, offering a closed-form solution for its non-convex loss landscape in high-dimensional data, which reveals a phase transition from positional to semantic mechanisms as sample complexity increases. We also show that the dot-product attention layer surpasses a linear positional baseline through the semantic mechanism with adequate data."
    },
    {
      "title": "The loss landscape of deep linear neural networks: a second-order analysis",
      "url": "/research/2022/03/11/The-loss-landscape-of-deep-linear-neural-networks-a-second-order-analysis.html",
      "date": "March 11, 2022",
      "summary": "This study investigates the optimization landscape of deep linear neural networks with square loss, focusing on the role of non-strict saddle points in algorithm dynamics, which has been previously understudied. the authors conduct a comprehensive second-order analysis, identifying global minimizers, strict, and non-strict saddle points among all critical points, alongside their critical values. Our findings, based on conditions related to the ranks of partial matrix products, contribute insights into global convergence and implicit regularization observed in optimizing linear neural networks. Additionally, the authors offer an explicit parameterization of global minimizers and identify extensive sets of strict and non-strict saddle points."
    }
  ],
  "undergo": [
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "This paper explores Gated Linear Units (GLUs), which involve the component-wise product of two linear projections, with one undergoing a sigmoid function. The authors investigate GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers. They find that certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    }
  ],
  "sublayer": [
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "This paper explores Gated Linear Units (GLUs), which involve the component-wise product of two linear projections, with one undergoing a sigmoid function. The authors investigate GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers. They find that certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    },
    {
      "title": "FNet: Mixing Tokens with Fourier Transforms",
      "url": "/research/2022/05/26/FNet-Mixing-Tokens-with-Fourier-Transforms.html",
      "date": "May 26, 2022",
      "summary": "We demonstrate that Transformer encoder architectures can be accelerated with minimal impact on accuracy by substituting self-attention sublayers with simple linear transformations. Remarkably, using a standard Fourier Transform instead of the self-attention sublayer in a Transformer encoder achieves 92-97% of BERT models' accuracy on the GLUE benchmark, while training 80% faster on GPUs and 70% faster on TPUs for standard 512 input lengths. Our FNet model significantly outperforms in speed at longer input lengths, matching the accuracy of the most accurate models on the Long Range Arena benchmark and surpassing the fastest models in speed across all sequence lengths on GPUs and shorter lengths on TPUs. Additionally, FNet is lightweight in memory use and exceptionally efficient in smaller sizes, outperforming Transformer models under the same speed and accuracy constraints."
    }
  ],
  "variants": [
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "This paper explores Gated Linear Units (GLUs), which involve the component-wise product of two linear projections, with one undergoing a sigmoid function. The authors investigate GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers. They find that certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    }
  ],
  "gelu": [
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "This paper explores Gated Linear Units (GLUs), which involve the component-wise product of two linear projections, with one undergoing a sigmoid function. The authors investigate GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers. They find that certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    },
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "This paper introduces the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. The authors' empirical evaluation across computer vision, natural language processing, and speech tasks demonstrates that GELU offers performance improvements over ReLU and ELU activations."
    },
    {
      "title": "SinLU: Sinu-Sigmoidal Linear Uni",
      "url": "/research/2021/12/02/SinLU-Sinu-Sigmoidal-Linear-Uni.html",
      "date": "December 2, 2021",
      "summary": "This paper introduces a non-linear activation function called the Sinu-sigmoidal Linear Unit (SinLU), formulated as SinLU(x) = (x + a sin bx) · σ(x). It incorporates a sine wave for added functionality over traditional linear units, with two trainable parameters controlling the sinusoidal participation. The authors compare SinLU's performance to ReLU, GELU, and SiLU across various domains, models, and standard datasets. The results demonstrate SinLU's robustness and superior performance due to the incorporation of the trainable sine wave parameters, facilitating easy training and fast convergence."
    }
  ],
  "magnitude": [
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "This paper introduces the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. The authors' empirical evaluation across computer vision, natural language processing, and speech tasks demonstrates that GELU offers performance improvements over ReLU and ELU activations."
    }
  ],
  "speech": [
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "This paper introduces the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. The authors' empirical evaluation across computer vision, natural language processing, and speech tasks demonstrates that GELU offers performance improvements over ReLU and ELU activations."
    }
  ],
  "elu": [
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "This paper introduces the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. The authors' empirical evaluation across computer vision, natural language processing, and speech tasks demonstrates that GELU offers performance improvements over ReLU and ELU activations."
    }
  ],
  "grokking": [
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper introduces the concept of grokking, a sudden onset of perfect generalisation long after overfitting, on small, algorithmically generated datasets. The study also finds that generalization on smaller datasets requires more optimization. The authors suggest that such datasets are ideal for investigating the puzzling phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    }
  ],
  "onset": [
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper introduces the concept of grokking, a sudden onset of perfect generalisation long after overfitting, on small, algorithmically generated datasets. The study also finds that generalization on smaller datasets requires more optimization. The authors suggest that such datasets are ideal for investigating the puzzling phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    }
  ],
  "puzzling": [
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper introduces the concept of grokking, a sudden onset of perfect generalisation long after overfitting, on small, algorithmically generated datasets. The study also finds that generalization on smaller datasets requires more optimization. The authors suggest that such datasets are ideal for investigating the puzzling phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    }
  ],
  "overparametrize": [
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper introduces the concept of grokking, a sudden onset of perfect generalisation long after overfitting, on small, algorithmically generated datasets. The study also finds that generalization on smaller datasets requires more optimization. The authors suggest that such datasets are ideal for investigating the puzzling phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    },
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    }
  ],
  "memorize": [
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper introduces the concept of grokking, a sudden onset of perfect generalisation long after overfitting, on small, algorithmically generated datasets. The study also finds that generalization on smaller datasets requires more optimization. The authors suggest that such datasets are ideal for investigating the puzzling phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    },
    {
      "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
      "url": "/research/2023/11/30/Dichotomy-of-Early-and-Late-Phase-Implicit-Biases-Can-Provably-Induce-Grokking.html",
      "date": "November 30, 2023",
      "summary": "Power et al. (2022) discovered that neural networks initially memorize arithmetic tasks, achieving perfect training accuracy but poor test accuracy, then suddenly achieve perfect test accuracy after extended training. This study explains this grokking phenomenon with the theory that early and late phase implicit biases cause this shift. Training homogeneous neural networks with large initialization and small weight decay on classification and regression tasks results in a prolonged period where the network acts like a kernel predictor, followed by a sudden shift to min-norm/max-margin predictors, significantly improving test accuracy."
    }
  ],
  "cifar10100": [
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of Vision Transformers (ViTs) without relying on large-scale pre-training datasets like ImageNet and JFT or requiring modifications to the architecture or loss functions. The authors show that this approach improves ViT performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, while maintaining ViT's attention to relevant regions and robustness, despite ViT's inherent lack of inductive biases and typical dependence on large-scale pre-training."
    }
  ],
  "cinic10": [
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of Vision Transformers (ViTs) without relying on large-scale pre-training datasets like ImageNet and JFT or requiring modifications to the architecture or loss functions. The authors show that this approach improves ViT performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, while maintaining ViT's attention to relevant regions and robustness, despite ViT's inherent lack of inductive biases and typical dependence on large-scale pre-training."
    }
  ],
  "aircraft": [
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of Vision Transformers (ViTs) without relying on large-scale pre-training datasets like ImageNet and JFT or requiring modifications to the architecture or loss functions. The authors show that this approach improves ViT performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, while maintaining ViT's attention to relevant regions and robustness, despite ViT's inherent lack of inductive biases and typical dependence on large-scale pre-training."
    }
  ],
  "car": [
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of Vision Transformers (ViTs) without relying on large-scale pre-training datasets like ImageNet and JFT or requiring modifications to the architecture or loss functions. The authors show that this approach improves ViT performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, while maintaining ViT's attention to relevant regions and robustness, despite ViT's inherent lack of inductive biases and typical dependence on large-scale pre-training."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "The paper introduces a system that trains deep neural networks for object detection using only synthetic images generated with domain randomization. This involves randomizing non-realistic simulator parameters like lighting and object textures, allowing the network to identify essential object features. Their findings show that networks can achieve impressive performance using just synthetic data, and further improve with fine-tuning on real data. This suggests the potential of using cost-effective synthetic data for training, avoiding the need to acquire vast amounts of real-world data or create detailed synthetic environments. They validate their method on car bounding box detection using the KITTI dataset."
    }
  ],
  "dependence": [
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of Vision Transformers (ViTs) without relying on large-scale pre-training datasets like ImageNet and JFT or requiring modifications to the architecture or loss functions. The authors show that this approach improves ViT performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, while maintaining ViT's attention to relevant regions and robustness, despite ViT's inherent lack of inductive biases and typical dependence on large-scale pre-training."
    }
  ],
  "favor": [
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "This study challenges the traditional understanding of how Convolutional Neural Networks (CNNs) recognize objects, revealing that they are more biased towards recognizing textures rather than learning complex shapes. The authors found that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, training a ResNet-50 on a stylized version of ImageNet, designed to emphasize shape, aligned the network's performance more closely with human behavior. This shape-based training matched human performance in controlled experiments and enhanced object detection and robustness against image distortions, highlighting the benefits of shape-based representations in visual recognition systems."
    },
    {
      "title": "Learning Rate Perturbation: A Generic Plugin of Learning Rate schedule towards Flatter Local Minima",
      "url": "/research/2022/08/25/Learning-Rate-Perturbation-A-Generic-Plugin-of-Learning-Rate-schedule-towards-Flatter-Local-Minima.html",
      "date": "August 25, 2022",
      "summary": "Learning rate is crucial in neural network training, yet existing schedules lack theoretical backing, often leading to suboptimal choices made through trial and error. To address this, we propose LEAP, a plugin enhancing various learning rate schedules by introducing perturbations. This simple yet effective strategy favors flat minima, ensuring better generalization. Extensive experiments demonstrate LEAP's ability to improve performance across diverse datasets and learning rate schedules, including constant ones."
    },
    {
      "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
      "url": "/research/2022/03/19/Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training.html",
      "date": "March 19, 2022",
      "summary": "Sharpness-Aware Minimization (SAM) enhances generalization by optimizing a neighborhood-based perturbed loss, but it doesn't always favor flat minima due to both sharp and flat minima having low perturbed losses. We introduce a new measure, the surrogate gap, reflecting the dominant Hessian eigenvalue at small neighborhood radii, which is simple to compute and can be minimized during training. We propose the Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), an advancement over SAM with minimal additional computational cost. GSAM employs a dual-step approach: first, minimizing the perturbed loss similar to SAM, and second, reducing the surrogate gap without affecting the perturbed loss to target regions with low loss and sharpness, thus achieving superior generalization. GSAM is theoretically robust, showing better convergence and empirical generalization improvements, notably a +3.2% gain over SAM and +5.4% over AdamW in ImageNet accuracy for ViT-B/32."
    },
    {
      "title": "Suppressing Pink Elephants with Direct Principle Feedback",
      "url": "/research/2024/02/13/Suppressing-Pink-Elephants-with-Direct-Principle-Feedback.html",
      "date": "February 13, 2024",
      "summary": "Existing methods for controlling language models focus on training desired behaviors, but often lack the flexibility needed for diverse applications. The authors address this with the Pink Elephant Problem, demonstrating the need for language models to adapt to different contexts by avoiding certain topics (the Pink Elephant) in favor of others (Grey Elephant). The authors introduce Direct Principle Feedback (DPF), an adaptation of Constitutional AI that improves control by directly applying critiques and revisions without ranking responses. Our study shows that a 13B LLaMA 2 model fine-tuned with DPF on a synthetic dataset outperforms existing models and matches GPT-4 in managing the Pink Elephant Problem."
    }
  ],
  "stylize": [
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "This study challenges the traditional understanding of how Convolutional Neural Networks (CNNs) recognize objects, revealing that they are more biased towards recognizing textures rather than learning complex shapes. The authors found that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, training a ResNet-50 on a stylized version of ImageNet, designed to emphasize shape, aligned the network's performance more closely with human behavior. This shape-based training matched human performance in controlled experiments and enhanced object detection and robustness against image distortions, highlighting the benefits of shape-based representations in visual recognition systems."
    }
  ],
  "siren": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces sinusoidal representation networks (SIRENs), which utilize periodic activation functions like Sin to effectively capture complex natural signals and their derivatives, addressing the limitations of neural networks parameterized for continuous, differentiable signal representations. The authors' analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. SIRENs are also applied in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations. The authors extend SIRENs' use with hypernetworks to learn priors for SIREN functions."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/2020/06/17/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "June 17, 2020",
      "summary": "Implicit neural representations, parameterized by continuous, differentiable neural networks, offer numerous advantages over conventional representations. However, existing architectures struggle with fine detail and fail to capture spatial and temporal derivatives, crucial for many physical signals described by partial differential equations. We introduce sinusoidal representation networks (SIRENs) utilizing periodic activation functions, ideal for representing complex signals and their derivatives. Analyzing SIREN activation statistics informs a principled initialization scheme. We demonstrate SIRENs' efficacy in representing various signals and solving boundary value problems, including Eikonal, Poisson, Helmholtz, and wave equations. Additionally, we integrate SIRENs with hypernetworks to learn priors over the SIREN function space. Visit the project website for detailed demonstrations."
    }
  ],
  "sin": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces sinusoidal representation networks (SIRENs), which utilize periodic activation functions like Sin to effectively capture complex natural signals and their derivatives, addressing the limitations of neural networks parameterized for continuous, differentiable signal representations. The authors' analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. SIRENs are also applied in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations. The authors extend SIRENs' use with hypernetworks to learn priors for SIREN functions."
    },
    {
      "title": "SinLU: Sinu-Sigmoidal Linear Uni",
      "url": "/research/2021/12/02/SinLU-Sinu-Sigmoidal-Linear-Uni.html",
      "date": "December 2, 2021",
      "summary": "This paper introduces a non-linear activation function called the Sinu-sigmoidal Linear Unit (SinLU), formulated as SinLU(x) = (x + a sin bx) · σ(x). It incorporates a sine wave for added functionality over traditional linear units, with two trainable parameters controlling the sinusoidal participation. The authors compare SinLU's performance to ReLU, GELU, and SiLU across various domains, models, and standard datasets. The results demonstrate SinLU's robustness and superior performance due to the incorporation of the trainable sine wave parameters, facilitating easy training and fast convergence."
    }
  ],
  "derivative": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces sinusoidal representation networks (SIRENs), which utilize periodic activation functions like Sin to effectively capture complex natural signals and their derivatives, addressing the limitations of neural networks parameterized for continuous, differentiable signal representations. The authors' analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. SIRENs are also applied in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations. The authors extend SIRENs' use with hypernetworks to learn priors for SIREN functions."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/2020/06/17/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "June 17, 2020",
      "summary": "Implicit neural representations, parameterized by continuous, differentiable neural networks, offer numerous advantages over conventional representations. However, existing architectures struggle with fine detail and fail to capture spatial and temporal derivatives, crucial for many physical signals described by partial differential equations. We introduce sinusoidal representation networks (SIRENs) utilizing periodic activation functions, ideal for representing complex signals and their derivatives. Analyzing SIREN activation statistics informs a principled initialization scheme. We demonstrate SIRENs' efficacy in representing various signals and solving boundary value problems, including Eikonal, Poisson, Helmholtz, and wave equations. Additionally, we integrate SIRENs with hypernetworks to learn priors over the SIREN function space. Visit the project website for detailed demonstrations."
    },
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    }
  ],
  "initialization": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces sinusoidal representation networks (SIRENs), which utilize periodic activation functions like Sin to effectively capture complex natural signals and their derivatives, addressing the limitations of neural networks parameterized for continuous, differentiable signal representations. The authors' analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. SIRENs are also applied in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations. The authors extend SIRENs' use with hypernetworks to learn priors for SIREN functions."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "This paper demonstrates that artificial neural networks (ANNs) are equivalent to Gaussian processes at initialization in the infinite-width limit and introduces the Neural Tangent Kernel (NTK), which describes ANNs' behavior during training. The NTK stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. The authors prove the positive-definiteness of the limiting NTK under certain conditions and show that the network function follows a linear differential equation during training for least-squares regression. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "The paper proposes three normalization-centric modifications to improve Transformer training: PRENORM which introduces pre-norm residual connections and smaller initializations, enabling warmup-free, validation-based training with large learning rates; SCALENORM which suggests L2 normalization with a single scale parameter for faster training and better performance; and FIXNORM which reaffirms the efficacy of normalizing word embeddings to a fixed length."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/2020/06/17/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "June 17, 2020",
      "summary": "Implicit neural representations, parameterized by continuous, differentiable neural networks, offer numerous advantages over conventional representations. However, existing architectures struggle with fine detail and fail to capture spatial and temporal derivatives, crucial for many physical signals described by partial differential equations. We introduce sinusoidal representation networks (SIRENs) utilizing periodic activation functions, ideal for representing complex signals and their derivatives. Analyzing SIREN activation statistics informs a principled initialization scheme. We demonstrate SIRENs' efficacy in representing various signals and solving boundary value problems, including Eikonal, Poisson, Helmholtz, and wave equations. Additionally, we integrate SIRENs with hypernetworks to learn priors over the SIREN function space. Visit the project website for detailed demonstrations."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "url": "/research/2014/02/19/Exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks.html",
      "date": "February 19, 2014",
      "summary": "This study aims to enhance our comprehension of deep learning dynamics by analyzing deep linear neural networks. Despite their linear input-output mapping, these networks display nonlinear gradient descent dynamics, resulting in phenomena like prolonged plateaus and swift transitions to improved solutions. Through analytical exploration, we reveal that as network depth increases indefinitely, learning speed can remain finite under specific initial conditions. Unsupervised pretraining, under certain data conditions, can discover these favorable initial conditions, unlike random Gaussian initializations. We introduce a novel class of random orthogonal initial conditions that, akin to unsupervised pretraining, enable depth-independent learning. These conditions also ensure gradient propagation in deep nonlinear networks, particularly when operating at the edge of chaos."
    },
    {
      "title": "Normalized Attention Without Probability Cage",
      "url": "/research/2020/05/19/Normalized-Attention-Without-Probability-Cage.html",
      "date": "May 19, 2020",
      "summary": "Softmax-attention architectures, especially popularized by Transformers, have seen significant advancements in various tasks. However, the geometric implications of softmax-attention remain underexplored. In this study, we demonstrate limitations arising from constraining attention weights to the probability simplex and its impact on the convex hull of value vectors. We reveal sequence length-dependent biases in Transformers towards token isolation at initialization and compare them with max- and sum-pooling, which are strong but often overlooked baselines. To address these issues, we propose a novel approach of replacing softmax with normalization in self-attention, resulting in a robust and widely applicable architecture. Our findings are supported by empirical results from over 25,000 trained models, and all results and implementations are publicly available."
    },
    {
      "title": "On Layer Normalization in the Transformer Architecture",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architecture.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is widely used in natural language processing. Training it typically requires a carefully designed learning rate warm-up stage, crucial for performance but slowing optimization and requiring more hyperparameter tuning. We provide theoretical insights into the necessity of the warm-up stage, demonstrating that the location of layer normalization affects gradient behavior. For Post-LN Transformers, with layer normalization between residual blocks, large gradients near the output layer at initialization make training unstable without warm-up. Conversely, for Pre-LN Transformers, with layer normalization inside residual blocks, gradients are well-behaved, suggesting the removal of the warm-up stage. Experimental results show that Pre-LN Transformers without warm-up achieve comparable performance with less training time and hyperparameter tuning."
    },
    {
      "title": "DeepNet: Scaling Transformers to 1,000 Layers",
      "url": "/research/2022/03/01/DeepNet-Scaling-Transformers-to-1,000-Layers.html",
      "date": "March 1, 2022",
      "summary": "This paper introduces DEEPNORM, a novel method to stabilize extremely deep Transformers through a new normalization function and a theoretically derived initialization for the residual connection. DEEPNORM offers a stable and efficient training regime, blending the benefits of Post-LN's performance with Pre-LN's stability, allowing for the scaling of Transformers up to 1,000 layers without difficulty. This represents a significant advancement over previous models, with a 200-layer, 3.2B parameter model outperforming a 48-layer, 12B parameter state-of-the-art model by 5 BLEU points on a multilingual benchmark with 7,482 translation directions, suggesting a promising direction for scaling."
    },
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    },
    {
      "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
      "url": "/research/2023/11/30/Dichotomy-of-Early-and-Late-Phase-Implicit-Biases-Can-Provably-Induce-Grokking.html",
      "date": "November 30, 2023",
      "summary": "Power et al. (2022) discovered that neural networks initially memorize arithmetic tasks, achieving perfect training accuracy but poor test accuracy, then suddenly achieve perfect test accuracy after extended training. This study explains this grokking phenomenon with the theory that early and late phase implicit biases cause this shift. Training homogeneous neural networks with large initialization and small weight decay on classification and regression tasks results in a prolonged period where the network acts like a kernel predictor, followed by a sudden shift to min-norm/max-margin predictors, significantly improving test accuracy."
    }
  ],
  "wavefield": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces sinusoidal representation networks (SIRENs), which utilize periodic activation functions like Sin to effectively capture complex natural signals and their derivatives, addressing the limitations of neural networks parameterized for continuous, differentiable signal representations. The authors' analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. SIRENs are also applied in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations. The authors extend SIRENs' use with hypernetworks to learn priors for SIREN functions."
    }
  ],
  "sound": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces sinusoidal representation networks (SIRENs), which utilize periodic activation functions like Sin to effectively capture complex natural signals and their derivatives, addressing the limitations of neural networks parameterized for continuous, differentiable signal representations. The authors' analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. SIRENs are also applied in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations. The authors extend SIRENs' use with hypernetworks to learn priors for SIREN functions."
    }
  ],
  "poisson": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces sinusoidal representation networks (SIRENs), which utilize periodic activation functions like Sin to effectively capture complex natural signals and their derivatives, addressing the limitations of neural networks parameterized for continuous, differentiable signal representations. The authors' analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. SIRENs are also applied in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations. The authors extend SIRENs' use with hypernetworks to learn priors for SIREN functions."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/2020/06/17/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "June 17, 2020",
      "summary": "Implicit neural representations, parameterized by continuous, differentiable neural networks, offer numerous advantages over conventional representations. However, existing architectures struggle with fine detail and fail to capture spatial and temporal derivatives, crucial for many physical signals described by partial differential equations. We introduce sinusoidal representation networks (SIRENs) utilizing periodic activation functions, ideal for representing complex signals and their derivatives. Analyzing SIREN activation statistics informs a principled initialization scheme. We demonstrate SIRENs' efficacy in representing various signals and solving boundary value problems, including Eikonal, Poisson, Helmholtz, and wave equations. Additionally, we integrate SIRENs with hypernetworks to learn priors over the SIREN function space. Visit the project website for detailed demonstrations."
    }
  ],
  "helmholtz": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces sinusoidal representation networks (SIRENs), which utilize periodic activation functions like Sin to effectively capture complex natural signals and their derivatives, addressing the limitations of neural networks parameterized for continuous, differentiable signal representations. The authors' analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. SIRENs are also applied in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations. The authors extend SIRENs' use with hypernetworks to learn priors for SIREN functions."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/2020/06/17/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "June 17, 2020",
      "summary": "Implicit neural representations, parameterized by continuous, differentiable neural networks, offer numerous advantages over conventional representations. However, existing architectures struggle with fine detail and fail to capture spatial and temporal derivatives, crucial for many physical signals described by partial differential equations. We introduce sinusoidal representation networks (SIRENs) utilizing periodic activation functions, ideal for representing complex signals and their derivatives. Analyzing SIREN activation statistics informs a principled initialization scheme. We demonstrate SIRENs' efficacy in representing various signals and solving boundary value problems, including Eikonal, Poisson, Helmholtz, and wave equations. Additionally, we integrate SIRENs with hypernetworks to learn priors over the SIREN function space. Visit the project website for detailed demonstrations."
    }
  ],
  "hypernetwork": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces sinusoidal representation networks (SIRENs), which utilize periodic activation functions like Sin to effectively capture complex natural signals and their derivatives, addressing the limitations of neural networks parameterized for continuous, differentiable signal representations. The authors' analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. SIRENs are also applied in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations. The authors extend SIRENs' use with hypernetworks to learn priors for SIREN functions."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/2020/06/17/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "June 17, 2020",
      "summary": "Implicit neural representations, parameterized by continuous, differentiable neural networks, offer numerous advantages over conventional representations. However, existing architectures struggle with fine detail and fail to capture spatial and temporal derivatives, crucial for many physical signals described by partial differential equations. We introduce sinusoidal representation networks (SIRENs) utilizing periodic activation functions, ideal for representing complex signals and their derivatives. Analyzing SIREN activation statistics informs a principled initialization scheme. We demonstrate SIRENs' efficacy in representing various signals and solving boundary value problems, including Eikonal, Poisson, Helmholtz, and wave equations. Additionally, we integrate SIRENs with hypernetworks to learn priors over the SIREN function space. Visit the project website for detailed demonstrations."
    }
  ],
  "prior": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces sinusoidal representation networks (SIRENs), which utilize periodic activation functions like Sin to effectively capture complex natural signals and their derivatives, addressing the limitations of neural networks parameterized for continuous, differentiable signal representations. The authors' analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. SIRENs are also applied in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations. The authors extend SIRENs' use with hypernetworks to learn priors for SIREN functions."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/2020/06/17/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "June 17, 2020",
      "summary": "Implicit neural representations, parameterized by continuous, differentiable neural networks, offer numerous advantages over conventional representations. However, existing architectures struggle with fine detail and fail to capture spatial and temporal derivatives, crucial for many physical signals described by partial differential equations. We introduce sinusoidal representation networks (SIRENs) utilizing periodic activation functions, ideal for representing complex signals and their derivatives. Analyzing SIREN activation statistics informs a principled initialization scheme. We demonstrate SIRENs' efficacy in representing various signals and solving boundary value problems, including Eikonal, Poisson, Helmholtz, and wave equations. Additionally, we integrate SIRENs with hypernetworks to learn priors over the SIREN function space. Visit the project website for detailed demonstrations."
    }
  ],
  "child": [
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "This paper introduces a curriculum learning framework for image-caption pretraining, inspired by children's language learning from cognitive science, to address the challenges of aligning multiple concepts from captions to objects in images. The method starts with simple image-caption pairs and gradually increases complexity by adding more concepts, leveraging knowledge from each phase for subsequent learning. This approach outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios."
    }
  ],
  "concepts": [
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "This paper introduces a curriculum learning framework for image-caption pretraining, inspired by children's language learning from cognitive science, to address the challenges of aligning multiple concepts from captions to objects in images. The method starts with simple image-caption pairs and gradually increases complexity by adding more concepts, leveraging knowledge from each phase for subsequent learning. This approach outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios."
    }
  ],
  "lesion": [
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "This paper introduces a novel approach called mixed-order self-paced curriculum learning (Mo-SCL) to address the challenges faced by self-paced curriculum learning (SCL) in medical image analysis tasks, such as universal lesion detection. These challenges include inaccurate difficulty estimation and the under-utilization of hard samples. Mo-SCL combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches. Through theoretical analysis and experiments on the DeepLesion dataset, the authors demonstrate that Mo-SCL enhances lesion detection accuracy in state-of-the-art methods without requiring additional network modifications."
    }
  ],
  "mix": [
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "This paper introduces a novel approach called mixed-order self-paced curriculum learning (Mo-SCL) to address the challenges faced by self-paced curriculum learning (SCL) in medical image analysis tasks, such as universal lesion detection. These challenges include inaccurate difficulty estimation and the under-utilization of hard samples. Mo-SCL combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches. Through theoretical analysis and experiments on the DeepLesion dataset, the authors demonstrate that Mo-SCL enhances lesion detection accuracy in state-of-the-art methods without requiring additional network modifications."
    },
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "This paper introduces tools for training monocular depth estimation models with multiple datasets despite incompatible annotations. Their approach uses a robust training objective, multi-objective learning for data integration, and pretraining encoders on auxiliary tasks. Tested across five datasets, including 3D films as a novel data source, their methods achieve superior zero-shot cross-dataset generalization, outperforming existing benchmarks through principled multi-dataset training."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    },
    {
      "title": "FNet: Mixing Tokens with Fourier Transforms",
      "url": "/research/2022/05/26/FNet-Mixing-Tokens-with-Fourier-Transforms.html",
      "date": "May 26, 2022",
      "summary": "We demonstrate that Transformer encoder architectures can be accelerated with minimal impact on accuracy by substituting self-attention sublayers with simple linear transformations. Remarkably, using a standard Fourier Transform instead of the self-attention sublayer in a Transformer encoder achieves 92-97% of BERT models' accuracy on the GLUE benchmark, while training 80% faster on GPUs and 70% faster on TPUs for standard 512 input lengths. Our FNet model significantly outperforms in speed at longer input lengths, matching the accuracy of the most accurate models on the Long Range Arena benchmark and surpassing the fastest models in speed across all sequence lengths on GPUs and shorter lengths on TPUs. Additionally, FNet is lightweight in memory use and exceptionally efficient in smaller sizes, outperforming Transformer models under the same speed and accuracy constraints."
    },
    {
      "title": "Chronos: Learning the Language of Time Series",
      "url": "/research/2024/03/12/Chronos-Learning-the-Language-of-Time-Series.html",
      "date": "March 12, 2024",
      "summary": "Chronos is a framework that enhances pretrained probabilistic time series models by tokenizing time series data for training with transformer-based architectures, notably the T5 family. Pretrained on a mix of public and synthetic datasets created via Gaussian processes, Chronos models outshine others in 42 benchmark datasets by showing superior performance on familiar datasets and competitive or better zero-shot capabilities on new datasets. This illustrates Chronos's ability to generalize across different domains, offering a simpler approach to forecasting tasks."
    },
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "moscl": [
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "This paper introduces a novel approach called mixed-order self-paced curriculum learning (Mo-SCL) to address the challenges faced by self-paced curriculum learning (SCL) in medical image analysis tasks, such as universal lesion detection. These challenges include inaccurate difficulty estimation and the under-utilization of hard samples. Mo-SCL combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches. Through theoretical analysis and experiments on the DeepLesion dataset, the authors demonstrate that Mo-SCL enhances lesion detection accuracy in state-of-the-art methods without requiring additional network modifications."
    }
  ],
  "scl": [
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "This paper introduces a novel approach called mixed-order self-paced curriculum learning (Mo-SCL) to address the challenges faced by self-paced curriculum learning (SCL) in medical image analysis tasks, such as universal lesion detection. These challenges include inaccurate difficulty estimation and the under-utilization of hard samples. Mo-SCL combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches. Through theoretical analysis and experiments on the DeepLesion dataset, the authors demonstrate that Mo-SCL enhances lesion detection accuracy in state-of-the-art methods without requiring additional network modifications."
    }
  ],
  "mo": [
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "This paper introduces a novel approach called mixed-order self-paced curriculum learning (Mo-SCL) to address the challenges faced by self-paced curriculum learning (SCL) in medical image analysis tasks, such as universal lesion detection. These challenges include inaccurate difficulty estimation and the under-utilization of hard samples. Mo-SCL combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches. Through theoretical analysis and experiments on the DeepLesion dataset, the authors demonstrate that Mo-SCL enhances lesion detection accuracy in state-of-the-art methods without requiring additional network modifications."
    }
  ],
  "uncertainty": [
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "This paper introduces a novel approach called mixed-order self-paced curriculum learning (Mo-SCL) to address the challenges faced by self-paced curriculum learning (SCL) in medical image analysis tasks, such as universal lesion detection. These challenges include inaccurate difficulty estimation and the under-utilization of hard samples. Mo-SCL combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches. Through theoretical analysis and experiments on the DeepLesion dataset, the authors demonstrate that Mo-SCL enhances lesion detection accuracy in state-of-the-art methods without requiring additional network modifications."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "SAM as an Optimal Relaxation of Bayes",
      "url": "/research/2023/12/10/SAM-as-an-Optimal-Relaxation-of-Bayes.html",
      "date": "December 10, 2023",
      "summary": "We introduce SAM, a method enhancing generalization in deep learning, as a relaxation of the Bayes objective. SAM replaces the expected negative-loss with an optimal convex lower bound derived using the Fenchel biconjugate. This connection enables an Adam-like extension of SAM, offering automatic uncertainty estimates and potential accuracy improvements. Bridging adversarial and Bayesian methods, our work paves the way for robustness enhancement."
    }
  ],
  "deeplesion": [
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "This paper introduces a novel approach called mixed-order self-paced curriculum learning (Mo-SCL) to address the challenges faced by self-paced curriculum learning (SCL) in medical image analysis tasks, such as universal lesion detection. These challenges include inaccurate difficulty estimation and the under-utilization of hard samples. Mo-SCL combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches. Through theoretical analysis and experiments on the DeepLesion dataset, the authors demonstrate that Mo-SCL enhances lesion detection accuracy in state-of-the-art methods without requiring additional network modifications."
    }
  ],
  "tangent": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "This paper demonstrates that artificial neural networks (ANNs) are equivalent to Gaussian processes at initialization in the infinite-width limit and introduces the Neural Tangent Kernel (NTK), which describes ANNs' behavior during training. The NTK stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. The authors prove the positive-definiteness of the limiting NTK under certain conditions and show that the network function follows a linear differential equation during training for least-squares regression. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "ann": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "This paper demonstrates that artificial neural networks (ANNs) are equivalent to Gaussian processes at initialization in the infinite-width limit and introduces the Neural Tangent Kernel (NTK), which describes ANNs' behavior during training. The NTK stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. The authors prove the positive-definiteness of the limiting NTK under certain conditions and show that the network function follows a linear differential equation during training for least-squares regression. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "ntk": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "This paper demonstrates that artificial neural networks (ANNs) are equivalent to Gaussian processes at initialization in the infinite-width limit and introduces the Neural Tangent Kernel (NTK), which describes ANNs' behavior during training. The NTK stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. The authors prove the positive-definiteness of the limiting NTK under certain conditions and show that the network function follows a linear differential equation during training for least-squares regression. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "describes": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "This paper demonstrates that artificial neural networks (ANNs) are equivalent to Gaussian processes at initialization in the infinite-width limit and introduces the Neural Tangent Kernel (NTK), which describes ANNs' behavior during training. The NTK stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. The authors prove the positive-definiteness of the limiting NTK under certain conditions and show that the network function follows a linear differential equation during training for least-squares regression. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "anns": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "This paper demonstrates that artificial neural networks (ANNs) are equivalent to Gaussian processes at initialization in the infinite-width limit and introduces the Neural Tangent Kernel (NTK), which describes ANNs' behavior during training. The NTK stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. The authors prove the positive-definiteness of the limiting NTK under certain conditions and show that the network function follows a linear differential equation during training for least-squares regression. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "constant": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "This paper demonstrates that artificial neural networks (ANNs) are equivalent to Gaussian processes at initialization in the infinite-width limit and introduces the Neural Tangent Kernel (NTK), which describes ANNs' behavior during training. The NTK stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. The authors prove the positive-definiteness of the limiting NTK under certain conditions and show that the network function follows a linear differential equation during training for least-squares regression. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "definiteness": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "This paper demonstrates that artificial neural networks (ANNs) are equivalent to Gaussian processes at initialization in the infinite-width limit and introduces the Neural Tangent Kernel (NTK), which describes ANNs' behavior during training. The NTK stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. The authors prove the positive-definiteness of the limiting NTK under certain conditions and show that the network function follows a linear differential equation during training for least-squares regression. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "limiting": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "This paper demonstrates that artificial neural networks (ANNs) are equivalent to Gaussian processes at initialization in the infinite-width limit and introduces the Neural Tangent Kernel (NTK), which describes ANNs' behavior during training. The NTK stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. The authors prove the positive-definiteness of the limiting NTK under certain conditions and show that the network function follows a linear differential equation during training for least-squares regression. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "squares": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "This paper demonstrates that artificial neural networks (ANNs) are equivalent to Gaussian processes at initialization in the infinite-width limit and introduces the Neural Tangent Kernel (NTK), which describes ANNs' behavior during training. The NTK stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. The authors prove the positive-definiteness of the limiting NTK under certain conditions and show that the network function follows a linear differential equation during training for least-squares regression. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "regression": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "This paper demonstrates that artificial neural networks (ANNs) are equivalent to Gaussian processes at initialization in the infinite-width limit and introduces the Neural Tangent Kernel (NTK), which describes ANNs' behavior during training. The NTK stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. The authors prove the positive-definiteness of the limiting NTK under certain conditions and show that the network function follows a linear differential equation during training for least-squares regression. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    },
    {
      "title": "Optimal learning rate schedules in high-dimensional non-convex optimization problems",
      "url": "/research/2022/02/09/Optimal-learning-rate-schedules-in-high-dimensional-non-convex-optimization-problems.html",
      "date": "February 9, 2022",
      "summary": "This paper explores the role of learning rate schedules in high-dimensional and non-convex optimization problems, focusing on Langevin optimization with a decaying learning rate. Analyzing models with Gaussian random functions on N-dimensional spheres, the study reveals that to accelerate optimization without getting trapped in saddles, a decay rate β < 1 is optimal, contrary to convex settings where β = 1 is preferred. Introducing a signal recovery component, the dynamics involve an exploration phase navigating through rough landscape parts and a convergence phase entering convex basins. It's found optimal to maintain a large learning rate during exploration to swiftly exit non-convex regions, then transition to β = 1 for rapid convergence to the solution. These findings are validated in a neural network regression task."
    },
    {
      "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
      "url": "/research/2023/11/30/Dichotomy-of-Early-and-Late-Phase-Implicit-Biases-Can-Provably-Induce-Grokking.html",
      "date": "November 30, 2023",
      "summary": "Power et al. (2022) discovered that neural networks initially memorize arithmetic tasks, achieving perfect training accuracy but poor test accuracy, then suddenly achieve perfect test accuracy after extended training. This study explains this grokking phenomenon with the theory that early and late phase implicit biases cause this shift. Training homogeneous neural networks with large initialization and small weight decay on classification and regression tasks results in a prolonged period where the network acts like a kernel predictor, followed by a sudden shift to min-norm/max-margin predictors, significantly improving test accuracy."
    }
  ],
  "architectur": [
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "This paper investigates the importance of the warm-up stage in training transformers, and the impact of layer normalization placement. Using mean field theory, the authors demonstrate that the original Post-LN Transformer's design results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    }
  ],
  "placement": [
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "This paper investigates the importance of the warm-up stage in training transformers, and the impact of layer normalization placement. Using mean field theory, the authors demonstrate that the original Post-LN Transformer's design results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    }
  ],
  "post": [
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "This paper investigates the importance of the warm-up stage in training transformers, and the impact of layer normalization placement. Using mean field theory, the authors demonstrate that the original Post-LN Transformer's design results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    },
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    },
    {
      "title": "On Layer Normalization in the Transformer Architecture",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architecture.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is widely used in natural language processing. Training it typically requires a carefully designed learning rate warm-up stage, crucial for performance but slowing optimization and requiring more hyperparameter tuning. We provide theoretical insights into the necessity of the warm-up stage, demonstrating that the location of layer normalization affects gradient behavior. For Post-LN Transformers, with layer normalization between residual blocks, large gradients near the output layer at initialization make training unstable without warm-up. Conversely, for Pre-LN Transformers, with layer normalization inside residual blocks, gradients are well-behaved, suggesting the removal of the warm-up stage. Experimental results show that Pre-LN Transformers without warm-up achieve comparable performance with less training time and hyperparameter tuning."
    },
    {
      "title": "DeepNet: Scaling Transformers to 1,000 Layers",
      "url": "/research/2022/03/01/DeepNet-Scaling-Transformers-to-1,000-Layers.html",
      "date": "March 1, 2022",
      "summary": "This paper introduces DEEPNORM, a novel method to stabilize extremely deep Transformers through a new normalization function and a theoretically derived initialization for the residual connection. DEEPNORM offers a stable and efficient training regime, blending the benefits of Post-LN's performance with Pre-LN's stability, allowing for the scaling of Transformers up to 1,000 layers without difficulty. This represents a significant advancement over previous models, with a 200-layer, 3.2B parameter model outperforming a 48-layer, 12B parameter state-of-the-art model by 5 BLEU points on a multilingual benchmark with 7,482 translation directions, suggesting a promising direction for scaling."
    }
  ],
  "ln": [
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "This paper investigates the importance of the warm-up stage in training transformers, and the impact of layer normalization placement. Using mean field theory, the authors demonstrate that the original Post-LN Transformer's design results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    },
    {
      "title": "On Layer Normalization in the Transformer Architecture",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architecture.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is widely used in natural language processing. Training it typically requires a carefully designed learning rate warm-up stage, crucial for performance but slowing optimization and requiring more hyperparameter tuning. We provide theoretical insights into the necessity of the warm-up stage, demonstrating that the location of layer normalization affects gradient behavior. For Post-LN Transformers, with layer normalization between residual blocks, large gradients near the output layer at initialization make training unstable without warm-up. Conversely, for Pre-LN Transformers, with layer normalization inside residual blocks, gradients are well-behaved, suggesting the removal of the warm-up stage. Experimental results show that Pre-LN Transformers without warm-up achieve comparable performance with less training time and hyperparameter tuning."
    }
  ],
  "necessitate": [
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "This paper investigates the importance of the warm-up stage in training transformers, and the impact of layer normalization placement. Using mean field theory, the authors demonstrate that the original Post-LN Transformer's design results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    },
    {
      "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
      "url": "/research/2024/01/24/Better-Call-GPT,-Comparing-Large-Language-Models-Against-Lawyers.html",
      "date": "January 24, 2024",
      "summary": "This paper compares Large Language Models (LLMs) with traditional legal contract reviewers—Junior Lawyers and Legal Process Outsourcers (LPOs). It evaluates whether LLMs outperform humans in accuracy, speed, and cost-efficiency during contract review. Empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, revealing that advanced models match or exceed human accuracy in identifying legal issues. LLMs complete reviews in seconds, vastly outpacing the hours required by humans. Cost-wise, LLMs achieve a 99.97 percent reduction compared to traditional methods. These findings indicate a transformative shift in legal practice, with LLMs enhancing the accessibility and efficiency of legal services. The research suggests that LLM dominance in legal contract review challenges the status quo, necessitating a reimagined future for legal workflows."
    }
  ],
  "warm": [
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "This paper investigates the importance of the warm-up stage in training transformers, and the impact of layer normalization placement. Using mean field theory, the authors demonstrate that the original Post-LN Transformer's design results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    },
    {
      "title": "Large Batch Training of Convolutional Networks",
      "url": "/research/2017/09/13/Large-Batch-Training-of-Convolutional-Networks.html",
      "date": "September 13, 2017",
      "summary": "To expedite the training of large convolutional networks, computational units are added and trained with data-parallel synchronous Stochastic Gradient Descent (SGD) across units, increasing batch size with node count. However, larger batch sizes can reduce model accuracy. The existing method of large batch training—linear learning rate scaling with warm-up—is not universally effective and may cause training divergence. To address these challenges, we introduce a novel training algorithm, Layer-wise Adaptive Rate Scaling (LARS), enabling us to train Alexnet with a batch size of 8K and Resnet-50 with a batch size of 32K, without compromising accuracy."
    },
    {
      "title": "On Layer Normalization in the Transformer Architecture",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architecture.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is widely used in natural language processing. Training it typically requires a carefully designed learning rate warm-up stage, crucial for performance but slowing optimization and requiring more hyperparameter tuning. We provide theoretical insights into the necessity of the warm-up stage, demonstrating that the location of layer normalization affects gradient behavior. For Post-LN Transformers, with layer normalization between residual blocks, large gradients near the output layer at initialization make training unstable without warm-up. Conversely, for Pre-LN Transformers, with layer normalization inside residual blocks, gradients are well-behaved, suggesting the removal of the warm-up stage. Experimental results show that Pre-LN Transformers without warm-up achieve comparable performance with less training time and hyperparameter tuning."
    }
  ],
  "place": [
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "This paper investigates the importance of the warm-up stage in training transformers, and the impact of layer normalization placement. Using mean field theory, the authors demonstrate that the original Post-LN Transformer's design results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    },
    {
      "title": "Deep Residual Learning for Image Recognition",
      "url": "/research/2015/12/10/Deep-Residual-Learning-for-Image-Recognition.html",
      "date": "December 10, 2015",
      "summary": "We introduce a residual learning framework to train deeper neural networks effectively. By reformulating layers to learn residual functions relative to inputs, our approach facilitates optimization and achieves higher accuracy with increased depth. Evaluations on ImageNet demonstrate the effectiveness of residual networks up to 152 layers deep, outperforming previous architectures. Our method achieves 3.57% error on the ImageNet test set, winning 1st place in the ILSVRC 2015 classification task. Additionally, our deep representations lead to a 28% improvement on the COCO object detection dataset. In the ILSVRC & COCO 2015 competitions, our approach secured 1st place in ImageNet detection, localization, COCO detection, and segmentation tasks."
    }
  ],
  "see": [
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "This paper investigates the importance of the warm-up stage in training transformers, and the impact of layer normalization placement. Using mean field theory, the authors demonstrate that the original Post-LN Transformer's design results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    },
    {
      "title": "Normalized Attention Without Probability Cage",
      "url": "/research/2020/05/19/Normalized-Attention-Without-Probability-Cage.html",
      "date": "May 19, 2020",
      "summary": "Softmax-attention architectures, especially popularized by Transformers, have seen significant advancements in various tasks. However, the geometric implications of softmax-attention remain underexplored. In this study, we demonstrate limitations arising from constraining attention weights to the probability simplex and its impact on the convex hull of value vectors. We reveal sequence length-dependent biases in Transformers towards token isolation at initialization and compare them with max- and sum-pooling, which are strong but often overlooked baselines. To address these issues, we propose a novel approach of replacing softmax with normalization in self-attention, resulting in a robust and widely applicable architecture. Our findings are supported by empirical results from over 25,000 trained models, and all results and implementations are publicly available."
    },
    {
      "title": "Do Vision Transformers See Like Convolutional Neural Networks?",
      "url": "/research/2022/03/03/Do-Vision-Transformers-See-Like-Convolutional-Neural-Networks.html",
      "date": "March 3, 2022",
      "summary": "Convolutional Neural Networks (CNNs) have been prominent in processing visual data, but recent studies show Vision Transformers (ViTs) can match or outperform CNNs in image classification. This work examines how ViTs solve classification tasks, discovering significant differences from CNNs, including uniformity in ViT representations across layers due to self-attention and residual connections that enhance feature propagation. ViTs also maintain spatial information effectively, influenced by classification methods. The study further explores how dataset scale affects ViT features and their transferability, linking these findings to new architectures like MLP-Mixer."
    }
  ],
  "preln": [
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "This paper investigates the importance of the warm-up stage in training transformers, and the impact of layer normalization placement. Using mean field theory, the authors demonstrate that the original Post-LN Transformer's design results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    },
    {
      "title": "On Layer Normalization in the Transformer Architecture",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architecture.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is widely used in natural language processing. Training it typically requires a carefully designed learning rate warm-up stage, crucial for performance but slowing optimization and requiring more hyperparameter tuning. We provide theoretical insights into the necessity of the warm-up stage, demonstrating that the location of layer normalization affects gradient behavior. For Post-LN Transformers, with layer normalization between residual blocks, large gradients near the output layer at initialization make training unstable without warm-up. Conversely, for Pre-LN Transformers, with layer normalization inside residual blocks, gradients are well-behaved, suggesting the removal of the warm-up stage. Experimental results show that Pre-LN Transformers without warm-up achieve comparable performance with less training time and hyperparameter tuning."
    }
  ],
  "elimination": [
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "This paper investigates the importance of the warm-up stage in training transformers, and the impact of layer normalization placement. Using mean field theory, the authors demonstrate that the original Post-LN Transformer's design results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    }
  ],
  "flaw": [
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "This paper introduces Online Normalization, a novel method for normalizing neural network hidden activations that offers a batch-independent alternative with comparable accuracy to Batch Normalization. Online Normalization addresses Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. The method is applicable to recurrent, fully connected networks, and those with high activation memory requirements. The authors demonstrate its effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    }
  ],
  "differentiation": [
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "This paper introduces Online Normalization, a novel method for normalizing neural network hidden activations that offers a batch-independent alternative with comparable accuracy to Batch Normalization. Online Normalization addresses Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. The method is applicable to recurrent, fully connected networks, and those with high activation memory requirements. The authors demonstrate its effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    }
  ],
  "connect": [
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "This paper introduces Online Normalization, a novel method for normalizing neural network hidden activations that offers a batch-independent alternative with comparable accuracy to Batch Normalization. Online Normalization addresses Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. The method is applicable to recurrent, fully connected networks, and those with high activation memory requirements. The authors demonstrate its effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    },
    {
      "title": "Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks",
      "url": "/research/2017/10/23/Cosine-Normalization-Using-Cosine-Similarity-Instead-of-Dot-Product-in-Neural-Networks.html",
      "date": "October 23, 2017",
      "summary": "In traditional multi-layer neural networks, the dot product between the output and weight vectors of preceding layers serves as input to the activation function, resulting in unbounded outputs and increased variance. This variance can lead to poor generalization and hinder training by exacerbating internal covariate shift. To address this, we propose cosine normalization, which replaces the dot product with cosine similarity or centered cosine similarity (Pearson Correlation Coefficient). We evaluate cosine normalization against batch, weight, and layer normalization in fully-connected and convolutional neural networks across various datasets. Our experiments demonstrate that cosine normalization outperforms other normalization techniques."
    }
  ],
  "ptb": [
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "This paper introduces Online Normalization, a novel method for normalizing neural network hidden activations that offers a batch-independent alternative with comparable accuracy to Batch Normalization. Online Normalization addresses Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. The method is applicable to recurrent, fully connected networks, and those with high activation memory requirements. The authors demonstrate its effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    }
  ],
  "qlora": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "This paper introduces QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU while maintaining 16-bit task performance. QLORA utilizes 4-bit quantized language models and Low Rank Adapters (LoRA) and incorporates several memory-saving innovations without compromising performance. The authors' best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. The authors applied QLORA to finetune over 1,000 models and analyzed performance across various datasets, model types, and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Their findings suggest that GPT-4 evaluations are a viable substitute for human assessments and question the reliability of current chatbot benchmarks. The authors make their models and 4-bit training code public."
    }
  ],
  "finetuning": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "This paper introduces QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU while maintaining 16-bit task performance. QLORA utilizes 4-bit quantized language models and Low Rank Adapters (LoRA) and incorporates several memory-saving innovations without compromising performance. The authors' best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. The authors applied QLORA to finetune over 1,000 models and analyzed performance across various datasets, model types, and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Their findings suggest that GPT-4 evaluations are a viable substitute for human assessments and question the reliability of current chatbot benchmarks. The authors make their models and 4-bit training code public."
    }
  ],
  "llm": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "This paper introduces QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU while maintaining 16-bit task performance. QLORA utilizes 4-bit quantized language models and Low Rank Adapters (LoRA) and incorporates several memory-saving innovations without compromising performance. The authors' best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. The authors applied QLORA to finetune over 1,000 models and analyzed performance across various datasets, model types, and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Their findings suggest that GPT-4 evaluations are a viable substitute for human assessments and question the reliability of current chatbot benchmarks. The authors make their models and 4-bit training code public."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    },
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    },
    {
      "title": "Neural Networks Learn Statistics of Increasing Complexity",
      "url": "/research/2024/02/13/Neural-Networks-Learn-Statistics-of-Increasing-Complexity.html",
      "date": "February 13, 2024",
      "summary": "The distributional simplicity bias (DSB) theory suggests neural networks first learn basic patterns in data before understanding more complex correlations. We provide new evidence supporting DSB, showing networks initially excel with data matching training set's simple statistics but this ability diminishes later. Extending DSB to discrete domains, we demonstrate an equivalence between n-gram frequencies and vector moments, also observing this bias in large language models (LLMs). Additionally, by adjusting low-level statistics of images to resemble another class, we reveal that networks in early training phases misclassify these edited images as if they belonged to the target class."
    },
    {
      "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
      "url": "/research/2024/03/13/Simple-and-Scalable-Strategies-to-Continually-Pre-train-Large-Language-Models.html",
      "date": "March 13, 2024",
      "summary": "Large language models (LLMs) often require re-training on new data, consuming extensive computational resources. We present an efficient method that combines learning rate re-warming, re-decaying, and data replay, effectively maintaining performance without full re-training. This approach works well across different data distributions, including minor shifts (English→English) and significant shifts (English→German), tested up to 405M and 10B parameter models. Our findings suggest that continual learning strategies can update LLMs with minimal computational cost, rivaling traditional re-training methods. Additionally, we propose alternatives to the cosine learning rate schedule to reduce forgetting, offering more flexibility in learning without a fixed token budget."
    },
    {
      "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
      "url": "/research/2024/02/27/The-Era-of-1-bit-LLMs-All-Large-Language-Models-are-in-1.58-Bits.html",
      "date": "February 27, 2024",
      "summary": "Recent research introduces BitNet b1.58, a ternary (values of -1, 0, 1) 1-bit Large Language Model (LLM) that matches traditional full-precision LLMs in performance and perplexity but is significantly more efficient in terms of latency, memory, throughput, and energy. This work establishes a new scaling law and training approach for future high-performance, cost-effective LLMs, while also facilitating the design of specialized hardware optimized for 1-bit LLMs."
    },
    {
      "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
      "url": "/research/2024/01/24/Better-Call-GPT,-Comparing-Large-Language-Models-Against-Lawyers.html",
      "date": "January 24, 2024",
      "summary": "This paper compares Large Language Models (LLMs) with traditional legal contract reviewers—Junior Lawyers and Legal Process Outsourcers (LPOs). It evaluates whether LLMs outperform humans in accuracy, speed, and cost-efficiency during contract review. Empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, revealing that advanced models match or exceed human accuracy in identifying legal issues. LLMs complete reviews in seconds, vastly outpacing the hours required by humans. Cost-wise, LLMs achieve a 99.97 percent reduction compared to traditional methods. These findings indicate a transformative shift in legal practice, with LLMs enhancing the accessibility and efficiency of legal services. The research suggests that LLM dominance in legal contract review challenges the status quo, necessitating a reimagined future for legal workflows."
    },
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "finetune": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "This paper introduces QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU while maintaining 16-bit task performance. QLORA utilizes 4-bit quantized language models and Low Rank Adapters (LoRA) and incorporates several memory-saving innovations without compromising performance. The authors' best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. The authors applied QLORA to finetune over 1,000 models and analyzed performance across various datasets, model types, and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Their findings suggest that GPT-4 evaluations are a viable substitute for human assessments and question the reliability of current chatbot benchmarks. The authors make their models and 4-bit training code public."
    }
  ],
  "gb": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "This paper introduces QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU while maintaining 16-bit task performance. QLORA utilizes 4-bit quantized language models and Low Rank Adapters (LoRA) and incorporates several memory-saving innovations without compromising performance. The authors' best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. The authors applied QLORA to finetune over 1,000 models and analyzed performance across various datasets, model types, and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Their findings suggest that GPT-4 evaluations are a viable substitute for human assessments and question the reliability of current chatbot benchmarks. The authors make their models and 4-bit training code public."
    }
  ],
  "bit": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "This paper introduces QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU while maintaining 16-bit task performance. QLORA utilizes 4-bit quantized language models and Low Rank Adapters (LoRA) and incorporates several memory-saving innovations without compromising performance. The authors' best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. The authors applied QLORA to finetune over 1,000 models and analyzed performance across various datasets, model types, and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Their findings suggest that GPT-4 evaluations are a viable substitute for human assessments and question the reliability of current chatbot benchmarks. The authors make their models and 4-bit training code public."
    },
    {
      "title": "A comprehensive review of Binary Neural Network",
      "url": "/research/2023/05/11/A-comprehensive-review-of-Binary-Neural-Network.html",
      "date": "May 11, 2023",
      "summary": "Deep learning (DL) is widely used in intelligent systems and real-life applications, but its deployment on computationally limited and energy-constrained devices requires efficient technologies like Binary Neural Networks (BNN). BNNs save significant storage, computation cost, and energy, making them ideal for small devices, despite the trade-offs in memory and performance. This article offers a comprehensive review of BNN developments, focusing on 1-bit activations and weights. It covers the evolution of BNN, from early models to advanced algorithms and design aspects, and addresses BNN optimization, deployment, computing architectures, and diverse applications. It also outlines potential research directions in BNN technology."
    },
    {
      "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
      "url": "/research/2024/02/27/The-Era-of-1-bit-LLMs-All-Large-Language-Models-are-in-1.58-Bits.html",
      "date": "February 27, 2024",
      "summary": "Recent research introduces BitNet b1.58, a ternary (values of -1, 0, 1) 1-bit Large Language Model (LLM) that matches traditional full-precision LLMs in performance and perplexity but is significantly more efficient in terms of latency, memory, throughput, and energy. This work establishes a new scaling law and training approach for future high-performance, cost-effective LLMs, while also facilitating the design of specialized hardware optimized for 1-bit LLMs."
    }
  ],
  "rank": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "This paper introduces QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU while maintaining 16-bit task performance. QLORA utilizes 4-bit quantized language models and Low Rank Adapters (LoRA) and incorporates several memory-saving innovations without compromising performance. The authors' best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. The authors applied QLORA to finetune over 1,000 models and analyzed performance across various datasets, model types, and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Their findings suggest that GPT-4 evaluations are a viable substitute for human assessments and question the reliability of current chatbot benchmarks. The authors make their models and 4-bit training code public."
    },
    {
      "title": "Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks",
      "url": "/research/2020/06/11/Batch-Normalization-Provably-Avoids-Rank-Collapse-for-Randomly-Initialised-Deep-Networks.html",
      "date": "June 11, 2020",
      "summary": "We investigate the challenge of training randomly initialized deep neural networks due to spectral instabilities in products of random matrices. Batch normalization emerges as an effective solution to prevent rank collapse in both linear and ReLU networks. Leveraging tools from Markov chain theory, we establish a lower rank bound for deep linear networks. Empirical findings show that this rank robustness extends to ReLU networks. Our experiments on real-world datasets underscore the significance of rank stability in training modern deep neural architectures."
    },
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    },
    {
      "title": "Suppressing Pink Elephants with Direct Principle Feedback",
      "url": "/research/2024/02/13/Suppressing-Pink-Elephants-with-Direct-Principle-Feedback.html",
      "date": "February 13, 2024",
      "summary": "Existing methods for controlling language models focus on training desired behaviors, but often lack the flexibility needed for diverse applications. The authors address this with the Pink Elephant Problem, demonstrating the need for language models to adapt to different contexts by avoiding certain topics (the Pink Elephant) in favor of others (Grey Elephant). The authors introduce Direct Principle Feedback (DPF), an adaptation of Constitutional AI that improves control by directly applying critiques and revisions without ranking responses. Our study shows that a 13B LLaMA 2 model fine-tuned with DPF on a synthetic dataset outperforms existing models and matches GPT-4 in managing the Pink Elephant Problem."
    },
    {
      "title": "The loss landscape of deep linear neural networks: a second-order analysis",
      "url": "/research/2022/03/11/The-loss-landscape-of-deep-linear-neural-networks-a-second-order-analysis.html",
      "date": "March 11, 2022",
      "summary": "This study investigates the optimization landscape of deep linear neural networks with square loss, focusing on the role of non-strict saddle points in algorithm dynamics, which has been previously understudied. the authors conduct a comprehensive second-order analysis, identifying global minimizers, strict, and non-strict saddle points among all critical points, alongside their critical values. Our findings, based on conditions related to the ranks of partial matrix products, contribute insights into global convergence and implicit regularization observed in optimizing linear neural networks. Additionally, the authors offer an explicit parameterization of global minimizers and identify extensive sets of strict and non-strict saddle points."
    },
    {
      "title": "Towards a Unified Theoretical Understanding of Non-contrastive Learning via Rank Differential Mechanism",
      "url": "/research/2023/03/04/Towards-a-Unified-Theoretical-Understanding-of-Non-contrastive-Learning-via-Rank-Differential-Mechanism.html",
      "date": "March 4, 2023",
      "summary": "Recent non-contrastive learning methods like BYOL, SimSiam, SwAV, and DINO have shown that asymmetric architectural designs can achieve good self-supervised visual learning performance by aligning positive pairs alone, despite a lack of unified theoretical understanding on how these designs prevent feature collapse. This study introduces the Rank Differential Mechanism (RDM) as a unified theoretical framework for non-contrastive learning. RDM demonstrates that these asymmetric designs maintain a consistent rank difference in output features, enhancing effective dimensionality and preventing feature collapse. Unlike previous theories, RDM applies to designs both with and without a predictor, offering a comprehensive understanding of non-contrastive learning methods and guiding the development of new variants. Experiments confirm that these variants perform comparably, if not better than, existing methods on benchmark datasets."
    }
  ],
  "adapters": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "This paper introduces QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU while maintaining 16-bit task performance. QLORA utilizes 4-bit quantized language models and Low Rank Adapters (LoRA) and incorporates several memory-saving innovations without compromising performance. The authors' best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. The authors applied QLORA to finetune over 1,000 models and analyzed performance across various datasets, model types, and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Their findings suggest that GPT-4 evaluations are a viable substitute for human assessments and question the reliability of current chatbot benchmarks. The authors make their models and 4-bit training code public."
    }
  ],
  "lora": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "This paper introduces QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU while maintaining 16-bit task performance. QLORA utilizes 4-bit quantized language models and Low Rank Adapters (LoRA) and incorporates several memory-saving innovations without compromising performance. The authors' best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. The authors applied QLORA to finetune over 1,000 models and analyzed performance across various datasets, model types, and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Their findings suggest that GPT-4 evaluations are a viable substitute for human assessments and question the reliability of current chatbot benchmarks. The authors make their models and 4-bit training code public."
    }
  ],
  "saving": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "This paper introduces QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU while maintaining 16-bit task performance. QLORA utilizes 4-bit quantized language models and Low Rank Adapters (LoRA) and incorporates several memory-saving innovations without compromising performance. The authors' best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. The authors applied QLORA to finetune over 1,000 models and analyzed performance across various datasets, model types, and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Their findings suggest that GPT-4 evaluations are a viable substitute for human assessments and question the reliability of current chatbot benchmarks. The authors make their models and 4-bit training code public."
    }
  ],
  "compromise": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "This paper introduces QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU while maintaining 16-bit task performance. QLORA utilizes 4-bit quantized language models and Low Rank Adapters (LoRA) and incorporates several memory-saving innovations without compromising performance. The authors' best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. The authors applied QLORA to finetune over 1,000 models and analyzed performance across various datasets, model types, and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Their findings suggest that GPT-4 evaluations are a viable substitute for human assessments and question the reliability of current chatbot benchmarks. The authors make their models and 4-bit training code public."
    },
    {
      "title": "Large Batch Training of Convolutional Networks",
      "url": "/research/2017/09/13/Large-Batch-Training-of-Convolutional-Networks.html",
      "date": "September 13, 2017",
      "summary": "To expedite the training of large convolutional networks, computational units are added and trained with data-parallel synchronous Stochastic Gradient Descent (SGD) across units, increasing batch size with node count. However, larger batch sizes can reduce model accuracy. The existing method of large batch training—linear learning rate scaling with warm-up—is not universally effective and may cause training divergence. To address these challenges, we introduce a novel training algorithm, Layer-wise Adaptive Rate Scaling (LARS), enabling us to train Alexnet with a batch size of 8K and Resnet-50 with a batch size of 32K, without compromising accuracy."
    },
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    }
  ],
  "performing": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "This paper introduces QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU while maintaining 16-bit task performance. QLORA utilizes 4-bit quantized language models and Low Rank Adapters (LoRA) and incorporates several memory-saving innovations without compromising performance. The authors' best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. The authors applied QLORA to finetune over 1,000 models and analyzed performance across various datasets, model types, and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Their findings suggest that GPT-4 evaluations are a viable substitute for human assessments and question the reliability of current chatbot benchmarks. The authors make their models and 4-bit training code public."
    },
    {
      "title": "Neural Network Diffusion",
      "url": "/research/2024/02/20/Neural-Network-Diffusion.html",
      "date": "February 20, 2024",
      "summary": "Diffusion models, known for their success in image and video generation, can also generate high-performing neural network parameters, as demonstrated in this study. By employing a simple combination of an autoencoder and a standard latent diffusion model, our method involves extracting latent representations of network parameters, which are then synthesized from random noise by the diffusion model. These new representations, processed through the autoencoder's decoder, serve as fresh network parameters. Tested across various architectures and datasets, this diffusion approach consistently produces models with comparable or superior performance to traditionally trained networks at minimal extra cost. Importantly, the generated models show distinct performance differences from the trained ones, suggesting further exploration into the versatile applications of diffusion models."
    }
  ],
  "guanaco": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "This paper introduces QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU while maintaining 16-bit task performance. QLORA utilizes 4-bit quantized language models and Low Rank Adapters (LoRA) and incorporates several memory-saving innovations without compromising performance. The authors' best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. The authors applied QLORA to finetune over 1,000 models and analyzed performance across various datasets, model types, and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Their findings suggest that GPT-4 evaluations are a viable substitute for human assessments and question the reliability of current chatbot benchmarks. The authors make their models and 4-bit training code public."
    }
  ],
  "vicuna": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "This paper introduces QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU while maintaining 16-bit task performance. QLORA utilizes 4-bit quantized language models and Low Rank Adapters (LoRA) and incorporates several memory-saving innovations without compromising performance. The authors' best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. The authors applied QLORA to finetune over 1,000 models and analyzed performance across various datasets, model types, and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Their findings suggest that GPT-4 evaluations are a viable substitute for human assessments and question the reliability of current chatbot benchmarks. The authors make their models and 4-bit training code public."
    }
  ],
  "scales": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "This paper introduces QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU while maintaining 16-bit task performance. QLORA utilizes 4-bit quantized language models and Low Rank Adapters (LoRA) and incorporates several memory-saving innovations without compromising performance. The authors' best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. The authors applied QLORA to finetune over 1,000 models and analyzed performance across various datasets, model types, and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Their findings suggest that GPT-4 evaluations are a viable substitute for human assessments and question the reliability of current chatbot benchmarks. The authors make their models and 4-bit training code public."
    }
  ],
  "gpt": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "This paper introduces QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU while maintaining 16-bit task performance. QLORA utilizes 4-bit quantized language models and Low Rank Adapters (LoRA) and incorporates several memory-saving innovations without compromising performance. The authors' best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. The authors applied QLORA to finetune over 1,000 models and analyzed performance across various datasets, model types, and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Their findings suggest that GPT-4 evaluations are a viable substitute for human assessments and question the reliability of current chatbot benchmarks. The authors make their models and 4-bit training code public."
    },
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    },
    {
      "title": "Suppressing Pink Elephants with Direct Principle Feedback",
      "url": "/research/2024/02/13/Suppressing-Pink-Elephants-with-Direct-Principle-Feedback.html",
      "date": "February 13, 2024",
      "summary": "Existing methods for controlling language models focus on training desired behaviors, but often lack the flexibility needed for diverse applications. The authors address this with the Pink Elephant Problem, demonstrating the need for language models to adapt to different contexts by avoiding certain topics (the Pink Elephant) in favor of others (Grey Elephant). The authors introduce Direct Principle Feedback (DPF), an adaptation of Constitutional AI that improves control by directly applying critiques and revisions without ranking responses. Our study shows that a 13B LLaMA 2 model fine-tuned with DPF on a synthetic dataset outperforms existing models and matches GPT-4 in managing the Pink Elephant Problem."
    },
    {
      "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
      "url": "/research/2024/01/24/Better-Call-GPT,-Comparing-Large-Language-Models-Against-Lawyers.html",
      "date": "January 24, 2024",
      "summary": "This paper compares Large Language Models (LLMs) with traditional legal contract reviewers—Junior Lawyers and Legal Process Outsourcers (LPOs). It evaluates whether LLMs outperform humans in accuracy, speed, and cost-efficiency during contract review. Empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, revealing that advanced models match or exceed human accuracy in identifying legal issues. LLMs complete reviews in seconds, vastly outpacing the hours required by humans. Cost-wise, LLMs achieve a 99.97 percent reduction compared to traditional methods. These findings indicate a transformative shift in legal practice, with LLMs enhancing the accessibility and efficiency of legal services. The research suggests that LLM dominance in legal contract review challenges the status quo, necessitating a reimagined future for legal workflows."
    }
  ],
  "reliability": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "This paper introduces QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU while maintaining 16-bit task performance. QLORA utilizes 4-bit quantized language models and Low Rank Adapters (LoRA) and incorporates several memory-saving innovations without compromising performance. The authors' best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. The authors applied QLORA to finetune over 1,000 models and analyzed performance across various datasets, model types, and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Their findings suggest that GPT-4 evaluations are a viable substitute for human assessments and question the reliability of current chatbot benchmarks. The authors make their models and 4-bit training code public."
    },
    {
      "title": "Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding",
      "url": "/research/2021/11/16/Improved-Robustness-of-Vision-Transformer-via-PreLayerNorm-in-Patch-Embedding.html",
      "date": "November 16, 2021",
      "summary": "Recent advancements in vision transformers (ViTs) have shown superior performance across diverse visual tasks, surpassing convolutional neural networks (CNNs). Given ViT's distinct architecture, understanding its behavior and reliability is imperative. This paper investigates ViT's robustness by comparing it with CNNs under various image corruptions relevant to real-world vision tasks. While ViT generally exhibits comparable or improved robustness over CNNs, it consistently underperforms in contrast enhancement tasks. Analysis suggests that positional embedding in ViT's patch embedding may malfunction with color scale changes. We propose PreLayerNorm, a modified patch embedding structure, to address this issue and ensure scale-invariant behavior in ViT. ViT with PreLayerNorm demonstrates enhanced robustness across various corruptions, particularly in contrast-varying environments."
    }
  ],
  "chatbot": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "This paper introduces QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU while maintaining 16-bit task performance. QLORA utilizes 4-bit quantized language models and Low Rank Adapters (LoRA) and incorporates several memory-saving innovations without compromising performance. The authors' best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. The authors applied QLORA to finetune over 1,000 models and analyzed performance across various datasets, model types, and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Their findings suggest that GPT-4 evaluations are a viable substitute for human assessments and question the reliability of current chatbot benchmarks. The authors make their models and 4-bit training code public."
    }
  ],
  "quo": [
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "This paper introduces the Kinetics Human Action Video dataset. The authors note that the limited number of videos in existing datasets like UCF-101 and HMDB-51 hinders the ability to effectively evaluate architectures due to similar performance across these small benchmarks. Kinetics contains 400 classes with over 400 video clips per class from challenging YouTube videos. The paper analyzes the impact of this larger dataset on the performance of existing architectures as well as the benefits of pre-training models on Kinetics. The authors introduce the Two-Stream Inflated 3D ConvNet (I3D), an extension of 2D ConvNets to 3D for improved video feature extraction. When pre-trained on Kinetics, this I3D model sets new benchmarks for action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    },
    {
      "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
      "url": "/research/2024/01/24/Better-Call-GPT,-Comparing-Large-Language-Models-Against-Lawyers.html",
      "date": "January 24, 2024",
      "summary": "This paper compares Large Language Models (LLMs) with traditional legal contract reviewers—Junior Lawyers and Legal Process Outsourcers (LPOs). It evaluates whether LLMs outperform humans in accuracy, speed, and cost-efficiency during contract review. Empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, revealing that advanced models match or exceed human accuracy in identifying legal issues. LLMs complete reviews in seconds, vastly outpacing the hours required by humans. Cost-wise, LLMs achieve a 99.97 percent reduction compared to traditional methods. These findings indicate a transformative shift in legal practice, with LLMs enhancing the accessibility and efficiency of legal services. The research suggests that LLM dominance in legal contract review challenges the status quo, necessitating a reimagined future for legal workflows."
    }
  ],
  "vadis": [
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "This paper introduces the Kinetics Human Action Video dataset. The authors note that the limited number of videos in existing datasets like UCF-101 and HMDB-51 hinders the ability to effectively evaluate architectures due to similar performance across these small benchmarks. Kinetics contains 400 classes with over 400 video clips per class from challenging YouTube videos. The paper analyzes the impact of this larger dataset on the performance of existing architectures as well as the benefits of pre-training models on Kinetics. The authors introduce the Two-Stream Inflated 3D ConvNet (I3D), an extension of 2D ConvNets to 3D for improved video feature extraction. When pre-trained on Kinetics, this I3D model sets new benchmarks for action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    }
  ],
  "kinetic": [
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "This paper introduces the Kinetics Human Action Video dataset. The authors note that the limited number of videos in existing datasets like UCF-101 and HMDB-51 hinders the ability to effectively evaluate architectures due to similar performance across these small benchmarks. Kinetics contains 400 classes with over 400 video clips per class from challenging YouTube videos. The paper analyzes the impact of this larger dataset on the performance of existing architectures as well as the benefits of pre-training models on Kinetics. The authors introduce the Two-Stream Inflated 3D ConvNet (I3D), an extension of 2D ConvNets to 3D for improved video feature extraction. When pre-trained on Kinetics, this I3D model sets new benchmarks for action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    },
    {
      "title": "V-Jepa: Latent Video Prediction For Visual Representation Learning",
      "url": "/research/2023/02/15/V-Jepa-Latent-Video-Prediction-For-Visual-Representation-Learning.html",
      "date": "February 15, 2023",
      "summary": "This paper introduces V-JEPA, a self-supervised video learning method that predicts masked spatio-temporal regions in latent space, effectively applying the masked-modelling principle of large language models to video. This approach generates visual features useful across various image and video tasks without needing model adjustment, achieving significant improvements on Kinetics-400 (82.1%) and Something-Something-v2 (71.2%) benchmarks, outperforming prior video models. V-JEPA also excels in motion understanding tasks, surpassing leading image models like DINOv2 and OpenCLIP, and achieves 77.9% on ImageNet classification with video training alone, setting a new standard for video models."
    }
  ],
  "datase": [
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "This paper introduces the Kinetics Human Action Video dataset. The authors note that the limited number of videos in existing datasets like UCF-101 and HMDB-51 hinders the ability to effectively evaluate architectures due to similar performance across these small benchmarks. Kinetics contains 400 classes with over 400 video clips per class from challenging YouTube videos. The paper analyzes the impact of this larger dataset on the performance of existing architectures as well as the benefits of pre-training models on Kinetics. The authors introduce the Two-Stream Inflated 3D ConvNet (I3D), an extension of 2D ConvNets to 3D for improved video feature extraction. When pre-trained on Kinetics, this I3D model sets new benchmarks for action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    }
  ],
  "kinetics": [
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "This paper introduces the Kinetics Human Action Video dataset. The authors note that the limited number of videos in existing datasets like UCF-101 and HMDB-51 hinders the ability to effectively evaluate architectures due to similar performance across these small benchmarks. Kinetics contains 400 classes with over 400 video clips per class from challenging YouTube videos. The paper analyzes the impact of this larger dataset on the performance of existing architectures as well as the benefits of pre-training models on Kinetics. The authors introduce the Two-Stream Inflated 3D ConvNet (I3D), an extension of 2D ConvNets to 3D for improved video feature extraction. When pre-trained on Kinetics, this I3D model sets new benchmarks for action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    },
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "This paper introduces SlowFast networks, a new convolutional architecture for video recognition tasks. These networks have two pathways: a Slow pathway operating at low frame rates to capture spatial semantics, and a Fast pathway working at high frame rates with reduced channel capacity to efficiently capture motion details. The SlowFast models achieve top accuracy on major video benchmarks like Kinetics, Charades, and AVA, significantly improving performance for action classification and detection tasks."
    }
  ],
  "ucf": [
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "This paper introduces the Kinetics Human Action Video dataset. The authors note that the limited number of videos in existing datasets like UCF-101 and HMDB-51 hinders the ability to effectively evaluate architectures due to similar performance across these small benchmarks. Kinetics contains 400 classes with over 400 video clips per class from challenging YouTube videos. The paper analyzes the impact of this larger dataset on the performance of existing architectures as well as the benefits of pre-training models on Kinetics. The authors introduce the Two-Stream Inflated 3D ConvNet (I3D), an extension of 2D ConvNets to 3D for improved video feature extraction. When pre-trained on Kinetics, this I3D model sets new benchmarks for action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    }
  ],
  "hmdb": [
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "This paper introduces the Kinetics Human Action Video dataset. The authors note that the limited number of videos in existing datasets like UCF-101 and HMDB-51 hinders the ability to effectively evaluate architectures due to similar performance across these small benchmarks. Kinetics contains 400 classes with over 400 video clips per class from challenging YouTube videos. The paper analyzes the impact of this larger dataset on the performance of existing architectures as well as the benefits of pre-training models on Kinetics. The authors introduce the Two-Stream Inflated 3D ConvNet (I3D), an extension of 2D ConvNets to 3D for improved video feature extraction. When pre-trained on Kinetics, this I3D model sets new benchmarks for action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    }
  ],
  "hinder": [
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "This paper introduces the Kinetics Human Action Video dataset. The authors note that the limited number of videos in existing datasets like UCF-101 and HMDB-51 hinders the ability to effectively evaluate architectures due to similar performance across these small benchmarks. Kinetics contains 400 classes with over 400 video clips per class from challenging YouTube videos. The paper analyzes the impact of this larger dataset on the performance of existing architectures as well as the benefits of pre-training models on Kinetics. The authors introduce the Two-Stream Inflated 3D ConvNet (I3D), an extension of 2D ConvNets to 3D for improved video feature extraction. When pre-trained on Kinetics, this I3D model sets new benchmarks for action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    },
    {
      "title": "Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks",
      "url": "/research/2017/10/23/Cosine-Normalization-Using-Cosine-Similarity-Instead-of-Dot-Product-in-Neural-Networks.html",
      "date": "October 23, 2017",
      "summary": "In traditional multi-layer neural networks, the dot product between the output and weight vectors of preceding layers serves as input to the activation function, resulting in unbounded outputs and increased variance. This variance can lead to poor generalization and hinder training by exacerbating internal covariate shift. To address this, we propose cosine normalization, which replaces the dot product with cosine similarity or centered cosine similarity (Pearson Correlation Coefficient). We evaluate cosine normalization against batch, weight, and layer normalization in fully-connected and convolutional neural networks across various datasets. Our experiments demonstrate that cosine normalization outperforms other normalization techniques."
    }
  ],
  "youtube": [
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "This paper introduces the Kinetics Human Action Video dataset. The authors note that the limited number of videos in existing datasets like UCF-101 and HMDB-51 hinders the ability to effectively evaluate architectures due to similar performance across these small benchmarks. Kinetics contains 400 classes with over 400 video clips per class from challenging YouTube videos. The paper analyzes the impact of this larger dataset on the performance of existing architectures as well as the benefits of pre-training models on Kinetics. The authors introduce the Two-Stream Inflated 3D ConvNet (I3D), an extension of 2D ConvNets to 3D for improved video feature extraction. When pre-trained on Kinetics, this I3D model sets new benchmarks for action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    }
  ],
  "stream": [
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "This paper introduces the Kinetics Human Action Video dataset. The authors note that the limited number of videos in existing datasets like UCF-101 and HMDB-51 hinders the ability to effectively evaluate architectures due to similar performance across these small benchmarks. Kinetics contains 400 classes with over 400 video clips per class from challenging YouTube videos. The paper analyzes the impact of this larger dataset on the performance of existing architectures as well as the benefits of pre-training models on Kinetics. The authors introduce the Two-Stream Inflated 3D ConvNet (I3D), an extension of 2D ConvNets to 3D for improved video feature extraction. When pre-trained on Kinetics, this I3D model sets new benchmarks for action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    }
  ],
  "i3d": [
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "This paper introduces the Kinetics Human Action Video dataset. The authors note that the limited number of videos in existing datasets like UCF-101 and HMDB-51 hinders the ability to effectively evaluate architectures due to similar performance across these small benchmarks. Kinetics contains 400 classes with over 400 video clips per class from challenging YouTube videos. The paper analyzes the impact of this larger dataset on the performance of existing architectures as well as the benefits of pre-training models on Kinetics. The authors introduce the Two-Stream Inflated 3D ConvNet (I3D), an extension of 2D ConvNets to 3D for improved video feature extraction. When pre-trained on Kinetics, this I3D model sets new benchmarks for action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    }
  ],
  "extension": [
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "This paper introduces the Kinetics Human Action Video dataset. The authors note that the limited number of videos in existing datasets like UCF-101 and HMDB-51 hinders the ability to effectively evaluate architectures due to similar performance across these small benchmarks. Kinetics contains 400 classes with over 400 video clips per class from challenging YouTube videos. The paper analyzes the impact of this larger dataset on the performance of existing architectures as well as the benefits of pre-training models on Kinetics. The authors introduce the Two-Stream Inflated 3D ConvNet (I3D), an extension of 2D ConvNets to 3D for improved video feature extraction. When pre-trained on Kinetics, this I3D model sets new benchmarks for action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    },
    {
      "title": "SAM as an Optimal Relaxation of Bayes",
      "url": "/research/2023/12/10/SAM-as-an-Optimal-Relaxation-of-Bayes.html",
      "date": "December 10, 2023",
      "summary": "We introduce SAM, a method enhancing generalization in deep learning, as a relaxation of the Bayes objective. SAM replaces the expected negative-loss with an optimal convex lower bound derived using the Fenchel biconjugate. This connection enables an Adam-like extension of SAM, offering automatic uncertainty estimates and potential accuracy improvements. Bridging adversarial and Bayesian methods, our work paves the way for robustness enhancement."
    },
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "to": [
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "This paper introduces the Kinetics Human Action Video dataset. The authors note that the limited number of videos in existing datasets like UCF-101 and HMDB-51 hinders the ability to effectively evaluate architectures due to similar performance across these small benchmarks. Kinetics contains 400 classes with over 400 video clips per class from challenging YouTube videos. The paper analyzes the impact of this larger dataset on the performance of existing architectures as well as the benefits of pre-training models on Kinetics. The authors introduce the Two-Stream Inflated 3D ConvNet (I3D), an extension of 2D ConvNets to 3D for improved video feature extraction. When pre-trained on Kinetics, this I3D model sets new benchmarks for action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    }
  ],
  "flow": [
    {
      "title": "Stable Diffusion 3: Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "This paper focuses on improving diffusion models for high-dimensional data like images. The authors enhance noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Their large-scale study shows superior performance over established diffusion methods in high-resolution text-to-image synthesis. A new transformer-based architecture is introduced for better text comprehension, typography, and human preference in text-to-image generation. The findings demonstrate predictable scaling trends, with lower validation loss tied to improved synthesis. The largest models outperform state-of-the-art approaches."
    }
  ],
  "typography": [
    {
      "title": "Stable Diffusion 3: Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "This paper focuses on improving diffusion models for high-dimensional data like images. The authors enhance noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Their large-scale study shows superior performance over established diffusion methods in high-resolution text-to-image synthesis. A new transformer-based architecture is introduced for better text comprehension, typography, and human preference in text-to-image generation. The findings demonstrate predictable scaling trends, with lower validation loss tied to improved synthesis. The largest models outperform state-of-the-art approaches."
    }
  ],
  "preference": [
    {
      "title": "Stable Diffusion 3: Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "This paper focuses on improving diffusion models for high-dimensional data like images. The authors enhance noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Their large-scale study shows superior performance over established diffusion methods in high-resolution text-to-image synthesis. A new transformer-based architecture is introduced for better text comprehension, typography, and human preference in text-to-image generation. The findings demonstrate predictable scaling trends, with lower validation loss tied to improved synthesis. The largest models outperform state-of-the-art approaches."
    }
  ],
  "tie": [
    {
      "title": "Stable Diffusion 3: Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "This paper focuses on improving diffusion models for high-dimensional data like images. The authors enhance noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Their large-scale study shows superior performance over established diffusion methods in high-resolution text-to-image synthesis. A new transformer-based architecture is introduced for better text comprehension, typography, and human preference in text-to-image generation. The findings demonstrate predictable scaling trends, with lower validation loss tied to improved synthesis. The largest models outperform state-of-the-art approaches."
    }
  ],
  "spcl": [
    {
      "title": "Self-Paced Curriculum Learning",
      "url": "/research/2015/01/01/Self-Paced-Curriculum-Learning.html",
      "date": "January 1, 2015",
      "summary": "This paper introduces self-paced curriculum learning (SPCL), a unified framework combining curriculum learning (CL) and self-paced learning (SPL). SPCL leverages prior knowledge and ongoing learning progress through an optimization problem. It mimics collaborative instructor-student learning, exhibiting empirical advantages in two tasks."
    }
  ],
  "spl": [
    {
      "title": "Self-Paced Curriculum Learning",
      "url": "/research/2015/01/01/Self-Paced-Curriculum-Learning.html",
      "date": "January 1, 2015",
      "summary": "This paper introduces self-paced curriculum learning (SPCL), a unified framework combining curriculum learning (CL) and self-paced learning (SPL). SPCL leverages prior knowledge and ongoing learning progress through an optimization problem. It mimics collaborative instructor-student learning, exhibiting empirical advantages in two tasks."
    }
  ],
  "instructor": [
    {
      "title": "Self-Paced Curriculum Learning",
      "url": "/research/2015/01/01/Self-Paced-Curriculum-Learning.html",
      "date": "January 1, 2015",
      "summary": "This paper introduces self-paced curriculum learning (SPCL), a unified framework combining curriculum learning (CL) and self-paced learning (SPL). SPCL leverages prior knowledge and ongoing learning progress through an optimization problem. It mimics collaborative instructor-student learning, exhibiting empirical advantages in two tasks."
    }
  ],
  "approximation": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions, SiLU and dSiLU. It challenges the need for experience replay and separate target networks in deep reinforcement learning. By using on-policy learning with eligibility traces and softmax action selection, it achieves state-of-the-art results in stochastic SZ-Tetris and small-board Tetris with TD(λ) learning and shallow dSiLU agents. It also outperforms DQN in Atari 2600 with a deep Sarsa(λ) agent using SiLU/dSiLU, suggesting an alternative to traditional DQN approaches."
    },
    {
      "title": "Penalizing Gradient Norm for Efficiently Improving Generalization in Deep Learning",
      "url": "/research/2022/06/26/Penalizing-Gradient-Norm-for-Efficiently-Improving-Generalization-in-Deep-Learning.html",
      "date": "June 26, 2022",
      "summary": "This paper proposes a method to enhance the generalization of deep neural networks (DNNs) by penalizing the gradient norm of the loss function during optimization. By constraining the gradient norm, the optimizers tend to find flat minima, improving generalization. We efficiently implement this method using first-order approximation within the gradient descent framework. Experimental results demonstrate improved generalization across various models and datasets. Additionally, we show that a recent method, sharpness-aware minimization, is a special case of our approach, with our method achieving new state-of-the-art performance on tested tasks."
    },
    {
      "title": "How Does Sharpness-Aware Minimization Minimize Sharpness?",
      "url": "/research/2023/01/05/How-Does-Sharpness-Aware-Minimization-Minimize-Sharpness.html",
      "date": "January 5, 2023",
      "summary": "Sharpness-Aware Minimization (SAM) enhances deep neural networks' generalization across various settings. Although SAM aims to penalize model sharpness using a computationally efficient approach, its exact operational mechanism and the sharpness notion it regularizes remain unclear, partly due to differing sharpness concepts used in its theoretical framework and empirical validation. This study identifies the precise sharpness concept SAM regulates and explains its mechanism. It reveals that the combined effect of SAM's two-step approximations, despite being individually misleading, correctly improves generalization when using full-batch gradients. Moreover, we demonstrate that the stochastic SAM version indeed regularizes a third sharpness notion, aligning more closely with practical performance. This effectiveness is attributed to the gradient's alignment with the Hessian's top eigenvector under SAM."
    }
  ],
  "reinforcement": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions, SiLU and dSiLU. It challenges the need for experience replay and separate target networks in deep reinforcement learning. By using on-policy learning with eligibility traces and softmax action selection, it achieves state-of-the-art results in stochastic SZ-Tetris and small-board Tetris with TD(λ) learning and shallow dSiLU agents. It also outperforms DQN in Atari 2600 with a deep Sarsa(λ) agent using SiLU/dSiLU, suggesting an alternative to traditional DQN approaches."
    },
    {
      "title": "Searching for Activation Functions",
      "url": "/research/2017/10/27/Searching-for-Activation-Functions.html",
      "date": "October 27, 2017",
      "summary": "This study explores the impact of activation functions on deep network training and performance. While Rectified Linear Unit (ReLU) is widely used, alternatives have not consistently outperformed it. We propose using automatic search techniques to discover new activation functions. Through exhaustive and reinforcement learning-based searches, we identify novel functions. Empirical evaluation shows that our best discovered function, Swish (f(x) = x · sigmoid(βx)), performs better than ReLU on deeper models across challenging datasets. Replacing ReLUs with Swish units improves classification accuracy on ImageNet, for example, by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. Swish's simplicity and similarity to ReLU facilitate its adoption in neural networks."
    },
    {
      "title": "Weight Agnostic Neural Networks",
      "url": "/research/2019/09/05/Weight-Agnostic-Neural-Networks.html",
      "date": "September 5, 2019",
      "summary": "This study investigates the importance of neural network architectures versus weight parameters for task performance. We introduce a method to find architectures capable of performing tasks without weight training. By assigning random weights, we show that minimal architectures can achieve notable performance on various tasks, including reinforcement learning and MNIST classification."
    },
    {
      "title": "Self-Paced Absolute Learning Progress as a Regularized Approach to Curriculum Learning",
      "url": "/research/2023/06/09/Self-Paced-Absolute-Learning-Progress-as-a-Regularized-Approach-to-Curriculum-Learning.html",
      "date": "June 9, 2023",
      "summary": "Reinforcement Learning's usability is limited by its high computation times. Curriculum Reinforcement Learning accelerates learning by ordering tasks from simple to hard. Curricula based on Absolute Learning Progress (ALP) have shown success in various environments but waste computation on redundant tasks. This issue is addressed by introducing Self-Paced Absolute Learning Progress (SPALP), a regularization method inspired by Self-Paced Learning. Evaluated in three environments, SPALP achieves performance comparable to ALP in all cases and reaches it faster in two. Further improvements in SPALP's efficiency and performance are also discussed."
    }
  ],
  "silu": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions, SiLU and dSiLU. It challenges the need for experience replay and separate target networks in deep reinforcement learning. By using on-policy learning with eligibility traces and softmax action selection, it achieves state-of-the-art results in stochastic SZ-Tetris and small-board Tetris with TD(λ) learning and shallow dSiLU agents. It also outperforms DQN in Atari 2600 with a deep Sarsa(λ) agent using SiLU/dSiLU, suggesting an alternative to traditional DQN approaches."
    },
    {
      "title": "SinLU: Sinu-Sigmoidal Linear Uni",
      "url": "/research/2021/12/02/SinLU-Sinu-Sigmoidal-Linear-Uni.html",
      "date": "December 2, 2021",
      "summary": "This paper introduces a non-linear activation function called the Sinu-sigmoidal Linear Unit (SinLU), formulated as SinLU(x) = (x + a sin bx) · σ(x). It incorporates a sine wave for added functionality over traditional linear units, with two trainable parameters controlling the sinusoidal participation. The authors compare SinLU's performance to ReLU, GELU, and SiLU across various domains, models, and standard datasets. The results demonstrate SinLU's robustness and superior performance due to the incorporation of the trainable sine wave parameters, facilitating easy training and fast convergence."
    }
  ],
  "dsilu": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions, SiLU and dSiLU. It challenges the need for experience replay and separate target networks in deep reinforcement learning. By using on-policy learning with eligibility traces and softmax action selection, it achieves state-of-the-art results in stochastic SZ-Tetris and small-board Tetris with TD(λ) learning and shallow dSiLU agents. It also outperforms DQN in Atari 2600 with a deep Sarsa(λ) agent using SiLU/dSiLU, suggesting an alternative to traditional DQN approaches."
    }
  ],
  "experience": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions, SiLU and dSiLU. It challenges the need for experience replay and separate target networks in deep reinforcement learning. By using on-policy learning with eligibility traces and softmax action selection, it achieves state-of-the-art results in stochastic SZ-Tetris and small-board Tetris with TD(λ) learning and shallow dSiLU agents. It also outperforms DQN in Atari 2600 with a deep Sarsa(λ) agent using SiLU/dSiLU, suggesting an alternative to traditional DQN approaches."
    }
  ],
  "replay": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions, SiLU and dSiLU. It challenges the need for experience replay and separate target networks in deep reinforcement learning. By using on-policy learning with eligibility traces and softmax action selection, it achieves state-of-the-art results in stochastic SZ-Tetris and small-board Tetris with TD(λ) learning and shallow dSiLU agents. It also outperforms DQN in Atari 2600 with a deep Sarsa(λ) agent using SiLU/dSiLU, suggesting an alternative to traditional DQN approaches."
    },
    {
      "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
      "url": "/research/2024/03/13/Simple-and-Scalable-Strategies-to-Continually-Pre-train-Large-Language-Models.html",
      "date": "March 13, 2024",
      "summary": "Large language models (LLMs) often require re-training on new data, consuming extensive computational resources. We present an efficient method that combines learning rate re-warming, re-decaying, and data replay, effectively maintaining performance without full re-training. This approach works well across different data distributions, including minor shifts (English→English) and significant shifts (English→German), tested up to 405M and 10B parameter models. Our findings suggest that continual learning strategies can update LLMs with minimal computational cost, rivaling traditional re-training methods. Additionally, we propose alternatives to the cosine learning rate schedule to reduce forgetting, offering more flexibility in learning without a fixed token budget."
    }
  ],
  "policy": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions, SiLU and dSiLU. It challenges the need for experience replay and separate target networks in deep reinforcement learning. By using on-policy learning with eligibility traces and softmax action selection, it achieves state-of-the-art results in stochastic SZ-Tetris and small-board Tetris with TD(λ) learning and shallow dSiLU agents. It also outperforms DQN in Atari 2600 with a deep Sarsa(λ) agent using SiLU/dSiLU, suggesting an alternative to traditional DQN approaches."
    },
    {
      "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum",
      "url": "/research/2023/09/30/When-to-Learn-What:-Model-Adaptive-Data-Augmentation-Curriculum.html",
      "date": "September 30, 2023",
      "summary": "Data augmentation (DA) improves neural network generalization by enforcing invariances and symmetries to predefined transformations applied to input data. However, fixed augmentation policies affect samples differently at various training stages, and existing approaches cannot adapt policies to individual samples and the training model. This paper proposes Model-Adaptive Data Augmentation (MADAug) which trains an augmentation policy network to determine when to learn what. Unlike previous work, MADAug selects augmentation operators for each input image with a model-adaptive policy that varies between training stages, creating an optimized data augmentation curriculum. The policy is trained using a bi-level optimization scheme to minimize validation-set loss of a model trained with policy-produced augmentations. Extensive evaluations on multiple image classification tasks and network architectures show that MADAug outperforms or matches existing DA approaches, enhances fairness by improving all classes, particularly the difficult ones, and performs better when transferred to fine-grained datasets. Additionally, the auto-optimized policy in MADAug gradually increases perturbations, forming an easy-to-hard curriculum."
    }
  ],
  "eligibility": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions, SiLU and dSiLU. It challenges the need for experience replay and separate target networks in deep reinforcement learning. By using on-policy learning with eligibility traces and softmax action selection, it achieves state-of-the-art results in stochastic SZ-Tetris and small-board Tetris with TD(λ) learning and shallow dSiLU agents. It also outperforms DQN in Atari 2600 with a deep Sarsa(λ) agent using SiLU/dSiLU, suggesting an alternative to traditional DQN approaches."
    }
  ],
  "trace": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions, SiLU and dSiLU. It challenges the need for experience replay and separate target networks in deep reinforcement learning. By using on-policy learning with eligibility traces and softmax action selection, it achieves state-of-the-art results in stochastic SZ-Tetris and small-board Tetris with TD(λ) learning and shallow dSiLU agents. It also outperforms DQN in Atari 2600 with a deep Sarsa(λ) agent using SiLU/dSiLU, suggesting an alternative to traditional DQN approaches."
    }
  ],
  "selection": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions, SiLU and dSiLU. It challenges the need for experience replay and separate target networks in deep reinforcement learning. By using on-policy learning with eligibility traces and softmax action selection, it achieves state-of-the-art results in stochastic SZ-Tetris and small-board Tetris with TD(λ) learning and shallow dSiLU agents. It also outperforms DQN in Atari 2600 with a deep Sarsa(λ) agent using SiLU/dSiLU, suggesting an alternative to traditional DQN approaches."
    },
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces an iterative self-training framework for sim-to-real 6D object pose estimation to enable cost-effective robotic bin-picking. The authors create a photo-realistic simulator to synthesize virtual data for training an initial pose estimation network (teacher model). This teacher predicts poses on unlabeled real data, and an adaptive selection scheme filters reliable predictions to generate pseudo-labels for training a student model on real data. Iteratively refining the teacher with the student improves pseudo-label quality. Evaluated on public benchmarks and a new dataset, their method shows 11.49% and 22.62% ADD(-S) improvements and a 19.54% increase in bin-picking success, demonstrating the effectiveness of this iterative sim-to-real approach."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    },
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    },
    {
      "title": "Non-negative Contrastive Learning",
      "url": "/research/2024/03/19/Non-negative-Contrastive-Learning.html",
      "date": "March 19, 2024",
      "summary": "Deep representations offer promising performance for downstream tasks but lack interpretability, posing a significant challenge. In this paper, we introduce Non-negative Contrastive Learning (NCL), a refinement of Non-negative Matrix Factorization (NMF) aimed at generating interpretable features. NCL enforces non-negativity constraints, akin to NMF, resulting in sparse and disentangled representations, unlike standard contrastive learning (CL). We establish theoretical guarantees on NCL's identifiability and downstream generalization. Empirically, NCL outperforms CL in feature disentanglement, selection, and downstream classification tasks. Moreover, NCL can be extended to other learning scenarios, benefiting supervised learning."
    },
    {
      "title": "Transformers with Learnable Activation Functions",
      "url": "/research/2023/02/14/Transformers-with-Learnable-Activation-Functions.html",
      "date": "February 14, 2023",
      "summary": "Activation functions significantly affect model performance by reducing data complexity, yet their selection in Transformer-based language models is often overlooked. This paper explores the impact of using rational activation functions (RAFs), which unlike fixed activation functions (FAFs), can learn optimal functions from data. Our experiments demonstrate that Transformer models with RAFs (RAFT) outperform those with FAFs (FAFT), achieving a 5.71 point higher score on the GLUE benchmark with only 100 training examples and a 2.05 point increase on SQuAD with full data. The varied shapes of learned RAFs across layers and tasks suggest a new method for analyzing and understanding large pre-trained language models."
    }
  ],
  "sz": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions, SiLU and dSiLU. It challenges the need for experience replay and separate target networks in deep reinforcement learning. By using on-policy learning with eligibility traces and softmax action selection, it achieves state-of-the-art results in stochastic SZ-Tetris and small-board Tetris with TD(λ) learning and shallow dSiLU agents. It also outperforms DQN in Atari 2600 with a deep Sarsa(λ) agent using SiLU/dSiLU, suggesting an alternative to traditional DQN approaches."
    }
  ],
  "tetris": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions, SiLU and dSiLU. It challenges the need for experience replay and separate target networks in deep reinforcement learning. By using on-policy learning with eligibility traces and softmax action selection, it achieves state-of-the-art results in stochastic SZ-Tetris and small-board Tetris with TD(λ) learning and shallow dSiLU agents. It also outperforms DQN in Atari 2600 with a deep Sarsa(λ) agent using SiLU/dSiLU, suggesting an alternative to traditional DQN approaches."
    }
  ],
  "board": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions, SiLU and dSiLU. It challenges the need for experience replay and separate target networks in deep reinforcement learning. By using on-policy learning with eligibility traces and softmax action selection, it achieves state-of-the-art results in stochastic SZ-Tetris and small-board Tetris with TD(λ) learning and shallow dSiLU agents. It also outperforms DQN in Atari 2600 with a deep Sarsa(λ) agent using SiLU/dSiLU, suggesting an alternative to traditional DQN approaches."
    }
  ],
  "tda": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions, SiLU and dSiLU. It challenges the need for experience replay and separate target networks in deep reinforcement learning. By using on-policy learning with eligibility traces and softmax action selection, it achieves state-of-the-art results in stochastic SZ-Tetris and small-board Tetris with TD(λ) learning and shallow dSiLU agents. It also outperforms DQN in Atari 2600 with a deep Sarsa(λ) agent using SiLU/dSiLU, suggesting an alternative to traditional DQN approaches."
    }
  ],
  "agent": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions, SiLU and dSiLU. It challenges the need for experience replay and separate target networks in deep reinforcement learning. By using on-policy learning with eligibility traces and softmax action selection, it achieves state-of-the-art results in stochastic SZ-Tetris and small-board Tetris with TD(λ) learning and shallow dSiLU agents. It also outperforms DQN in Atari 2600 with a deep Sarsa(λ) agent using SiLU/dSiLU, suggesting an alternative to traditional DQN approaches."
    }
  ],
  "dqn": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions, SiLU and dSiLU. It challenges the need for experience replay and separate target networks in deep reinforcement learning. By using on-policy learning with eligibility traces and softmax action selection, it achieves state-of-the-art results in stochastic SZ-Tetris and small-board Tetris with TD(λ) learning and shallow dSiLU agents. It also outperforms DQN in Atari 2600 with a deep Sarsa(λ) agent using SiLU/dSiLU, suggesting an alternative to traditional DQN approaches."
    }
  ],
  "atari": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions, SiLU and dSiLU. It challenges the need for experience replay and separate target networks in deep reinforcement learning. By using on-policy learning with eligibility traces and softmax action selection, it achieves state-of-the-art results in stochastic SZ-Tetris and small-board Tetris with TD(λ) learning and shallow dSiLU agents. It also outperforms DQN in Atari 2600 with a deep Sarsa(λ) agent using SiLU/dSiLU, suggesting an alternative to traditional DQN approaches."
    }
  ],
  "sarsaa": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions, SiLU and dSiLU. It challenges the need for experience replay and separate target networks in deep reinforcement learning. By using on-policy learning with eligibility traces and softmax action selection, it achieves state-of-the-art results in stochastic SZ-Tetris and small-board Tetris with TD(λ) learning and shallow dSiLU agents. It also outperforms DQN in Atari 2600 with a deep Sarsa(λ) agent using SiLU/dSiLU, suggesting an alternative to traditional DQN approaches."
    }
  ],
  "siludsilu": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions, SiLU and dSiLU. It challenges the need for experience replay and separate target networks in deep reinforcement learning. By using on-policy learning with eligibility traces and softmax action selection, it achieves state-of-the-art results in stochastic SZ-Tetris and small-board Tetris with TD(λ) learning and shallow dSiLU agents. It also outperforms DQN in Atari 2600 with a deep Sarsa(λ) agent using SiLU/dSiLU, suggesting an alternative to traditional DQN approaches."
    }
  ],
  "sim": [
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces an iterative self-training framework for sim-to-real 6D object pose estimation to enable cost-effective robotic bin-picking. The authors create a photo-realistic simulator to synthesize virtual data for training an initial pose estimation network (teacher model). This teacher predicts poses on unlabeled real data, and an adaptive selection scheme filters reliable predictions to generate pseudo-labels for training a student model on real data. Iteratively refining the teacher with the student improves pseudo-label quality. Evaluated on public benchmarks and a new dataset, their method shows 11.49% and 22.62% ADD(-S) improvements and a 19.54% increase in bin-picking success, demonstrating the effectiveness of this iterative sim-to-real approach."
    }
  ],
  "bin": [
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces an iterative self-training framework for sim-to-real 6D object pose estimation to enable cost-effective robotic bin-picking. The authors create a photo-realistic simulator to synthesize virtual data for training an initial pose estimation network (teacher model). This teacher predicts poses on unlabeled real data, and an adaptive selection scheme filters reliable predictions to generate pseudo-labels for training a student model on real data. Iteratively refining the teacher with the student improves pseudo-label quality. Evaluated on public benchmarks and a new dataset, their method shows 11.49% and 22.62% ADD(-S) improvements and a 19.54% increase in bin-picking success, demonstrating the effectiveness of this iterative sim-to-real approach."
    }
  ],
  "pick": [
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces an iterative self-training framework for sim-to-real 6D object pose estimation to enable cost-effective robotic bin-picking. The authors create a photo-realistic simulator to synthesize virtual data for training an initial pose estimation network (teacher model). This teacher predicts poses on unlabeled real data, and an adaptive selection scheme filters reliable predictions to generate pseudo-labels for training a student model on real data. Iteratively refining the teacher with the student improves pseudo-label quality. Evaluated on public benchmarks and a new dataset, their method shows 11.49% and 22.62% ADD(-S) improvements and a 19.54% increase in bin-picking success, demonstrating the effectiveness of this iterative sim-to-real approach."
    }
  ],
  "picking": [
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces an iterative self-training framework for sim-to-real 6D object pose estimation to enable cost-effective robotic bin-picking. The authors create a photo-realistic simulator to synthesize virtual data for training an initial pose estimation network (teacher model). This teacher predicts poses on unlabeled real data, and an adaptive selection scheme filters reliable predictions to generate pseudo-labels for training a student model on real data. Iteratively refining the teacher with the student improves pseudo-label quality. Evaluated on public benchmarks and a new dataset, their method shows 11.49% and 22.62% ADD(-S) improvements and a 19.54% increase in bin-picking success, demonstrating the effectiveness of this iterative sim-to-real approach."
    }
  ],
  "simulator": [
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces an iterative self-training framework for sim-to-real 6D object pose estimation to enable cost-effective robotic bin-picking. The authors create a photo-realistic simulator to synthesize virtual data for training an initial pose estimation network (teacher model). This teacher predicts poses on unlabeled real data, and an adaptive selection scheme filters reliable predictions to generate pseudo-labels for training a student model on real data. Iteratively refining the teacher with the student improves pseudo-label quality. Evaluated on public benchmarks and a new dataset, their method shows 11.49% and 22.62% ADD(-S) improvements and a 19.54% increase in bin-picking success, demonstrating the effectiveness of this iterative sim-to-real approach."
    },
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "The paper investigates the use of synthetic data generated by graphics simulators for pre-training computer vision models. The authors find that model performance on downstream tasks varies with different simulation parameters used to generate the synthetic data. They introduce Task2Sim, a model that maps the requirements of a downstream task to the optimal simulation parameters for generating synthetic pre-training data."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "The paper introduces a system that trains deep neural networks for object detection using only synthetic images generated with domain randomization. This involves randomizing non-realistic simulator parameters like lighting and object textures, allowing the network to identify essential object features. Their findings show that networks can achieve impressive performance using just synthetic data, and further improve with fine-tuning on real data. This suggests the potential of using cost-effective synthetic data for training, avoiding the need to acquire vast amounts of real-world data or create detailed synthetic environments. They validate their method on car bounding box detection using the KITTI dataset."
    }
  ],
  "adds": [
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "This paper introduces an iterative self-training framework for sim-to-real 6D object pose estimation to enable cost-effective robotic bin-picking. The authors create a photo-realistic simulator to synthesize virtual data for training an initial pose estimation network (teacher model). This teacher predicts poses on unlabeled real data, and an adaptive selection scheme filters reliable predictions to generate pseudo-labels for training a student model on real data. Iteratively refining the teacher with the student improves pseudo-label quality. Evaluated on public benchmarks and a new dataset, their method shows 11.49% and 22.62% ADD(-S) improvements and a 19.54% increase in bin-picking success, demonstrating the effectiveness of this iterative sim-to-real approach."
    }
  ],
  "net": [
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "This paper introduces an algorithm that identifies simple and highly generalizable neural networks by searching for extensive flat minima regions in the error function, where the error rate is relatively stable. Such flat minima are associated with lower overfitting risks based on minimum description length (MDL) principles. Despite requiring second-order derivative calculations, the algorithm has a complexity level comparable to backpropagation. When tested on feedforward and recurrent networks, as well as stock market prediction tasks, this algorithm outperformed traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    },
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    }
  ],
  "minima": [
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "This paper introduces an algorithm that identifies simple and highly generalizable neural networks by searching for extensive flat minima regions in the error function, where the error rate is relatively stable. Such flat minima are associated with lower overfitting risks based on minimum description length (MDL) principles. Despite requiring second-order derivative calculations, the algorithm has a complexity level comparable to backpropagation. When tested on feedforward and recurrent networks, as well as stock market prediction tasks, this algorithm outperformed traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    },
    {
      "title": "Taming the Waves: Sine as Activation Function in Deep Neural Networks",
      "url": "/research/2016/11/04/Taming-the-Waves-Sine-as-Activation-Function-in-Deep-Neural-Networks.html",
      "date": "November 4, 2016",
      "summary": "This paper explores the challenges in training deep neural networks that use sinusoidal activation functions. The authors explain that the difficulty arises from the emergence of numerous shallow local minima in the loss landscape. The study reveals that successful learning in typical classification tasks occurs when the network effectively ignores the periodic cycles of the sinusoidal functions. However, for certain non-trivial tasks, networks with sinusoidal activations can outperform those using traditional monotonic activation functions."
    },
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    },
    {
      "title": "Learning Rate Perturbation: A Generic Plugin of Learning Rate schedule towards Flatter Local Minima",
      "url": "/research/2022/08/25/Learning-Rate-Perturbation-A-Generic-Plugin-of-Learning-Rate-schedule-towards-Flatter-Local-Minima.html",
      "date": "August 25, 2022",
      "summary": "Learning rate is crucial in neural network training, yet existing schedules lack theoretical backing, often leading to suboptimal choices made through trial and error. To address this, we propose LEAP, a plugin enhancing various learning rate schedules by introducing perturbations. This simple yet effective strategy favors flat minima, ensuring better generalization. Extensive experiments demonstrate LEAP's ability to improve performance across diverse datasets and learning rate schedules, including constant ones."
    },
    {
      "title": "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization",
      "url": "/research/2023/07/04/Gradient-Norm-Aware-Minimization-Seeks-First-Order-Flatness-and-Improves-Generalization.html",
      "date": "July 4, 2023",
      "summary": "Recent advancements have highlighted the effectiveness of flat minima in enhancing generalization, particularly through Sharpness-Aware Minimization (SAM). However, existing definitions of flatness, such as zeroth-order flatness, have limitations in discerning between minima with low and high generalization errors. To address this, we propose first-order flatness, which considers maximal gradient norm within a perturbation radius. We introduce Gradient norm Aware Minimization (GAM) as a novel training approach to achieve uniformly small curvature across all directions. Experimental results demonstrate GAM's ability to enhance generalization compared to standard optimizers like SGD and AdamW across various datasets and networks. Moreover, GAM facilitates SAM in identifying flatter minima, leading to improved generalization."
    },
    {
      "title": "Penalizing Gradient Norm for Efficiently Improving Generalization in Deep Learning",
      "url": "/research/2022/06/26/Penalizing-Gradient-Norm-for-Efficiently-Improving-Generalization-in-Deep-Learning.html",
      "date": "June 26, 2022",
      "summary": "This paper proposes a method to enhance the generalization of deep neural networks (DNNs) by penalizing the gradient norm of the loss function during optimization. By constraining the gradient norm, the optimizers tend to find flat minima, improving generalization. We efficiently implement this method using first-order approximation within the gradient descent framework. Experimental results demonstrate improved generalization across various models and datasets. Additionally, we show that a recent method, sharpness-aware minimization, is a special case of our approach, with our method achieving new state-of-the-art performance on tested tasks."
    },
    {
      "title": "Sharpness-Aware Training for Free",
      "url": "/research/2023/03/02/Sharpness-Aware-Training-for-Free.html",
      "date": "March 2, 2023",
      "summary": "Modern deep neural networks (DNNs) excel in performance but often suffer from over-parameterization, leading to increased generalization error without tailored training strategies. Sharpness-Aware Minimization (SAM) has proven effective in reducing generalization error by minimizing sharpness in the loss landscape. However, SAM incurs a significant computational overhead. This paper introduces Sharpness-Aware Training for Free (SAF), which mitigates sharpness at nearly zero additional computational cost. SAF achieves this by preventing sudden drops in loss within sharp local minima during weight updates. A novel trajectory loss, based on KL-divergence between current and past DNN outputs, replaces SAM's sharpness measure, guiding convergence towards flat minima for enhanced generalization. Empirical results demonstrate SAF's effectiveness on ImageNet with comparable computational efficiency to the base optimizer."
    },
    {
      "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
      "url": "/research/2022/03/19/Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training.html",
      "date": "March 19, 2022",
      "summary": "Sharpness-Aware Minimization (SAM) enhances generalization by optimizing a neighborhood-based perturbed loss, but it doesn't always favor flat minima due to both sharp and flat minima having low perturbed losses. We introduce a new measure, the surrogate gap, reflecting the dominant Hessian eigenvalue at small neighborhood radii, which is simple to compute and can be minimized during training. We propose the Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), an advancement over SAM with minimal additional computational cost. GSAM employs a dual-step approach: first, minimizing the perturbed loss similar to SAM, and second, reducing the surrogate gap without affecting the perturbed loss to target regions with low loss and sharpness, thus achieving superior generalization. GSAM is theoretically robust, showing better convergence and empirical generalization improvements, notably a +3.2% gain over SAM and +5.4% over AdamW in ImageNet accuracy for ViT-B/32."
    },
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    },
    {
      "title": "Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes",
      "url": "/research/2017/11/28/Towards-Understanding-Generalization-of-Deep-Learning-Perspective-of-Loss-Landscapes.html",
      "date": "November 28, 2017",
      "summary": "This study examines why deep learning models, despite having more parameters than training samples, generalize well. It highlights the importance of the loss function's landscape, showing that areas leading to better generalization (good minima) are more prevalent than those leading to poor outcomes. This predominance facilitates the convergence of optimization methods to these beneficial minima. The research provides theoretical backing by analyzing 2-layer neural networks, noting that solutions with better generalization have a smaller Hessian matrix norm. For deeper networks, extensive numerical data supports these conclusions."
    }
  ],
  "risk": [
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "This paper introduces an algorithm that identifies simple and highly generalizable neural networks by searching for extensive flat minima regions in the error function, where the error rate is relatively stable. Such flat minima are associated with lower overfitting risks based on minimum description length (MDL) principles. Despite requiring second-order derivative calculations, the algorithm has a complexity level comparable to backpropagation. When tested on feedforward and recurrent networks, as well as stock market prediction tasks, this algorithm outperformed traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    }
  ],
  "mdl": [
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "This paper introduces an algorithm that identifies simple and highly generalizable neural networks by searching for extensive flat minima regions in the error function, where the error rate is relatively stable. Such flat minima are associated with lower overfitting risks based on minimum description length (MDL) principles. Despite requiring second-order derivative calculations, the algorithm has a complexity level comparable to backpropagation. When tested on feedforward and recurrent networks, as well as stock market prediction tasks, this algorithm outperformed traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    }
  ],
  "stock": [
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "This paper introduces an algorithm that identifies simple and highly generalizable neural networks by searching for extensive flat minima regions in the error function, where the error rate is relatively stable. Such flat minima are associated with lower overfitting risks based on minimum description length (MDL) principles. Despite requiring second-order derivative calculations, the algorithm has a complexity level comparable to backpropagation. When tested on feedforward and recurrent networks, as well as stock market prediction tasks, this algorithm outperformed traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    }
  ],
  "market": [
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "This paper introduces an algorithm that identifies simple and highly generalizable neural networks by searching for extensive flat minima regions in the error function, where the error rate is relatively stable. Such flat minima are associated with lower overfitting risks based on minimum description length (MDL) principles. Despite requiring second-order derivative calculations, the algorithm has a complexity level comparable to backpropagation. When tested on feedforward and recurrent networks, as well as stock market prediction tasks, this algorithm outperformed traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    }
  ],
  "brain": [
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "This paper introduces an algorithm that identifies simple and highly generalizable neural networks by searching for extensive flat minima regions in the error function, where the error rate is relatively stable. Such flat minima are associated with lower overfitting risks based on minimum description length (MDL) principles. Despite requiring second-order derivative calculations, the algorithm has a complexity level comparable to backpropagation. When tested on feedforward and recurrent networks, as well as stock market prediction tasks, this algorithm outperformed traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    }
  ],
  "surgeon": [
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "This paper introduces an algorithm that identifies simple and highly generalizable neural networks by searching for extensive flat minima regions in the error function, where the error rate is relatively stable. Such flat minima are associated with lower overfitting risks based on minimum description length (MDL) principles. Despite requiring second-order derivative calculations, the algorithm has a complexity level comparable to backpropagation. When tested on feedforward and recurrent networks, as well as stock market prediction tasks, this algorithm outperformed traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    }
  ],
  "sinlu": [
    {
      "title": "SinLU: Sinu-Sigmoidal Linear Uni",
      "url": "/research/2021/12/02/SinLU-Sinu-Sigmoidal-Linear-Uni.html",
      "date": "December 2, 2021",
      "summary": "This paper introduces a non-linear activation function called the Sinu-sigmoidal Linear Unit (SinLU), formulated as SinLU(x) = (x + a sin bx) · σ(x). It incorporates a sine wave for added functionality over traditional linear units, with two trainable parameters controlling the sinusoidal participation. The authors compare SinLU's performance to ReLU, GELU, and SiLU across various domains, models, and standard datasets. The results demonstrate SinLU's robustness and superior performance due to the incorporation of the trainable sine wave parameters, facilitating easy training and fast convergence."
    }
  ],
  "uni": [
    {
      "title": "SinLU: Sinu-Sigmoidal Linear Uni",
      "url": "/research/2021/12/02/SinLU-Sinu-Sigmoidal-Linear-Uni.html",
      "date": "December 2, 2021",
      "summary": "This paper introduces a non-linear activation function called the Sinu-sigmoidal Linear Unit (SinLU), formulated as SinLU(x) = (x + a sin bx) · σ(x). It incorporates a sine wave for added functionality over traditional linear units, with two trainable parameters controlling the sinusoidal participation. The authors compare SinLU's performance to ReLU, GELU, and SiLU across various domains, models, and standard datasets. The results demonstrate SinLU's robustness and superior performance due to the incorporation of the trainable sine wave parameters, facilitating easy training and fast convergence."
    }
  ],
  "sinlux": [
    {
      "title": "SinLU: Sinu-Sigmoidal Linear Uni",
      "url": "/research/2021/12/02/SinLU-Sinu-Sigmoidal-Linear-Uni.html",
      "date": "December 2, 2021",
      "summary": "This paper introduces a non-linear activation function called the Sinu-sigmoidal Linear Unit (SinLU), formulated as SinLU(x) = (x + a sin bx) · σ(x). It incorporates a sine wave for added functionality over traditional linear units, with two trainable parameters controlling the sinusoidal participation. The authors compare SinLU's performance to ReLU, GELU, and SiLU across various domains, models, and standard datasets. The results demonstrate SinLU's robustness and superior performance due to the incorporation of the trainable sine wave parameters, facilitating easy training and fast convergence."
    }
  ],
  "bx": [
    {
      "title": "SinLU: Sinu-Sigmoidal Linear Uni",
      "url": "/research/2021/12/02/SinLU-Sinu-Sigmoidal-Linear-Uni.html",
      "date": "December 2, 2021",
      "summary": "This paper introduces a non-linear activation function called the Sinu-sigmoidal Linear Unit (SinLU), formulated as SinLU(x) = (x + a sin bx) · σ(x). It incorporates a sine wave for added functionality over traditional linear units, with two trainable parameters controlling the sinusoidal participation. The authors compare SinLU's performance to ReLU, GELU, and SiLU across various domains, models, and standard datasets. The results demonstrate SinLU's robustness and superior performance due to the incorporation of the trainable sine wave parameters, facilitating easy training and fast convergence."
    }
  ],
  "ox": [
    {
      "title": "SinLU: Sinu-Sigmoidal Linear Uni",
      "url": "/research/2021/12/02/SinLU-Sinu-Sigmoidal-Linear-Uni.html",
      "date": "December 2, 2021",
      "summary": "This paper introduces a non-linear activation function called the Sinu-sigmoidal Linear Unit (SinLU), formulated as SinLU(x) = (x + a sin bx) · σ(x). It incorporates a sine wave for added functionality over traditional linear units, with two trainable parameters controlling the sinusoidal participation. The authors compare SinLU's performance to ReLU, GELU, and SiLU across various domains, models, and standard datasets. The results demonstrate SinLU's robustness and superior performance due to the incorporation of the trainable sine wave parameters, facilitating easy training and fast convergence."
    }
  ],
  "sine": [
    {
      "title": "SinLU: Sinu-Sigmoidal Linear Uni",
      "url": "/research/2021/12/02/SinLU-Sinu-Sigmoidal-Linear-Uni.html",
      "date": "December 2, 2021",
      "summary": "This paper introduces a non-linear activation function called the Sinu-sigmoidal Linear Unit (SinLU), formulated as SinLU(x) = (x + a sin bx) · σ(x). It incorporates a sine wave for added functionality over traditional linear units, with two trainable parameters controlling the sinusoidal participation. The authors compare SinLU's performance to ReLU, GELU, and SiLU across various domains, models, and standard datasets. The results demonstrate SinLU's robustness and superior performance due to the incorporation of the trainable sine wave parameters, facilitating easy training and fast convergence."
    },
    {
      "title": "Taming the Waves: Sine as Activation Function in Deep Neural Networks",
      "url": "/research/2016/11/04/Taming-the-Waves-Sine-as-Activation-Function-in-Deep-Neural-Networks.html",
      "date": "November 4, 2016",
      "summary": "This paper explores the challenges in training deep neural networks that use sinusoidal activation functions. The authors explain that the difficulty arises from the emergence of numerous shallow local minima in the loss landscape. The study reveals that successful learning in typical classification tasks occurs when the network effectively ignores the periodic cycles of the sinusoidal functions. However, for certain non-trivial tasks, networks with sinusoidal activations can outperform those using traditional monotonic activation functions."
    }
  ],
  "functionality": [
    {
      "title": "SinLU: Sinu-Sigmoidal Linear Uni",
      "url": "/research/2021/12/02/SinLU-Sinu-Sigmoidal-Linear-Uni.html",
      "date": "December 2, 2021",
      "summary": "This paper introduces a non-linear activation function called the Sinu-sigmoidal Linear Unit (SinLU), formulated as SinLU(x) = (x + a sin bx) · σ(x). It incorporates a sine wave for added functionality over traditional linear units, with two trainable parameters controlling the sinusoidal participation. The authors compare SinLU's performance to ReLU, GELU, and SiLU across various domains, models, and standard datasets. The results demonstrate SinLU's robustness and superior performance due to the incorporation of the trainable sine wave parameters, facilitating easy training and fast convergence."
    }
  ],
  "participation": [
    {
      "title": "SinLU: Sinu-Sigmoidal Linear Uni",
      "url": "/research/2021/12/02/SinLU-Sinu-Sigmoidal-Linear-Uni.html",
      "date": "December 2, 2021",
      "summary": "This paper introduces a non-linear activation function called the Sinu-sigmoidal Linear Unit (SinLU), formulated as SinLU(x) = (x + a sin bx) · σ(x). It incorporates a sine wave for added functionality over traditional linear units, with two trainable parameters controlling the sinusoidal participation. The authors compare SinLU's performance to ReLU, GELU, and SiLU across various domains, models, and standard datasets. The results demonstrate SinLU's robustness and superior performance due to the incorporation of the trainable sine wave parameters, facilitating easy training and fast convergence."
    }
  ],
  "incorporation": [
    {
      "title": "SinLU: Sinu-Sigmoidal Linear Uni",
      "url": "/research/2021/12/02/SinLU-Sinu-Sigmoidal-Linear-Uni.html",
      "date": "December 2, 2021",
      "summary": "This paper introduces a non-linear activation function called the Sinu-sigmoidal Linear Unit (SinLU), formulated as SinLU(x) = (x + a sin bx) · σ(x). It incorporates a sine wave for added functionality over traditional linear units, with two trainable parameters controlling the sinusoidal participation. The authors compare SinLU's performance to ReLU, GELU, and SiLU across various domains, models, and standard datasets. The results demonstrate SinLU's robustness and superior performance due to the incorporation of the trainable sine wave parameters, facilitating easy training and fast convergence."
    }
  ],
  "slowfast": [
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "This paper introduces SlowFast networks, a new convolutional architecture for video recognition tasks. These networks have two pathways: a Slow pathway operating at low frame rates to capture spatial semantics, and a Fast pathway working at high frame rates with reduced channel capacity to efficiently capture motion details. The SlowFast models achieve top accuracy on major video benchmarks like Kinetics, Charades, and AVA, significantly improving performance for action classification and detection tasks."
    }
  ],
  "pathway": [
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "This paper introduces SlowFast networks, a new convolutional architecture for video recognition tasks. These networks have two pathways: a Slow pathway operating at low frame rates to capture spatial semantics, and a Fast pathway working at high frame rates with reduced channel capacity to efficiently capture motion details. The SlowFast models achieve top accuracy on major video benchmarks like Kinetics, Charades, and AVA, significantly improving performance for action classification and detection tasks."
    }
  ],
  "operating": [
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "This paper introduces SlowFast networks, a new convolutional architecture for video recognition tasks. These networks have two pathways: a Slow pathway operating at low frame rates to capture spatial semantics, and a Fast pathway working at high frame rates with reduced channel capacity to efficiently capture motion details. The SlowFast models achieve top accuracy on major video benchmarks like Kinetics, Charades, and AVA, significantly improving performance for action classification and detection tasks."
    }
  ],
  "capacity": [
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "This paper introduces SlowFast networks, a new convolutional architecture for video recognition tasks. These networks have two pathways: a Slow pathway operating at low frame rates to capture spatial semantics, and a Fast pathway working at high frame rates with reduced channel capacity to efficiently capture motion details. The SlowFast models achieve top accuracy on major video benchmarks like Kinetics, Charades, and AVA, significantly improving performance for action classification and detection tasks."
    },
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    },
    {
      "title": "Learning and Leveraging World Models in Visual Representation Learning",
      "url": "/research/2024/03/01/Learning-and-Leveraging-World-Models-in-Visual-Representation-Learning.html",
      "date": "March 1, 2024",
      "summary": "The Joint-Embedding Predictive Architecture (JEPA) is a self-supervised method previously used for predicting missing input parts. This study expands JEPA to predict a wider range of corruptions by introducing Image World Models (IWM), which learn the effects of global photometric transformations in latent space. Key to effective IWM learning are conditioning, prediction difficulty, and model capacity. The study demonstrates that IWM's predictive world model, when fine-tuned, can tackle diverse tasks and either matches or outperforms existing self-supervised techniques. Additionally, it enables control over the abstraction level of learned representations, achieving either invariant or equivariant representations, similar to contrastive methods or masked image modeling, respectively."
    }
  ],
  "motion": [
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "This paper introduces SlowFast networks, a new convolutional architecture for video recognition tasks. These networks have two pathways: a Slow pathway operating at low frame rates to capture spatial semantics, and a Fast pathway working at high frame rates with reduced channel capacity to efficiently capture motion details. The SlowFast models achieve top accuracy on major video benchmarks like Kinetics, Charades, and AVA, significantly improving performance for action classification and detection tasks."
    },
    {
      "title": "V-Jepa: Latent Video Prediction For Visual Representation Learning",
      "url": "/research/2023/02/15/V-Jepa-Latent-Video-Prediction-For-Visual-Representation-Learning.html",
      "date": "February 15, 2023",
      "summary": "This paper introduces V-JEPA, a self-supervised video learning method that predicts masked spatio-temporal regions in latent space, effectively applying the masked-modelling principle of large language models to video. This approach generates visual features useful across various image and video tasks without needing model adjustment, achieving significant improvements on Kinetics-400 (82.1%) and Something-Something-v2 (71.2%) benchmarks, outperforming prior video models. V-JEPA also excels in motion understanding tasks, surpassing leading image models like DINOv2 and OpenCLIP, and achieves 77.9% on ImageNet classification with video training alone, setting a new standard for video models."
    },
    {
      "title": "VideoMamba: State Space Model for Efficient Video Understanding",
      "url": "/research/2024/03/12/VideoMamba-State-Space-Model-for-Efficient-Video-Understanding.html",
      "date": "March 12, 2024",
      "summary": "This work introduces VideoMamba, an innovative approach that addresses local redundancy and global dependencies in video understanding by adapting the Mamba framework to video analysis. VideoMamba outperforms existing 3D convolutional neural networks and video transformers through its linear-complexity operator, which facilitates efficient long-term modeling for high-resolution, long-duration videos. Its effectiveness is demonstrated across four main areas: scalability in the visual domain without needing extensive dataset pretraining, thanks to a novel self-distillation technique; the ability to recognize short-term actions with fine-grained motion differences; superior performance in long-term video understanding compared to traditional feature-based models; and robust compatibility with multiple modalities, enhancing multi-modal video analysis. VideoMamba establishes a new standard for comprehensive and efficient video understanding."
    }
  ],
  "charade": [
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "This paper introduces SlowFast networks, a new convolutional architecture for video recognition tasks. These networks have two pathways: a Slow pathway operating at low frame rates to capture spatial semantics, and a Fast pathway working at high frame rates with reduced channel capacity to efficiently capture motion details. The SlowFast models achieve top accuracy on major video benchmarks like Kinetics, Charades, and AVA, significantly improving performance for action classification and detection tasks."
    }
  ],
  "software": [
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This paper investigates using high-quality rendering software and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle. This synthetic dataset is used to augment limited real training data for image classification and semantic segmentation tasks. While models trained solely on synthetic images showed low accuracy on real validation data, including even small amounts of real data significantly improved performance. Augmenting real training data with synthetic images outperformed using only real images. Furthermore, pretraining models on the synthetic dataset before transfer learning significantly reduced training costs by allowing most of the training to be completed upfront using the synthetic data."
    }
  ],
  "randomization": [
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This paper investigates using high-quality rendering software and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle. This synthetic dataset is used to augment limited real training data for image classification and semantic segmentation tasks. While models trained solely on synthetic images showed low accuracy on real validation data, including even small amounts of real data significantly improved performance. Augmenting real training data with synthetic images outperformed using only real images. Furthermore, pretraining models on the synthetic dataset before transfer learning significantly reduced training costs by allowing most of the training to be completed upfront using the synthetic data."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "The paper introduces a system that trains deep neural networks for object detection using only synthetic images generated with domain randomization. This involves randomizing non-realistic simulator parameters like lighting and object textures, allowing the network to identify essential object features. Their findings show that networks can achieve impressive performance using just synthetic data, and further improve with fine-tuning on real data. This suggests the potential of using cost-effective synthetic data for training, avoiding the need to acquire vast amounts of real-world data or create detailed synthetic environments. They validate their method on car bounding box detection using the KITTI dataset."
    }
  ],
  "of": [
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This paper investigates using high-quality rendering software and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle. This synthetic dataset is used to augment limited real training data for image classification and semantic segmentation tasks. While models trained solely on synthetic images showed low accuracy on real validation data, including even small amounts of real data significantly improved performance. Augmenting real training data with synthetic images outperformed using only real images. Furthermore, pretraining models on the synthetic dataset before transfer learning significantly reduced training costs by allowing most of the training to be completed upfront using the synthetic data."
    }
  ],
  "vehicle": [
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This paper investigates using high-quality rendering software and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle. This synthetic dataset is used to augment limited real training data for image classification and semantic segmentation tasks. While models trained solely on synthetic images showed low accuracy on real validation data, including even small amounts of real data significantly improved performance. Augmenting real training data with synthetic images outperformed using only real images. Furthermore, pretraining models on the synthetic dataset before transfer learning significantly reduced training costs by allowing most of the training to be completed upfront using the synthetic data."
    }
  ],
  "complete": [
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This paper investigates using high-quality rendering software and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle. This synthetic dataset is used to augment limited real training data for image classification and semantic segmentation tasks. While models trained solely on synthetic images showed low accuracy on real validation data, including even small amounts of real data significantly improved performance. Augmenting real training data with synthetic images outperformed using only real images. Furthermore, pretraining models on the synthetic dataset before transfer learning significantly reduced training costs by allowing most of the training to be completed upfront using the synthetic data."
    },
    {
      "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
      "url": "/research/2024/01/24/Better-Call-GPT,-Comparing-Large-Language-Models-Against-Lawyers.html",
      "date": "January 24, 2024",
      "summary": "This paper compares Large Language Models (LLMs) with traditional legal contract reviewers—Junior Lawyers and Legal Process Outsourcers (LPOs). It evaluates whether LLMs outperform humans in accuracy, speed, and cost-efficiency during contract review. Empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, revealing that advanced models match or exceed human accuracy in identifying legal issues. LLMs complete reviews in seconds, vastly outpacing the hours required by humans. Cost-wise, LLMs achieve a 99.97 percent reduction compared to traditional methods. These findings indicate a transformative shift in legal practice, with LLMs enhancing the accessibility and efficiency of legal services. The research suggests that LLM dominance in legal contract review challenges the status quo, necessitating a reimagined future for legal workflows."
    }
  ],
  "upfront": [
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This paper investigates using high-quality rendering software and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle. This synthetic dataset is used to augment limited real training data for image classification and semantic segmentation tasks. While models trained solely on synthetic images showed low accuracy on real validation data, including even small amounts of real data significantly improved performance. Augmenting real training data with synthetic images outperformed using only real images. Furthermore, pretraining models on the synthetic dataset before transfer learning significantly reduced training costs by allowing most of the training to be completed upfront using the synthetic data."
    }
  ],
  "taming": [
    {
      "title": "Taming the Waves: Sine as Activation Function in Deep Neural Networks",
      "url": "/research/2016/11/04/Taming-the-Waves-Sine-as-Activation-Function-in-Deep-Neural-Networks.html",
      "date": "November 4, 2016",
      "summary": "This paper explores the challenges in training deep neural networks that use sinusoidal activation functions. The authors explain that the difficulty arises from the emergence of numerous shallow local minima in the loss landscape. The study reveals that successful learning in typical classification tasks occurs when the network effectively ignores the periodic cycles of the sinusoidal functions. However, for certain non-trivial tasks, networks with sinusoidal activations can outperform those using traditional monotonic activation functions."
    }
  ],
  "waves": [
    {
      "title": "Taming the Waves: Sine as Activation Function in Deep Neural Networks",
      "url": "/research/2016/11/04/Taming-the-Waves-Sine-as-Activation-Function-in-Deep-Neural-Networks.html",
      "date": "November 4, 2016",
      "summary": "This paper explores the challenges in training deep neural networks that use sinusoidal activation functions. The authors explain that the difficulty arises from the emergence of numerous shallow local minima in the loss landscape. The study reveals that successful learning in typical classification tasks occurs when the network effectively ignores the periodic cycles of the sinusoidal functions. However, for certain non-trivial tasks, networks with sinusoidal activations can outperform those using traditional monotonic activation functions."
    }
  ],
  "arise": [
    {
      "title": "Taming the Waves: Sine as Activation Function in Deep Neural Networks",
      "url": "/research/2016/11/04/Taming-the-Waves-Sine-as-Activation-Function-in-Deep-Neural-Networks.html",
      "date": "November 4, 2016",
      "summary": "This paper explores the challenges in training deep neural networks that use sinusoidal activation functions. The authors explain that the difficulty arises from the emergence of numerous shallow local minima in the loss landscape. The study reveals that successful learning in typical classification tasks occurs when the network effectively ignores the periodic cycles of the sinusoidal functions. However, for certain non-trivial tasks, networks with sinusoidal activations can outperform those using traditional monotonic activation functions."
    },
    {
      "title": "Normalized Attention Without Probability Cage",
      "url": "/research/2020/05/19/Normalized-Attention-Without-Probability-Cage.html",
      "date": "May 19, 2020",
      "summary": "Softmax-attention architectures, especially popularized by Transformers, have seen significant advancements in various tasks. However, the geometric implications of softmax-attention remain underexplored. In this study, we demonstrate limitations arising from constraining attention weights to the probability simplex and its impact on the convex hull of value vectors. We reveal sequence length-dependent biases in Transformers towards token isolation at initialization and compare them with max- and sum-pooling, which are strong but often overlooked baselines. To address these issues, we propose a novel approach of replacing softmax with normalization in self-attention, resulting in a robust and widely applicable architecture. Our findings are supported by empirical results from over 25,000 trained models, and all results and implementations are publicly available."
    }
  ],
  "occur": [
    {
      "title": "Taming the Waves: Sine as Activation Function in Deep Neural Networks",
      "url": "/research/2016/11/04/Taming-the-Waves-Sine-as-Activation-Function-in-Deep-Neural-Networks.html",
      "date": "November 4, 2016",
      "summary": "This paper explores the challenges in training deep neural networks that use sinusoidal activation functions. The authors explain that the difficulty arises from the emergence of numerous shallow local minima in the loss landscape. The study reveals that successful learning in typical classification tasks occurs when the network effectively ignores the periodic cycles of the sinusoidal functions. However, for certain non-trivial tasks, networks with sinusoidal activations can outperform those using traditional monotonic activation functions."
    }
  ],
  "cycle": [
    {
      "title": "Taming the Waves: Sine as Activation Function in Deep Neural Networks",
      "url": "/research/2016/11/04/Taming-the-Waves-Sine-as-Activation-Function-in-Deep-Neural-Networks.html",
      "date": "November 4, 2016",
      "summary": "This paper explores the challenges in training deep neural networks that use sinusoidal activation functions. The authors explain that the difficulty arises from the emergence of numerous shallow local minima in the loss landscape. The study reveals that successful learning in typical classification tasks occurs when the network effectively ignores the periodic cycles of the sinusoidal functions. However, for certain non-trivial tasks, networks with sinusoidal activations can outperform those using traditional monotonic activation functions."
    }
  ],
  "task2sim": [
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "The paper investigates the use of synthetic data generated by graphics simulators for pre-training computer vision models. The authors find that model performance on downstream tasks varies with different simulation parameters used to generate the synthetic data. They introduce Task2Sim, a model that maps the requirements of a downstream task to the optimal simulation parameters for generating synthetic pre-training data."
    }
  ],
  "graphics": [
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "The paper investigates the use of synthetic data generated by graphics simulators for pre-training computer vision models. The authors find that model performance on downstream tasks varies with different simulation parameters used to generate the synthetic data. They introduce Task2Sim, a model that maps the requirements of a downstream task to the optimal simulation parameters for generating synthetic pre-training data."
    }
  ],
  "simulation": [
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "The paper investigates the use of synthetic data generated by graphics simulators for pre-training computer vision models. The authors find that model performance on downstream tasks varies with different simulation parameters used to generate the synthetic data. They introduce Task2Sim, a model that maps the requirements of a downstream task to the optimal simulation parameters for generating synthetic pre-training data."
    }
  ],
  "film": [
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "This paper introduces tools for training monocular depth estimation models with multiple datasets despite incompatible annotations. Their approach uses a robust training objective, multi-objective learning for data integration, and pretraining encoders on auxiliary tasks. Tested across five datasets, including 3D films as a novel data source, their methods achieve superior zero-shot cross-dataset generalization, outperforming existing benchmarks through principled multi-dataset training."
    }
  ],
  "multidataset": [
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "This paper introduces tools for training monocular depth estimation models with multiple datasets despite incompatible annotations. Their approach uses a robust training objective, multi-objective learning for data integration, and pretraining encoders on auxiliary tasks. Tested across five datasets, including 3D films as a novel data source, their methods achieve superior zero-shot cross-dataset generalization, outperforming existing benchmarks through principled multi-dataset training."
    }
  ],
  "bridge": [
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "The paper introduces a system that trains deep neural networks for object detection using only synthetic images generated with domain randomization. This involves randomizing non-realistic simulator parameters like lighting and object textures, allowing the network to identify essential object features. Their findings show that networks can achieve impressive performance using just synthetic data, and further improve with fine-tuning on real data. This suggests the potential of using cost-effective synthetic data for training, avoiding the need to acquire vast amounts of real-world data or create detailed synthetic environments. They validate their method on car bounding box detection using the KITTI dataset."
    },
    {
      "title": "SAM as an Optimal Relaxation of Bayes",
      "url": "/research/2023/12/10/SAM-as-an-Optimal-Relaxation-of-Bayes.html",
      "date": "December 10, 2023",
      "summary": "We introduce SAM, a method enhancing generalization in deep learning, as a relaxation of the Bayes objective. SAM replaces the expected negative-loss with an optimal convex lower bound derived using the Fenchel biconjugate. This connection enables an Adam-like extension of SAM, offering automatic uncertainty estimates and potential accuracy improvements. Bridging adversarial and Bayesian methods, our work paves the way for robustness enhancement."
    }
  ],
  "reality": [
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "The paper introduces a system that trains deep neural networks for object detection using only synthetic images generated with domain randomization. This involves randomizing non-realistic simulator parameters like lighting and object textures, allowing the network to identify essential object features. Their findings show that networks can achieve impressive performance using just synthetic data, and further improve with fine-tuning on real data. This suggests the potential of using cost-effective synthetic data for training, avoiding the need to acquire vast amounts of real-world data or create detailed synthetic environments. They validate their method on car bounding box detection using the KITTI dataset."
    },
    {
      "title": "The Platonic Representation Hypothesis",
      "url": "/research/2024/05/13/The-Platonic-Representation-Hypothesis.html",
      "date": "May 13, 2024",
      "summary": "This paper argues that AI model representations, especially in deep networks, are converging. It surveys examples in the literature showing how neural network representations align over time and across domains. It demonstrates convergence across data modalities, noting that as vision and language models grow, they measure distances between data points similarly. The authors hypothesize that this convergence leads to a shared statistical model of reality, termed the platonic representation and discuss possible selective pressures toward it. The paper also addresses the implications, limitations, and counterexamples to this trend."
    }
  ],
  "randomizatio": [
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "The paper introduces a system that trains deep neural networks for object detection using only synthetic images generated with domain randomization. This involves randomizing non-realistic simulator parameters like lighting and object textures, allowing the network to identify essential object features. Their findings show that networks can achieve impressive performance using just synthetic data, and further improve with fine-tuning on real data. This suggests the potential of using cost-effective synthetic data for training, avoiding the need to acquire vast amounts of real-world data or create detailed synthetic environments. They validate their method on car bounding box detection using the KITTI dataset."
    }
  ],
  "randomize": [
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "The paper introduces a system that trains deep neural networks for object detection using only synthetic images generated with domain randomization. This involves randomizing non-realistic simulator parameters like lighting and object textures, allowing the network to identify essential object features. Their findings show that networks can achieve impressive performance using just synthetic data, and further improve with fine-tuning on real data. This suggests the potential of using cost-effective synthetic data for training, avoiding the need to acquire vast amounts of real-world data or create detailed synthetic environments. They validate their method on car bounding box detection using the KITTI dataset."
    }
  ],
  "light": [
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "The paper introduces a system that trains deep neural networks for object detection using only synthetic images generated with domain randomization. This involves randomizing non-realistic simulator parameters like lighting and object textures, allowing the network to identify essential object features. Their findings show that networks can achieve impressive performance using just synthetic data, and further improve with fine-tuning on real data. This suggests the potential of using cost-effective synthetic data for training, avoiding the need to acquire vast amounts of real-world data or create detailed synthetic environments. They validate their method on car bounding box detection using the KITTI dataset."
    }
  ],
  "acquire": [
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "The paper introduces a system that trains deep neural networks for object detection using only synthetic images generated with domain randomization. This involves randomizing non-realistic simulator parameters like lighting and object textures, allowing the network to identify essential object features. Their findings show that networks can achieve impressive performance using just synthetic data, and further improve with fine-tuning on real data. This suggests the potential of using cost-effective synthetic data for training, avoiding the need to acquire vast amounts of real-world data or create detailed synthetic environments. They validate their method on car bounding box detection using the KITTI dataset."
    }
  ],
  "environment": [
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "The paper introduces a system that trains deep neural networks for object detection using only synthetic images generated with domain randomization. This involves randomizing non-realistic simulator parameters like lighting and object textures, allowing the network to identify essential object features. Their findings show that networks can achieve impressive performance using just synthetic data, and further improve with fine-tuning on real data. This suggests the potential of using cost-effective synthetic data for training, avoiding the need to acquire vast amounts of real-world data or create detailed synthetic environments. They validate their method on car bounding box detection using the KITTI dataset."
    },
    {
      "title": "Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding",
      "url": "/research/2021/11/16/Improved-Robustness-of-Vision-Transformer-via-PreLayerNorm-in-Patch-Embedding.html",
      "date": "November 16, 2021",
      "summary": "Recent advancements in vision transformers (ViTs) have shown superior performance across diverse visual tasks, surpassing convolutional neural networks (CNNs). Given ViT's distinct architecture, understanding its behavior and reliability is imperative. This paper investigates ViT's robustness by comparing it with CNNs under various image corruptions relevant to real-world vision tasks. While ViT generally exhibits comparable or improved robustness over CNNs, it consistently underperforms in contrast enhancement tasks. Analysis suggests that positional embedding in ViT's patch embedding may malfunction with color scale changes. We propose PreLayerNorm, a modified patch embedding structure, to address this issue and ensure scale-invariant behavior in ViT. ViT with PreLayerNorm demonstrates enhanced robustness across various corruptions, particularly in contrast-varying environments."
    },
    {
      "title": "From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning",
      "url": "/research/2023/03/16/From-MNIST-to-ImageNet-and-Back:-Benchmarking-Continual-Curriculum-Learning.html",
      "date": "March 16, 2023",
      "summary": "Continual learning (CL) is a promising trend in machine learning, aiming to develop robust models and strategies for dynamic environments by incorporating new knowledge while retaining past knowledge. CL research is fragmented, with various protocols, tasks, datasets, and metrics, often not reflecting real-world complexity and tailored to specific strategies. This work addresses this gap by introducing two novel CL benchmarks with heterogeneous tasks from six image datasets, varying in complexity and quality. These benchmarks fairly evaluate state-of-the-art CL strategies in scenarios closer to real-world conditions. The benchmarks present tasks in both increasing and decreasing complexity, assessing models' ability to exploit task structure. The work emphasizes a rigorous, reproducible evaluation protocol for measuring generalization and memory retention in models. Experimental results show that popular CL strategies perform poorly on these benchmarks, exhibit high forgetting, and struggle with curriculum task ordering. These findings underscore the need for rigorous comparisons and the development of new CL strategies for complex scenarios."
    },
    {
      "title": "Self-Paced Absolute Learning Progress as a Regularized Approach to Curriculum Learning",
      "url": "/research/2023/06/09/Self-Paced-Absolute-Learning-Progress-as-a-Regularized-Approach-to-Curriculum-Learning.html",
      "date": "June 9, 2023",
      "summary": "Reinforcement Learning's usability is limited by its high computation times. Curriculum Reinforcement Learning accelerates learning by ordering tasks from simple to hard. Curricula based on Absolute Learning Progress (ALP) have shown success in various environments but waste computation on redundant tasks. This issue is addressed by introducing Self-Paced Absolute Learning Progress (SPALP), a regularization method inspired by Self-Paced Learning. Evaluated in three environments, SPALP achieves performance comparable to ALP in all cases and reaches it faster in two. Further improvements in SPALP's efficiency and performance are also discussed."
    }
  ],
  "kitti": [
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "The paper introduces a system that trains deep neural networks for object detection using only synthetic images generated with domain randomization. This involves randomizing non-realistic simulator parameters like lighting and object textures, allowing the network to identify essential object features. Their findings show that networks can achieve impressive performance using just synthetic data, and further improve with fine-tuning on real data. This suggests the potential of using cost-effective synthetic data for training, avoiding the need to acquire vast amounts of real-world data or create detailed synthetic environments. They validate their method on car bounding box detection using the KITTI dataset."
    }
  ],
  "tear": [
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "The paper proposes three normalization-centric modifications to improve Transformer training: PRENORM which introduces pre-norm residual connections and smaller initializations, enabling warmup-free, validation-based training with large learning rates; SCALENORM which suggests L2 normalization with a single scale parameter for faster training and better performance; and FIXNORM which reaffirms the efficacy of normalizing word embeddings to a fixed length."
    }
  ],
  "prenorm": [
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "The paper proposes three normalization-centric modifications to improve Transformer training: PRENORM which introduces pre-norm residual connections and smaller initializations, enabling warmup-free, validation-based training with large learning rates; SCALENORM which suggests L2 normalization with a single scale parameter for faster training and better performance; and FIXNORM which reaffirms the efficacy of normalizing word embeddings to a fixed length."
    }
  ],
  "scalenorm": [
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "The paper proposes three normalization-centric modifications to improve Transformer training: PRENORM which introduces pre-norm residual connections and smaller initializations, enabling warmup-free, validation-based training with large learning rates; SCALENORM which suggests L2 normalization with a single scale parameter for faster training and better performance; and FIXNORM which reaffirms the efficacy of normalizing word embeddings to a fixed length."
    }
  ],
  "fixnorm": [
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "The paper proposes three normalization-centric modifications to improve Transformer training: PRENORM which introduces pre-norm residual connections and smaller initializations, enabling warmup-free, validation-based training with large learning rates; SCALENORM which suggests L2 normalization with a single scale parameter for faster training and better performance; and FIXNORM which reaffirms the efficacy of normalizing word embeddings to a fixed length."
    }
  ],
  "reaffirm": [
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "The paper proposes three normalization-centric modifications to improve Transformer training: PRENORM which introduces pre-norm residual connections and smaller initializations, enabling warmup-free, validation-based training with large learning rates; SCALENORM which suggests L2 normalization with a single scale parameter for faster training and better performance; and FIXNORM which reaffirms the efficacy of normalizing word embeddings to a fixed length."
    }
  ],
  "word": [
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "The paper proposes three normalization-centric modifications to improve Transformer training: PRENORM which introduces pre-norm residual connections and smaller initializations, enabling warmup-free, validation-based training with large learning rates; SCALENORM which suggests L2 normalization with a single scale parameter for faster training and better performance; and FIXNORM which reaffirms the efficacy of normalizing word embeddings to a fixed length."
    },
    {
      "title": "Language Modeling with Gated Convolutional Networks",
      "url": "/research/2017/09/08/Language-Modeling-with-Gated-Convolutional-Networks.html",
      "date": "September 8, 2017",
      "summary": "This paper introduces a novel language modeling approach using stacked convolutions that enable efficient parallel processing, in contrast to the traditionally used recurrent neural networks known for handling unbounded context. The proposed method features a simplified gating mechanism that outperforms previous models and demonstrates superior performance on the WikiText-103 benchmark, showing its capability to manage long-term dependencies. It also delivers competitive results on the Google Billion Words benchmark and significantly reduces sentence scoring latency compared to recurrent models. This marks the first instance of a non-recurrent model achieving comparable success to strong recurrent models on major language tasks."
    }
  ],
  "trivialaugment": [
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "The paper introduces TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method that applies a single augmentation to each image. Despite its minimal complexity and cost, TrivialAugment outperforms existing methods across various image classification scenarios, as validated through extensive experiments against state-of-the-art methods and multiple ablation studies involving different augmentation spaces and methods. The work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Noting a stagnation in automatic augmentation research, the authors conclude by proposing best practices for future advancements in the field."
    }
  ],
  "interface": [
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "The paper introduces TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method that applies a single augmentation to each image. Despite its minimal complexity and cost, TrivialAugment outperforms existing methods across various image classification scenarios, as validated through extensive experiments against state-of-the-art methods and multiple ablation studies involving different augmentation spaces and methods. The work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Noting a stagnation in automatic augmentation research, the authors conclude by proposing best practices for future advancements in the field."
    }
  ],
  "codebase": [
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "The paper introduces TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method that applies a single augmentation to each image. Despite its minimal complexity and cost, TrivialAugment outperforms existing methods across various image classification scenarios, as validated through extensive experiments against state-of-the-art methods and multiple ablation studies involving different augmentation spaces and methods. The work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Noting a stagnation in automatic augmentation research, the authors conclude by proposing best practices for future advancements in the field."
    }
  ],
  "adoption": [
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "The paper introduces TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method that applies a single augmentation to each image. Despite its minimal complexity and cost, TrivialAugment outperforms existing methods across various image classification scenarios, as validated through extensive experiments against state-of-the-art methods and multiple ablation studies involving different augmentation spaces and methods. The work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Noting a stagnation in automatic augmentation research, the authors conclude by proposing best practices for future advancements in the field."
    },
    {
      "title": "Searching for Activation Functions",
      "url": "/research/2017/10/27/Searching-for-Activation-Functions.html",
      "date": "October 27, 2017",
      "summary": "This study explores the impact of activation functions on deep network training and performance. While Rectified Linear Unit (ReLU) is widely used, alternatives have not consistently outperformed it. We propose using automatic search techniques to discover new activation functions. Through exhaustive and reinforcement learning-based searches, we identify novel functions. Empirical evaluation shows that our best discovered function, Swish (f(x) = x · sigmoid(βx)), performs better than ReLU on deeper models across challenging datasets. Replacing ReLUs with Swish units improves classification accuracy on ImageNet, for example, by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. Swish's simplicity and similarity to ReLU facilitate its adoption in neural networks."
    }
  ],
  "reproducibility": [
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "The paper introduces TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method that applies a single augmentation to each image. Despite its minimal complexity and cost, TrivialAugment outperforms existing methods across various image classification scenarios, as validated through extensive experiments against state-of-the-art methods and multiple ablation studies involving different augmentation spaces and methods. The work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Noting a stagnation in automatic augmentation research, the authors conclude by proposing best practices for future advancements in the field."
    }
  ],
  "stagnation": [
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "The paper introduces TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method that applies a single augmentation to each image. Despite its minimal complexity and cost, TrivialAugment outperforms existing methods across various image classification scenarios, as validated through extensive experiments against state-of-the-art methods and multiple ablation studies involving different augmentation spaces and methods. The work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Noting a stagnation in automatic augmentation research, the authors conclude by proposing best practices for future advancements in the field."
    }
  ],
  "attack": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    },
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    }
  ],
  "prompting": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "attach": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "suffix": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "query": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    },
    {
      "title": "A Phase Transition between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention",
      "url": "/research/2024/02/06/A-Phase-Transition-between-Positional-and-Semantic-Learning-in-a-Solvable-Model-of-Dot-Product-Attention.html",
      "date": "February 6, 2024",
      "summary": "We explore the learning process of a dot-product attention layer, which learns both positional and semantic attention matrices, enabling tokens to attend based on position or meaning. Through experiments on an algorithmic task, we demonstrate that this architecture can use either mechanism for solving the task. Theoretically, we examine a non-linear self-attention layer with special query and key matrices, offering a closed-form solution for its non-convex loss landscape in high-dimensional data, which reveals a phase transition from positional to semantic mechanisms as sample complexity increases. We also show that the dot-product attention layer surpasses a linear positional baseline through the semantic mechanism with adequate data."
    }
  ],
  "release": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "chatgpt": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "bard": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "claude": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    },
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    }
  ],
  "llama": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    },
    {
      "title": "Suppressing Pink Elephants with Direct Principle Feedback",
      "url": "/research/2024/02/13/Suppressing-Pink-Elephants-with-Direct-Principle-Feedback.html",
      "date": "February 13, 2024",
      "summary": "Existing methods for controlling language models focus on training desired behaviors, but often lack the flexibility needed for diverse applications. The authors address this with the Pink Elephant Problem, demonstrating the need for language models to adapt to different contexts by avoiding certain topics (the Pink Elephant) in favor of others (Grey Elephant). The authors introduce Direct Principle Feedback (DPF), an adaptation of Constitutional AI that improves control by directly applying critiques and revisions without ranking responses. Our study shows that a 13B LLaMA 2 model fine-tuned with DPF on a synthetic dataset outperforms existing models and matches GPT-4 in managing the Pink Elephant Problem."
    }
  ],
  "chat": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "pythia": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "falcon": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "etc": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "security": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "urge": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "mitigation": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "videomae": [
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "The authors introduce VideoMAE, a data-efficient self-supervised video pre-training (SSVP) approach that utilizes video masked autoencoders with a novel high-ratio video tube masking technique inspired by ImageMAE. Key findings include the effectiveness of high masking ratios (90-95%) due to video's temporal redundancy, strong performance on small datasets (~3k-4k videos) without extra data highlighting the importance of high-level structure learning, and data quality being more crucial than quantity with domain shift being significant. Notably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone without extra data."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    }
  ],
  "ssvp": [
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "The authors introduce VideoMAE, a data-efficient self-supervised video pre-training (SSVP) approach that utilizes video masked autoencoders with a novel high-ratio video tube masking technique inspired by ImageMAE. Key findings include the effectiveness of high masking ratios (90-95%) due to video's temporal redundancy, strong performance on small datasets (~3k-4k videos) without extra data highlighting the importance of high-level structure learning, and data quality being more crucial than quantity with domain shift being significant. Notably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone without extra data."
    }
  ],
  "tube": [
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "The authors introduce VideoMAE, a data-efficient self-supervised video pre-training (SSVP) approach that utilizes video masked autoencoders with a novel high-ratio video tube masking technique inspired by ImageMAE. Key findings include the effectiveness of high masking ratios (90-95%) due to video's temporal redundancy, strong performance on small datasets (~3k-4k videos) without extra data highlighting the importance of high-level structure learning, and data quality being more crucial than quantity with domain shift being significant. Notably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone without extra data."
    }
  ],
  "imagemae": [
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "The authors introduce VideoMAE, a data-efficient self-supervised video pre-training (SSVP) approach that utilizes video masked autoencoders with a novel high-ratio video tube masking technique inspired by ImageMAE. Key findings include the effectiveness of high masking ratios (90-95%) due to video's temporal redundancy, strong performance on small datasets (~3k-4k videos) without extra data highlighting the importance of high-level structure learning, and data quality being more crucial than quantity with domain shift being significant. Notably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone without extra data."
    }
  ],
  "ratios": [
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "The authors introduce VideoMAE, a data-efficient self-supervised video pre-training (SSVP) approach that utilizes video masked autoencoders with a novel high-ratio video tube masking technique inspired by ImageMAE. Key findings include the effectiveness of high masking ratios (90-95%) due to video's temporal redundancy, strong performance on small datasets (~3k-4k videos) without extra data highlighting the importance of high-level structure learning, and data quality being more crucial than quantity with domain shift being significant. Notably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone without extra data."
    }
  ],
  "redundancy": [
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "The authors introduce VideoMAE, a data-efficient self-supervised video pre-training (SSVP) approach that utilizes video masked autoencoders with a novel high-ratio video tube masking technique inspired by ImageMAE. Key findings include the effectiveness of high masking ratios (90-95%) due to video's temporal redundancy, strong performance on small datasets (~3k-4k videos) without extra data highlighting the importance of high-level structure learning, and data quality being more crucial than quantity with domain shift being significant. Notably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone without extra data."
    },
    {
      "title": "VideoMamba: State Space Model for Efficient Video Understanding",
      "url": "/research/2024/03/12/VideoMamba-State-Space-Model-for-Efficient-Video-Understanding.html",
      "date": "March 12, 2024",
      "summary": "This work introduces VideoMamba, an innovative approach that addresses local redundancy and global dependencies in video understanding by adapting the Mamba framework to video analysis. VideoMamba outperforms existing 3D convolutional neural networks and video transformers through its linear-complexity operator, which facilitates efficient long-term modeling for high-resolution, long-duration videos. Its effectiveness is demonstrated across four main areas: scalability in the visual domain without needing extensive dataset pretraining, thanks to a novel self-distillation technique; the ability to recognize short-term actions with fine-grained motion differences; superior performance in long-term video understanding compared to traditional feature-based models; and robust compatibility with multiple modalities, enhancing multi-modal video analysis. VideoMamba establishes a new standard for comprehensive and efficient video understanding."
    }
  ],
  "backbone": [
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "The authors introduce VideoMAE, a data-efficient self-supervised video pre-training (SSVP) approach that utilizes video masked autoencoders with a novel high-ratio video tube masking technique inspired by ImageMAE. Key findings include the effectiveness of high masking ratios (90-95%) due to video's temporal redundancy, strong performance on small datasets (~3k-4k videos) without extra data highlighting the importance of high-level structure learning, and data quality being more crucial than quantity with domain shift being significant. Notably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone without extra data."
    }
  ],
  "handling": [
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    },
    {
      "title": "Language Modeling with Gated Convolutional Networks",
      "url": "/research/2017/09/08/Language-Modeling-with-Gated-Convolutional-Networks.html",
      "date": "September 8, 2017",
      "summary": "This paper introduces a novel language modeling approach using stacked convolutions that enable efficient parallel processing, in contrast to the traditionally used recurrent neural networks known for handling unbounded context. The proposed method features a simplified gating mechanism that outperforms previous models and demonstrates superior performance on the WikiText-103 benchmark, showing its capability to manage long-term dependencies. It also delivers competitive results on the Google Billion Words benchmark and significantly reduces sentence scoring latency compared to recurrent models. This marks the first instance of a non-recurrent model achieving comparable success to strong recurrent models on major language tasks."
    }
  ],
  "multisource": [
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    }
  ],
  "record": [
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    },
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    }
  ],
  "something": [
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    },
    {
      "title": "V-Jepa: Latent Video Prediction For Visual Representation Learning",
      "url": "/research/2023/02/15/V-Jepa-Latent-Video-Prediction-For-Visual-Representation-Learning.html",
      "date": "February 15, 2023",
      "summary": "This paper introduces V-JEPA, a self-supervised video learning method that predicts masked spatio-temporal regions in latent space, effectively applying the masked-modelling principle of large language models to video. This approach generates visual features useful across various image and video tasks without needing model adjustment, achieving significant improvements on Kinetics-400 (82.1%) and Something-Something-v2 (71.2%) benchmarks, outperforming prior video models. V-JEPA also excels in motion understanding tasks, surpassing leading image models like DINOv2 and OpenCLIP, and achieves 77.9% on ImageNet classification with video training alone, setting a new standard for video models."
    }
  ],
  "article": [
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article provides an extensive review of vision transformer techniques applied specifically to human action recognition. These action transformers are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also examines optimizing spatio-temporal attention in transformers for longer sequences and different learning strategies like self-supervised and zero-shot learning with their respective loss functions. Additionally, it assesses progress in benchmark evaluation scores and discusses challenges and future directions in this research area."
    },
    {
      "title": "A comprehensive review of Binary Neural Network",
      "url": "/research/2023/05/11/A-comprehensive-review-of-Binary-Neural-Network.html",
      "date": "May 11, 2023",
      "summary": "Deep learning (DL) is widely used in intelligent systems and real-life applications, but its deployment on computationally limited and energy-constrained devices requires efficient technologies like Binary Neural Networks (BNN). BNNs save significant storage, computation cost, and energy, making them ideal for small devices, despite the trade-offs in memory and performance. This article offers a comprehensive review of BNN developments, focusing on 1-bit activations and weights. It covers the evolution of BNN, from early models to advanced algorithms and design aspects, and addresses BNN optimization, deployment, computing architectures, and diverse applications. It also outlines potential research directions in BNN technology."
    }
  ],
  "categorize": [
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article provides an extensive review of vision transformer techniques applied specifically to human action recognition. These action transformers are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also examines optimizing spatio-temporal attention in transformers for longer sequences and different learning strategies like self-supervised and zero-shot learning with their respective loss functions. Additionally, it assesses progress in benchmark evaluation scores and discusses challenges and future directions in this research area."
    }
  ],
  "modality": [
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article provides an extensive review of vision transformer techniques applied specifically to human action recognition. These action transformers are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also examines optimizing spatio-temporal attention in transformers for longer sequences and different learning strategies like self-supervised and zero-shot learning with their respective loss functions. Additionally, it assesses progress in benchmark evaluation scores and discusses challenges and future directions in this research area."
    },
    {
      "title": "VideoMamba: State Space Model for Efficient Video Understanding",
      "url": "/research/2024/03/12/VideoMamba-State-Space-Model-for-Efficient-Video-Understanding.html",
      "date": "March 12, 2024",
      "summary": "This work introduces VideoMamba, an innovative approach that addresses local redundancy and global dependencies in video understanding by adapting the Mamba framework to video analysis. VideoMamba outperforms existing 3D convolutional neural networks and video transformers through its linear-complexity operator, which facilitates efficient long-term modeling for high-resolution, long-duration videos. Its effectiveness is demonstrated across four main areas: scalability in the visual domain without needing extensive dataset pretraining, thanks to a novel self-distillation technique; the ability to recognize short-term actions with fine-grained motion differences; superior performance in long-term video understanding compared to traditional feature-based models; and robust compatibility with multiple modalities, enhancing multi-modal video analysis. VideoMamba establishes a new standard for comprehensive and efficient video understanding."
    },
    {
      "title": "The Platonic Representation Hypothesis",
      "url": "/research/2024/05/13/The-Platonic-Representation-Hypothesis.html",
      "date": "May 13, 2024",
      "summary": "This paper argues that AI model representations, especially in deep networks, are converging. It surveys examples in the literature showing how neural network representations align over time and across domains. It demonstrates convergence across data modalities, noting that as vision and language models grow, they measure distances between data points similarly. The authors hypothesize that this convergence leads to a shared statistical model of reality, termed the platonic representation and discuss possible selective pressures toward it. The paper also addresses the implications, limitations, and counterexamples to this trend."
    }
  ],
  "cube": [
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article provides an extensive review of vision transformer techniques applied specifically to human action recognition. These action transformers are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also examines optimizing spatio-temporal attention in transformers for longer sequences and different learning strategies like self-supervised and zero-shot learning with their respective loss functions. Additionally, it assesses progress in benchmark evaluation scores and discusses challenges and future directions in this research area."
    }
  ],
  "examines": [
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article provides an extensive review of vision transformer techniques applied specifically to human action recognition. These action transformers are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also examines optimizing spatio-temporal attention in transformers for longer sequences and different learning strategies like self-supervised and zero-shot learning with their respective loss functions. Additionally, it assesses progress in benchmark evaluation scores and discusses challenges and future directions in this research area."
    }
  ],
  "score": [
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article provides an extensive review of vision transformer techniques applied specifically to human action recognition. These action transformers are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also examines optimizing spatio-temporal attention in transformers for longer sequences and different learning strategies like self-supervised and zero-shot learning with their respective loss functions. Additionally, it assesses progress in benchmark evaluation scores and discusses challenges and future directions in this research area."
    },
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    },
    {
      "title": "Transformers with Learnable Activation Functions",
      "url": "/research/2023/02/14/Transformers-with-Learnable-Activation-Functions.html",
      "date": "February 14, 2023",
      "summary": "Activation functions significantly affect model performance by reducing data complexity, yet their selection in Transformer-based language models is often overlooked. This paper explores the impact of using rational activation functions (RAFs), which unlike fixed activation functions (FAFs), can learn optimal functions from data. Our experiments demonstrate that Transformer models with RAFs (RAFT) outperform those with FAFs (FAFT), achieving a 5.71 point higher score on the GLUE benchmark with only 100 training examples and a 2.05 point increase on SQuAD with full data. The varied shapes of learned RAFs across layers and tasks suggest a new method for analyzing and understanding large pre-trained language models."
    }
  ],
  "update": [
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "The paper focuses on evaluating the performance of recent advancements in image transformers, such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, on the Tiny ImageNet dataset for transfer learning tasks. While these models are typically trained on large datasets like ImageNet-21k and then fine-tuned on ImageNet-1k, their assessments often overlook the Tiny ImageNet benchmark. The study updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. This underscores the importance of evaluating these models on the Tiny ImageNet dataset for transfer learning performance."
    },
    {
      "title": "Don't Decay the Learning Rate, Increase the Batch Size",
      "url": "/research/2017/11/01/Don't-Decay-the-Learning-Rate,-Increase-the-Batch-Size.html",
      "date": "November 1, 2017",
      "summary": "This study demonstrates that increasing the batch size during training achieves similar learning outcomes as the common practice of decaying the learning rate, applicable to stochastic gradient descent and its variants, including with momentum and Adam optimization. This approach not only matches test accuracies within the same number of epochs but also enhances parallelism and reduces training time due to fewer parameter updates. Efficiency can be further improved by adjusting the learning rate and batch size proportionally, and although increasing the momentum coefficient and scaling the batch size accordingly may slightly lower test accuracy, it enables the use of large batch training without needing to tune hyper-parameters. Using these methods, ResNet-50 was trained on ImageNet to a 76.1% validation accuracy in less than 30 minutes."
    },
    {
      "title": "Sharpness-Aware Training for Free",
      "url": "/research/2023/03/02/Sharpness-Aware-Training-for-Free.html",
      "date": "March 2, 2023",
      "summary": "Modern deep neural networks (DNNs) excel in performance but often suffer from over-parameterization, leading to increased generalization error without tailored training strategies. Sharpness-Aware Minimization (SAM) has proven effective in reducing generalization error by minimizing sharpness in the loss landscape. However, SAM incurs a significant computational overhead. This paper introduces Sharpness-Aware Training for Free (SAF), which mitigates sharpness at nearly zero additional computational cost. SAF achieves this by preventing sudden drops in loss within sharp local minima during weight updates. A novel trajectory loss, based on KL-divergence between current and past DNN outputs, replaces SAM's sharpness measure, guiding convergence towards flat minima for enhanced generalization. Empirical results demonstrate SAF's effectiveness on ImageNet with comparable computational efficiency to the base optimizer."
    },
    {
      "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
      "url": "/research/2024/02/28/CL-MAE:-Curriculum-Learned-Masked-Autoencoders.html",
      "date": "February 28, 2024",
      "summary": "This paper proposes a curriculum learning approach that updates the masking strategy of Masked Auto Encoders (MAE) to progressively increase the complexity of the self-supervised reconstruction task. To achieve this, a novel learnable masking module is introduced, capable of generating masks of varying complexities, and integrated into masked autoencoders (MAE). This module is jointly trained with the MAE, adjusting its behavior from partner (optimizing the same reconstruction loss) to adversary (optimizing the opposite loss), with a smooth transition regulated by a factor multiplied with the reconstruction loss. This training procedure creates an easy-to-hard curriculum. The Curriculum-Learned Masked Autoencoder (CL-MAE) is trained on ImageNet and demonstrates superior representation learning compared to MAE. Empirical results on five downstream tasks confirm that curriculum learning can successfully self-supervise masked autoencoders."
    },
    {
      "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
      "url": "/research/2024/03/13/Simple-and-Scalable-Strategies-to-Continually-Pre-train-Large-Language-Models.html",
      "date": "March 13, 2024",
      "summary": "Large language models (LLMs) often require re-training on new data, consuming extensive computational resources. We present an efficient method that combines learning rate re-warming, re-decaying, and data replay, effectively maintaining performance without full re-training. This approach works well across different data distributions, including minor shifts (English→English) and significant shifts (English→German), tested up to 405M and 10B parameter models. Our findings suggest that continual learning strategies can update LLMs with minimal computational cost, rivaling traditional re-training methods. Additionally, we propose alternatives to the cosine learning rate schedule to reduce forgetting, offering more flexibility in learning without a fixed token budget."
    },
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "datum": [
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "The paper focuses on evaluating the performance of recent advancements in image transformers, such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, on the Tiny ImageNet dataset for transfer learning tasks. While these models are typically trained on large datasets like ImageNet-21k and then fine-tuned on ImageNet-1k, their assessments often overlook the Tiny ImageNet benchmark. The study updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. This underscores the importance of evaluating these models on the Tiny ImageNet dataset for transfer learning performance."
    },
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    }
  ],
  "cait": [
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "The paper focuses on evaluating the performance of recent advancements in image transformers, such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, on the Tiny ImageNet dataset for transfer learning tasks. While these models are typically trained on large datasets like ImageNet-21k and then fine-tuned on ImageNet-1k, their assessments often overlook the Tiny ImageNet benchmark. The study updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. This underscores the importance of evaluating these models on the Tiny ImageNet dataset for transfer learning performance."
    }
  ],
  "overlook": [
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "The paper focuses on evaluating the performance of recent advancements in image transformers, such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, on the Tiny ImageNet dataset for transfer learning tasks. While these models are typically trained on large datasets like ImageNet-21k and then fine-tuned on ImageNet-1k, their assessments often overlook the Tiny ImageNet benchmark. The study updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. This underscores the importance of evaluating these models on the Tiny ImageNet dataset for transfer learning performance."
    },
    {
      "title": "Normalized Attention Without Probability Cage",
      "url": "/research/2020/05/19/Normalized-Attention-Without-Probability-Cage.html",
      "date": "May 19, 2020",
      "summary": "Softmax-attention architectures, especially popularized by Transformers, have seen significant advancements in various tasks. However, the geometric implications of softmax-attention remain underexplored. In this study, we demonstrate limitations arising from constraining attention weights to the probability simplex and its impact on the convex hull of value vectors. We reveal sequence length-dependent biases in Transformers towards token isolation at initialization and compare them with max- and sum-pooling, which are strong but often overlooked baselines. To address these issues, we propose a novel approach of replacing softmax with normalization in self-attention, resulting in a robust and widely applicable architecture. Our findings are supported by empirical results from over 25,000 trained models, and all results and implementations are publicly available."
    },
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    },
    {
      "title": "Transformers with Learnable Activation Functions",
      "url": "/research/2023/02/14/Transformers-with-Learnable-Activation-Functions.html",
      "date": "February 14, 2023",
      "summary": "Activation functions significantly affect model performance by reducing data complexity, yet their selection in Transformer-based language models is often overlooked. This paper explores the impact of using rational activation functions (RAFs), which unlike fixed activation functions (FAFs), can learn optimal functions from data. Our experiments demonstrate that Transformer models with RAFs (RAFT) outperform those with FAFs (FAFT), achieving a 5.71 point higher score on the GLUE benchmark with only 100 training examples and a 2.05 point increase on SQuAD with full data. The varied shapes of learned RAFs across layers and tasks suggest a new method for analyzing and understanding large pre-trained language models."
    }
  ],
  "underscore": [
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "The paper focuses on evaluating the performance of recent advancements in image transformers, such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, on the Tiny ImageNet dataset for transfer learning tasks. While these models are typically trained on large datasets like ImageNet-21k and then fine-tuned on ImageNet-1k, their assessments often overlook the Tiny ImageNet benchmark. The study updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. This underscores the importance of evaluating these models on the Tiny ImageNet dataset for transfer learning performance."
    },
    {
      "title": "Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks",
      "url": "/research/2020/06/11/Batch-Normalization-Provably-Avoids-Rank-Collapse-for-Randomly-Initialised-Deep-Networks.html",
      "date": "June 11, 2020",
      "summary": "We investigate the challenge of training randomly initialized deep neural networks due to spectral instabilities in products of random matrices. Batch normalization emerges as an effective solution to prevent rank collapse in both linear and ReLU networks. Leveraging tools from Markov chain theory, we establish a lower rank bound for deep linear networks. Empirical findings show that this rank robustness extends to ReLU networks. Our experiments on real-world datasets underscore the significance of rank stability in training modern deep neural architectures."
    },
    {
      "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
      "url": "/research/2021/07/24/The-Many-Faces-of-Robustness-A-Critical-Analysis-of-Out-of-Distribution-Generalization.html",
      "date": "July 24, 2021",
      "summary": "We present four novel real-world distribution shift datasets encompassing changes in image style, blurriness, location, camera settings, and more. Evaluating existing methods for enhancing out-of-distribution robustness, we discover that employing larger models and artificial data augmentations can enhance robustness against real-world distribution shifts, contradicting prior claims. Our findings demonstrate that improvements in artificial robustness benchmarks can indeed transfer to real-world distribution shifts, contrary to prior assumptions. Additionally, we introduce a novel data augmentation technique that surpasses models pretrained with significantly more labeled data, emphasizing its efficacy in addressing real-world distribution shifts. While some methods consistently mitigate texture and local image statistics shifts, they fail to address other shifts like geographic changes. Our results underscore the necessity for future research to examine multiple distribution shifts concurrently, as no single method consistently improves robustness across all evaluated scenarios."
    },
    {
      "title": "From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning",
      "url": "/research/2023/03/16/From-MNIST-to-ImageNet-and-Back:-Benchmarking-Continual-Curriculum-Learning.html",
      "date": "March 16, 2023",
      "summary": "Continual learning (CL) is a promising trend in machine learning, aiming to develop robust models and strategies for dynamic environments by incorporating new knowledge while retaining past knowledge. CL research is fragmented, with various protocols, tasks, datasets, and metrics, often not reflecting real-world complexity and tailored to specific strategies. This work addresses this gap by introducing two novel CL benchmarks with heterogeneous tasks from six image datasets, varying in complexity and quality. These benchmarks fairly evaluate state-of-the-art CL strategies in scenarios closer to real-world conditions. The benchmarks present tasks in both increasing and decreasing complexity, assessing models' ability to exploit task structure. The work emphasizes a rigorous, reproducible evaluation protocol for measuring generalization and memory retention in models. Experimental results show that popular CL strategies perform poorly on these benchmarks, exhibit high forgetting, and struggle with curriculum task ordering. These findings underscore the need for rigorous comparisons and the development of new CL strategies for complex scenarios."
    }
  ],
  "identification": [
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "The study presents a novel approach for detection identification of Holstein-Friesian cattle. This technique offers a completely hands-off solution for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training when new cattle are introduced. The system utilizes convolutional neural networks and deep metric learning, achieving high accuracy with a 93.8% success rate in identifying cattle unseen during training, using only half of the population."
    }
  ],
  "holstein": [
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "The study presents a novel approach for detection identification of Holstein-Friesian cattle. This technique offers a completely hands-off solution for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training when new cattle are introduced. The system utilizes convolutional neural networks and deep metric learning, achieving high accuracy with a 93.8% success rate in identifying cattle unseen during training, using only half of the population."
    }
  ],
  "friesian": [
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "The study presents a novel approach for detection identification of Holstein-Friesian cattle. This technique offers a completely hands-off solution for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training when new cattle are introduced. The system utilizes convolutional neural networks and deep metric learning, achieving high accuracy with a 93.8% success rate in identifying cattle unseen during training, using only half of the population."
    }
  ],
  "cattle": [
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "The study presents a novel approach for detection identification of Holstein-Friesian cattle. This technique offers a completely hands-off solution for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training when new cattle are introduced. The system utilizes convolutional neural networks and deep metric learning, achieving high accuracy with a 93.8% success rate in identifying cattle unseen during training, using only half of the population."
    }
  ],
  "hands": [
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "The study presents a novel approach for detection identification of Holstein-Friesian cattle. This technique offers a completely hands-off solution for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training when new cattle are introduced. The system utilizes convolutional neural networks and deep metric learning, achieving high accuracy with a 93.8% success rate in identifying cattle unseen during training, using only half of the population."
    }
  ],
  "off": [
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "The study presents a novel approach for detection identification of Holstein-Friesian cattle. This technique offers a completely hands-off solution for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training when new cattle are introduced. The system utilizes convolutional neural networks and deep metric learning, achieving high accuracy with a 93.8% success rate in identifying cattle unseen during training, using only half of the population."
    },
    {
      "title": "A comprehensive review of Binary Neural Network",
      "url": "/research/2023/05/11/A-comprehensive-review-of-Binary-Neural-Network.html",
      "date": "May 11, 2023",
      "summary": "Deep learning (DL) is widely used in intelligent systems and real-life applications, but its deployment on computationally limited and energy-constrained devices requires efficient technologies like Binary Neural Networks (BNN). BNNs save significant storage, computation cost, and energy, making them ideal for small devices, despite the trade-offs in memory and performance. This article offers a comprehensive review of BNN developments, focusing on 1-bit activations and weights. It covers the evolution of BNN, from early models to advanced algorithms and design aspects, and addresses BNN optimization, deployment, computing architectures, and diverse applications. It also outlines potential research directions in BNN technology."
    }
  ],
  "herd": [
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "The study presents a novel approach for detection identification of Holstein-Friesian cattle. This technique offers a completely hands-off solution for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training when new cattle are introduced. The system utilizes convolutional neural networks and deep metric learning, achieving high accuracy with a 93.8% success rate in identifying cattle unseen during training, using only half of the population."
    }
  ],
  "retrain": [
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "The study presents a novel approach for detection identification of Holstein-Friesian cattle. This technique offers a completely hands-off solution for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training when new cattle are introduced. The system utilizes convolutional neural networks and deep metric learning, achieving high accuracy with a 93.8% success rate in identifying cattle unseen during training, using only half of the population."
    },
    {
      "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
      "url": "/research/2024/03/13/Simple-and-Scalable-Strategies-to-Continually-Pre-train-Large-Language-Models.html",
      "date": "March 13, 2024",
      "summary": "Large language models (LLMs) often require re-training on new data, consuming extensive computational resources. We present an efficient method that combines learning rate re-warming, re-decaying, and data replay, effectively maintaining performance without full re-training. This approach works well across different data distributions, including minor shifts (English→English) and significant shifts (English→German), tested up to 405M and 10B parameter models. Our findings suggest that continual learning strategies can update LLMs with minimal computational cost, rivaling traditional re-training methods. Additionally, we propose alternatives to the cosine learning rate schedule to reduce forgetting, offering more flexibility in learning without a fixed token budget."
    }
  ],
  "average": [
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    },
    {
      "title": "Fine-tuning with Very Large Dropout",
      "url": "/research/2024/03/01/Fine-tuning-with-Very-Large-Dropout.html",
      "date": "March 1, 2024",
      "summary": "This study challenges the notion that machine learning practices assume training and testing data have the same distribution. It explores the effectiveness of high dropout rates, as opposed to ensemble techniques, in developing rich data representations suitable for multiple distribution scenarios. These representations surpass those achieved by traditional in-distribution performance regularization and the implicit sparsity induced by common stochastic gradient methods. While training deep networks from scratch with high dropout rates is impractical, fine-tuning pre-trained models under these conditions is feasible and yields better out-of-distribution performance than ensembles and model averaging techniques like model soups. This finding is significant due to the growing relevance of fine-tuning with large pre-trained models, offering insights into the nature of rich representations and the linear characteristics of fine-tuning large networks with small datasets."
    }
  ],
  "swa": [
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    }
  ],
  "minimization": [
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    },
    {
      "title": "On the Maximum Hessian Eigenvalue and Generalization",
      "url": "/research/2023/05/23/On-the-Maximum-Hessian-Eigenvalue-and-Generalization.html",
      "date": "May 23, 2023",
      "summary": "This study investigates the relationship between training interventions and the generalization of deep networks. While previous research suggests that flatter solutions generalize better than sharper ones, particularly measured by λmax, the largest eigenvalue of the Hessian of the loss, this paper challenges this notion. Through experiments, we demonstrate that larger learning rates reduce λmax for all batch sizes but do not consistently improve generalization. Additionally, scaling batch size and learning rate simultaneously can change λmax without affecting generalization. Sharpness-Aware Minimization (SAM) produces smaller λmax but does not consistently enhance generalization, especially with larger batch sizes. Excessive dropout probabilities can degrade generalization, despite promoting smaller λmax. Batch normalization, while not consistently reducing λmax, still improves generalization. These findings question λmax's role in explaining generalization in neural networks, highlighting the limits of its explanatory power."
    },
    {
      "title": "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization",
      "url": "/research/2023/07/04/Gradient-Norm-Aware-Minimization-Seeks-First-Order-Flatness-and-Improves-Generalization.html",
      "date": "July 4, 2023",
      "summary": "Recent advancements have highlighted the effectiveness of flat minima in enhancing generalization, particularly through Sharpness-Aware Minimization (SAM). However, existing definitions of flatness, such as zeroth-order flatness, have limitations in discerning between minima with low and high generalization errors. To address this, we propose first-order flatness, which considers maximal gradient norm within a perturbation radius. We introduce Gradient norm Aware Minimization (GAM) as a novel training approach to achieve uniformly small curvature across all directions. Experimental results demonstrate GAM's ability to enhance generalization compared to standard optimizers like SGD and AdamW across various datasets and networks. Moreover, GAM facilitates SAM in identifying flatter minima, leading to improved generalization."
    },
    {
      "title": "Model Generalization: A Sharpness Aware Optimization Perspective",
      "url": "/research/2022/08/14/Model-Generalization-A-Sharpness-Aware-Optimization-Perspective.html",
      "date": "August 14, 2022",
      "summary": "This study investigates the effectiveness of Sharpness-Aware Minimization (SAM) and adaptive Sharpness-Aware Minimization (ASAM) in enhancing model generalization. Through three experiments, we assess their impact from a sharpness-aware perspective. Results demonstrate that optimization techniques based on sharpness awareness can bolster model generalization. Furthermore, ASAM exhibits potential for enhancing generalization performance on un-normalized data, though additional research is required for validation."
    },
    {
      "title": "Penalizing Gradient Norm for Efficiently Improving Generalization in Deep Learning",
      "url": "/research/2022/06/26/Penalizing-Gradient-Norm-for-Efficiently-Improving-Generalization-in-Deep-Learning.html",
      "date": "June 26, 2022",
      "summary": "This paper proposes a method to enhance the generalization of deep neural networks (DNNs) by penalizing the gradient norm of the loss function during optimization. By constraining the gradient norm, the optimizers tend to find flat minima, improving generalization. We efficiently implement this method using first-order approximation within the gradient descent framework. Experimental results demonstrate improved generalization across various models and datasets. Additionally, we show that a recent method, sharpness-aware minimization, is a special case of our approach, with our method achieving new state-of-the-art performance on tested tasks."
    },
    {
      "title": "Sharpness-Aware Training for Free",
      "url": "/research/2023/03/02/Sharpness-Aware-Training-for-Free.html",
      "date": "March 2, 2023",
      "summary": "Modern deep neural networks (DNNs) excel in performance but often suffer from over-parameterization, leading to increased generalization error without tailored training strategies. Sharpness-Aware Minimization (SAM) has proven effective in reducing generalization error by minimizing sharpness in the loss landscape. However, SAM incurs a significant computational overhead. This paper introduces Sharpness-Aware Training for Free (SAF), which mitigates sharpness at nearly zero additional computational cost. SAF achieves this by preventing sudden drops in loss within sharp local minima during weight updates. A novel trajectory loss, based on KL-divergence between current and past DNN outputs, replaces SAM's sharpness measure, guiding convergence towards flat minima for enhanced generalization. Empirical results demonstrate SAF's effectiveness on ImageNet with comparable computational efficiency to the base optimizer."
    },
    {
      "title": "AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks",
      "url": "/research/2023/03/01/AdaSAM-Boosting-Sharpness-Aware-Minimization-with-Adaptive-Learning-Rate-and-Momentum-for-Training-Deep-Neural-Networks.html",
      "date": "March 1, 2023",
      "summary": "The Sharpness Aware Minimization (SAM) optimizer, known for enhancing the generalization of deep neural networks by introducing extra perturbation steps, is further developed into AdaSAM by integrating adaptive learning rates and momentum acceleration. Although AdaSAM has been empirically applied to large-scale networks, a theoretical understanding of its performance, considering the complexity of its components, was lacking. This study presents a theoretical analysis of AdaSAM's convergence in stochastic non-convex settings, demonstrating a convergence rate of O(1/√bT) that scales linearly with mini-batch size. By introducing a delayed second-order momentum term, the study successfully decouples and analyzes the intertwined effects of stochastic gradients, adaptive learning rates, and perturbations. This is the first work to offer a detailed convergence rate for SAM with adaptive mechanisms. Experimental results on various NLP tasks indicate AdaSAM's superior performance over SGD, AMSGrad, and SAM optimizers."
    },
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    },
    {
      "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
      "url": "/research/2022/03/19/Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training.html",
      "date": "March 19, 2022",
      "summary": "Sharpness-Aware Minimization (SAM) enhances generalization by optimizing a neighborhood-based perturbed loss, but it doesn't always favor flat minima due to both sharp and flat minima having low perturbed losses. We introduce a new measure, the surrogate gap, reflecting the dominant Hessian eigenvalue at small neighborhood radii, which is simple to compute and can be minimized during training. We propose the Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), an advancement over SAM with minimal additional computational cost. GSAM employs a dual-step approach: first, minimizing the perturbed loss similar to SAM, and second, reducing the surrogate gap without affecting the perturbed loss to target regions with low loss and sharpness, thus achieving superior generalization. GSAM is theoretically robust, showing better convergence and empirical generalization improvements, notably a +3.2% gain over SAM and +5.4% over AdamW in ImageNet accuracy for ViT-B/32."
    },
    {
      "title": "ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Network",
      "url": "/research/2021/06/29/ASAM-Adaptive-Sharpness-Aware-Minimization-for-Scale-Invariant-Learning-of-Deep-Neural-Network.html",
      "date": "June 29, 2021",
      "summary": "Recent advances in learning algorithms have highlighted the importance of the sharpness of the loss surface as a key indicator of the generalization gap, achieving state-of-the-art results. However, traditional sharpness measures, defined within a fixed region, suffer from sensitivity to parameter scaling, weakening their predictive value for the generalization gap. This paper introduces a scale-invariant measure, adaptive sharpness, along with a new generalization bound. We present a new learning method, adaptive sharpness-aware minimization (ASAM), which leverages this bound. Our experiments across various benchmark datasets demonstrate that ASAM significantly enhances model generalization."
    },
    {
      "title": "How Does Sharpness-Aware Minimization Minimize Sharpness?",
      "url": "/research/2023/01/05/How-Does-Sharpness-Aware-Minimization-Minimize-Sharpness.html",
      "date": "January 5, 2023",
      "summary": "Sharpness-Aware Minimization (SAM) enhances deep neural networks' generalization across various settings. Although SAM aims to penalize model sharpness using a computationally efficient approach, its exact operational mechanism and the sharpness notion it regularizes remain unclear, partly due to differing sharpness concepts used in its theoretical framework and empirical validation. This study identifies the precise sharpness concept SAM regulates and explains its mechanism. It reveals that the combined effect of SAM's two-step approximations, despite being individually misleading, correctly improves generalization when using full-batch gradients. Moreover, we demonstrate that the stochastic SAM version indeed regularizes a third sharpness notion, aligning more closely with practical performance. This effectiveness is attributed to the gradient's alignment with the Hessian's top eigenvector under SAM."
    }
  ],
  "sam": [
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    },
    {
      "title": "On the Maximum Hessian Eigenvalue and Generalization",
      "url": "/research/2023/05/23/On-the-Maximum-Hessian-Eigenvalue-and-Generalization.html",
      "date": "May 23, 2023",
      "summary": "This study investigates the relationship between training interventions and the generalization of deep networks. While previous research suggests that flatter solutions generalize better than sharper ones, particularly measured by λmax, the largest eigenvalue of the Hessian of the loss, this paper challenges this notion. Through experiments, we demonstrate that larger learning rates reduce λmax for all batch sizes but do not consistently improve generalization. Additionally, scaling batch size and learning rate simultaneously can change λmax without affecting generalization. Sharpness-Aware Minimization (SAM) produces smaller λmax but does not consistently enhance generalization, especially with larger batch sizes. Excessive dropout probabilities can degrade generalization, despite promoting smaller λmax. Batch normalization, while not consistently reducing λmax, still improves generalization. These findings question λmax's role in explaining generalization in neural networks, highlighting the limits of its explanatory power."
    },
    {
      "title": "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization",
      "url": "/research/2023/07/04/Gradient-Norm-Aware-Minimization-Seeks-First-Order-Flatness-and-Improves-Generalization.html",
      "date": "July 4, 2023",
      "summary": "Recent advancements have highlighted the effectiveness of flat minima in enhancing generalization, particularly through Sharpness-Aware Minimization (SAM). However, existing definitions of flatness, such as zeroth-order flatness, have limitations in discerning between minima with low and high generalization errors. To address this, we propose first-order flatness, which considers maximal gradient norm within a perturbation radius. We introduce Gradient norm Aware Minimization (GAM) as a novel training approach to achieve uniformly small curvature across all directions. Experimental results demonstrate GAM's ability to enhance generalization compared to standard optimizers like SGD and AdamW across various datasets and networks. Moreover, GAM facilitates SAM in identifying flatter minima, leading to improved generalization."
    },
    {
      "title": "Model Generalization: A Sharpness Aware Optimization Perspective",
      "url": "/research/2022/08/14/Model-Generalization-A-Sharpness-Aware-Optimization-Perspective.html",
      "date": "August 14, 2022",
      "summary": "This study investigates the effectiveness of Sharpness-Aware Minimization (SAM) and adaptive Sharpness-Aware Minimization (ASAM) in enhancing model generalization. Through three experiments, we assess their impact from a sharpness-aware perspective. Results demonstrate that optimization techniques based on sharpness awareness can bolster model generalization. Furthermore, ASAM exhibits potential for enhancing generalization performance on un-normalized data, though additional research is required for validation."
    },
    {
      "title": "SAM as an Optimal Relaxation of Bayes",
      "url": "/research/2023/12/10/SAM-as-an-Optimal-Relaxation-of-Bayes.html",
      "date": "December 10, 2023",
      "summary": "We introduce SAM, a method enhancing generalization in deep learning, as a relaxation of the Bayes objective. SAM replaces the expected negative-loss with an optimal convex lower bound derived using the Fenchel biconjugate. This connection enables an Adam-like extension of SAM, offering automatic uncertainty estimates and potential accuracy improvements. Bridging adversarial and Bayesian methods, our work paves the way for robustness enhancement."
    },
    {
      "title": "Sharpness-Aware Training for Free",
      "url": "/research/2023/03/02/Sharpness-Aware-Training-for-Free.html",
      "date": "March 2, 2023",
      "summary": "Modern deep neural networks (DNNs) excel in performance but often suffer from over-parameterization, leading to increased generalization error without tailored training strategies. Sharpness-Aware Minimization (SAM) has proven effective in reducing generalization error by minimizing sharpness in the loss landscape. However, SAM incurs a significant computational overhead. This paper introduces Sharpness-Aware Training for Free (SAF), which mitigates sharpness at nearly zero additional computational cost. SAF achieves this by preventing sudden drops in loss within sharp local minima during weight updates. A novel trajectory loss, based on KL-divergence between current and past DNN outputs, replaces SAM's sharpness measure, guiding convergence towards flat minima for enhanced generalization. Empirical results demonstrate SAF's effectiveness on ImageNet with comparable computational efficiency to the base optimizer."
    },
    {
      "title": "AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks",
      "url": "/research/2023/03/01/AdaSAM-Boosting-Sharpness-Aware-Minimization-with-Adaptive-Learning-Rate-and-Momentum-for-Training-Deep-Neural-Networks.html",
      "date": "March 1, 2023",
      "summary": "The Sharpness Aware Minimization (SAM) optimizer, known for enhancing the generalization of deep neural networks by introducing extra perturbation steps, is further developed into AdaSAM by integrating adaptive learning rates and momentum acceleration. Although AdaSAM has been empirically applied to large-scale networks, a theoretical understanding of its performance, considering the complexity of its components, was lacking. This study presents a theoretical analysis of AdaSAM's convergence in stochastic non-convex settings, demonstrating a convergence rate of O(1/√bT) that scales linearly with mini-batch size. By introducing a delayed second-order momentum term, the study successfully decouples and analyzes the intertwined effects of stochastic gradients, adaptive learning rates, and perturbations. This is the first work to offer a detailed convergence rate for SAM with adaptive mechanisms. Experimental results on various NLP tasks indicate AdaSAM's superior performance over SGD, AMSGrad, and SAM optimizers."
    },
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    },
    {
      "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
      "url": "/research/2022/03/19/Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training.html",
      "date": "March 19, 2022",
      "summary": "Sharpness-Aware Minimization (SAM) enhances generalization by optimizing a neighborhood-based perturbed loss, but it doesn't always favor flat minima due to both sharp and flat minima having low perturbed losses. We introduce a new measure, the surrogate gap, reflecting the dominant Hessian eigenvalue at small neighborhood radii, which is simple to compute and can be minimized during training. We propose the Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), an advancement over SAM with minimal additional computational cost. GSAM employs a dual-step approach: first, minimizing the perturbed loss similar to SAM, and second, reducing the surrogate gap without affecting the perturbed loss to target regions with low loss and sharpness, thus achieving superior generalization. GSAM is theoretically robust, showing better convergence and empirical generalization improvements, notably a +3.2% gain over SAM and +5.4% over AdamW in ImageNet accuracy for ViT-B/32."
    },
    {
      "title": "How Does Sharpness-Aware Minimization Minimize Sharpness?",
      "url": "/research/2023/01/05/How-Does-Sharpness-Aware-Minimization-Minimize-Sharpness.html",
      "date": "January 5, 2023",
      "summary": "Sharpness-Aware Minimization (SAM) enhances deep neural networks' generalization across various settings. Although SAM aims to penalize model sharpness using a computationally efficient approach, its exact operational mechanism and the sharpness notion it regularizes remain unclear, partly due to differing sharpness concepts used in its theoretical framework and empirical validation. This study identifies the precise sharpness concept SAM regulates and explains its mechanism. It reveals that the combined effect of SAM's two-step approximations, despite being individually misleading, correctly improves generalization when using full-batch gradients. Moreover, we demonstrate that the stochastic SAM version indeed regularizes a third sharpness notion, aligning more closely with practical performance. This effectiveness is attributed to the gradient's alignment with the Hessian's top eigenvector under SAM."
    }
  ],
  "surface": [
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    },
    {
      "title": "ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Network",
      "url": "/research/2021/06/29/ASAM-Adaptive-Sharpness-Aware-Minimization-for-Scale-Invariant-Learning-of-Deep-Neural-Network.html",
      "date": "June 29, 2021",
      "summary": "Recent advances in learning algorithms have highlighted the importance of the sharpness of the loss surface as a key indicator of the generalization gap, achieving state-of-the-art results. However, traditional sharpness measures, defined within a fixed region, suffer from sensitivity to parameter scaling, weakening their predictive value for the generalization gap. This paper introduces a scale-invariant measure, adaptive sharpness, along with a new generalization bound. We present a new learning method, adaptive sharpness-aware minimization (ASAM), which leverages this bound. Our experiments across various benchmark datasets demonstrate that ASAM significantly enhances model generalization."
    }
  ],
  "choose": [
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    }
  ],
  "x3d": [
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    }
  ],
  "recognitio": [
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    }
  ],
  "adopt": [
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    }
  ],
  "stepwise": [
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    }
  ],
  "deliver": [
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    },
    {
      "title": "Language Modeling with Gated Convolutional Networks",
      "url": "/research/2017/09/08/Language-Modeling-with-Gated-Convolutional-Networks.html",
      "date": "September 8, 2017",
      "summary": "This paper introduces a novel language modeling approach using stacked convolutions that enable efficient parallel processing, in contrast to the traditionally used recurrent neural networks known for handling unbounded context. The proposed method features a simplified gating mechanism that outperforms previous models and demonstrates superior performance on the WikiText-103 benchmark, showing its capability to manage long-term dependencies. It also delivers competitive results on the Google Billion Words benchmark and significantly reduces sentence scoring latency compared to recurrent models. This marks the first instance of a non-recurrent model achieving comparable success to strong recurrent models on major language tasks."
    }
  ],
  "humanlike": [
    {
      "title": "Are Convolutional Neural Networks or Transformers more like human vision?",
      "url": "/research/2021/07/01/Are-Convolutional-Neural-Networks-or-Transformers-more-like-human-vision.html",
      "date": "July 1, 2021",
      "summary": "This study investigates the performance of Convolutional Neural Networks (CNNs) and attention-based Vision Transformers (ViTs) in computer vision tasks. While CNNs excel in accuracy, ViTs offer a different approach with weaker inductive biases. By analyzing error patterns, we find that ViTs exhibit consistency with human errors, suggesting potential for more human-like vision models and insights into human object recognition."
    }
  ],
  "all": [
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    }
  ],
  "recurrence": [
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    }
  ],
  "translation": [
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    },
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    },
    {
      "title": "DeepNet: Scaling Transformers to 1,000 Layers",
      "url": "/research/2022/03/01/DeepNet-Scaling-Transformers-to-1,000-Layers.html",
      "date": "March 1, 2022",
      "summary": "This paper introduces DEEPNORM, a novel method to stabilize extremely deep Transformers through a new normalization function and a theoretically derived initialization for the residual connection. DEEPNORM offers a stable and efficient training regime, blending the benefits of Post-LN's performance with Pre-LN's stability, allowing for the scaling of Transformers up to 1,000 layers without difficulty. This represents a significant advancement over previous models, with a 200-layer, 3.2B parameter model outperforming a 48-layer, 12B parameter state-of-the-art model by 5 BLEU points on a multilingual benchmark with 7,482 translation directions, suggesting a promising direction for scaling."
    }
  ],
  "parallelization": [
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    }
  ],
  "bleu": [
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    },
    {
      "title": "DeepNet: Scaling Transformers to 1,000 Layers",
      "url": "/research/2022/03/01/DeepNet-Scaling-Transformers-to-1,000-Layers.html",
      "date": "March 1, 2022",
      "summary": "This paper introduces DEEPNORM, a novel method to stabilize extremely deep Transformers through a new normalization function and a theoretically derived initialization for the residual connection. DEEPNORM offers a stable and efficient training regime, blending the benefits of Post-LN's performance with Pre-LN's stability, allowing for the scaling of Transformers up to 1,000 layers without difficulty. This represents a significant advancement over previous models, with a 200-layer, 3.2B parameter model outperforming a 48-layer, 12B parameter state-of-the-art model by 5 BLEU points on a multilingual benchmark with 7,482 translation directions, suggesting a promising direction for scaling."
    }
  ],
  "wmt": [
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    }
  ],
  "german": [
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    }
  ],
  "french": [
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    }
  ],
  "gpus": [
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    },
    {
      "title": "FNet: Mixing Tokens with Fourier Transforms",
      "url": "/research/2022/05/26/FNet-Mixing-Tokens-with-Fourier-Transforms.html",
      "date": "May 26, 2022",
      "summary": "We demonstrate that Transformer encoder architectures can be accelerated with minimal impact on accuracy by substituting self-attention sublayers with simple linear transformations. Remarkably, using a standard Fourier Transform instead of the self-attention sublayer in a Transformer encoder achieves 92-97% of BERT models' accuracy on the GLUE benchmark, while training 80% faster on GPUs and 70% faster on TPUs for standard 512 input lengths. Our FNet model significantly outperforms in speed at longer input lengths, matching the accuracy of the most accurate models on the Long Range Arena benchmark and surpassing the fastest models in speed across all sequence lengths on GPUs and shorter lengths on TPUs. Additionally, FNet is lightweight in memory use and exceptionally efficient in smaller sizes, outperforming Transformer models under the same speed and accuracy constraints."
    }
  ],
  "constituency": [
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    }
  ],
  "parse": [
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    }
  ],
  "reformulate": [
    {
      "title": "Deep Residual Learning for Image Recognition",
      "url": "/research/2015/12/10/Deep-Residual-Learning-for-Image-Recognition.html",
      "date": "December 10, 2015",
      "summary": "We introduce a residual learning framework to train deeper neural networks effectively. By reformulating layers to learn residual functions relative to inputs, our approach facilitates optimization and achieves higher accuracy with increased depth. Evaluations on ImageNet demonstrate the effectiveness of residual networks up to 152 layers deep, outperforming previous architectures. Our method achieves 3.57% error on the ImageNet test set, winning 1st place in the ILSVRC 2015 classification task. Additionally, our deep representations lead to a 28% improvement on the COCO object detection dataset. In the ILSVRC & COCO 2015 competitions, our approach secured 1st place in ImageNet detection, localization, COCO detection, and segmentation tasks."
    }
  ],
  "functions": [
    {
      "title": "Deep Residual Learning for Image Recognition",
      "url": "/research/2015/12/10/Deep-Residual-Learning-for-Image-Recognition.html",
      "date": "December 10, 2015",
      "summary": "We introduce a residual learning framework to train deeper neural networks effectively. By reformulating layers to learn residual functions relative to inputs, our approach facilitates optimization and achieves higher accuracy with increased depth. Evaluations on ImageNet demonstrate the effectiveness of residual networks up to 152 layers deep, outperforming previous architectures. Our method achieves 3.57% error on the ImageNet test set, winning 1st place in the ILSVRC 2015 classification task. Additionally, our deep representations lead to a 28% improvement on the COCO object detection dataset. In the ILSVRC & COCO 2015 competitions, our approach secured 1st place in ImageNet detection, localization, COCO detection, and segmentation tasks."
    }
  ],
  "relative": [
    {
      "title": "Deep Residual Learning for Image Recognition",
      "url": "/research/2015/12/10/Deep-Residual-Learning-for-Image-Recognition.html",
      "date": "December 10, 2015",
      "summary": "We introduce a residual learning framework to train deeper neural networks effectively. By reformulating layers to learn residual functions relative to inputs, our approach facilitates optimization and achieves higher accuracy with increased depth. Evaluations on ImageNet demonstrate the effectiveness of residual networks up to 152 layers deep, outperforming previous architectures. Our method achieves 3.57% error on the ImageNet test set, winning 1st place in the ILSVRC 2015 classification task. Additionally, our deep representations lead to a 28% improvement on the COCO object detection dataset. In the ILSVRC & COCO 2015 competitions, our approach secured 1st place in ImageNet detection, localization, COCO detection, and segmentation tasks."
    }
  ],
  "facilitates": [
    {
      "title": "Deep Residual Learning for Image Recognition",
      "url": "/research/2015/12/10/Deep-Residual-Learning-for-Image-Recognition.html",
      "date": "December 10, 2015",
      "summary": "We introduce a residual learning framework to train deeper neural networks effectively. By reformulating layers to learn residual functions relative to inputs, our approach facilitates optimization and achieves higher accuracy with increased depth. Evaluations on ImageNet demonstrate the effectiveness of residual networks up to 152 layers deep, outperforming previous architectures. Our method achieves 3.57% error on the ImageNet test set, winning 1st place in the ILSVRC 2015 classification task. Additionally, our deep representations lead to a 28% improvement on the COCO object detection dataset. In the ILSVRC & COCO 2015 competitions, our approach secured 1st place in ImageNet detection, localization, COCO detection, and segmentation tasks."
    },
    {
      "title": "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization",
      "url": "/research/2023/07/04/Gradient-Norm-Aware-Minimization-Seeks-First-Order-Flatness-and-Improves-Generalization.html",
      "date": "July 4, 2023",
      "summary": "Recent advancements have highlighted the effectiveness of flat minima in enhancing generalization, particularly through Sharpness-Aware Minimization (SAM). However, existing definitions of flatness, such as zeroth-order flatness, have limitations in discerning between minima with low and high generalization errors. To address this, we propose first-order flatness, which considers maximal gradient norm within a perturbation radius. We introduce Gradient norm Aware Minimization (GAM) as a novel training approach to achieve uniformly small curvature across all directions. Experimental results demonstrate GAM's ability to enhance generalization compared to standard optimizers like SGD and AdamW across various datasets and networks. Moreover, GAM facilitates SAM in identifying flatter minima, leading to improved generalization."
    },
    {
      "title": "VideoMamba: State Space Model for Efficient Video Understanding",
      "url": "/research/2024/03/12/VideoMamba-State-Space-Model-for-Efficient-Video-Understanding.html",
      "date": "March 12, 2024",
      "summary": "This work introduces VideoMamba, an innovative approach that addresses local redundancy and global dependencies in video understanding by adapting the Mamba framework to video analysis. VideoMamba outperforms existing 3D convolutional neural networks and video transformers through its linear-complexity operator, which facilitates efficient long-term modeling for high-resolution, long-duration videos. Its effectiveness is demonstrated across four main areas: scalability in the visual domain without needing extensive dataset pretraining, thanks to a novel self-distillation technique; the ability to recognize short-term actions with fine-grained motion differences; superior performance in long-term video understanding compared to traditional feature-based models; and robust compatibility with multiple modalities, enhancing multi-modal video analysis. VideoMamba establishes a new standard for comprehensive and efficient video understanding."
    }
  ],
  "win": [
    {
      "title": "Deep Residual Learning for Image Recognition",
      "url": "/research/2015/12/10/Deep-Residual-Learning-for-Image-Recognition.html",
      "date": "December 10, 2015",
      "summary": "We introduce a residual learning framework to train deeper neural networks effectively. By reformulating layers to learn residual functions relative to inputs, our approach facilitates optimization and achieves higher accuracy with increased depth. Evaluations on ImageNet demonstrate the effectiveness of residual networks up to 152 layers deep, outperforming previous architectures. Our method achieves 3.57% error on the ImageNet test set, winning 1st place in the ILSVRC 2015 classification task. Additionally, our deep representations lead to a 28% improvement on the COCO object detection dataset. In the ILSVRC & COCO 2015 competitions, our approach secured 1st place in ImageNet detection, localization, COCO detection, and segmentation tasks."
    }
  ],
  "ilsvrc": [
    {
      "title": "Deep Residual Learning for Image Recognition",
      "url": "/research/2015/12/10/Deep-Residual-Learning-for-Image-Recognition.html",
      "date": "December 10, 2015",
      "summary": "We introduce a residual learning framework to train deeper neural networks effectively. By reformulating layers to learn residual functions relative to inputs, our approach facilitates optimization and achieves higher accuracy with increased depth. Evaluations on ImageNet demonstrate the effectiveness of residual networks up to 152 layers deep, outperforming previous architectures. Our method achieves 3.57% error on the ImageNet test set, winning 1st place in the ILSVRC 2015 classification task. Additionally, our deep representations lead to a 28% improvement on the COCO object detection dataset. In the ILSVRC & COCO 2015 competitions, our approach secured 1st place in ImageNet detection, localization, COCO detection, and segmentation tasks."
    }
  ],
  "competition": [
    {
      "title": "Deep Residual Learning for Image Recognition",
      "url": "/research/2015/12/10/Deep-Residual-Learning-for-Image-Recognition.html",
      "date": "December 10, 2015",
      "summary": "We introduce a residual learning framework to train deeper neural networks effectively. By reformulating layers to learn residual functions relative to inputs, our approach facilitates optimization and achieves higher accuracy with increased depth. Evaluations on ImageNet demonstrate the effectiveness of residual networks up to 152 layers deep, outperforming previous architectures. Our method achieves 3.57% error on the ImageNet test set, winning 1st place in the ILSVRC 2015 classification task. Additionally, our deep representations lead to a 28% improvement on the COCO object detection dataset. In the ILSVRC & COCO 2015 competitions, our approach secured 1st place in ImageNet detection, localization, COCO detection, and segmentation tasks."
    }
  ],
  "secure": [
    {
      "title": "Deep Residual Learning for Image Recognition",
      "url": "/research/2015/12/10/Deep-Residual-Learning-for-Image-Recognition.html",
      "date": "December 10, 2015",
      "summary": "We introduce a residual learning framework to train deeper neural networks effectively. By reformulating layers to learn residual functions relative to inputs, our approach facilitates optimization and achieves higher accuracy with increased depth. Evaluations on ImageNet demonstrate the effectiveness of residual networks up to 152 layers deep, outperforming previous architectures. Our method achieves 3.57% error on the ImageNet test set, winning 1st place in the ILSVRC 2015 classification task. Additionally, our deep representations lead to a 28% improvement on the COCO object detection dataset. In the ILSVRC & COCO 2015 competitions, our approach secured 1st place in ImageNet detection, localization, COCO detection, and segmentation tasks."
    }
  ],
  "fail": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/2020/06/17/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "June 17, 2020",
      "summary": "Implicit neural representations, parameterized by continuous, differentiable neural networks, offer numerous advantages over conventional representations. However, existing architectures struggle with fine detail and fail to capture spatial and temporal derivatives, crucial for many physical signals described by partial differential equations. We introduce sinusoidal representation networks (SIRENs) utilizing periodic activation functions, ideal for representing complex signals and their derivatives. Analyzing SIREN activation statistics informs a principled initialization scheme. We demonstrate SIRENs' efficacy in representing various signals and solving boundary value problems, including Eikonal, Poisson, Helmholtz, and wave equations. Additionally, we integrate SIRENs with hypernetworks to learn priors over the SIREN function space. Visit the project website for detailed demonstrations."
    },
    {
      "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
      "url": "/research/2021/07/24/The-Many-Faces-of-Robustness-A-Critical-Analysis-of-Out-of-Distribution-Generalization.html",
      "date": "July 24, 2021",
      "summary": "We present four novel real-world distribution shift datasets encompassing changes in image style, blurriness, location, camera settings, and more. Evaluating existing methods for enhancing out-of-distribution robustness, we discover that employing larger models and artificial data augmentations can enhance robustness against real-world distribution shifts, contradicting prior claims. Our findings demonstrate that improvements in artificial robustness benchmarks can indeed transfer to real-world distribution shifts, contrary to prior assumptions. Additionally, we introduce a novel data augmentation technique that surpasses models pretrained with significantly more labeled data, emphasizing its efficacy in addressing real-world distribution shifts. While some methods consistently mitigate texture and local image statistics shifts, they fail to address other shifts like geographic changes. Our results underscore the necessity for future research to examine multiple distribution shifts concurrently, as no single method consistently improves robustness across all evaluated scenarios."
    }
  ],
  "sirens": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/2020/06/17/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "June 17, 2020",
      "summary": "Implicit neural representations, parameterized by continuous, differentiable neural networks, offer numerous advantages over conventional representations. However, existing architectures struggle with fine detail and fail to capture spatial and temporal derivatives, crucial for many physical signals described by partial differential equations. We introduce sinusoidal representation networks (SIRENs) utilizing periodic activation functions, ideal for representing complex signals and their derivatives. Analyzing SIREN activation statistics informs a principled initialization scheme. We demonstrate SIRENs' efficacy in representing various signals and solving boundary value problems, including Eikonal, Poisson, Helmholtz, and wave equations. Additionally, we integrate SIRENs with hypernetworks to learn priors over the SIREN function space. Visit the project website for detailed demonstrations."
    }
  ],
  "inform": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/2020/06/17/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "June 17, 2020",
      "summary": "Implicit neural representations, parameterized by continuous, differentiable neural networks, offer numerous advantages over conventional representations. However, existing architectures struggle with fine detail and fail to capture spatial and temporal derivatives, crucial for many physical signals described by partial differential equations. We introduce sinusoidal representation networks (SIRENs) utilizing periodic activation functions, ideal for representing complex signals and their derivatives. Analyzing SIREN activation statistics informs a principled initialization scheme. We demonstrate SIRENs' efficacy in representing various signals and solving boundary value problems, including Eikonal, Poisson, Helmholtz, and wave equations. Additionally, we integrate SIRENs with hypernetworks to learn priors over the SIREN function space. Visit the project website for detailed demonstrations."
    }
  ],
  "visit": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/2020/06/17/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "June 17, 2020",
      "summary": "Implicit neural representations, parameterized by continuous, differentiable neural networks, offer numerous advantages over conventional representations. However, existing architectures struggle with fine detail and fail to capture spatial and temporal derivatives, crucial for many physical signals described by partial differential equations. We introduce sinusoidal representation networks (SIRENs) utilizing periodic activation functions, ideal for representing complex signals and their derivatives. Analyzing SIREN activation statistics informs a principled initialization scheme. We demonstrate SIRENs' efficacy in representing various signals and solving boundary value problems, including Eikonal, Poisson, Helmholtz, and wave equations. Additionally, we integrate SIRENs with hypernetworks to learn priors over the SIREN function space. Visit the project website for detailed demonstrations."
    }
  ],
  "website": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/2020/06/17/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "June 17, 2020",
      "summary": "Implicit neural representations, parameterized by continuous, differentiable neural networks, offer numerous advantages over conventional representations. However, existing architectures struggle with fine detail and fail to capture spatial and temporal derivatives, crucial for many physical signals described by partial differential equations. We introduce sinusoidal representation networks (SIRENs) utilizing periodic activation functions, ideal for representing complex signals and their derivatives. Analyzing SIREN activation statistics informs a principled initialization scheme. We demonstrate SIRENs' efficacy in representing various signals and solving boundary value problems, including Eikonal, Poisson, Helmholtz, and wave equations. Additionally, we integrate SIRENs with hypernetworks to learn priors over the SIREN function space. Visit the project website for detailed demonstrations."
    }
  ],
  "demonstration": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/2020/06/17/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "June 17, 2020",
      "summary": "Implicit neural representations, parameterized by continuous, differentiable neural networks, offer numerous advantages over conventional representations. However, existing architectures struggle with fine detail and fail to capture spatial and temporal derivatives, crucial for many physical signals described by partial differential equations. We introduce sinusoidal representation networks (SIRENs) utilizing periodic activation functions, ideal for representing complex signals and their derivatives. Analyzing SIREN activation statistics informs a principled initialization scheme. We demonstrate SIRENs' efficacy in representing various signals and solving boundary value problems, including Eikonal, Poisson, Helmholtz, and wave equations. Additionally, we integrate SIRENs with hypernetworks to learn priors over the SIREN function space. Visit the project website for detailed demonstrations."
    }
  ],
  "cosine": [
    {
      "title": "Is Cosine-Similarity of Embeddings Really About Similarity?",
      "url": "/research/2024/03/08/Is-Cosine-Similarity-of-Embeddings-Really-About-Similarity.html",
      "date": "March 8, 2024",
      "summary": "Cosine similarity, often used to gauge semantic similarity between high-dimensional objects by comparing low-dimensional feature embeddings, can yield variable results compared to unnormalized dot products. We investigate this phenomenon by analyzing embeddings from regularized linear models, revealing that cosine similarity can produce arbitrary and even meaningless similarities. This applies not only to linear models but also to deep models due to the implicit effects of various regularizations. Consequently, we advise against relying solely on cosine similarity and suggest exploring alternative approaches."
    },
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    },
    {
      "title": "Eigencurve: Optimal Learning Rate Schedule For Sgd on Quadratic Objectives With Skewed Hessian Spectrums",
      "url": "/research/2022/06/14/Eigencurve-Optimal-Learning-Rate-Schedule-For-Sgd-on-Quadratic-Objectives-With-Skewed-Hessian-Spectrums.html",
      "date": "June 14, 2022",
      "summary": "Learning rate schedulers are essential in deep neural network training, but there's a gap between practical usage and theoretical understanding. This paper introduces Eigencurve, the first set of learning rate schedules achieving minimax optimal convergence rates for SGD on quadratic objectives with skewed eigenvalue distributions of the Hessian matrix. This condition is common in practice. Experimental results on CIFAR-10 image classification tasks demonstrate Eigencurve's superiority over step decay, particularly with fewer epochs. The theory inspires practical schedulers approximating Eigencurve, resembling cosine decay for some problems while outperforming it in others."
    },
    {
      "title": "Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks",
      "url": "/research/2017/10/23/Cosine-Normalization-Using-Cosine-Similarity-Instead-of-Dot-Product-in-Neural-Networks.html",
      "date": "October 23, 2017",
      "summary": "In traditional multi-layer neural networks, the dot product between the output and weight vectors of preceding layers serves as input to the activation function, resulting in unbounded outputs and increased variance. This variance can lead to poor generalization and hinder training by exacerbating internal covariate shift. To address this, we propose cosine normalization, which replaces the dot product with cosine similarity or centered cosine similarity (Pearson Correlation Coefficient). We evaluate cosine normalization against batch, weight, and layer normalization in fully-connected and convolutional neural networks across various datasets. Our experiments demonstrate that cosine normalization outperforms other normalization techniques."
    },
    {
      "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
      "url": "/research/2024/03/13/Simple-and-Scalable-Strategies-to-Continually-Pre-train-Large-Language-Models.html",
      "date": "March 13, 2024",
      "summary": "Large language models (LLMs) often require re-training on new data, consuming extensive computational resources. We present an efficient method that combines learning rate re-warming, re-decaying, and data replay, effectively maintaining performance without full re-training. This approach works well across different data distributions, including minor shifts (English→English) and significant shifts (English→German), tested up to 405M and 10B parameter models. Our findings suggest that continual learning strategies can update LLMs with minimal computational cost, rivaling traditional re-training methods. Additionally, we propose alternatives to the cosine learning rate schedule to reduce forgetting, offering more flexibility in learning without a fixed token budget."
    }
  ],
  "gauge": [
    {
      "title": "Is Cosine-Similarity of Embeddings Really About Similarity?",
      "url": "/research/2024/03/08/Is-Cosine-Similarity-of-Embeddings-Really-About-Similarity.html",
      "date": "March 8, 2024",
      "summary": "Cosine similarity, often used to gauge semantic similarity between high-dimensional objects by comparing low-dimensional feature embeddings, can yield variable results compared to unnormalized dot products. We investigate this phenomenon by analyzing embeddings from regularized linear models, revealing that cosine similarity can produce arbitrary and even meaningless similarities. This applies not only to linear models but also to deep models due to the implicit effects of various regularizations. Consequently, we advise against relying solely on cosine similarity and suggest exploring alternative approaches."
    }
  ],
  "unnormalize": [
    {
      "title": "Is Cosine-Similarity of Embeddings Really About Similarity?",
      "url": "/research/2024/03/08/Is-Cosine-Similarity-of-Embeddings-Really-About-Similarity.html",
      "date": "March 8, 2024",
      "summary": "Cosine similarity, often used to gauge semantic similarity between high-dimensional objects by comparing low-dimensional feature embeddings, can yield variable results compared to unnormalized dot products. We investigate this phenomenon by analyzing embeddings from regularized linear models, revealing that cosine similarity can produce arbitrary and even meaningless similarities. This applies not only to linear models but also to deep models due to the implicit effects of various regularizations. Consequently, we advise against relying solely on cosine similarity and suggest exploring alternative approaches."
    },
    {
      "title": "Model Generalization: A Sharpness Aware Optimization Perspective",
      "url": "/research/2022/08/14/Model-Generalization-A-Sharpness-Aware-Optimization-Perspective.html",
      "date": "August 14, 2022",
      "summary": "This study investigates the effectiveness of Sharpness-Aware Minimization (SAM) and adaptive Sharpness-Aware Minimization (ASAM) in enhancing model generalization. Through three experiments, we assess their impact from a sharpness-aware perspective. Results demonstrate that optimization techniques based on sharpness awareness can bolster model generalization. Furthermore, ASAM exhibits potential for enhancing generalization performance on un-normalized data, though additional research is required for validation."
    }
  ],
  "dot": [
    {
      "title": "Is Cosine-Similarity of Embeddings Really About Similarity?",
      "url": "/research/2024/03/08/Is-Cosine-Similarity-of-Embeddings-Really-About-Similarity.html",
      "date": "March 8, 2024",
      "summary": "Cosine similarity, often used to gauge semantic similarity between high-dimensional objects by comparing low-dimensional feature embeddings, can yield variable results compared to unnormalized dot products. We investigate this phenomenon by analyzing embeddings from regularized linear models, revealing that cosine similarity can produce arbitrary and even meaningless similarities. This applies not only to linear models but also to deep models due to the implicit effects of various regularizations. Consequently, we advise against relying solely on cosine similarity and suggest exploring alternative approaches."
    },
    {
      "title": "Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks",
      "url": "/research/2017/10/23/Cosine-Normalization-Using-Cosine-Similarity-Instead-of-Dot-Product-in-Neural-Networks.html",
      "date": "October 23, 2017",
      "summary": "In traditional multi-layer neural networks, the dot product between the output and weight vectors of preceding layers serves as input to the activation function, resulting in unbounded outputs and increased variance. This variance can lead to poor generalization and hinder training by exacerbating internal covariate shift. To address this, we propose cosine normalization, which replaces the dot product with cosine similarity or centered cosine similarity (Pearson Correlation Coefficient). We evaluate cosine normalization against batch, weight, and layer normalization in fully-connected and convolutional neural networks across various datasets. Our experiments demonstrate that cosine normalization outperforms other normalization techniques."
    },
    {
      "title": "A Phase Transition between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention",
      "url": "/research/2024/02/06/A-Phase-Transition-between-Positional-and-Semantic-Learning-in-a-Solvable-Model-of-Dot-Product-Attention.html",
      "date": "February 6, 2024",
      "summary": "We explore the learning process of a dot-product attention layer, which learns both positional and semantic attention matrices, enabling tokens to attend based on position or meaning. Through experiments on an algorithmic task, we demonstrate that this architecture can use either mechanism for solving the task. Theoretically, we examine a non-linear self-attention layer with special query and key matrices, offering a closed-form solution for its non-convex loss landscape in high-dimensional data, which reveals a phase transition from positional to semantic mechanisms as sample complexity increases. We also show that the dot-product attention layer surpasses a linear positional baseline through the semantic mechanism with adequate data."
    }
  ],
  "advise": [
    {
      "title": "Is Cosine-Similarity of Embeddings Really About Similarity?",
      "url": "/research/2024/03/08/Is-Cosine-Similarity-of-Embeddings-Really-About-Similarity.html",
      "date": "March 8, 2024",
      "summary": "Cosine similarity, often used to gauge semantic similarity between high-dimensional objects by comparing low-dimensional feature embeddings, can yield variable results compared to unnormalized dot products. We investigate this phenomenon by analyzing embeddings from regularized linear models, revealing that cosine similarity can produce arbitrary and even meaningless similarities. This applies not only to linear models but also to deep models due to the implicit effects of various regularizations. Consequently, we advise against relying solely on cosine similarity and suggest exploring alternative approaches."
    }
  ],
  "relying": [
    {
      "title": "Is Cosine-Similarity of Embeddings Really About Similarity?",
      "url": "/research/2024/03/08/Is-Cosine-Similarity-of-Embeddings-Really-About-Similarity.html",
      "date": "March 8, 2024",
      "summary": "Cosine similarity, often used to gauge semantic similarity between high-dimensional objects by comparing low-dimensional feature embeddings, can yield variable results compared to unnormalized dot products. We investigate this phenomenon by analyzing embeddings from regularized linear models, revealing that cosine similarity can produce arbitrary and even meaningless similarities. This applies not only to linear models but also to deep models due to the implicit effects of various regularizations. Consequently, we advise against relying solely on cosine similarity and suggest exploring alternative approaches."
    }
  ],
  "multitask": [
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    }
  ],
  "analyzing": [
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    }
  ],
  "propensity": [
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    }
  ],
  "stl": [
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    }
  ],
  "oxford": [
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    }
  ],
  "caltech": [
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    }
  ],
  "widely": [
    {
      "title": "Searching for Activation Functions",
      "url": "/research/2017/10/27/Searching-for-Activation-Functions.html",
      "date": "October 27, 2017",
      "summary": "This study explores the impact of activation functions on deep network training and performance. While Rectified Linear Unit (ReLU) is widely used, alternatives have not consistently outperformed it. We propose using automatic search techniques to discover new activation functions. Through exhaustive and reinforcement learning-based searches, we identify novel functions. Empirical evaluation shows that our best discovered function, Swish (f(x) = x · sigmoid(βx)), performs better than ReLU on deeper models across challenging datasets. Replacing ReLUs with Swish units improves classification accuracy on ImageNet, for example, by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. Swish's simplicity and similarity to ReLU facilitate its adoption in neural networks."
    }
  ],
  "fx": [
    {
      "title": "Searching for Activation Functions",
      "url": "/research/2017/10/27/Searching-for-Activation-Functions.html",
      "date": "October 27, 2017",
      "summary": "This study explores the impact of activation functions on deep network training and performance. While Rectified Linear Unit (ReLU) is widely used, alternatives have not consistently outperformed it. We propose using automatic search techniques to discover new activation functions. Through exhaustive and reinforcement learning-based searches, we identify novel functions. Empirical evaluation shows that our best discovered function, Swish (f(x) = x · sigmoid(βx)), performs better than ReLU on deeper models across challenging datasets. Replacing ReLUs with Swish units improves classification accuracy on ImageNet, for example, by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. Swish's simplicity and similarity to ReLU facilitate its adoption in neural networks."
    }
  ],
  "sigmoidbx": [
    {
      "title": "Searching for Activation Functions",
      "url": "/research/2017/10/27/Searching-for-Activation-Functions.html",
      "date": "October 27, 2017",
      "summary": "This study explores the impact of activation functions on deep network training and performance. While Rectified Linear Unit (ReLU) is widely used, alternatives have not consistently outperformed it. We propose using automatic search techniques to discover new activation functions. Through exhaustive and reinforcement learning-based searches, we identify novel functions. Empirical evaluation shows that our best discovered function, Swish (f(x) = x · sigmoid(βx)), performs better than ReLU on deeper models across challenging datasets. Replacing ReLUs with Swish units improves classification accuracy on ImageNet, for example, by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. Swish's simplicity and similarity to ReLU facilitate its adoption in neural networks."
    }
  ],
  "relus": [
    {
      "title": "Searching for Activation Functions",
      "url": "/research/2017/10/27/Searching-for-Activation-Functions.html",
      "date": "October 27, 2017",
      "summary": "This study explores the impact of activation functions on deep network training and performance. While Rectified Linear Unit (ReLU) is widely used, alternatives have not consistently outperformed it. We propose using automatic search techniques to discover new activation functions. Through exhaustive and reinforcement learning-based searches, we identify novel functions. Empirical evaluation shows that our best discovered function, Swish (f(x) = x · sigmoid(βx)), performs better than ReLU on deeper models across challenging datasets. Replacing ReLUs with Swish units improves classification accuracy on ImageNet, for example, by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. Swish's simplicity and similarity to ReLU facilitate its adoption in neural networks."
    }
  ],
  "nasnet": [
    {
      "title": "Searching for Activation Functions",
      "url": "/research/2017/10/27/Searching-for-Activation-Functions.html",
      "date": "October 27, 2017",
      "summary": "This study explores the impact of activation functions on deep network training and performance. While Rectified Linear Unit (ReLU) is widely used, alternatives have not consistently outperformed it. We propose using automatic search techniques to discover new activation functions. Through exhaustive and reinforcement learning-based searches, we identify novel functions. Empirical evaluation shows that our best discovered function, Swish (f(x) = x · sigmoid(βx)), performs better than ReLU on deeper models across challenging datasets. Replacing ReLUs with Swish units improves classification accuracy on ImageNet, for example, by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. Swish's simplicity and similarity to ReLU facilitate its adoption in neural networks."
    }
  ],
  "inception": [
    {
      "title": "Searching for Activation Functions",
      "url": "/research/2017/10/27/Searching-for-Activation-Functions.html",
      "date": "October 27, 2017",
      "summary": "This study explores the impact of activation functions on deep network training and performance. While Rectified Linear Unit (ReLU) is widely used, alternatives have not consistently outperformed it. We propose using automatic search techniques to discover new activation functions. Through exhaustive and reinforcement learning-based searches, we identify novel functions. Empirical evaluation shows that our best discovered function, Swish (f(x) = x · sigmoid(βx)), performs better than ReLU on deeper models across challenging datasets. Replacing ReLUs with Swish units improves classification accuracy on ImageNet, for example, by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. Swish's simplicity and similarity to ReLU facilitate its adoption in neural networks."
    }
  ],
  "simplicity": [
    {
      "title": "Searching for Activation Functions",
      "url": "/research/2017/10/27/Searching-for-Activation-Functions.html",
      "date": "October 27, 2017",
      "summary": "This study explores the impact of activation functions on deep network training and performance. While Rectified Linear Unit (ReLU) is widely used, alternatives have not consistently outperformed it. We propose using automatic search techniques to discover new activation functions. Through exhaustive and reinforcement learning-based searches, we identify novel functions. Empirical evaluation shows that our best discovered function, Swish (f(x) = x · sigmoid(βx)), performs better than ReLU on deeper models across challenging datasets. Replacing ReLUs with Swish units improves classification accuracy on ImageNet, for example, by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. Swish's simplicity and similarity to ReLU facilitate its adoption in neural networks."
    },
    {
      "title": "Neural Networks Learn Statistics of Increasing Complexity",
      "url": "/research/2024/02/13/Neural-Networks-Learn-Statistics-of-Increasing-Complexity.html",
      "date": "February 13, 2024",
      "summary": "The distributional simplicity bias (DSB) theory suggests neural networks first learn basic patterns in data before understanding more complex correlations. We provide new evidence supporting DSB, showing networks initially excel with data matching training set's simple statistics but this ability diminishes later. Extending DSB to discrete domains, we demonstrate an equivalence between n-gram frequencies and vector moments, also observing this bias in large language models (LLMs). Additionally, by adjusting low-level statistics of images to resemble another class, we reveal that networks in early training phases misclassify these edited images as if they belonged to the target class."
    }
  ],
  "hunger": [
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    }
  ],
  "position": [
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    },
    {
      "title": "A Phase Transition between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention",
      "url": "/research/2024/02/06/A-Phase-Transition-between-Positional-and-Semantic-Learning-in-a-Solvable-Model-of-Dot-Product-Attention.html",
      "date": "February 6, 2024",
      "summary": "We explore the learning process of a dot-product attention layer, which learns both positional and semantic attention matrices, enabling tokens to attend based on position or meaning. Through experiments on an algorithmic task, we demonstrate that this architecture can use either mechanism for solving the task. Theoretically, we examine a non-linear self-attention layer with special query and key matrices, offering a closed-form solution for its non-convex loss landscape in high-dimensional data, which reveals a phase transition from positional to semantic mechanisms as sample complexity increases. We also show that the dot-product attention layer surpasses a linear positional baseline through the semantic mechanism with adequate data."
    }
  ],
  "simmim": [
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    }
  ],
  "times": [
    {
      "title": "Swin Transformer V2 Scaling Up Capacity and Resolution",
      "url": "/research/2022/04/11/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution.html",
      "date": "April 11, 2022",
      "summary": "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
    }
  ],
  "re": [
    {
      "title": "Understanding Deep Learning Requires Re- Thinking Generalization",
      "url": "/research/2017/02/26/Understanding-Deep-Learning-Requires-Re--Thinking-Generalization.html",
      "date": "February 26, 2017",
      "summary": "Extensive experiments reveal that traditional explanations for the strong generalization performance of large neural networks fall short. Despite conventional wisdom attributing success to model properties or regularization techniques, our findings show that state-of-the-art convolutional networks for image classification can easily overfit random labels. This phenomenon persists regardless of explicit regularization or substituting true images with random noise. Theoretical analysis suggests that shallow neural networks achieve perfect finite sample expressivity when parameters outnumber data points, as commonly observed. We interpret these results in contrast to traditional models."
    }
  ],
  "thinking": [
    {
      "title": "Understanding Deep Learning Requires Re- Thinking Generalization",
      "url": "/research/2017/02/26/Understanding-Deep-Learning-Requires-Re--Thinking-Generalization.html",
      "date": "February 26, 2017",
      "summary": "Extensive experiments reveal that traditional explanations for the strong generalization performance of large neural networks fall short. Despite conventional wisdom attributing success to model properties or regularization techniques, our findings show that state-of-the-art convolutional networks for image classification can easily overfit random labels. This phenomenon persists regardless of explicit regularization or substituting true images with random noise. Theoretical analysis suggests that shallow neural networks achieve perfect finite sample expressivity when parameters outnumber data points, as commonly observed. We interpret these results in contrast to traditional models."
    }
  ],
  "wisdom": [
    {
      "title": "Understanding Deep Learning Requires Re- Thinking Generalization",
      "url": "/research/2017/02/26/Understanding-Deep-Learning-Requires-Re--Thinking-Generalization.html",
      "date": "February 26, 2017",
      "summary": "Extensive experiments reveal that traditional explanations for the strong generalization performance of large neural networks fall short. Despite conventional wisdom attributing success to model properties or regularization techniques, our findings show that state-of-the-art convolutional networks for image classification can easily overfit random labels. This phenomenon persists regardless of explicit regularization or substituting true images with random noise. Theoretical analysis suggests that shallow neural networks achieve perfect finite sample expressivity when parameters outnumber data points, as commonly observed. We interpret these results in contrast to traditional models."
    }
  ],
  "expressivity": [
    {
      "title": "Understanding Deep Learning Requires Re- Thinking Generalization",
      "url": "/research/2017/02/26/Understanding-Deep-Learning-Requires-Re--Thinking-Generalization.html",
      "date": "February 26, 2017",
      "summary": "Extensive experiments reveal that traditional explanations for the strong generalization performance of large neural networks fall short. Despite conventional wisdom attributing success to model properties or regularization techniques, our findings show that state-of-the-art convolutional networks for image classification can easily overfit random labels. This phenomenon persists regardless of explicit regularization or substituting true images with random noise. Theoretical analysis suggests that shallow neural networks achieve perfect finite sample expressivity when parameters outnumber data points, as commonly observed. We interpret these results in contrast to traditional models."
    }
  ],
  "outnumber": [
    {
      "title": "Understanding Deep Learning Requires Re- Thinking Generalization",
      "url": "/research/2017/02/26/Understanding-Deep-Learning-Requires-Re--Thinking-Generalization.html",
      "date": "February 26, 2017",
      "summary": "Extensive experiments reveal that traditional explanations for the strong generalization performance of large neural networks fall short. Despite conventional wisdom attributing success to model properties or regularization techniques, our findings show that state-of-the-art convolutional networks for image classification can easily overfit random labels. This phenomenon persists regardless of explicit regularization or substituting true images with random noise. Theoretical analysis suggests that shallow neural networks achieve perfect finite sample expressivity when parameters outnumber data points, as commonly observed. We interpret these results in contrast to traditional models."
    }
  ],
  "layer normalization": [
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    },
    {
      "title": "Root Mean Square Layer Normalization",
      "url": "/research/2019/10/16/Root-Mean-Square-Layer-Normalization.html",
      "date": "October 16, 2019",
      "summary": "Layer normalization (LayerNorm) enhances deep neural network stability and convergence by re-centering and re-scaling inputs and weight matrices. However, its computational overhead slows networks, particularly RNNs. We introduce RMSNorm, which replaces re-centering with root mean square (RMS) regularization. RMSNorm maintains re-scaling invariance and adapts learning rates implicitly, while being computationally simpler than LayerNorm. We also propose partial RMSNorm (pRMSNorm), estimating RMS from a subset of inputs. Empirical results across various tasks and architectures demonstrate that RMSNorm achieves comparable performance to LayerNorm while reducing running time by 7%∼64%."
    }
  ],
  "smoothness": [
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    }
  ],
  "recenter": [
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    },
    {
      "title": "Root Mean Square Layer Normalization",
      "url": "/research/2019/10/16/Root-Mean-Square-Layer-Normalization.html",
      "date": "October 16, 2019",
      "summary": "Layer normalization (LayerNorm) enhances deep neural network stability and convergence by re-centering and re-scaling inputs and weight matrices. However, its computational overhead slows networks, particularly RNNs. We introduce RMSNorm, which replaces re-centering with root mean square (RMS) regularization. RMSNorm maintains re-scaling invariance and adapts learning rates implicitly, while being computationally simpler than LayerNorm. We also propose partial RMSNorm (pRMSNorm), estimating RMS from a subset of inputs. Empirical results across various tasks and architectures demonstrate that RMSNorm achieves comparable performance to LayerNorm while reducing running time by 7%∼64%."
    }
  ],
  "play": [
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    },
    {
      "title": "Atomically Accurate De Novo Design of Single-domain Antibodies",
      "url": "/research/2024/03/18/Atomically-Accurate-De-Novo-Design-of-Single-domain-Antibodies.html",
      "date": "March 18, 2024",
      "summary": "Despite the critical role antibodies play in medicine, current methods for designing new antibodies targeting specific epitopes are time-consuming. Here, we showcase the effectiveness of a refined RFdiffusion network in designing novel antibody variable heavy chains (VHH’s) to target user-specified epitopes. Through experiments, we validate the binding capability of these designed VHH's to four disease-relevant epitopes. Furthermore, the cryo-EM structure analysis reveals that a designed VHH bound to influenza hemagglutinin closely matches the design model, affirming the accuracy of our approach."
    }
  ],
  "exacerbate": [
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    },
    {
      "title": "Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks",
      "url": "/research/2017/10/23/Cosine-Normalization-Using-Cosine-Similarity-Instead-of-Dot-Product-in-Neural-Networks.html",
      "date": "October 23, 2017",
      "summary": "In traditional multi-layer neural networks, the dot product between the output and weight vectors of preceding layers serves as input to the activation function, resulting in unbounded outputs and increased variance. This variance can lead to poor generalization and hinder training by exacerbating internal covariate shift. To address this, we propose cosine normalization, which replaces the dot product with cosine similarity or centered cosine similarity (Pearson Correlation Coefficient). We evaluate cosine normalization against batch, weight, and layer normalization in fully-connected and convolutional neural networks across various datasets. Our experiments demonstrate that cosine normalization outperforms other normalization techniques."
    }
  ],
  "layernormsimple": [
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    }
  ],
  "en": [
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    }
  ],
  "vi": [
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    }
  ],
  "adanorm": [
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    }
  ],
  "majority": [
    {
      "title": "Understanding and Improving Layer Normalization",
      "url": "/research/2019/12/08/Understanding-and-Improving-Layer-Normalization.html",
      "date": "December 8, 2019",
      "summary": "Layer normalization (LayerNorm) enhances gradient smoothness, accelerates training, and improves generalization accuracy. While its effectiveness has been attributed to forward normalization in previous studies, our research reveals that re-centering and re-scaling backward gradients through derivatives of mean and variance play a crucial role. Moreover, we find that parameters like bias and gain in LayerNorm exacerbate overfitting and are often ineffective. Experiments demonstrate that a simplified version of LayerNorm (LayerNorm-simple) without bias and gain outperforms traditional LayerNorm on multiple datasets, achieving state-of-the-art results in En-Vi machine translation. To mitigate overfitting, we introduce Adaptive Normalization (AdaNorm), which replaces bias and gain with a new transformation function. Experimental results indicate that AdaNorm outperforms LayerNorm on the majority of datasets, suggesting its efficacy in addressing overfitting concerns."
    }
  ],
  "visualize": [
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    }
  ],
  "quest": [
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    }
  ],
  "minimizer": [
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    },
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    },
    {
      "title": "The loss landscape of deep linear neural networks: a second-order analysis",
      "url": "/research/2022/03/11/The-loss-landscape-of-deep-linear-neural-networks-a-second-order-analysis.html",
      "date": "March 11, 2022",
      "summary": "This study investigates the optimization landscape of deep linear neural networks with square loss, focusing on the role of non-strict saddle points in algorithm dynamics, which has been previously understudied. the authors conduct a comprehensive second-order analysis, identifying global minimizers, strict, and non-strict saddle points among all critical points, alongside their critical values. Our findings, based on conditions related to the ranks of partial matrix products, contribute insights into global convergence and implicit regularization observed in optimizing linear neural networks. Additionally, the authors offer an explicit parameterization of global minimizers and identify extensive sets of strict and non-strict saddle points."
    }
  ],
  "curvature": [
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "Deep learning via Hessian-free optimization",
      "url": "/research/2010/06/21/Deep-learning-via-Hessian-free-optimization.html",
      "date": "June 21, 2010",
      "summary": "We present a novel 2nd-order optimization method inspired by the Hessian-free approach, applied to deep auto-encoders. Achieving superior results without pre-training compared to Hinton & Salakhutdinov (2006), our method is practical, user-friendly, scalable to large datasets, and versatile across various model classes. Additionally, we address the challenge of pathological curvature in deep learning, highlighting how our method effectively mitigates this issue."
    },
    {
      "title": "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization",
      "url": "/research/2023/07/04/Gradient-Norm-Aware-Minimization-Seeks-First-Order-Flatness-and-Improves-Generalization.html",
      "date": "July 4, 2023",
      "summary": "Recent advancements have highlighted the effectiveness of flat minima in enhancing generalization, particularly through Sharpness-Aware Minimization (SAM). However, existing definitions of flatness, such as zeroth-order flatness, have limitations in discerning between minima with low and high generalization errors. To address this, we propose first-order flatness, which considers maximal gradient norm within a perturbation radius. We introduce Gradient norm Aware Minimization (GAM) as a novel training approach to achieve uniformly small curvature across all directions. Experimental results demonstrate GAM's ability to enhance generalization compared to standard optimizers like SGD and AdamW across various datasets and networks. Moreover, GAM facilitates SAM in identifying flatter minima, leading to improved generalization."
    }
  ],
  "dl": [
    {
      "title": "A comprehensive review of Binary Neural Network",
      "url": "/research/2023/05/11/A-comprehensive-review-of-Binary-Neural-Network.html",
      "date": "May 11, 2023",
      "summary": "Deep learning (DL) is widely used in intelligent systems and real-life applications, but its deployment on computationally limited and energy-constrained devices requires efficient technologies like Binary Neural Networks (BNN). BNNs save significant storage, computation cost, and energy, making them ideal for small devices, despite the trade-offs in memory and performance. This article offers a comprehensive review of BNN developments, focusing on 1-bit activations and weights. It covers the evolution of BNN, from early models to advanced algorithms and design aspects, and addresses BNN optimization, deployment, computing architectures, and diverse applications. It also outlines potential research directions in BNN technology."
    }
  ],
  "life": [
    {
      "title": "A comprehensive review of Binary Neural Network",
      "url": "/research/2023/05/11/A-comprehensive-review-of-Binary-Neural-Network.html",
      "date": "May 11, 2023",
      "summary": "Deep learning (DL) is widely used in intelligent systems and real-life applications, but its deployment on computationally limited and energy-constrained devices requires efficient technologies like Binary Neural Networks (BNN). BNNs save significant storage, computation cost, and energy, making them ideal for small devices, despite the trade-offs in memory and performance. This article offers a comprehensive review of BNN developments, focusing on 1-bit activations and weights. It covers the evolution of BNN, from early models to advanced algorithms and design aspects, and addresses BNN optimization, deployment, computing architectures, and diverse applications. It also outlines potential research directions in BNN technology."
    }
  ],
  "technology": [
    {
      "title": "A comprehensive review of Binary Neural Network",
      "url": "/research/2023/05/11/A-comprehensive-review-of-Binary-Neural-Network.html",
      "date": "May 11, 2023",
      "summary": "Deep learning (DL) is widely used in intelligent systems and real-life applications, but its deployment on computationally limited and energy-constrained devices requires efficient technologies like Binary Neural Networks (BNN). BNNs save significant storage, computation cost, and energy, making them ideal for small devices, despite the trade-offs in memory and performance. This article offers a comprehensive review of BNN developments, focusing on 1-bit activations and weights. It covers the evolution of BNN, from early models to advanced algorithms and design aspects, and addresses BNN optimization, deployment, computing architectures, and diverse applications. It also outlines potential research directions in BNN technology."
    },
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "bnn": [
    {
      "title": "A comprehensive review of Binary Neural Network",
      "url": "/research/2023/05/11/A-comprehensive-review-of-Binary-Neural-Network.html",
      "date": "May 11, 2023",
      "summary": "Deep learning (DL) is widely used in intelligent systems and real-life applications, but its deployment on computationally limited and energy-constrained devices requires efficient technologies like Binary Neural Networks (BNN). BNNs save significant storage, computation cost, and energy, making them ideal for small devices, despite the trade-offs in memory and performance. This article offers a comprehensive review of BNN developments, focusing on 1-bit activations and weights. It covers the evolution of BNN, from early models to advanced algorithms and design aspects, and addresses BNN optimization, deployment, computing architectures, and diverse applications. It also outlines potential research directions in BNN technology."
    }
  ],
  "storage": [
    {
      "title": "A comprehensive review of Binary Neural Network",
      "url": "/research/2023/05/11/A-comprehensive-review-of-Binary-Neural-Network.html",
      "date": "May 11, 2023",
      "summary": "Deep learning (DL) is widely used in intelligent systems and real-life applications, but its deployment on computationally limited and energy-constrained devices requires efficient technologies like Binary Neural Networks (BNN). BNNs save significant storage, computation cost, and energy, making them ideal for small devices, despite the trade-offs in memory and performance. This article offers a comprehensive review of BNN developments, focusing on 1-bit activations and weights. It covers the evolution of BNN, from early models to advanced algorithms and design aspects, and addresses BNN optimization, deployment, computing architectures, and diverse applications. It also outlines potential research directions in BNN technology."
    }
  ],
  "trade": [
    {
      "title": "A comprehensive review of Binary Neural Network",
      "url": "/research/2023/05/11/A-comprehensive-review-of-Binary-Neural-Network.html",
      "date": "May 11, 2023",
      "summary": "Deep learning (DL) is widely used in intelligent systems and real-life applications, but its deployment on computationally limited and energy-constrained devices requires efficient technologies like Binary Neural Networks (BNN). BNNs save significant storage, computation cost, and energy, making them ideal for small devices, despite the trade-offs in memory and performance. This article offers a comprehensive review of BNN developments, focusing on 1-bit activations and weights. It covers the evolution of BNN, from early models to advanced algorithms and design aspects, and addresses BNN optimization, deployment, computing architectures, and diverse applications. It also outlines potential research directions in BNN technology."
    }
  ],
  "adahessian": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    }
  ],
  "hessian": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "Eigencurve: Optimal Learning Rate Schedule For Sgd on Quadratic Objectives With Skewed Hessian Spectrums",
      "url": "/research/2022/06/14/Eigencurve-Optimal-Learning-Rate-Schedule-For-Sgd-on-Quadratic-Objectives-With-Skewed-Hessian-Spectrums.html",
      "date": "June 14, 2022",
      "summary": "Learning rate schedulers are essential in deep neural network training, but there's a gap between practical usage and theoretical understanding. This paper introduces Eigencurve, the first set of learning rate schedules achieving minimax optimal convergence rates for SGD on quadratic objectives with skewed eigenvalue distributions of the Hessian matrix. This condition is common in practice. Experimental results on CIFAR-10 image classification tasks demonstrate Eigencurve's superiority over step decay, particularly with fewer epochs. The theory inspires practical schedulers approximating Eigencurve, resembling cosine decay for some problems while outperforming it in others."
    },
    {
      "title": "On the Maximum Hessian Eigenvalue and Generalization",
      "url": "/research/2023/05/23/On-the-Maximum-Hessian-Eigenvalue-and-Generalization.html",
      "date": "May 23, 2023",
      "summary": "This study investigates the relationship between training interventions and the generalization of deep networks. While previous research suggests that flatter solutions generalize better than sharper ones, particularly measured by λmax, the largest eigenvalue of the Hessian of the loss, this paper challenges this notion. Through experiments, we demonstrate that larger learning rates reduce λmax for all batch sizes but do not consistently improve generalization. Additionally, scaling batch size and learning rate simultaneously can change λmax without affecting generalization. Sharpness-Aware Minimization (SAM) produces smaller λmax but does not consistently enhance generalization, especially with larger batch sizes. Excessive dropout probabilities can degrade generalization, despite promoting smaller λmax. Batch normalization, while not consistently reducing λmax, still improves generalization. These findings question λmax's role in explaining generalization in neural networks, highlighting the limits of its explanatory power."
    },
    {
      "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
      "url": "/research/2022/03/19/Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training.html",
      "date": "March 19, 2022",
      "summary": "Sharpness-Aware Minimization (SAM) enhances generalization by optimizing a neighborhood-based perturbed loss, but it doesn't always favor flat minima due to both sharp and flat minima having low perturbed losses. We introduce a new measure, the surrogate gap, reflecting the dominant Hessian eigenvalue at small neighborhood radii, which is simple to compute and can be minimized during training. We propose the Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), an advancement over SAM with minimal additional computational cost. GSAM employs a dual-step approach: first, minimizing the perturbed loss similar to SAM, and second, reducing the surrogate gap without affecting the perturbed loss to target regions with low loss and sharpness, thus achieving superior generalization. GSAM is theoretically robust, showing better convergence and empirical generalization improvements, notably a +3.2% gain over SAM and +5.4% over AdamW in ImageNet accuracy for ViT-B/32."
    },
    {
      "title": "Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes",
      "url": "/research/2017/11/28/Towards-Understanding-Generalization-of-Deep-Learning-Perspective-of-Loss-Landscapes.html",
      "date": "November 28, 2017",
      "summary": "This study examines why deep learning models, despite having more parameters than training samples, generalize well. It highlights the importance of the loss function's landscape, showing that areas leading to better generalization (good minima) are more prevalent than those leading to poor outcomes. This predominance facilitates the convergence of optimization methods to these beneficial minima. The research provides theoretical backing by analyzing 2-layer neural networks, noting that solutions with better generalization have a smaller Hessian matrix norm. For deeper networks, extensive numerical data supports these conclusions."
    }
  ],
  "iteration": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    }
  ],
  "overhead": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "Automated Learning Rate Scheduler for Large-batch",
      "url": "/research/2021/07/13/Automated-Learning-Rate-Scheduler-for-Large-batch.html",
      "date": "July 13, 2021",
      "summary": "Large-batch training is crucial for deep learning with large datasets and models, but requires specific learning rate (LR) schedules for optimal performance, especially under limited training epochs. This work introduces an automated LR scheduling algorithm for large-batch neural network training within a fixed epoch budget, consisting of adaptive warmup and predefined decay phases. The LR is dynamically adjusted based on training loss monitored through Gaussian process smoothing, facilitating low computational overhead. When integrated with adaptive stochastic optimizers like AdamP and LAMB, this scheduler eliminates the need for extensive hyperparameter tuning and achieves competitive or superior results on various image classification tasks across different batch sizes and architectures."
    },
    {
      "title": "Root Mean Square Layer Normalization",
      "url": "/research/2019/10/16/Root-Mean-Square-Layer-Normalization.html",
      "date": "October 16, 2019",
      "summary": "Layer normalization (LayerNorm) enhances deep neural network stability and convergence by re-centering and re-scaling inputs and weight matrices. However, its computational overhead slows networks, particularly RNNs. We introduce RMSNorm, which replaces re-centering with root mean square (RMS) regularization. RMSNorm maintains re-scaling invariance and adapts learning rates implicitly, while being computationally simpler than LayerNorm. We also propose partial RMSNorm (pRMSNorm), estimating RMS from a subset of inputs. Empirical results across various tasks and architectures demonstrate that RMSNorm achieves comparable performance to LayerNorm while reducing running time by 7%∼64%."
    },
    {
      "title": "Sharpness-Aware Training for Free",
      "url": "/research/2023/03/02/Sharpness-Aware-Training-for-Free.html",
      "date": "March 2, 2023",
      "summary": "Modern deep neural networks (DNNs) excel in performance but often suffer from over-parameterization, leading to increased generalization error without tailored training strategies. Sharpness-Aware Minimization (SAM) has proven effective in reducing generalization error by minimizing sharpness in the loss landscape. However, SAM incurs a significant computational overhead. This paper introduces Sharpness-Aware Training for Free (SAF), which mitigates sharpness at nearly zero additional computational cost. SAF achieves this by preventing sudden drops in loss within sharp local minima during weight updates. A novel trajectory loss, based on KL-divergence between current and past DNN outputs, replaces SAM's sharpness measure, guiding convergence towards flat minima for enhanced generalization. Empirical results demonstrate SAF's effectiveness on ImageNet with comparable computational efficiency to the base optimizer."
    },
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    }
  ],
  "approximate": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "Eigencurve: Optimal Learning Rate Schedule For Sgd on Quadratic Objectives With Skewed Hessian Spectrums",
      "url": "/research/2022/06/14/Eigencurve-Optimal-Learning-Rate-Schedule-For-Sgd-on-Quadratic-Objectives-With-Skewed-Hessian-Spectrums.html",
      "date": "June 14, 2022",
      "summary": "Learning rate schedulers are essential in deep neural network training, but there's a gap between practical usage and theoretical understanding. This paper introduces Eigencurve, the first set of learning rate schedules achieving minimax optimal convergence rates for SGD on quadratic objectives with skewed eigenvalue distributions of the Hessian matrix. This condition is common in practice. Experimental results on CIFAR-10 image classification tasks demonstrate Eigencurve's superiority over step decay, particularly with fewer epochs. The theory inspires practical schedulers approximating Eigencurve, resembling cosine decay for some problems while outperforming it in others."
    },
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    }
  ],
  "square": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "Root Mean Square Layer Normalization",
      "url": "/research/2019/10/16/Root-Mean-Square-Layer-Normalization.html",
      "date": "October 16, 2019",
      "summary": "Layer normalization (LayerNorm) enhances deep neural network stability and convergence by re-centering and re-scaling inputs and weight matrices. However, its computational overhead slows networks, particularly RNNs. We introduce RMSNorm, which replaces re-centering with root mean square (RMS) regularization. RMSNorm maintains re-scaling invariance and adapts learning rates implicitly, while being computationally simpler than LayerNorm. We also propose partial RMSNorm (pRMSNorm), estimating RMS from a subset of inputs. Empirical results across various tasks and architectures demonstrate that RMSNorm achieves comparable performance to LayerNorm while reducing running time by 7%∼64%."
    },
    {
      "title": "The loss landscape of deep linear neural networks: a second-order analysis",
      "url": "/research/2022/03/11/The-loss-landscape-of-deep-linear-neural-networks-a-second-order-analysis.html",
      "date": "March 11, 2022",
      "summary": "This study investigates the optimization landscape of deep linear neural networks with square loss, focusing on the role of non-strict saddle points in algorithm dynamics, which has been previously understudied. the authors conduct a comprehensive second-order analysis, identifying global minimizers, strict, and non-strict saddle points among all critical points, alongside their critical values. Our findings, based on conditions related to the ranks of partial matrix products, contribute insights into global convergence and implicit regularization observed in optimizing linear neural networks. Additionally, the authors offer an explicit parameterization of global minimizers and identify extensive sets of strict and non-strict saddle points."
    }
  ],
  "averaging": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    }
  ],
  "glue": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "We present AdaHessian, a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function via adaptive estimates of the Hessian. Despite the superior convergence properties of second-order methods over first-order methods like SGD and Adam, traditional second-order methods suffer from heavier per-iteration computation and poor accuracy. AdaHessian addresses these issues through innovative approaches, including a fast Hutchinson-based method for low computational overhead in approximating the curvature matrix, a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, and block diagonal averaging to reduce the variance of Hessian diagonal elements. Empirical results demonstrate that AdaHessian significantly outperforms other adaptive optimization methods, including variants of Adam, across various tasks such as computer vision, natural language processing, and recommendation systems. Specifically, AdaHessian achieves higher accuracy in image classification tasks, outperforms AdamW in transformer models, and achieves superior performance in tasks such as GLUE and recommendation systems. Importantly, AdaHessian demonstrates comparable per-iteration cost to first-order methods and robustness to hyperparameters."
    },
    {
      "title": "FNet: Mixing Tokens with Fourier Transforms",
      "url": "/research/2022/05/26/FNet-Mixing-Tokens-with-Fourier-Transforms.html",
      "date": "May 26, 2022",
      "summary": "We demonstrate that Transformer encoder architectures can be accelerated with minimal impact on accuracy by substituting self-attention sublayers with simple linear transformations. Remarkably, using a standard Fourier Transform instead of the self-attention sublayer in a Transformer encoder achieves 92-97% of BERT models' accuracy on the GLUE benchmark, while training 80% faster on GPUs and 70% faster on TPUs for standard 512 input lengths. Our FNet model significantly outperforms in speed at longer input lengths, matching the accuracy of the most accurate models on the Long Range Arena benchmark and surpassing the fastest models in speed across all sequence lengths on GPUs and shorter lengths on TPUs. Additionally, FNet is lightweight in memory use and exceptionally efficient in smaller sizes, outperforming Transformer models under the same speed and accuracy constraints."
    },
    {
      "title": "Transformers with Learnable Activation Functions",
      "url": "/research/2023/02/14/Transformers-with-Learnable-Activation-Functions.html",
      "date": "February 14, 2023",
      "summary": "Activation functions significantly affect model performance by reducing data complexity, yet their selection in Transformer-based language models is often overlooked. This paper explores the impact of using rational activation functions (RAFs), which unlike fixed activation functions (FAFs), can learn optimal functions from data. Our experiments demonstrate that Transformer models with RAFs (RAFT) outperform those with FAFs (FAFT), achieving a 5.71 point higher score on the GLUE benchmark with only 100 training examples and a 2.05 point increase on SQuAD with full data. The varied shapes of learned RAFs across layers and tasks suggest a new method for analyzing and understanding large pre-trained language models."
    }
  ],
  "lr": [
    {
      "title": "Automated Learning Rate Scheduler for Large-batch",
      "url": "/research/2021/07/13/Automated-Learning-Rate-Scheduler-for-Large-batch.html",
      "date": "July 13, 2021",
      "summary": "Large-batch training is crucial for deep learning with large datasets and models, but requires specific learning rate (LR) schedules for optimal performance, especially under limited training epochs. This work introduces an automated LR scheduling algorithm for large-batch neural network training within a fixed epoch budget, consisting of adaptive warmup and predefined decay phases. The LR is dynamically adjusted based on training loss monitored through Gaussian process smoothing, facilitating low computational overhead. When integrated with adaptive stochastic optimizers like AdamP and LAMB, this scheduler eliminates the need for extensive hyperparameter tuning and achieves competitive or superior results on various image classification tasks across different batch sizes and architectures."
    }
  ],
  "scheduling": [
    {
      "title": "Automated Learning Rate Scheduler for Large-batch",
      "url": "/research/2021/07/13/Automated-Learning-Rate-Scheduler-for-Large-batch.html",
      "date": "July 13, 2021",
      "summary": "Large-batch training is crucial for deep learning with large datasets and models, but requires specific learning rate (LR) schedules for optimal performance, especially under limited training epochs. This work introduces an automated LR scheduling algorithm for large-batch neural network training within a fixed epoch budget, consisting of adaptive warmup and predefined decay phases. The LR is dynamically adjusted based on training loss monitored through Gaussian process smoothing, facilitating low computational overhead. When integrated with adaptive stochastic optimizers like AdamP and LAMB, this scheduler eliminates the need for extensive hyperparameter tuning and achieves competitive or superior results on various image classification tasks across different batch sizes and architectures."
    }
  ],
  "warmup": [
    {
      "title": "Automated Learning Rate Scheduler for Large-batch",
      "url": "/research/2021/07/13/Automated-Learning-Rate-Scheduler-for-Large-batch.html",
      "date": "July 13, 2021",
      "summary": "Large-batch training is crucial for deep learning with large datasets and models, but requires specific learning rate (LR) schedules for optimal performance, especially under limited training epochs. This work introduces an automated LR scheduling algorithm for large-batch neural network training within a fixed epoch budget, consisting of adaptive warmup and predefined decay phases. The LR is dynamically adjusted based on training loss monitored through Gaussian process smoothing, facilitating low computational overhead. When integrated with adaptive stochastic optimizers like AdamP and LAMB, this scheduler eliminates the need for extensive hyperparameter tuning and achieves competitive or superior results on various image classification tasks across different batch sizes and architectures."
    }
  ],
  "monitor": [
    {
      "title": "Automated Learning Rate Scheduler for Large-batch",
      "url": "/research/2021/07/13/Automated-Learning-Rate-Scheduler-for-Large-batch.html",
      "date": "July 13, 2021",
      "summary": "Large-batch training is crucial for deep learning with large datasets and models, but requires specific learning rate (LR) schedules for optimal performance, especially under limited training epochs. This work introduces an automated LR scheduling algorithm for large-batch neural network training within a fixed epoch budget, consisting of adaptive warmup and predefined decay phases. The LR is dynamically adjusted based on training loss monitored through Gaussian process smoothing, facilitating low computational overhead. When integrated with adaptive stochastic optimizers like AdamP and LAMB, this scheduler eliminates the need for extensive hyperparameter tuning and achieves competitive or superior results on various image classification tasks across different batch sizes and architectures."
    }
  ],
  "adamp": [
    {
      "title": "Automated Learning Rate Scheduler for Large-batch",
      "url": "/research/2021/07/13/Automated-Learning-Rate-Scheduler-for-Large-batch.html",
      "date": "July 13, 2021",
      "summary": "Large-batch training is crucial for deep learning with large datasets and models, but requires specific learning rate (LR) schedules for optimal performance, especially under limited training epochs. This work introduces an automated LR scheduling algorithm for large-batch neural network training within a fixed epoch budget, consisting of adaptive warmup and predefined decay phases. The LR is dynamically adjusted based on training loss monitored through Gaussian process smoothing, facilitating low computational overhead. When integrated with adaptive stochastic optimizers like AdamP and LAMB, this scheduler eliminates the need for extensive hyperparameter tuning and achieves competitive or superior results on various image classification tasks across different batch sizes and architectures."
    }
  ],
  "lamb": [
    {
      "title": "Automated Learning Rate Scheduler for Large-batch",
      "url": "/research/2021/07/13/Automated-Learning-Rate-Scheduler-for-Large-batch.html",
      "date": "July 13, 2021",
      "summary": "Large-batch training is crucial for deep learning with large datasets and models, but requires specific learning rate (LR) schedules for optimal performance, especially under limited training epochs. This work introduces an automated LR scheduling algorithm for large-batch neural network training within a fixed epoch budget, consisting of adaptive warmup and predefined decay phases. The LR is dynamically adjusted based on training loss monitored through Gaussian process smoothing, facilitating low computational overhead. When integrated with adaptive stochastic optimizers like AdamP and LAMB, this scheduler eliminates the need for extensive hyperparameter tuning and achieves competitive or superior results on various image classification tasks across different batch sizes and architectures."
    }
  ],
  "collapse": [
    {
      "title": "Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks",
      "url": "/research/2020/06/11/Batch-Normalization-Provably-Avoids-Rank-Collapse-for-Randomly-Initialised-Deep-Networks.html",
      "date": "June 11, 2020",
      "summary": "We investigate the challenge of training randomly initialized deep neural networks due to spectral instabilities in products of random matrices. Batch normalization emerges as an effective solution to prevent rank collapse in both linear and ReLU networks. Leveraging tools from Markov chain theory, we establish a lower rank bound for deep linear networks. Empirical findings show that this rank robustness extends to ReLU networks. Our experiments on real-world datasets underscore the significance of rank stability in training modern deep neural architectures."
    },
    {
      "title": "Towards a Unified Theoretical Understanding of Non-contrastive Learning via Rank Differential Mechanism",
      "url": "/research/2023/03/04/Towards-a-Unified-Theoretical-Understanding-of-Non-contrastive-Learning-via-Rank-Differential-Mechanism.html",
      "date": "March 4, 2023",
      "summary": "Recent non-contrastive learning methods like BYOL, SimSiam, SwAV, and DINO have shown that asymmetric architectural designs can achieve good self-supervised visual learning performance by aligning positive pairs alone, despite a lack of unified theoretical understanding on how these designs prevent feature collapse. This study introduces the Rank Differential Mechanism (RDM) as a unified theoretical framework for non-contrastive learning. RDM demonstrates that these asymmetric designs maintain a consistent rank difference in output features, enhancing effective dimensionality and preventing feature collapse. Unlike previous theories, RDM applies to designs both with and without a predictor, offering a comprehensive understanding of non-contrastive learning methods and guiding the development of new variants. Experiments confirm that these variants perform comparably, if not better than, existing methods on benchmark datasets."
    }
  ],
  "initialise": [
    {
      "title": "Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks",
      "url": "/research/2020/06/11/Batch-Normalization-Provably-Avoids-Rank-Collapse-for-Randomly-Initialised-Deep-Networks.html",
      "date": "June 11, 2020",
      "summary": "We investigate the challenge of training randomly initialized deep neural networks due to spectral instabilities in products of random matrices. Batch normalization emerges as an effective solution to prevent rank collapse in both linear and ReLU networks. Leveraging tools from Markov chain theory, we establish a lower rank bound for deep linear networks. Empirical findings show that this rank robustness extends to ReLU networks. Our experiments on real-world datasets underscore the significance of rank stability in training modern deep neural architectures."
    }
  ],
  "markov": [
    {
      "title": "Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks",
      "url": "/research/2020/06/11/Batch-Normalization-Provably-Avoids-Rank-Collapse-for-Randomly-Initialised-Deep-Networks.html",
      "date": "June 11, 2020",
      "summary": "We investigate the challenge of training randomly initialized deep neural networks due to spectral instabilities in products of random matrices. Batch normalization emerges as an effective solution to prevent rank collapse in both linear and ReLU networks. Leveraging tools from Markov chain theory, we establish a lower rank bound for deep linear networks. Empirical findings show that this rank robustness extends to ReLU networks. Our experiments on real-world datasets underscore the significance of rank stability in training modern deep neural architectures."
    }
  ],
  "chain": [
    {
      "title": "Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks",
      "url": "/research/2020/06/11/Batch-Normalization-Provably-Avoids-Rank-Collapse-for-Randomly-Initialised-Deep-Networks.html",
      "date": "June 11, 2020",
      "summary": "We investigate the challenge of training randomly initialized deep neural networks due to spectral instabilities in products of random matrices. Batch normalization emerges as an effective solution to prevent rank collapse in both linear and ReLU networks. Leveraging tools from Markov chain theory, we establish a lower rank bound for deep linear networks. Empirical findings show that this rank robustness extends to ReLU networks. Our experiments on real-world datasets underscore the significance of rank stability in training modern deep neural architectures."
    },
    {
      "title": "Atomically Accurate De Novo Design of Single-domain Antibodies",
      "url": "/research/2024/03/18/Atomically-Accurate-De-Novo-Design-of-Single-domain-Antibodies.html",
      "date": "March 18, 2024",
      "summary": "Despite the critical role antibodies play in medicine, current methods for designing new antibodies targeting specific epitopes are time-consuming. Here, we showcase the effectiveness of a refined RFdiffusion network in designing novel antibody variable heavy chains (VHH’s) to target user-specified epitopes. Through experiments, we validate the binding capability of these designed VHH's to four disease-relevant epitopes. Furthermore, the cryo-EM structure analysis reveals that a designed VHH bound to influenza hemagglutinin closely matches the design model, affirming the accuracy of our approach."
    }
  ],
  "descent": [
    {
      "title": "Don't Decay the Learning Rate, Increase the Batch Size",
      "url": "/research/2017/11/01/Don't-Decay-the-Learning-Rate,-Increase-the-Batch-Size.html",
      "date": "November 1, 2017",
      "summary": "This study demonstrates that increasing the batch size during training achieves similar learning outcomes as the common practice of decaying the learning rate, applicable to stochastic gradient descent and its variants, including with momentum and Adam optimization. This approach not only matches test accuracies within the same number of epochs but also enhances parallelism and reduces training time due to fewer parameter updates. Efficiency can be further improved by adjusting the learning rate and batch size proportionally, and although increasing the momentum coefficient and scaling the batch size accordingly may slightly lower test accuracy, it enables the use of large batch training without needing to tune hyper-parameters. Using these methods, ResNet-50 was trained on ImageNet to a 76.1% validation accuracy in less than 30 minutes."
    },
    {
      "title": "Large Batch Training of Convolutional Networks",
      "url": "/research/2017/09/13/Large-Batch-Training-of-Convolutional-Networks.html",
      "date": "September 13, 2017",
      "summary": "To expedite the training of large convolutional networks, computational units are added and trained with data-parallel synchronous Stochastic Gradient Descent (SGD) across units, increasing batch size with node count. However, larger batch sizes can reduce model accuracy. The existing method of large batch training—linear learning rate scaling with warm-up—is not universally effective and may cause training divergence. To address these challenges, we introduce a novel training algorithm, Layer-wise Adaptive Rate Scaling (LARS), enabling us to train Alexnet with a batch size of 8K and Resnet-50 with a batch size of 32K, without compromising accuracy."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "url": "/research/2014/02/19/Exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks.html",
      "date": "February 19, 2014",
      "summary": "This study aims to enhance our comprehension of deep learning dynamics by analyzing deep linear neural networks. Despite their linear input-output mapping, these networks display nonlinear gradient descent dynamics, resulting in phenomena like prolonged plateaus and swift transitions to improved solutions. Through analytical exploration, we reveal that as network depth increases indefinitely, learning speed can remain finite under specific initial conditions. Unsupervised pretraining, under certain data conditions, can discover these favorable initial conditions, unlike random Gaussian initializations. We introduce a novel class of random orthogonal initial conditions that, akin to unsupervised pretraining, enable depth-independent learning. These conditions also ensure gradient propagation in deep nonlinear networks, particularly when operating at the edge of chaos."
    },
    {
      "title": "Penalizing Gradient Norm for Efficiently Improving Generalization in Deep Learning",
      "url": "/research/2022/06/26/Penalizing-Gradient-Norm-for-Efficiently-Improving-Generalization-in-Deep-Learning.html",
      "date": "June 26, 2022",
      "summary": "This paper proposes a method to enhance the generalization of deep neural networks (DNNs) by penalizing the gradient norm of the loss function during optimization. By constraining the gradient norm, the optimizers tend to find flat minima, improving generalization. We efficiently implement this method using first-order approximation within the gradient descent framework. Experimental results demonstrate improved generalization across various models and datasets. Additionally, we show that a recent method, sharpness-aware minimization, is a special case of our approach, with our method achieving new state-of-the-art performance on tested tasks."
    },
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    },
    {
      "title": "SGDR: Stochastic Gradient Descent With Warm Restarts",
      "url": "/research/2017/05/02/SGDR-Stochastic-Gradient-Descent-With-Warm-Restarts.html",
      "date": "May 2, 2017",
      "summary": "This paper introduces a warm restart technique for stochastic gradient descent aimed at enhancing anytime performance in deep neural network training. It showcases empirical performance improvements on CIFAR-10 and CIFAR-100 datasets, achieving state-of-the-art results with 3.14% and 16.21% error rates, respectively. Additionally, the technique's benefits are demonstrated on an EEG dataset and a downsampled ImageNet dataset."
    }
  ],
  "parallelism": [
    {
      "title": "Don't Decay the Learning Rate, Increase the Batch Size",
      "url": "/research/2017/11/01/Don't-Decay-the-Learning-Rate,-Increase-the-Batch-Size.html",
      "date": "November 1, 2017",
      "summary": "This study demonstrates that increasing the batch size during training achieves similar learning outcomes as the common practice of decaying the learning rate, applicable to stochastic gradient descent and its variants, including with momentum and Adam optimization. This approach not only matches test accuracies within the same number of epochs but also enhances parallelism and reduces training time due to fewer parameter updates. Efficiency can be further improved by adjusting the learning rate and batch size proportionally, and although increasing the momentum coefficient and scaling the batch size accordingly may slightly lower test accuracy, it enables the use of large batch training without needing to tune hyper-parameters. Using these methods, ResNet-50 was trained on ImageNet to a 76.1% validation accuracy in less than 30 minutes."
    }
  ],
  "coefficient": [
    {
      "title": "Don't Decay the Learning Rate, Increase the Batch Size",
      "url": "/research/2017/11/01/Don't-Decay-the-Learning-Rate,-Increase-the-Batch-Size.html",
      "date": "November 1, 2017",
      "summary": "This study demonstrates that increasing the batch size during training achieves similar learning outcomes as the common practice of decaying the learning rate, applicable to stochastic gradient descent and its variants, including with momentum and Adam optimization. This approach not only matches test accuracies within the same number of epochs but also enhances parallelism and reduces training time due to fewer parameter updates. Efficiency can be further improved by adjusting the learning rate and batch size proportionally, and although increasing the momentum coefficient and scaling the batch size accordingly may slightly lower test accuracy, it enables the use of large batch training without needing to tune hyper-parameters. Using these methods, ResNet-50 was trained on ImageNet to a 76.1% validation accuracy in less than 30 minutes."
    },
    {
      "title": "Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks",
      "url": "/research/2017/10/23/Cosine-Normalization-Using-Cosine-Similarity-Instead-of-Dot-Product-in-Neural-Networks.html",
      "date": "October 23, 2017",
      "summary": "In traditional multi-layer neural networks, the dot product between the output and weight vectors of preceding layers serves as input to the activation function, resulting in unbounded outputs and increased variance. This variance can lead to poor generalization and hinder training by exacerbating internal covariate shift. To address this, we propose cosine normalization, which replaces the dot product with cosine similarity or centered cosine similarity (Pearson Correlation Coefficient). We evaluate cosine normalization against batch, weight, and layer normalization in fully-connected and convolutional neural networks across various datasets. Our experiments demonstrate that cosine normalization outperforms other normalization techniques."
    }
  ],
  "accordingly": [
    {
      "title": "Don't Decay the Learning Rate, Increase the Batch Size",
      "url": "/research/2017/11/01/Don't-Decay-the-Learning-Rate,-Increase-the-Batch-Size.html",
      "date": "November 1, 2017",
      "summary": "This study demonstrates that increasing the batch size during training achieves similar learning outcomes as the common practice of decaying the learning rate, applicable to stochastic gradient descent and its variants, including with momentum and Adam optimization. This approach not only matches test accuracies within the same number of epochs but also enhances parallelism and reduces training time due to fewer parameter updates. Efficiency can be further improved by adjusting the learning rate and batch size proportionally, and although increasing the momentum coefficient and scaling the batch size accordingly may slightly lower test accuracy, it enables the use of large batch training without needing to tune hyper-parameters. Using these methods, ResNet-50 was trained on ImageNet to a 76.1% validation accuracy in less than 30 minutes."
    }
  ],
  "slightly": [
    {
      "title": "Don't Decay the Learning Rate, Increase the Batch Size",
      "url": "/research/2017/11/01/Don't-Decay-the-Learning-Rate,-Increase-the-Batch-Size.html",
      "date": "November 1, 2017",
      "summary": "This study demonstrates that increasing the batch size during training achieves similar learning outcomes as the common practice of decaying the learning rate, applicable to stochastic gradient descent and its variants, including with momentum and Adam optimization. This approach not only matches test accuracies within the same number of epochs but also enhances parallelism and reduces training time due to fewer parameter updates. Efficiency can be further improved by adjusting the learning rate and batch size proportionally, and although increasing the momentum coefficient and scaling the batch size accordingly may slightly lower test accuracy, it enables the use of large batch training without needing to tune hyper-parameters. Using these methods, ResNet-50 was trained on ImageNet to a 76.1% validation accuracy in less than 30 minutes."
    }
  ],
  "eigencurve": [
    {
      "title": "Eigencurve: Optimal Learning Rate Schedule For Sgd on Quadratic Objectives With Skewed Hessian Spectrums",
      "url": "/research/2022/06/14/Eigencurve-Optimal-Learning-Rate-Schedule-For-Sgd-on-Quadratic-Objectives-With-Skewed-Hessian-Spectrums.html",
      "date": "June 14, 2022",
      "summary": "Learning rate schedulers are essential in deep neural network training, but there's a gap between practical usage and theoretical understanding. This paper introduces Eigencurve, the first set of learning rate schedules achieving minimax optimal convergence rates for SGD on quadratic objectives with skewed eigenvalue distributions of the Hessian matrix. This condition is common in practice. Experimental results on CIFAR-10 image classification tasks demonstrate Eigencurve's superiority over step decay, particularly with fewer epochs. The theory inspires practical schedulers approximating Eigencurve, resembling cosine decay for some problems while outperforming it in others."
    }
  ],
  "skew": [
    {
      "title": "Eigencurve: Optimal Learning Rate Schedule For Sgd on Quadratic Objectives With Skewed Hessian Spectrums",
      "url": "/research/2022/06/14/Eigencurve-Optimal-Learning-Rate-Schedule-For-Sgd-on-Quadratic-Objectives-With-Skewed-Hessian-Spectrums.html",
      "date": "June 14, 2022",
      "summary": "Learning rate schedulers are essential in deep neural network training, but there's a gap between practical usage and theoretical understanding. This paper introduces Eigencurve, the first set of learning rate schedules achieving minimax optimal convergence rates for SGD on quadratic objectives with skewed eigenvalue distributions of the Hessian matrix. This condition is common in practice. Experimental results on CIFAR-10 image classification tasks demonstrate Eigencurve's superiority over step decay, particularly with fewer epochs. The theory inspires practical schedulers approximating Eigencurve, resembling cosine decay for some problems while outperforming it in others."
    }
  ],
  "spectrums": [
    {
      "title": "Eigencurve: Optimal Learning Rate Schedule For Sgd on Quadratic Objectives With Skewed Hessian Spectrums",
      "url": "/research/2022/06/14/Eigencurve-Optimal-Learning-Rate-Schedule-For-Sgd-on-Quadratic-Objectives-With-Skewed-Hessian-Spectrums.html",
      "date": "June 14, 2022",
      "summary": "Learning rate schedulers are essential in deep neural network training, but there's a gap between practical usage and theoretical understanding. This paper introduces Eigencurve, the first set of learning rate schedules achieving minimax optimal convergence rates for SGD on quadratic objectives with skewed eigenvalue distributions of the Hessian matrix. This condition is common in practice. Experimental results on CIFAR-10 image classification tasks demonstrate Eigencurve's superiority over step decay, particularly with fewer epochs. The theory inspires practical schedulers approximating Eigencurve, resembling cosine decay for some problems while outperforming it in others."
    }
  ],
  "minimax": [
    {
      "title": "Eigencurve: Optimal Learning Rate Schedule For Sgd on Quadratic Objectives With Skewed Hessian Spectrums",
      "url": "/research/2022/06/14/Eigencurve-Optimal-Learning-Rate-Schedule-For-Sgd-on-Quadratic-Objectives-With-Skewed-Hessian-Spectrums.html",
      "date": "June 14, 2022",
      "summary": "Learning rate schedulers are essential in deep neural network training, but there's a gap between practical usage and theoretical understanding. This paper introduces Eigencurve, the first set of learning rate schedules achieving minimax optimal convergence rates for SGD on quadratic objectives with skewed eigenvalue distributions of the Hessian matrix. This condition is common in practice. Experimental results on CIFAR-10 image classification tasks demonstrate Eigencurve's superiority over step decay, particularly with fewer epochs. The theory inspires practical schedulers approximating Eigencurve, resembling cosine decay for some problems while outperforming it in others."
    }
  ],
  "eigenvalue": [
    {
      "title": "Eigencurve: Optimal Learning Rate Schedule For Sgd on Quadratic Objectives With Skewed Hessian Spectrums",
      "url": "/research/2022/06/14/Eigencurve-Optimal-Learning-Rate-Schedule-For-Sgd-on-Quadratic-Objectives-With-Skewed-Hessian-Spectrums.html",
      "date": "June 14, 2022",
      "summary": "Learning rate schedulers are essential in deep neural network training, but there's a gap between practical usage and theoretical understanding. This paper introduces Eigencurve, the first set of learning rate schedules achieving minimax optimal convergence rates for SGD on quadratic objectives with skewed eigenvalue distributions of the Hessian matrix. This condition is common in practice. Experimental results on CIFAR-10 image classification tasks demonstrate Eigencurve's superiority over step decay, particularly with fewer epochs. The theory inspires practical schedulers approximating Eigencurve, resembling cosine decay for some problems while outperforming it in others."
    },
    {
      "title": "On the Maximum Hessian Eigenvalue and Generalization",
      "url": "/research/2023/05/23/On-the-Maximum-Hessian-Eigenvalue-and-Generalization.html",
      "date": "May 23, 2023",
      "summary": "This study investigates the relationship between training interventions and the generalization of deep networks. While previous research suggests that flatter solutions generalize better than sharper ones, particularly measured by λmax, the largest eigenvalue of the Hessian of the loss, this paper challenges this notion. Through experiments, we demonstrate that larger learning rates reduce λmax for all batch sizes but do not consistently improve generalization. Additionally, scaling batch size and learning rate simultaneously can change λmax without affecting generalization. Sharpness-Aware Minimization (SAM) produces smaller λmax but does not consistently enhance generalization, especially with larger batch sizes. Excessive dropout probabilities can degrade generalization, despite promoting smaller λmax. Batch normalization, while not consistently reducing λmax, still improves generalization. These findings question λmax's role in explaining generalization in neural networks, highlighting the limits of its explanatory power."
    },
    {
      "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
      "url": "/research/2022/03/19/Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training.html",
      "date": "March 19, 2022",
      "summary": "Sharpness-Aware Minimization (SAM) enhances generalization by optimizing a neighborhood-based perturbed loss, but it doesn't always favor flat minima due to both sharp and flat minima having low perturbed losses. We introduce a new measure, the surrogate gap, reflecting the dominant Hessian eigenvalue at small neighborhood radii, which is simple to compute and can be minimized during training. We propose the Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), an advancement over SAM with minimal additional computational cost. GSAM employs a dual-step approach: first, minimizing the perturbed loss similar to SAM, and second, reducing the surrogate gap without affecting the perturbed loss to target regions with low loss and sharpness, thus achieving superior generalization. GSAM is theoretically robust, showing better convergence and empirical generalization improvements, notably a +3.2% gain over SAM and +5.4% over AdamW in ImageNet accuracy for ViT-B/32."
    }
  ],
  "resemble": [
    {
      "title": "Eigencurve: Optimal Learning Rate Schedule For Sgd on Quadratic Objectives With Skewed Hessian Spectrums",
      "url": "/research/2022/06/14/Eigencurve-Optimal-Learning-Rate-Schedule-For-Sgd-on-Quadratic-Objectives-With-Skewed-Hessian-Spectrums.html",
      "date": "June 14, 2022",
      "summary": "Learning rate schedulers are essential in deep neural network training, but there's a gap between practical usage and theoretical understanding. This paper introduces Eigencurve, the first set of learning rate schedules achieving minimax optimal convergence rates for SGD on quadratic objectives with skewed eigenvalue distributions of the Hessian matrix. This condition is common in practice. Experimental results on CIFAR-10 image classification tasks demonstrate Eigencurve's superiority over step decay, particularly with fewer epochs. The theory inspires practical schedulers approximating Eigencurve, resembling cosine decay for some problems while outperforming it in others."
    },
    {
      "title": "How to decay your learning rate",
      "url": "/research/2021/05/23/How-to-decay-your-learning-rate.html",
      "date": "May 23, 2021",
      "summary": "Empirical findings suggest that typical fine-tuned learning rate schedules decay the learning rate following weight norm fluctuations. This led to the development of ABEL, an automatic scheduler that adjusts the learning rate based on weight norm changes. ABEL performs comparably to tuned schedules but demonstrates greater robustness to parameter variations. Extensive experiments across various domains reveal that when the weight norm remains stable, simplified schedules yield equivalent performance to complex ones, resembling a constant learning rate with decay towards the end of training."
    },
    {
      "title": "Neural Networks Learn Statistics of Increasing Complexity",
      "url": "/research/2024/02/13/Neural-Networks-Learn-Statistics-of-Increasing-Complexity.html",
      "date": "February 13, 2024",
      "summary": "The distributional simplicity bias (DSB) theory suggests neural networks first learn basic patterns in data before understanding more complex correlations. We provide new evidence supporting DSB, showing networks initially excel with data matching training set's simple statistics but this ability diminishes later. Extending DSB to discrete domains, we demonstrate an equivalence between n-gram frequencies and vector moments, also observing this bias in large language models (LLMs). Additionally, by adjusting low-level statistics of images to resemble another class, we reveal that networks in early training phases misclassify these edited images as if they belonged to the target class."
    }
  ],
  "others": [
    {
      "title": "Eigencurve: Optimal Learning Rate Schedule For Sgd on Quadratic Objectives With Skewed Hessian Spectrums",
      "url": "/research/2022/06/14/Eigencurve-Optimal-Learning-Rate-Schedule-For-Sgd-on-Quadratic-Objectives-With-Skewed-Hessian-Spectrums.html",
      "date": "June 14, 2022",
      "summary": "Learning rate schedulers are essential in deep neural network training, but there's a gap between practical usage and theoretical understanding. This paper introduces Eigencurve, the first set of learning rate schedules achieving minimax optimal convergence rates for SGD on quadratic objectives with skewed eigenvalue distributions of the Hessian matrix. This condition is common in practice. Experimental results on CIFAR-10 image classification tasks demonstrate Eigencurve's superiority over step decay, particularly with fewer epochs. The theory inspires practical schedulers approximating Eigencurve, resembling cosine decay for some problems while outperforming it in others."
    },
    {
      "title": "Chronos: Learning the Language of Time Series",
      "url": "/research/2024/03/12/Chronos-Learning-the-Language-of-Time-Series.html",
      "date": "March 12, 2024",
      "summary": "Chronos is a framework that enhances pretrained probabilistic time series models by tokenizing time series data for training with transformer-based architectures, notably the T5 family. Pretrained on a mix of public and synthetic datasets created via Gaussian processes, Chronos models outshine others in 42 benchmark datasets by showing superior performance on familiar datasets and competitive or better zero-shot capabilities on new datasets. This illustrates Chronos's ability to generalize across different domains, offering a simpler approach to forecasting tasks."
    },
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    },
    {
      "title": "Suppressing Pink Elephants with Direct Principle Feedback",
      "url": "/research/2024/02/13/Suppressing-Pink-Elephants-with-Direct-Principle-Feedback.html",
      "date": "February 13, 2024",
      "summary": "Existing methods for controlling language models focus on training desired behaviors, but often lack the flexibility needed for diverse applications. The authors address this with the Pink Elephant Problem, demonstrating the need for language models to adapt to different contexts by avoiding certain topics (the Pink Elephant) in favor of others (Grey Elephant). The authors introduce Direct Principle Feedback (DPF), an adaptation of Constitutional AI that improves control by directly applying critiques and revisions without ranking responses. Our study shows that a 13B LLaMA 2 model fine-tuned with DPF on a synthetic dataset outperforms existing models and matches GPT-4 in managing the Pink Elephant Problem."
    }
  ],
  "fluctuation": [
    {
      "title": "How to decay your learning rate",
      "url": "/research/2021/05/23/How-to-decay-your-learning-rate.html",
      "date": "May 23, 2021",
      "summary": "Empirical findings suggest that typical fine-tuned learning rate schedules decay the learning rate following weight norm fluctuations. This led to the development of ABEL, an automatic scheduler that adjusts the learning rate based on weight norm changes. ABEL performs comparably to tuned schedules but demonstrates greater robustness to parameter variations. Extensive experiments across various domains reveal that when the weight norm remains stable, simplified schedules yield equivalent performance to complex ones, resembling a constant learning rate with decay towards the end of training."
    }
  ],
  "abel": [
    {
      "title": "How to decay your learning rate",
      "url": "/research/2021/05/23/How-to-decay-your-learning-rate.html",
      "date": "May 23, 2021",
      "summary": "Empirical findings suggest that typical fine-tuned learning rate schedules decay the learning rate following weight norm fluctuations. This led to the development of ABEL, an automatic scheduler that adjusts the learning rate based on weight norm changes. ABEL performs comparably to tuned schedules but demonstrates greater robustness to parameter variations. Extensive experiments across various domains reveal that when the weight norm remains stable, simplified schedules yield equivalent performance to complex ones, resembling a constant learning rate with decay towards the end of training."
    }
  ],
  "prelayernorm": [
    {
      "title": "Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding",
      "url": "/research/2021/11/16/Improved-Robustness-of-Vision-Transformer-via-PreLayerNorm-in-Patch-Embedding.html",
      "date": "November 16, 2021",
      "summary": "Recent advancements in vision transformers (ViTs) have shown superior performance across diverse visual tasks, surpassing convolutional neural networks (CNNs). Given ViT's distinct architecture, understanding its behavior and reliability is imperative. This paper investigates ViT's robustness by comparing it with CNNs under various image corruptions relevant to real-world vision tasks. While ViT generally exhibits comparable or improved robustness over CNNs, it consistently underperforms in contrast enhancement tasks. Analysis suggests that positional embedding in ViT's patch embedding may malfunction with color scale changes. We propose PreLayerNorm, a modified patch embedding structure, to address this issue and ensure scale-invariant behavior in ViT. ViT with PreLayerNorm demonstrates enhanced robustness across various corruptions, particularly in contrast-varying environments."
    }
  ],
  "may": [
    {
      "title": "Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding",
      "url": "/research/2021/11/16/Improved-Robustness-of-Vision-Transformer-via-PreLayerNorm-in-Patch-Embedding.html",
      "date": "November 16, 2021",
      "summary": "Recent advancements in vision transformers (ViTs) have shown superior performance across diverse visual tasks, surpassing convolutional neural networks (CNNs). Given ViT's distinct architecture, understanding its behavior and reliability is imperative. This paper investigates ViT's robustness by comparing it with CNNs under various image corruptions relevant to real-world vision tasks. While ViT generally exhibits comparable or improved robustness over CNNs, it consistently underperforms in contrast enhancement tasks. Analysis suggests that positional embedding in ViT's patch embedding may malfunction with color scale changes. We propose PreLayerNorm, a modified patch embedding structure, to address this issue and ensure scale-invariant behavior in ViT. ViT with PreLayerNorm demonstrates enhanced robustness across various corruptions, particularly in contrast-varying environments."
    }
  ],
  "malfunction": [
    {
      "title": "Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding",
      "url": "/research/2021/11/16/Improved-Robustness-of-Vision-Transformer-via-PreLayerNorm-in-Patch-Embedding.html",
      "date": "November 16, 2021",
      "summary": "Recent advancements in vision transformers (ViTs) have shown superior performance across diverse visual tasks, surpassing convolutional neural networks (CNNs). Given ViT's distinct architecture, understanding its behavior and reliability is imperative. This paper investigates ViT's robustness by comparing it with CNNs under various image corruptions relevant to real-world vision tasks. While ViT generally exhibits comparable or improved robustness over CNNs, it consistently underperforms in contrast enhancement tasks. Analysis suggests that positional embedding in ViT's patch embedding may malfunction with color scale changes. We propose PreLayerNorm, a modified patch embedding structure, to address this issue and ensure scale-invariant behavior in ViT. ViT with PreLayerNorm demonstrates enhanced robustness across various corruptions, particularly in contrast-varying environments."
    }
  ],
  "stack": [
    {
      "title": "Language Modeling with Gated Convolutional Networks",
      "url": "/research/2017/09/08/Language-Modeling-with-Gated-Convolutional-Networks.html",
      "date": "September 8, 2017",
      "summary": "This paper introduces a novel language modeling approach using stacked convolutions that enable efficient parallel processing, in contrast to the traditionally used recurrent neural networks known for handling unbounded context. The proposed method features a simplified gating mechanism that outperforms previous models and demonstrates superior performance on the WikiText-103 benchmark, showing its capability to manage long-term dependencies. It also delivers competitive results on the Google Billion Words benchmark and significantly reduces sentence scoring latency compared to recurrent models. This marks the first instance of a non-recurrent model achieving comparable success to strong recurrent models on major language tasks."
    },
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "unbind": [
    {
      "title": "Language Modeling with Gated Convolutional Networks",
      "url": "/research/2017/09/08/Language-Modeling-with-Gated-Convolutional-Networks.html",
      "date": "September 8, 2017",
      "summary": "This paper introduces a novel language modeling approach using stacked convolutions that enable efficient parallel processing, in contrast to the traditionally used recurrent neural networks known for handling unbounded context. The proposed method features a simplified gating mechanism that outperforms previous models and demonstrates superior performance on the WikiText-103 benchmark, showing its capability to manage long-term dependencies. It also delivers competitive results on the Google Billion Words benchmark and significantly reduces sentence scoring latency compared to recurrent models. This marks the first instance of a non-recurrent model achieving comparable success to strong recurrent models on major language tasks."
    }
  ],
  "gating": [
    {
      "title": "Language Modeling with Gated Convolutional Networks",
      "url": "/research/2017/09/08/Language-Modeling-with-Gated-Convolutional-Networks.html",
      "date": "September 8, 2017",
      "summary": "This paper introduces a novel language modeling approach using stacked convolutions that enable efficient parallel processing, in contrast to the traditionally used recurrent neural networks known for handling unbounded context. The proposed method features a simplified gating mechanism that outperforms previous models and demonstrates superior performance on the WikiText-103 benchmark, showing its capability to manage long-term dependencies. It also delivers competitive results on the Google Billion Words benchmark and significantly reduces sentence scoring latency compared to recurrent models. This marks the first instance of a non-recurrent model achieving comparable success to strong recurrent models on major language tasks."
    },
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "wikitext": [
    {
      "title": "Language Modeling with Gated Convolutional Networks",
      "url": "/research/2017/09/08/Language-Modeling-with-Gated-Convolutional-Networks.html",
      "date": "September 8, 2017",
      "summary": "This paper introduces a novel language modeling approach using stacked convolutions that enable efficient parallel processing, in contrast to the traditionally used recurrent neural networks known for handling unbounded context. The proposed method features a simplified gating mechanism that outperforms previous models and demonstrates superior performance on the WikiText-103 benchmark, showing its capability to manage long-term dependencies. It also delivers competitive results on the Google Billion Words benchmark and significantly reduces sentence scoring latency compared to recurrent models. This marks the first instance of a non-recurrent model achieving comparable success to strong recurrent models on major language tasks."
    }
  ],
  "google": [
    {
      "title": "Language Modeling with Gated Convolutional Networks",
      "url": "/research/2017/09/08/Language-Modeling-with-Gated-Convolutional-Networks.html",
      "date": "September 8, 2017",
      "summary": "This paper introduces a novel language modeling approach using stacked convolutions that enable efficient parallel processing, in contrast to the traditionally used recurrent neural networks known for handling unbounded context. The proposed method features a simplified gating mechanism that outperforms previous models and demonstrates superior performance on the WikiText-103 benchmark, showing its capability to manage long-term dependencies. It also delivers competitive results on the Google Billion Words benchmark and significantly reduces sentence scoring latency compared to recurrent models. This marks the first instance of a non-recurrent model achieving comparable success to strong recurrent models on major language tasks."
    }
  ],
  "sentence": [
    {
      "title": "Language Modeling with Gated Convolutional Networks",
      "url": "/research/2017/09/08/Language-Modeling-with-Gated-Convolutional-Networks.html",
      "date": "September 8, 2017",
      "summary": "This paper introduces a novel language modeling approach using stacked convolutions that enable efficient parallel processing, in contrast to the traditionally used recurrent neural networks known for handling unbounded context. The proposed method features a simplified gating mechanism that outperforms previous models and demonstrates superior performance on the WikiText-103 benchmark, showing its capability to manage long-term dependencies. It also delivers competitive results on the Google Billion Words benchmark and significantly reduces sentence scoring latency compared to recurrent models. This marks the first instance of a non-recurrent model achieving comparable success to strong recurrent models on major language tasks."
    }
  ],
  "scoring": [
    {
      "title": "Language Modeling with Gated Convolutional Networks",
      "url": "/research/2017/09/08/Language-Modeling-with-Gated-Convolutional-Networks.html",
      "date": "September 8, 2017",
      "summary": "This paper introduces a novel language modeling approach using stacked convolutions that enable efficient parallel processing, in contrast to the traditionally used recurrent neural networks known for handling unbounded context. The proposed method features a simplified gating mechanism that outperforms previous models and demonstrates superior performance on the WikiText-103 benchmark, showing its capability to manage long-term dependencies. It also delivers competitive results on the Google Billion Words benchmark and significantly reduces sentence scoring latency compared to recurrent models. This marks the first instance of a non-recurrent model achieving comparable success to strong recurrent models on major language tasks."
    },
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    }
  ],
  "latency": [
    {
      "title": "Language Modeling with Gated Convolutional Networks",
      "url": "/research/2017/09/08/Language-Modeling-with-Gated-Convolutional-Networks.html",
      "date": "September 8, 2017",
      "summary": "This paper introduces a novel language modeling approach using stacked convolutions that enable efficient parallel processing, in contrast to the traditionally used recurrent neural networks known for handling unbounded context. The proposed method features a simplified gating mechanism that outperforms previous models and demonstrates superior performance on the WikiText-103 benchmark, showing its capability to manage long-term dependencies. It also delivers competitive results on the Google Billion Words benchmark and significantly reduces sentence scoring latency compared to recurrent models. This marks the first instance of a non-recurrent model achieving comparable success to strong recurrent models on major language tasks."
    },
    {
      "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
      "url": "/research/2024/02/27/The-Era-of-1-bit-LLMs-All-Large-Language-Models-are-in-1.58-Bits.html",
      "date": "February 27, 2024",
      "summary": "Recent research introduces BitNet b1.58, a ternary (values of -1, 0, 1) 1-bit Large Language Model (LLM) that matches traditional full-precision LLMs in performance and perplexity but is significantly more efficient in terms of latency, memory, throughput, and energy. This work establishes a new scaling law and training approach for future high-performance, cost-effective LLMs, while also facilitating the design of specialized hardware optimized for 1-bit LLMs."
    }
  ],
  "mark": [
    {
      "title": "Language Modeling with Gated Convolutional Networks",
      "url": "/research/2017/09/08/Language-Modeling-with-Gated-Convolutional-Networks.html",
      "date": "September 8, 2017",
      "summary": "This paper introduces a novel language modeling approach using stacked convolutions that enable efficient parallel processing, in contrast to the traditionally used recurrent neural networks known for handling unbounded context. The proposed method features a simplified gating mechanism that outperforms previous models and demonstrates superior performance on the WikiText-103 benchmark, showing its capability to manage long-term dependencies. It also delivers competitive results on the Google Billion Words benchmark and significantly reduces sentence scoring latency compared to recurrent models. This marks the first instance of a non-recurrent model achieving comparable success to strong recurrent models on major language tasks."
    }
  ],
  "expedite": [
    {
      "title": "Large Batch Training of Convolutional Networks",
      "url": "/research/2017/09/13/Large-Batch-Training-of-Convolutional-Networks.html",
      "date": "September 13, 2017",
      "summary": "To expedite the training of large convolutional networks, computational units are added and trained with data-parallel synchronous Stochastic Gradient Descent (SGD) across units, increasing batch size with node count. However, larger batch sizes can reduce model accuracy. The existing method of large batch training—linear learning rate scaling with warm-up—is not universally effective and may cause training divergence. To address these challenges, we introduce a novel training algorithm, Layer-wise Adaptive Rate Scaling (LARS), enabling us to train Alexnet with a batch size of 8K and Resnet-50 with a batch size of 32K, without compromising accuracy."
    }
  ],
  "count": [
    {
      "title": "Large Batch Training of Convolutional Networks",
      "url": "/research/2017/09/13/Large-Batch-Training-of-Convolutional-Networks.html",
      "date": "September 13, 2017",
      "summary": "To expedite the training of large convolutional networks, computational units are added and trained with data-parallel synchronous Stochastic Gradient Descent (SGD) across units, increasing batch size with node count. However, larger batch sizes can reduce model accuracy. The existing method of large batch training—linear learning rate scaling with warm-up—is not universally effective and may cause training divergence. To address these challenges, we introduce a novel training algorithm, Layer-wise Adaptive Rate Scaling (LARS), enabling us to train Alexnet with a batch size of 8K and Resnet-50 with a batch size of 32K, without compromising accuracy."
    }
  ],
  "cause": [
    {
      "title": "Large Batch Training of Convolutional Networks",
      "url": "/research/2017/09/13/Large-Batch-Training-of-Convolutional-Networks.html",
      "date": "September 13, 2017",
      "summary": "To expedite the training of large convolutional networks, computational units are added and trained with data-parallel synchronous Stochastic Gradient Descent (SGD) across units, increasing batch size with node count. However, larger batch sizes can reduce model accuracy. The existing method of large batch training—linear learning rate scaling with warm-up—is not universally effective and may cause training divergence. To address these challenges, we introduce a novel training algorithm, Layer-wise Adaptive Rate Scaling (LARS), enabling us to train Alexnet with a batch size of 8K and Resnet-50 with a batch size of 32K, without compromising accuracy."
    },
    {
      "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
      "url": "/research/2023/11/30/Dichotomy-of-Early-and-Late-Phase-Implicit-Biases-Can-Provably-Induce-Grokking.html",
      "date": "November 30, 2023",
      "summary": "Power et al. (2022) discovered that neural networks initially memorize arithmetic tasks, achieving perfect training accuracy but poor test accuracy, then suddenly achieve perfect test accuracy after extended training. This study explains this grokking phenomenon with the theory that early and late phase implicit biases cause this shift. Training homogeneous neural networks with large initialization and small weight decay on classification and regression tasks results in a prolonged period where the network acts like a kernel predictor, followed by a sudden shift to min-norm/max-margin predictors, significantly improving test accuracy."
    }
  ],
  "divergence": [
    {
      "title": "Large Batch Training of Convolutional Networks",
      "url": "/research/2017/09/13/Large-Batch-Training-of-Convolutional-Networks.html",
      "date": "September 13, 2017",
      "summary": "To expedite the training of large convolutional networks, computational units are added and trained with data-parallel synchronous Stochastic Gradient Descent (SGD) across units, increasing batch size with node count. However, larger batch sizes can reduce model accuracy. The existing method of large batch training—linear learning rate scaling with warm-up—is not universally effective and may cause training divergence. To address these challenges, we introduce a novel training algorithm, Layer-wise Adaptive Rate Scaling (LARS), enabling us to train Alexnet with a batch size of 8K and Resnet-50 with a batch size of 32K, without compromising accuracy."
    },
    {
      "title": "Sharpness-Aware Training for Free",
      "url": "/research/2023/03/02/Sharpness-Aware-Training-for-Free.html",
      "date": "March 2, 2023",
      "summary": "Modern deep neural networks (DNNs) excel in performance but often suffer from over-parameterization, leading to increased generalization error without tailored training strategies. Sharpness-Aware Minimization (SAM) has proven effective in reducing generalization error by minimizing sharpness in the loss landscape. However, SAM incurs a significant computational overhead. This paper introduces Sharpness-Aware Training for Free (SAF), which mitigates sharpness at nearly zero additional computational cost. SAF achieves this by preventing sudden drops in loss within sharp local minima during weight updates. A novel trajectory loss, based on KL-divergence between current and past DNN outputs, replaces SAM's sharpness measure, guiding convergence towards flat minima for enhanced generalization. Empirical results demonstrate SAF's effectiveness on ImageNet with comparable computational efficiency to the base optimizer."
    }
  ],
  "lars": [
    {
      "title": "Large Batch Training of Convolutional Networks",
      "url": "/research/2017/09/13/Large-Batch-Training-of-Convolutional-Networks.html",
      "date": "September 13, 2017",
      "summary": "To expedite the training of large convolutional networks, computational units are added and trained with data-parallel synchronous Stochastic Gradient Descent (SGD) across units, increasing batch size with node count. However, larger batch sizes can reduce model accuracy. The existing method of large batch training—linear learning rate scaling with warm-up—is not universally effective and may cause training divergence. To address these challenges, we introduce a novel training algorithm, Layer-wise Adaptive Rate Scaling (LARS), enabling us to train Alexnet with a batch size of 8K and Resnet-50 with a batch size of 32K, without compromising accuracy."
    }
  ],
  "with": [
    {
      "title": "Large Batch Training of Convolutional Networks",
      "url": "/research/2017/09/13/Large-Batch-Training-of-Convolutional-Networks.html",
      "date": "September 13, 2017",
      "summary": "To expedite the training of large convolutional networks, computational units are added and trained with data-parallel synchronous Stochastic Gradient Descent (SGD) across units, increasing batch size with node count. However, larger batch sizes can reduce model accuracy. The existing method of large batch training—linear learning rate scaling with warm-up—is not universally effective and may cause training divergence. To address these challenges, we introduce a novel training algorithm, Layer-wise Adaptive Rate Scaling (LARS), enabling us to train Alexnet with a batch size of 8K and Resnet-50 with a batch size of 32K, without compromising accuracy."
    }
  ],
  "plugin": [
    {
      "title": "Learning Rate Perturbation: A Generic Plugin of Learning Rate schedule towards Flatter Local Minima",
      "url": "/research/2022/08/25/Learning-Rate-Perturbation-A-Generic-Plugin-of-Learning-Rate-schedule-towards-Flatter-Local-Minima.html",
      "date": "August 25, 2022",
      "summary": "Learning rate is crucial in neural network training, yet existing schedules lack theoretical backing, often leading to suboptimal choices made through trial and error. To address this, we propose LEAP, a plugin enhancing various learning rate schedules by introducing perturbations. This simple yet effective strategy favors flat minima, ensuring better generalization. Extensive experiments demonstrate LEAP's ability to improve performance across diverse datasets and learning rate schedules, including constant ones."
    }
  ],
  "flatter": [
    {
      "title": "Learning Rate Perturbation: A Generic Plugin of Learning Rate schedule towards Flatter Local Minima",
      "url": "/research/2022/08/25/Learning-Rate-Perturbation-A-Generic-Plugin-of-Learning-Rate-schedule-towards-Flatter-Local-Minima.html",
      "date": "August 25, 2022",
      "summary": "Learning rate is crucial in neural network training, yet existing schedules lack theoretical backing, often leading to suboptimal choices made through trial and error. To address this, we propose LEAP, a plugin enhancing various learning rate schedules by introducing perturbations. This simple yet effective strategy favors flat minima, ensuring better generalization. Extensive experiments demonstrate LEAP's ability to improve performance across diverse datasets and learning rate schedules, including constant ones."
    },
    {
      "title": "On the Maximum Hessian Eigenvalue and Generalization",
      "url": "/research/2023/05/23/On-the-Maximum-Hessian-Eigenvalue-and-Generalization.html",
      "date": "May 23, 2023",
      "summary": "This study investigates the relationship between training interventions and the generalization of deep networks. While previous research suggests that flatter solutions generalize better than sharper ones, particularly measured by λmax, the largest eigenvalue of the Hessian of the loss, this paper challenges this notion. Through experiments, we demonstrate that larger learning rates reduce λmax for all batch sizes but do not consistently improve generalization. Additionally, scaling batch size and learning rate simultaneously can change λmax without affecting generalization. Sharpness-Aware Minimization (SAM) produces smaller λmax but does not consistently enhance generalization, especially with larger batch sizes. Excessive dropout probabilities can degrade generalization, despite promoting smaller λmax. Batch normalization, while not consistently reducing λmax, still improves generalization. These findings question λmax's role in explaining generalization in neural networks, highlighting the limits of its explanatory power."
    },
    {
      "title": "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization",
      "url": "/research/2023/07/04/Gradient-Norm-Aware-Minimization-Seeks-First-Order-Flatness-and-Improves-Generalization.html",
      "date": "July 4, 2023",
      "summary": "Recent advancements have highlighted the effectiveness of flat minima in enhancing generalization, particularly through Sharpness-Aware Minimization (SAM). However, existing definitions of flatness, such as zeroth-order flatness, have limitations in discerning between minima with low and high generalization errors. To address this, we propose first-order flatness, which considers maximal gradient norm within a perturbation radius. We introduce Gradient norm Aware Minimization (GAM) as a novel training approach to achieve uniformly small curvature across all directions. Experimental results demonstrate GAM's ability to enhance generalization compared to standard optimizers like SGD and AdamW across various datasets and networks. Moreover, GAM facilitates SAM in identifying flatter minima, leading to improved generalization."
    }
  ],
  "backing": [
    {
      "title": "Learning Rate Perturbation: A Generic Plugin of Learning Rate schedule towards Flatter Local Minima",
      "url": "/research/2022/08/25/Learning-Rate-Perturbation-A-Generic-Plugin-of-Learning-Rate-schedule-towards-Flatter-Local-Minima.html",
      "date": "August 25, 2022",
      "summary": "Learning rate is crucial in neural network training, yet existing schedules lack theoretical backing, often leading to suboptimal choices made through trial and error. To address this, we propose LEAP, a plugin enhancing various learning rate schedules by introducing perturbations. This simple yet effective strategy favors flat minima, ensuring better generalization. Extensive experiments demonstrate LEAP's ability to improve performance across diverse datasets and learning rate schedules, including constant ones."
    },
    {
      "title": "Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes",
      "url": "/research/2017/11/28/Towards-Understanding-Generalization-of-Deep-Learning-Perspective-of-Loss-Landscapes.html",
      "date": "November 28, 2017",
      "summary": "This study examines why deep learning models, despite having more parameters than training samples, generalize well. It highlights the importance of the loss function's landscape, showing that areas leading to better generalization (good minima) are more prevalent than those leading to poor outcomes. This predominance facilitates the convergence of optimization methods to these beneficial minima. The research provides theoretical backing by analyzing 2-layer neural networks, noting that solutions with better generalization have a smaller Hessian matrix norm. For deeper networks, extensive numerical data supports these conclusions."
    }
  ],
  "choice": [
    {
      "title": "Learning Rate Perturbation: A Generic Plugin of Learning Rate schedule towards Flatter Local Minima",
      "url": "/research/2022/08/25/Learning-Rate-Perturbation-A-Generic-Plugin-of-Learning-Rate-schedule-towards-Flatter-Local-Minima.html",
      "date": "August 25, 2022",
      "summary": "Learning rate is crucial in neural network training, yet existing schedules lack theoretical backing, often leading to suboptimal choices made through trial and error. To address this, we propose LEAP, a plugin enhancing various learning rate schedules by introducing perturbations. This simple yet effective strategy favors flat minima, ensuring better generalization. Extensive experiments demonstrate LEAP's ability to improve performance across diverse datasets and learning rate schedules, including constant ones."
    }
  ],
  "leap": [
    {
      "title": "Learning Rate Perturbation: A Generic Plugin of Learning Rate schedule towards Flatter Local Minima",
      "url": "/research/2022/08/25/Learning-Rate-Perturbation-A-Generic-Plugin-of-Learning-Rate-schedule-towards-Flatter-Local-Minima.html",
      "date": "August 25, 2022",
      "summary": "Learning rate is crucial in neural network training, yet existing schedules lack theoretical backing, often leading to suboptimal choices made through trial and error. To address this, we propose LEAP, a plugin enhancing various learning rate schedules by introducing perturbations. This simple yet effective strategy favors flat minima, ensuring better generalization. Extensive experiments demonstrate LEAP's ability to improve performance across diverse datasets and learning rate schedules, including constant ones."
    }
  ],
  "amax": [
    {
      "title": "On the Maximum Hessian Eigenvalue and Generalization",
      "url": "/research/2023/05/23/On-the-Maximum-Hessian-Eigenvalue-and-Generalization.html",
      "date": "May 23, 2023",
      "summary": "This study investigates the relationship between training interventions and the generalization of deep networks. While previous research suggests that flatter solutions generalize better than sharper ones, particularly measured by λmax, the largest eigenvalue of the Hessian of the loss, this paper challenges this notion. Through experiments, we demonstrate that larger learning rates reduce λmax for all batch sizes but do not consistently improve generalization. Additionally, scaling batch size and learning rate simultaneously can change λmax without affecting generalization. Sharpness-Aware Minimization (SAM) produces smaller λmax but does not consistently enhance generalization, especially with larger batch sizes. Excessive dropout probabilities can degrade generalization, despite promoting smaller λmax. Batch normalization, while not consistently reducing λmax, still improves generalization. These findings question λmax's role in explaining generalization in neural networks, highlighting the limits of its explanatory power."
    }
  ],
  "notion": [
    {
      "title": "On the Maximum Hessian Eigenvalue and Generalization",
      "url": "/research/2023/05/23/On-the-Maximum-Hessian-Eigenvalue-and-Generalization.html",
      "date": "May 23, 2023",
      "summary": "This study investigates the relationship between training interventions and the generalization of deep networks. While previous research suggests that flatter solutions generalize better than sharper ones, particularly measured by λmax, the largest eigenvalue of the Hessian of the loss, this paper challenges this notion. Through experiments, we demonstrate that larger learning rates reduce λmax for all batch sizes but do not consistently improve generalization. Additionally, scaling batch size and learning rate simultaneously can change λmax without affecting generalization. Sharpness-Aware Minimization (SAM) produces smaller λmax but does not consistently enhance generalization, especially with larger batch sizes. Excessive dropout probabilities can degrade generalization, despite promoting smaller λmax. Batch normalization, while not consistently reducing λmax, still improves generalization. These findings question λmax's role in explaining generalization in neural networks, highlighting the limits of its explanatory power."
    },
    {
      "title": "Fine-tuning with Very Large Dropout",
      "url": "/research/2024/03/01/Fine-tuning-with-Very-Large-Dropout.html",
      "date": "March 1, 2024",
      "summary": "This study challenges the notion that machine learning practices assume training and testing data have the same distribution. It explores the effectiveness of high dropout rates, as opposed to ensemble techniques, in developing rich data representations suitable for multiple distribution scenarios. These representations surpass those achieved by traditional in-distribution performance regularization and the implicit sparsity induced by common stochastic gradient methods. While training deep networks from scratch with high dropout rates is impractical, fine-tuning pre-trained models under these conditions is feasible and yields better out-of-distribution performance than ensembles and model averaging techniques like model soups. This finding is significant due to the growing relevance of fine-tuning with large pre-trained models, offering insights into the nature of rich representations and the linear characteristics of fine-tuning large networks with small datasets."
    },
    {
      "title": "How Does Sharpness-Aware Minimization Minimize Sharpness?",
      "url": "/research/2023/01/05/How-Does-Sharpness-Aware-Minimization-Minimize-Sharpness.html",
      "date": "January 5, 2023",
      "summary": "Sharpness-Aware Minimization (SAM) enhances deep neural networks' generalization across various settings. Although SAM aims to penalize model sharpness using a computationally efficient approach, its exact operational mechanism and the sharpness notion it regularizes remain unclear, partly due to differing sharpness concepts used in its theoretical framework and empirical validation. This study identifies the precise sharpness concept SAM regulates and explains its mechanism. It reveals that the combined effect of SAM's two-step approximations, despite being individually misleading, correctly improves generalization when using full-batch gradients. Moreover, we demonstrate that the stochastic SAM version indeed regularizes a third sharpness notion, aligning more closely with practical performance. This effectiveness is attributed to the gradient's alignment with the Hessian's top eigenvector under SAM."
    }
  ],
  "promote": [
    {
      "title": "On the Maximum Hessian Eigenvalue and Generalization",
      "url": "/research/2023/05/23/On-the-Maximum-Hessian-Eigenvalue-and-Generalization.html",
      "date": "May 23, 2023",
      "summary": "This study investigates the relationship between training interventions and the generalization of deep networks. While previous research suggests that flatter solutions generalize better than sharper ones, particularly measured by λmax, the largest eigenvalue of the Hessian of the loss, this paper challenges this notion. Through experiments, we demonstrate that larger learning rates reduce λmax for all batch sizes but do not consistently improve generalization. Additionally, scaling batch size and learning rate simultaneously can change λmax without affecting generalization. Sharpness-Aware Minimization (SAM) produces smaller λmax but does not consistently enhance generalization, especially with larger batch sizes. Excessive dropout probabilities can degrade generalization, despite promoting smaller λmax. Batch normalization, while not consistently reducing λmax, still improves generalization. These findings question λmax's role in explaining generalization in neural networks, highlighting the limits of its explanatory power."
    }
  ],
  "power": [
    {
      "title": "On the Maximum Hessian Eigenvalue and Generalization",
      "url": "/research/2023/05/23/On-the-Maximum-Hessian-Eigenvalue-and-Generalization.html",
      "date": "May 23, 2023",
      "summary": "This study investigates the relationship between training interventions and the generalization of deep networks. While previous research suggests that flatter solutions generalize better than sharper ones, particularly measured by λmax, the largest eigenvalue of the Hessian of the loss, this paper challenges this notion. Through experiments, we demonstrate that larger learning rates reduce λmax for all batch sizes but do not consistently improve generalization. Additionally, scaling batch size and learning rate simultaneously can change λmax without affecting generalization. Sharpness-Aware Minimization (SAM) produces smaller λmax but does not consistently enhance generalization, especially with larger batch sizes. Excessive dropout probabilities can degrade generalization, despite promoting smaller λmax. Batch normalization, while not consistently reducing λmax, still improves generalization. These findings question λmax's role in explaining generalization in neural networks, highlighting the limits of its explanatory power."
    },
    {
      "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
      "url": "/research/2023/11/30/Dichotomy-of-Early-and-Late-Phase-Implicit-Biases-Can-Provably-Induce-Grokking.html",
      "date": "November 30, 2023",
      "summary": "Power et al. (2022) discovered that neural networks initially memorize arithmetic tasks, achieving perfect training accuracy but poor test accuracy, then suddenly achieve perfect test accuracy after extended training. This study explains this grokking phenomenon with the theory that early and late phase implicit biases cause this shift. Training homogeneous neural networks with large initialization and small weight decay on classification and regression tasks results in a prolonged period where the network acts like a kernel predictor, followed by a sudden shift to min-norm/max-margin predictors, significantly improving test accuracy."
    }
  ],
  "langevin": [
    {
      "title": "Optimal learning rate schedules in high-dimensional non-convex optimization problems",
      "url": "/research/2022/02/09/Optimal-learning-rate-schedules-in-high-dimensional-non-convex-optimization-problems.html",
      "date": "February 9, 2022",
      "summary": "This paper explores the role of learning rate schedules in high-dimensional and non-convex optimization problems, focusing on Langevin optimization with a decaying learning rate. Analyzing models with Gaussian random functions on N-dimensional spheres, the study reveals that to accelerate optimization without getting trapped in saddles, a decay rate β < 1 is optimal, contrary to convex settings where β = 1 is preferred. Introducing a signal recovery component, the dynamics involve an exploration phase navigating through rough landscape parts and a convergence phase entering convex basins. It's found optimal to maintain a large learning rate during exploration to swiftly exit non-convex regions, then transition to β = 1 for rapid convergence to the solution. These findings are validated in a neural network regression task."
    }
  ],
  "sphere": [
    {
      "title": "Optimal learning rate schedules in high-dimensional non-convex optimization problems",
      "url": "/research/2022/02/09/Optimal-learning-rate-schedules-in-high-dimensional-non-convex-optimization-problems.html",
      "date": "February 9, 2022",
      "summary": "This paper explores the role of learning rate schedules in high-dimensional and non-convex optimization problems, focusing on Langevin optimization with a decaying learning rate. Analyzing models with Gaussian random functions on N-dimensional spheres, the study reveals that to accelerate optimization without getting trapped in saddles, a decay rate β < 1 is optimal, contrary to convex settings where β = 1 is preferred. Introducing a signal recovery component, the dynamics involve an exploration phase navigating through rough landscape parts and a convergence phase entering convex basins. It's found optimal to maintain a large learning rate during exploration to swiftly exit non-convex regions, then transition to β = 1 for rapid convergence to the solution. These findings are validated in a neural network regression task."
    }
  ],
  "accelerate": [
    {
      "title": "Optimal learning rate schedules in high-dimensional non-convex optimization problems",
      "url": "/research/2022/02/09/Optimal-learning-rate-schedules-in-high-dimensional-non-convex-optimization-problems.html",
      "date": "February 9, 2022",
      "summary": "This paper explores the role of learning rate schedules in high-dimensional and non-convex optimization problems, focusing on Langevin optimization with a decaying learning rate. Analyzing models with Gaussian random functions on N-dimensional spheres, the study reveals that to accelerate optimization without getting trapped in saddles, a decay rate β < 1 is optimal, contrary to convex settings where β = 1 is preferred. Introducing a signal recovery component, the dynamics involve an exploration phase navigating through rough landscape parts and a convergence phase entering convex basins. It's found optimal to maintain a large learning rate during exploration to swiftly exit non-convex regions, then transition to β = 1 for rapid convergence to the solution. These findings are validated in a neural network regression task."
    },
    {
      "title": "FNet: Mixing Tokens with Fourier Transforms",
      "url": "/research/2022/05/26/FNet-Mixing-Tokens-with-Fourier-Transforms.html",
      "date": "May 26, 2022",
      "summary": "We demonstrate that Transformer encoder architectures can be accelerated with minimal impact on accuracy by substituting self-attention sublayers with simple linear transformations. Remarkably, using a standard Fourier Transform instead of the self-attention sublayer in a Transformer encoder achieves 92-97% of BERT models' accuracy on the GLUE benchmark, while training 80% faster on GPUs and 70% faster on TPUs for standard 512 input lengths. Our FNet model significantly outperforms in speed at longer input lengths, matching the accuracy of the most accurate models on the Long Range Arena benchmark and surpassing the fastest models in speed across all sequence lengths on GPUs and shorter lengths on TPUs. Additionally, FNet is lightweight in memory use and exceptionally efficient in smaller sizes, outperforming Transformer models under the same speed and accuracy constraints."
    }
  ],
  "trap": [
    {
      "title": "Optimal learning rate schedules in high-dimensional non-convex optimization problems",
      "url": "/research/2022/02/09/Optimal-learning-rate-schedules-in-high-dimensional-non-convex-optimization-problems.html",
      "date": "February 9, 2022",
      "summary": "This paper explores the role of learning rate schedules in high-dimensional and non-convex optimization problems, focusing on Langevin optimization with a decaying learning rate. Analyzing models with Gaussian random functions on N-dimensional spheres, the study reveals that to accelerate optimization without getting trapped in saddles, a decay rate β < 1 is optimal, contrary to convex settings where β = 1 is preferred. Introducing a signal recovery component, the dynamics involve an exploration phase navigating through rough landscape parts and a convergence phase entering convex basins. It's found optimal to maintain a large learning rate during exploration to swiftly exit non-convex regions, then transition to β = 1 for rapid convergence to the solution. These findings are validated in a neural network regression task."
    }
  ],
  "saddle": [
    {
      "title": "Optimal learning rate schedules in high-dimensional non-convex optimization problems",
      "url": "/research/2022/02/09/Optimal-learning-rate-schedules-in-high-dimensional-non-convex-optimization-problems.html",
      "date": "February 9, 2022",
      "summary": "This paper explores the role of learning rate schedules in high-dimensional and non-convex optimization problems, focusing on Langevin optimization with a decaying learning rate. Analyzing models with Gaussian random functions on N-dimensional spheres, the study reveals that to accelerate optimization without getting trapped in saddles, a decay rate β < 1 is optimal, contrary to convex settings where β = 1 is preferred. Introducing a signal recovery component, the dynamics involve an exploration phase navigating through rough landscape parts and a convergence phase entering convex basins. It's found optimal to maintain a large learning rate during exploration to swiftly exit non-convex regions, then transition to β = 1 for rapid convergence to the solution. These findings are validated in a neural network regression task."
    },
    {
      "title": "The loss landscape of deep linear neural networks: a second-order analysis",
      "url": "/research/2022/03/11/The-loss-landscape-of-deep-linear-neural-networks-a-second-order-analysis.html",
      "date": "March 11, 2022",
      "summary": "This study investigates the optimization landscape of deep linear neural networks with square loss, focusing on the role of non-strict saddle points in algorithm dynamics, which has been previously understudied. the authors conduct a comprehensive second-order analysis, identifying global minimizers, strict, and non-strict saddle points among all critical points, alongside their critical values. Our findings, based on conditions related to the ranks of partial matrix products, contribute insights into global convergence and implicit regularization observed in optimizing linear neural networks. Additionally, the authors offer an explicit parameterization of global minimizers and identify extensive sets of strict and non-strict saddle points."
    }
  ],
  "prefer": [
    {
      "title": "Optimal learning rate schedules in high-dimensional non-convex optimization problems",
      "url": "/research/2022/02/09/Optimal-learning-rate-schedules-in-high-dimensional-non-convex-optimization-problems.html",
      "date": "February 9, 2022",
      "summary": "This paper explores the role of learning rate schedules in high-dimensional and non-convex optimization problems, focusing on Langevin optimization with a decaying learning rate. Analyzing models with Gaussian random functions on N-dimensional spheres, the study reveals that to accelerate optimization without getting trapped in saddles, a decay rate β < 1 is optimal, contrary to convex settings where β = 1 is preferred. Introducing a signal recovery component, the dynamics involve an exploration phase navigating through rough landscape parts and a convergence phase entering convex basins. It's found optimal to maintain a large learning rate during exploration to swiftly exit non-convex regions, then transition to β = 1 for rapid convergence to the solution. These findings are validated in a neural network regression task."
    }
  ],
  "recovery": [
    {
      "title": "Optimal learning rate schedules in high-dimensional non-convex optimization problems",
      "url": "/research/2022/02/09/Optimal-learning-rate-schedules-in-high-dimensional-non-convex-optimization-problems.html",
      "date": "February 9, 2022",
      "summary": "This paper explores the role of learning rate schedules in high-dimensional and non-convex optimization problems, focusing on Langevin optimization with a decaying learning rate. Analyzing models with Gaussian random functions on N-dimensional spheres, the study reveals that to accelerate optimization without getting trapped in saddles, a decay rate β < 1 is optimal, contrary to convex settings where β = 1 is preferred. Introducing a signal recovery component, the dynamics involve an exploration phase navigating through rough landscape parts and a convergence phase entering convex basins. It's found optimal to maintain a large learning rate during exploration to swiftly exit non-convex regions, then transition to β = 1 for rapid convergence to the solution. These findings are validated in a neural network regression task."
    }
  ],
  "dynamic": [
    {
      "title": "Optimal learning rate schedules in high-dimensional non-convex optimization problems",
      "url": "/research/2022/02/09/Optimal-learning-rate-schedules-in-high-dimensional-non-convex-optimization-problems.html",
      "date": "February 9, 2022",
      "summary": "This paper explores the role of learning rate schedules in high-dimensional and non-convex optimization problems, focusing on Langevin optimization with a decaying learning rate. Analyzing models with Gaussian random functions on N-dimensional spheres, the study reveals that to accelerate optimization without getting trapped in saddles, a decay rate β < 1 is optimal, contrary to convex settings where β = 1 is preferred. Introducing a signal recovery component, the dynamics involve an exploration phase navigating through rough landscape parts and a convergence phase entering convex basins. It's found optimal to maintain a large learning rate during exploration to swiftly exit non-convex regions, then transition to β = 1 for rapid convergence to the solution. These findings are validated in a neural network regression task."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "url": "/research/2014/02/19/Exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks.html",
      "date": "February 19, 2014",
      "summary": "This study aims to enhance our comprehension of deep learning dynamics by analyzing deep linear neural networks. Despite their linear input-output mapping, these networks display nonlinear gradient descent dynamics, resulting in phenomena like prolonged plateaus and swift transitions to improved solutions. Through analytical exploration, we reveal that as network depth increases indefinitely, learning speed can remain finite under specific initial conditions. Unsupervised pretraining, under certain data conditions, can discover these favorable initial conditions, unlike random Gaussian initializations. We introduce a novel class of random orthogonal initial conditions that, akin to unsupervised pretraining, enable depth-independent learning. These conditions also ensure gradient propagation in deep nonlinear networks, particularly when operating at the edge of chaos."
    },
    {
      "title": "The loss landscape of deep linear neural networks: a second-order analysis",
      "url": "/research/2022/03/11/The-loss-landscape-of-deep-linear-neural-networks-a-second-order-analysis.html",
      "date": "March 11, 2022",
      "summary": "This study investigates the optimization landscape of deep linear neural networks with square loss, focusing on the role of non-strict saddle points in algorithm dynamics, which has been previously understudied. the authors conduct a comprehensive second-order analysis, identifying global minimizers, strict, and non-strict saddle points among all critical points, alongside their critical values. Our findings, based on conditions related to the ranks of partial matrix products, contribute insights into global convergence and implicit regularization observed in optimizing linear neural networks. Additionally, the authors offer an explicit parameterization of global minimizers and identify extensive sets of strict and non-strict saddle points."
    }
  ],
  "navigating": [
    {
      "title": "Optimal learning rate schedules in high-dimensional non-convex optimization problems",
      "url": "/research/2022/02/09/Optimal-learning-rate-schedules-in-high-dimensional-non-convex-optimization-problems.html",
      "date": "February 9, 2022",
      "summary": "This paper explores the role of learning rate schedules in high-dimensional and non-convex optimization problems, focusing on Langevin optimization with a decaying learning rate. Analyzing models with Gaussian random functions on N-dimensional spheres, the study reveals that to accelerate optimization without getting trapped in saddles, a decay rate β < 1 is optimal, contrary to convex settings where β = 1 is preferred. Introducing a signal recovery component, the dynamics involve an exploration phase navigating through rough landscape parts and a convergence phase entering convex basins. It's found optimal to maintain a large learning rate during exploration to swiftly exit non-convex regions, then transition to β = 1 for rapid convergence to the solution. These findings are validated in a neural network regression task."
    }
  ],
  "enter": [
    {
      "title": "Optimal learning rate schedules in high-dimensional non-convex optimization problems",
      "url": "/research/2022/02/09/Optimal-learning-rate-schedules-in-high-dimensional-non-convex-optimization-problems.html",
      "date": "February 9, 2022",
      "summary": "This paper explores the role of learning rate schedules in high-dimensional and non-convex optimization problems, focusing on Langevin optimization with a decaying learning rate. Analyzing models with Gaussian random functions on N-dimensional spheres, the study reveals that to accelerate optimization without getting trapped in saddles, a decay rate β < 1 is optimal, contrary to convex settings where β = 1 is preferred. Introducing a signal recovery component, the dynamics involve an exploration phase navigating through rough landscape parts and a convergence phase entering convex basins. It's found optimal to maintain a large learning rate during exploration to swiftly exit non-convex regions, then transition to β = 1 for rapid convergence to the solution. These findings are validated in a neural network regression task."
    }
  ],
  "basin": [
    {
      "title": "Optimal learning rate schedules in high-dimensional non-convex optimization problems",
      "url": "/research/2022/02/09/Optimal-learning-rate-schedules-in-high-dimensional-non-convex-optimization-problems.html",
      "date": "February 9, 2022",
      "summary": "This paper explores the role of learning rate schedules in high-dimensional and non-convex optimization problems, focusing on Langevin optimization with a decaying learning rate. Analyzing models with Gaussian random functions on N-dimensional spheres, the study reveals that to accelerate optimization without getting trapped in saddles, a decay rate β < 1 is optimal, contrary to convex settings where β = 1 is preferred. Introducing a signal recovery component, the dynamics involve an exploration phase navigating through rough landscape parts and a convergence phase entering convex basins. It's found optimal to maintain a large learning rate during exploration to swiftly exit non-convex regions, then transition to β = 1 for rapid convergence to the solution. These findings are validated in a neural network regression task."
    }
  ],
  "exit": [
    {
      "title": "Optimal learning rate schedules in high-dimensional non-convex optimization problems",
      "url": "/research/2022/02/09/Optimal-learning-rate-schedules-in-high-dimensional-non-convex-optimization-problems.html",
      "date": "February 9, 2022",
      "summary": "This paper explores the role of learning rate schedules in high-dimensional and non-convex optimization problems, focusing on Langevin optimization with a decaying learning rate. Analyzing models with Gaussian random functions on N-dimensional spheres, the study reveals that to accelerate optimization without getting trapped in saddles, a decay rate β < 1 is optimal, contrary to convex settings where β = 1 is preferred. Introducing a signal recovery component, the dynamics involve an exploration phase navigating through rough landscape parts and a convergence phase entering convex basins. It's found optimal to maintain a large learning rate during exploration to swiftly exit non-convex regions, then transition to β = 1 for rapid convergence to the solution. These findings are validated in a neural network regression task."
    }
  ],
  "root": [
    {
      "title": "Root Mean Square Layer Normalization",
      "url": "/research/2019/10/16/Root-Mean-Square-Layer-Normalization.html",
      "date": "October 16, 2019",
      "summary": "Layer normalization (LayerNorm) enhances deep neural network stability and convergence by re-centering and re-scaling inputs and weight matrices. However, its computational overhead slows networks, particularly RNNs. We introduce RMSNorm, which replaces re-centering with root mean square (RMS) regularization. RMSNorm maintains re-scaling invariance and adapts learning rates implicitly, while being computationally simpler than LayerNorm. We also propose partial RMSNorm (pRMSNorm), estimating RMS from a subset of inputs. Empirical results across various tasks and architectures demonstrate that RMSNorm achieves comparable performance to LayerNorm while reducing running time by 7%∼64%."
    }
  ],
  "rescale": [
    {
      "title": "Root Mean Square Layer Normalization",
      "url": "/research/2019/10/16/Root-Mean-Square-Layer-Normalization.html",
      "date": "October 16, 2019",
      "summary": "Layer normalization (LayerNorm) enhances deep neural network stability and convergence by re-centering and re-scaling inputs and weight matrices. However, its computational overhead slows networks, particularly RNNs. We introduce RMSNorm, which replaces re-centering with root mean square (RMS) regularization. RMSNorm maintains re-scaling invariance and adapts learning rates implicitly, while being computationally simpler than LayerNorm. We also propose partial RMSNorm (pRMSNorm), estimating RMS from a subset of inputs. Empirical results across various tasks and architectures demonstrate that RMSNorm achieves comparable performance to LayerNorm while reducing running time by 7%∼64%."
    }
  ],
  "rmsnorm": [
    {
      "title": "Root Mean Square Layer Normalization",
      "url": "/research/2019/10/16/Root-Mean-Square-Layer-Normalization.html",
      "date": "October 16, 2019",
      "summary": "Layer normalization (LayerNorm) enhances deep neural network stability and convergence by re-centering and re-scaling inputs and weight matrices. However, its computational overhead slows networks, particularly RNNs. We introduce RMSNorm, which replaces re-centering with root mean square (RMS) regularization. RMSNorm maintains re-scaling invariance and adapts learning rates implicitly, while being computationally simpler than LayerNorm. We also propose partial RMSNorm (pRMSNorm), estimating RMS from a subset of inputs. Empirical results across various tasks and architectures demonstrate that RMSNorm achieves comparable performance to LayerNorm while reducing running time by 7%∼64%."
    }
  ],
  "rms": [
    {
      "title": "Root Mean Square Layer Normalization",
      "url": "/research/2019/10/16/Root-Mean-Square-Layer-Normalization.html",
      "date": "October 16, 2019",
      "summary": "Layer normalization (LayerNorm) enhances deep neural network stability and convergence by re-centering and re-scaling inputs and weight matrices. However, its computational overhead slows networks, particularly RNNs. We introduce RMSNorm, which replaces re-centering with root mean square (RMS) regularization. RMSNorm maintains re-scaling invariance and adapts learning rates implicitly, while being computationally simpler than LayerNorm. We also propose partial RMSNorm (pRMSNorm), estimating RMS from a subset of inputs. Empirical results across various tasks and architectures demonstrate that RMSNorm achieves comparable performance to LayerNorm while reducing running time by 7%∼64%."
    }
  ],
  "maintains": [
    {
      "title": "Root Mean Square Layer Normalization",
      "url": "/research/2019/10/16/Root-Mean-Square-Layer-Normalization.html",
      "date": "October 16, 2019",
      "summary": "Layer normalization (LayerNorm) enhances deep neural network stability and convergence by re-centering and re-scaling inputs and weight matrices. However, its computational overhead slows networks, particularly RNNs. We introduce RMSNorm, which replaces re-centering with root mean square (RMS) regularization. RMSNorm maintains re-scaling invariance and adapts learning rates implicitly, while being computationally simpler than LayerNorm. We also propose partial RMSNorm (pRMSNorm), estimating RMS from a subset of inputs. Empirical results across various tasks and architectures demonstrate that RMSNorm achieves comparable performance to LayerNorm while reducing running time by 7%∼64%."
    }
  ],
  "adapts": [
    {
      "title": "Root Mean Square Layer Normalization",
      "url": "/research/2019/10/16/Root-Mean-Square-Layer-Normalization.html",
      "date": "October 16, 2019",
      "summary": "Layer normalization (LayerNorm) enhances deep neural network stability and convergence by re-centering and re-scaling inputs and weight matrices. However, its computational overhead slows networks, particularly RNNs. We introduce RMSNorm, which replaces re-centering with root mean square (RMS) regularization. RMSNorm maintains re-scaling invariance and adapts learning rates implicitly, while being computationally simpler than LayerNorm. We also propose partial RMSNorm (pRMSNorm), estimating RMS from a subset of inputs. Empirical results across various tasks and architectures demonstrate that RMSNorm achieves comparable performance to LayerNorm while reducing running time by 7%∼64%."
    }
  ],
  "prmsnorm": [
    {
      "title": "Root Mean Square Layer Normalization",
      "url": "/research/2019/10/16/Root-Mean-Square-Layer-Normalization.html",
      "date": "October 16, 2019",
      "summary": "Layer normalization (LayerNorm) enhances deep neural network stability and convergence by re-centering and re-scaling inputs and weight matrices. However, its computational overhead slows networks, particularly RNNs. We introduce RMSNorm, which replaces re-centering with root mean square (RMS) regularization. RMSNorm maintains re-scaling invariance and adapts learning rates implicitly, while being computationally simpler than LayerNorm. We also propose partial RMSNorm (pRMSNorm), estimating RMS from a subset of inputs. Empirical results across various tasks and architectures demonstrate that RMSNorm achieves comparable performance to LayerNorm while reducing running time by 7%∼64%."
    }
  ],
  "subset": [
    {
      "title": "Root Mean Square Layer Normalization",
      "url": "/research/2019/10/16/Root-Mean-Square-Layer-Normalization.html",
      "date": "October 16, 2019",
      "summary": "Layer normalization (LayerNorm) enhances deep neural network stability and convergence by re-centering and re-scaling inputs and weight matrices. However, its computational overhead slows networks, particularly RNNs. We introduce RMSNorm, which replaces re-centering with root mean square (RMS) regularization. RMSNorm maintains re-scaling invariance and adapts learning rates implicitly, while being computationally simpler than LayerNorm. We also propose partial RMSNorm (pRMSNorm), estimating RMS from a subset of inputs. Empirical results across various tasks and architectures demonstrate that RMSNorm achieves comparable performance to LayerNorm while reducing running time by 7%∼64%."
    },
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    }
  ],
  "run": [
    {
      "title": "Root Mean Square Layer Normalization",
      "url": "/research/2019/10/16/Root-Mean-Square-Layer-Normalization.html",
      "date": "October 16, 2019",
      "summary": "Layer normalization (LayerNorm) enhances deep neural network stability and convergence by re-centering and re-scaling inputs and weight matrices. However, its computational overhead slows networks, particularly RNNs. We introduce RMSNorm, which replaces re-centering with root mean square (RMS) regularization. RMSNorm maintains re-scaling invariance and adapts learning rates implicitly, while being computationally simpler than LayerNorm. We also propose partial RMSNorm (pRMSNorm), estimating RMS from a subset of inputs. Empirical results across various tasks and architectures demonstrate that RMSNorm achieves comparable performance to LayerNorm while reducing running time by 7%∼64%."
    }
  ],
  "encompass": [
    {
      "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
      "url": "/research/2021/07/24/The-Many-Faces-of-Robustness-A-Critical-Analysis-of-Out-of-Distribution-Generalization.html",
      "date": "July 24, 2021",
      "summary": "We present four novel real-world distribution shift datasets encompassing changes in image style, blurriness, location, camera settings, and more. Evaluating existing methods for enhancing out-of-distribution robustness, we discover that employing larger models and artificial data augmentations can enhance robustness against real-world distribution shifts, contradicting prior claims. Our findings demonstrate that improvements in artificial robustness benchmarks can indeed transfer to real-world distribution shifts, contrary to prior assumptions. Additionally, we introduce a novel data augmentation technique that surpasses models pretrained with significantly more labeled data, emphasizing its efficacy in addressing real-world distribution shifts. While some methods consistently mitigate texture and local image statistics shifts, they fail to address other shifts like geographic changes. Our results underscore the necessity for future research to examine multiple distribution shifts concurrently, as no single method consistently improves robustness across all evaluated scenarios."
    }
  ],
  "blurriness": [
    {
      "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
      "url": "/research/2021/07/24/The-Many-Faces-of-Robustness-A-Critical-Analysis-of-Out-of-Distribution-Generalization.html",
      "date": "July 24, 2021",
      "summary": "We present four novel real-world distribution shift datasets encompassing changes in image style, blurriness, location, camera settings, and more. Evaluating existing methods for enhancing out-of-distribution robustness, we discover that employing larger models and artificial data augmentations can enhance robustness against real-world distribution shifts, contradicting prior claims. Our findings demonstrate that improvements in artificial robustness benchmarks can indeed transfer to real-world distribution shifts, contrary to prior assumptions. Additionally, we introduce a novel data augmentation technique that surpasses models pretrained with significantly more labeled data, emphasizing its efficacy in addressing real-world distribution shifts. While some methods consistently mitigate texture and local image statistics shifts, they fail to address other shifts like geographic changes. Our results underscore the necessity for future research to examine multiple distribution shifts concurrently, as no single method consistently improves robustness across all evaluated scenarios."
    }
  ],
  "location": [
    {
      "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
      "url": "/research/2021/07/24/The-Many-Faces-of-Robustness-A-Critical-Analysis-of-Out-of-Distribution-Generalization.html",
      "date": "July 24, 2021",
      "summary": "We present four novel real-world distribution shift datasets encompassing changes in image style, blurriness, location, camera settings, and more. Evaluating existing methods for enhancing out-of-distribution robustness, we discover that employing larger models and artificial data augmentations can enhance robustness against real-world distribution shifts, contradicting prior claims. Our findings demonstrate that improvements in artificial robustness benchmarks can indeed transfer to real-world distribution shifts, contrary to prior assumptions. Additionally, we introduce a novel data augmentation technique that surpasses models pretrained with significantly more labeled data, emphasizing its efficacy in addressing real-world distribution shifts. While some methods consistently mitigate texture and local image statistics shifts, they fail to address other shifts like geographic changes. Our results underscore the necessity for future research to examine multiple distribution shifts concurrently, as no single method consistently improves robustness across all evaluated scenarios."
    },
    {
      "title": "On Layer Normalization in the Transformer Architecture",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architecture.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is widely used in natural language processing. Training it typically requires a carefully designed learning rate warm-up stage, crucial for performance but slowing optimization and requiring more hyperparameter tuning. We provide theoretical insights into the necessity of the warm-up stage, demonstrating that the location of layer normalization affects gradient behavior. For Post-LN Transformers, with layer normalization between residual blocks, large gradients near the output layer at initialization make training unstable without warm-up. Conversely, for Pre-LN Transformers, with layer normalization inside residual blocks, gradients are well-behaved, suggesting the removal of the warm-up stage. Experimental results show that Pre-LN Transformers without warm-up achieve comparable performance with less training time and hyperparameter tuning."
    }
  ],
  "contradict": [
    {
      "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
      "url": "/research/2021/07/24/The-Many-Faces-of-Robustness-A-Critical-Analysis-of-Out-of-Distribution-Generalization.html",
      "date": "July 24, 2021",
      "summary": "We present four novel real-world distribution shift datasets encompassing changes in image style, blurriness, location, camera settings, and more. Evaluating existing methods for enhancing out-of-distribution robustness, we discover that employing larger models and artificial data augmentations can enhance robustness against real-world distribution shifts, contradicting prior claims. Our findings demonstrate that improvements in artificial robustness benchmarks can indeed transfer to real-world distribution shifts, contrary to prior assumptions. Additionally, we introduce a novel data augmentation technique that surpasses models pretrained with significantly more labeled data, emphasizing its efficacy in addressing real-world distribution shifts. While some methods consistently mitigate texture and local image statistics shifts, they fail to address other shifts like geographic changes. Our results underscore the necessity for future research to examine multiple distribution shifts concurrently, as no single method consistently improves robustness across all evaluated scenarios."
    }
  ],
  "claim": [
    {
      "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
      "url": "/research/2021/07/24/The-Many-Faces-of-Robustness-A-Critical-Analysis-of-Out-of-Distribution-Generalization.html",
      "date": "July 24, 2021",
      "summary": "We present four novel real-world distribution shift datasets encompassing changes in image style, blurriness, location, camera settings, and more. Evaluating existing methods for enhancing out-of-distribution robustness, we discover that employing larger models and artificial data augmentations can enhance robustness against real-world distribution shifts, contradicting prior claims. Our findings demonstrate that improvements in artificial robustness benchmarks can indeed transfer to real-world distribution shifts, contrary to prior assumptions. Additionally, we introduce a novel data augmentation technique that surpasses models pretrained with significantly more labeled data, emphasizing its efficacy in addressing real-world distribution shifts. While some methods consistently mitigate texture and local image statistics shifts, they fail to address other shifts like geographic changes. Our results underscore the necessity for future research to examine multiple distribution shifts concurrently, as no single method consistently improves robustness across all evaluated scenarios."
    }
  ],
  "mnist": [
    {
      "title": "Weight Agnostic Neural Networks",
      "url": "/research/2019/09/05/Weight-Agnostic-Neural-Networks.html",
      "date": "September 5, 2019",
      "summary": "This study investigates the importance of neural network architectures versus weight parameters for task performance. We introduce a method to find architectures capable of performing tasks without weight training. By assigning random weights, we show that minimal architectures can achieve notable performance on various tasks, including reinforcement learning and MNIST classification."
    },
    {
      "title": "From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning",
      "url": "/research/2023/03/16/From-MNIST-to-ImageNet-and-Back:-Benchmarking-Continual-Curriculum-Learning.html",
      "date": "March 16, 2023",
      "summary": "Continual learning (CL) is a promising trend in machine learning, aiming to develop robust models and strategies for dynamic environments by incorporating new knowledge while retaining past knowledge. CL research is fragmented, with various protocols, tasks, datasets, and metrics, often not reflecting real-world complexity and tailored to specific strategies. This work addresses this gap by introducing two novel CL benchmarks with heterogeneous tasks from six image datasets, varying in complexity and quality. These benchmarks fairly evaluate state-of-the-art CL strategies in scenarios closer to real-world conditions. The benchmarks present tasks in both increasing and decreasing complexity, assessing models' ability to exploit task structure. The work emphasizes a rigorous, reproducible evaluation protocol for measuring generalization and memory retention in models. Experimental results show that popular CL strategies perform poorly on these benchmarks, exhibit high forgetting, and struggle with curriculum task ordering. These findings underscore the need for rigorous comparisons and the development of new CL strategies for complex scenarios."
    }
  ],
  "2ndorder": [
    {
      "title": "Deep learning via Hessian-free optimization",
      "url": "/research/2010/06/21/Deep-learning-via-Hessian-free-optimization.html",
      "date": "June 21, 2010",
      "summary": "We present a novel 2nd-order optimization method inspired by the Hessian-free approach, applied to deep auto-encoders. Achieving superior results without pre-training compared to Hinton & Salakhutdinov (2006), our method is practical, user-friendly, scalable to large datasets, and versatile across various model classes. Additionally, we address the challenge of pathological curvature in deep learning, highlighting how our method effectively mitigates this issue."
    }
  ],
  "hinton": [
    {
      "title": "Deep learning via Hessian-free optimization",
      "url": "/research/2010/06/21/Deep-learning-via-Hessian-free-optimization.html",
      "date": "June 21, 2010",
      "summary": "We present a novel 2nd-order optimization method inspired by the Hessian-free approach, applied to deep auto-encoders. Achieving superior results without pre-training compared to Hinton & Salakhutdinov (2006), our method is practical, user-friendly, scalable to large datasets, and versatile across various model classes. Additionally, we address the challenge of pathological curvature in deep learning, highlighting how our method effectively mitigates this issue."
    }
  ],
  "salakhutdinov": [
    {
      "title": "Deep learning via Hessian-free optimization",
      "url": "/research/2010/06/21/Deep-learning-via-Hessian-free-optimization.html",
      "date": "June 21, 2010",
      "summary": "We present a novel 2nd-order optimization method inspired by the Hessian-free approach, applied to deep auto-encoders. Achieving superior results without pre-training compared to Hinton & Salakhutdinov (2006), our method is practical, user-friendly, scalable to large datasets, and versatile across various model classes. Additionally, we address the challenge of pathological curvature in deep learning, highlighting how our method effectively mitigates this issue."
    }
  ],
  "akin": [
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "url": "/research/2014/02/19/Exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks.html",
      "date": "February 19, 2014",
      "summary": "This study aims to enhance our comprehension of deep learning dynamics by analyzing deep linear neural networks. Despite their linear input-output mapping, these networks display nonlinear gradient descent dynamics, resulting in phenomena like prolonged plateaus and swift transitions to improved solutions. Through analytical exploration, we reveal that as network depth increases indefinitely, learning speed can remain finite under specific initial conditions. Unsupervised pretraining, under certain data conditions, can discover these favorable initial conditions, unlike random Gaussian initializations. We introduce a novel class of random orthogonal initial conditions that, akin to unsupervised pretraining, enable depth-independent learning. These conditions also ensure gradient propagation in deep nonlinear networks, particularly when operating at the edge of chaos."
    },
    {
      "title": "Non-negative Contrastive Learning",
      "url": "/research/2024/03/19/Non-negative-Contrastive-Learning.html",
      "date": "March 19, 2024",
      "summary": "Deep representations offer promising performance for downstream tasks but lack interpretability, posing a significant challenge. In this paper, we introduce Non-negative Contrastive Learning (NCL), a refinement of Non-negative Matrix Factorization (NMF) aimed at generating interpretable features. NCL enforces non-negativity constraints, akin to NMF, resulting in sparse and disentangled representations, unlike standard contrastive learning (CL). We establish theoretical guarantees on NCL's identifiability and downstream generalization. Empirically, NCL outperforms CL in feature disentanglement, selection, and downstream classification tasks. Moreover, NCL can be extended to other learning scenarios, benefiting supervised learning."
    },
    {
      "title": "On the Relationship Between Self-Attention And Convolutional Layers",
      "url": "/research/2020/01/10/On-the-Relationship-Between-Self-Attention-And-Convolutional-Layers.html",
      "date": "January 10, 2020",
      "summary": "Recent studies have shown that attention mechanisms can rival or even outperform convolutional layers in vision tasks, challenging their dominance. Ramachandran et al. (2019) demonstrated that attention could entirely replace convolution, achieving top results. This work investigates whether attention layers function like convolutional layers, finding that multi-head self-attention layers, with enough heads, match the expressiveness of convolutional layers. Numerical experiments confirm that self-attention layers learn to focus on pixel-grid patterns akin to convolutional layers, supporting our findings."
    }
  ],
  "propagation": [
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "url": "/research/2014/02/19/Exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks.html",
      "date": "February 19, 2014",
      "summary": "This study aims to enhance our comprehension of deep learning dynamics by analyzing deep linear neural networks. Despite their linear input-output mapping, these networks display nonlinear gradient descent dynamics, resulting in phenomena like prolonged plateaus and swift transitions to improved solutions. Through analytical exploration, we reveal that as network depth increases indefinitely, learning speed can remain finite under specific initial conditions. Unsupervised pretraining, under certain data conditions, can discover these favorable initial conditions, unlike random Gaussian initializations. We introduce a novel class of random orthogonal initial conditions that, akin to unsupervised pretraining, enable depth-independent learning. These conditions also ensure gradient propagation in deep nonlinear networks, particularly when operating at the edge of chaos."
    },
    {
      "title": "Do Vision Transformers See Like Convolutional Neural Networks?",
      "url": "/research/2022/03/03/Do-Vision-Transformers-See-Like-Convolutional-Neural-Networks.html",
      "date": "March 3, 2022",
      "summary": "Convolutional Neural Networks (CNNs) have been prominent in processing visual data, but recent studies show Vision Transformers (ViTs) can match or outperform CNNs in image classification. This work examines how ViTs solve classification tasks, discovering significant differences from CNNs, including uniformity in ViT representations across layers due to self-attention and residual connections that enhance feature propagation. ViTs also maintain spatial information effectively, influenced by classification methods. The study further explores how dataset scale affects ViT features and their transferability, linking these findings to new architectures like MLP-Mixer."
    }
  ],
  "chaos": [
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "url": "/research/2014/02/19/Exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks.html",
      "date": "February 19, 2014",
      "summary": "This study aims to enhance our comprehension of deep learning dynamics by analyzing deep linear neural networks. Despite their linear input-output mapping, these networks display nonlinear gradient descent dynamics, resulting in phenomena like prolonged plateaus and swift transitions to improved solutions. Through analytical exploration, we reveal that as network depth increases indefinitely, learning speed can remain finite under specific initial conditions. Unsupervised pretraining, under certain data conditions, can discover these favorable initial conditions, unlike random Gaussian initializations. We introduce a novel class of random orthogonal initial conditions that, akin to unsupervised pretraining, enable depth-independent learning. These conditions also ensure gradient propagation in deep nonlinear networks, particularly when operating at the edge of chaos."
    }
  ],
  "painting": [
    {
      "title": "ImageNet-D: A new challenging robustness dataset inspired by domain adaptation",
      "url": "/research/2023/05/05/ImageNet-D-A-new-challenging-robustness-dataset-inspired-by-domain-adaptation.html",
      "date": "May 5, 2023",
      "summary": "We introduce ImageNet-D, a novel dataset designed to evaluate the robustness of ImageNet-trained models across various domains. With six distinct domains including Real, Painting, Clipart, Sketch, Infograph, and Quick-draw, ImageNet-D challenges even state-of-the-art models, revealing interpretable errors. For instance, our leading EfficientNet-L2 model exhibits a significant performance decrease, dropping from 11.6% on clean ImageNet to 29.2% on the Real domain."
    }
  ],
  "clipart": [
    {
      "title": "ImageNet-D: A new challenging robustness dataset inspired by domain adaptation",
      "url": "/research/2023/05/05/ImageNet-D-A-new-challenging-robustness-dataset-inspired-by-domain-adaptation.html",
      "date": "May 5, 2023",
      "summary": "We introduce ImageNet-D, a novel dataset designed to evaluate the robustness of ImageNet-trained models across various domains. With six distinct domains including Real, Painting, Clipart, Sketch, Infograph, and Quick-draw, ImageNet-D challenges even state-of-the-art models, revealing interpretable errors. For instance, our leading EfficientNet-L2 model exhibits a significant performance decrease, dropping from 11.6% on clean ImageNet to 29.2% on the Real domain."
    }
  ],
  "infograph": [
    {
      "title": "ImageNet-D: A new challenging robustness dataset inspired by domain adaptation",
      "url": "/research/2023/05/05/ImageNet-D-A-new-challenging-robustness-dataset-inspired-by-domain-adaptation.html",
      "date": "May 5, 2023",
      "summary": "We introduce ImageNet-D, a novel dataset designed to evaluate the robustness of ImageNet-trained models across various domains. With six distinct domains including Real, Painting, Clipart, Sketch, Infograph, and Quick-draw, ImageNet-D challenges even state-of-the-art models, revealing interpretable errors. For instance, our leading EfficientNet-L2 model exhibits a significant performance decrease, dropping from 11.6% on clean ImageNet to 29.2% on the Real domain."
    }
  ],
  "cage": [
    {
      "title": "Normalized Attention Without Probability Cage",
      "url": "/research/2020/05/19/Normalized-Attention-Without-Probability-Cage.html",
      "date": "May 19, 2020",
      "summary": "Softmax-attention architectures, especially popularized by Transformers, have seen significant advancements in various tasks. However, the geometric implications of softmax-attention remain underexplored. In this study, we demonstrate limitations arising from constraining attention weights to the probability simplex and its impact on the convex hull of value vectors. We reveal sequence length-dependent biases in Transformers towards token isolation at initialization and compare them with max- and sum-pooling, which are strong but often overlooked baselines. To address these issues, we propose a novel approach of replacing softmax with normalization in self-attention, resulting in a robust and widely applicable architecture. Our findings are supported by empirical results from over 25,000 trained models, and all results and implementations are publicly available."
    }
  ],
  "popularize": [
    {
      "title": "Normalized Attention Without Probability Cage",
      "url": "/research/2020/05/19/Normalized-Attention-Without-Probability-Cage.html",
      "date": "May 19, 2020",
      "summary": "Softmax-attention architectures, especially popularized by Transformers, have seen significant advancements in various tasks. However, the geometric implications of softmax-attention remain underexplored. In this study, we demonstrate limitations arising from constraining attention weights to the probability simplex and its impact on the convex hull of value vectors. We reveal sequence length-dependent biases in Transformers towards token isolation at initialization and compare them with max- and sum-pooling, which are strong but often overlooked baselines. To address these issues, we propose a novel approach of replacing softmax with normalization in self-attention, resulting in a robust and widely applicable architecture. Our findings are supported by empirical results from over 25,000 trained models, and all results and implementations are publicly available."
    }
  ],
  "implication": [
    {
      "title": "Normalized Attention Without Probability Cage",
      "url": "/research/2020/05/19/Normalized-Attention-Without-Probability-Cage.html",
      "date": "May 19, 2020",
      "summary": "Softmax-attention architectures, especially popularized by Transformers, have seen significant advancements in various tasks. However, the geometric implications of softmax-attention remain underexplored. In this study, we demonstrate limitations arising from constraining attention weights to the probability simplex and its impact on the convex hull of value vectors. We reveal sequence length-dependent biases in Transformers towards token isolation at initialization and compare them with max- and sum-pooling, which are strong but often overlooked baselines. To address these issues, we propose a novel approach of replacing softmax with normalization in self-attention, resulting in a robust and widely applicable architecture. Our findings are supported by empirical results from over 25,000 trained models, and all results and implementations are publicly available."
    }
  ],
  "underexplore": [
    {
      "title": "Normalized Attention Without Probability Cage",
      "url": "/research/2020/05/19/Normalized-Attention-Without-Probability-Cage.html",
      "date": "May 19, 2020",
      "summary": "Softmax-attention architectures, especially popularized by Transformers, have seen significant advancements in various tasks. However, the geometric implications of softmax-attention remain underexplored. In this study, we demonstrate limitations arising from constraining attention weights to the probability simplex and its impact on the convex hull of value vectors. We reveal sequence length-dependent biases in Transformers towards token isolation at initialization and compare them with max- and sum-pooling, which are strong but often overlooked baselines. To address these issues, we propose a novel approach of replacing softmax with normalization in self-attention, resulting in a robust and widely applicable architecture. Our findings are supported by empirical results from over 25,000 trained models, and all results and implementations are publicly available."
    }
  ],
  "simplex": [
    {
      "title": "Normalized Attention Without Probability Cage",
      "url": "/research/2020/05/19/Normalized-Attention-Without-Probability-Cage.html",
      "date": "May 19, 2020",
      "summary": "Softmax-attention architectures, especially popularized by Transformers, have seen significant advancements in various tasks. However, the geometric implications of softmax-attention remain underexplored. In this study, we demonstrate limitations arising from constraining attention weights to the probability simplex and its impact on the convex hull of value vectors. We reveal sequence length-dependent biases in Transformers towards token isolation at initialization and compare them with max- and sum-pooling, which are strong but often overlooked baselines. To address these issues, we propose a novel approach of replacing softmax with normalization in self-attention, resulting in a robust and widely applicable architecture. Our findings are supported by empirical results from over 25,000 trained models, and all results and implementations are publicly available."
    }
  ],
  "hull": [
    {
      "title": "Normalized Attention Without Probability Cage",
      "url": "/research/2020/05/19/Normalized-Attention-Without-Probability-Cage.html",
      "date": "May 19, 2020",
      "summary": "Softmax-attention architectures, especially popularized by Transformers, have seen significant advancements in various tasks. However, the geometric implications of softmax-attention remain underexplored. In this study, we demonstrate limitations arising from constraining attention weights to the probability simplex and its impact on the convex hull of value vectors. We reveal sequence length-dependent biases in Transformers towards token isolation at initialization and compare them with max- and sum-pooling, which are strong but often overlooked baselines. To address these issues, we propose a novel approach of replacing softmax with normalization in self-attention, resulting in a robust and widely applicable architecture. Our findings are supported by empirical results from over 25,000 trained models, and all results and implementations are publicly available."
    }
  ],
  "vector": [
    {
      "title": "Normalized Attention Without Probability Cage",
      "url": "/research/2020/05/19/Normalized-Attention-Without-Probability-Cage.html",
      "date": "May 19, 2020",
      "summary": "Softmax-attention architectures, especially popularized by Transformers, have seen significant advancements in various tasks. However, the geometric implications of softmax-attention remain underexplored. In this study, we demonstrate limitations arising from constraining attention weights to the probability simplex and its impact on the convex hull of value vectors. We reveal sequence length-dependent biases in Transformers towards token isolation at initialization and compare them with max- and sum-pooling, which are strong but often overlooked baselines. To address these issues, we propose a novel approach of replacing softmax with normalization in self-attention, resulting in a robust and widely applicable architecture. Our findings are supported by empirical results from over 25,000 trained models, and all results and implementations are publicly available."
    },
    {
      "title": "Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks",
      "url": "/research/2017/10/23/Cosine-Normalization-Using-Cosine-Similarity-Instead-of-Dot-Product-in-Neural-Networks.html",
      "date": "October 23, 2017",
      "summary": "In traditional multi-layer neural networks, the dot product between the output and weight vectors of preceding layers serves as input to the activation function, resulting in unbounded outputs and increased variance. This variance can lead to poor generalization and hinder training by exacerbating internal covariate shift. To address this, we propose cosine normalization, which replaces the dot product with cosine similarity or centered cosine similarity (Pearson Correlation Coefficient). We evaluate cosine normalization against batch, weight, and layer normalization in fully-connected and convolutional neural networks across various datasets. Our experiments demonstrate that cosine normalization outperforms other normalization techniques."
    },
    {
      "title": "Neural Networks Learn Statistics of Increasing Complexity",
      "url": "/research/2024/02/13/Neural-Networks-Learn-Statistics-of-Increasing-Complexity.html",
      "date": "February 13, 2024",
      "summary": "The distributional simplicity bias (DSB) theory suggests neural networks first learn basic patterns in data before understanding more complex correlations. We provide new evidence supporting DSB, showing networks initially excel with data matching training set's simple statistics but this ability diminishes later. Extending DSB to discrete domains, we demonstrate an equivalence between n-gram frequencies and vector moments, also observing this bias in large language models (LLMs). Additionally, by adjusting low-level statistics of images to resemble another class, we reveal that networks in early training phases misclassify these edited images as if they belonged to the target class."
    }
  ],
  "isolation": [
    {
      "title": "Normalized Attention Without Probability Cage",
      "url": "/research/2020/05/19/Normalized-Attention-Without-Probability-Cage.html",
      "date": "May 19, 2020",
      "summary": "Softmax-attention architectures, especially popularized by Transformers, have seen significant advancements in various tasks. However, the geometric implications of softmax-attention remain underexplored. In this study, we demonstrate limitations arising from constraining attention weights to the probability simplex and its impact on the convex hull of value vectors. We reveal sequence length-dependent biases in Transformers towards token isolation at initialization and compare them with max- and sum-pooling, which are strong but often overlooked baselines. To address these issues, we propose a novel approach of replacing softmax with normalization in self-attention, resulting in a robust and widely applicable architecture. Our findings are supported by empirical results from over 25,000 trained models, and all results and implementations are publicly available."
    }
  ],
  "max": [
    {
      "title": "Normalized Attention Without Probability Cage",
      "url": "/research/2020/05/19/Normalized-Attention-Without-Probability-Cage.html",
      "date": "May 19, 2020",
      "summary": "Softmax-attention architectures, especially popularized by Transformers, have seen significant advancements in various tasks. However, the geometric implications of softmax-attention remain underexplored. In this study, we demonstrate limitations arising from constraining attention weights to the probability simplex and its impact on the convex hull of value vectors. We reveal sequence length-dependent biases in Transformers towards token isolation at initialization and compare them with max- and sum-pooling, which are strong but often overlooked baselines. To address these issues, we propose a novel approach of replacing softmax with normalization in self-attention, resulting in a robust and widely applicable architecture. Our findings are supported by empirical results from over 25,000 trained models, and all results and implementations are publicly available."
    }
  ],
  "sum": [
    {
      "title": "Normalized Attention Without Probability Cage",
      "url": "/research/2020/05/19/Normalized-Attention-Without-Probability-Cage.html",
      "date": "May 19, 2020",
      "summary": "Softmax-attention architectures, especially popularized by Transformers, have seen significant advancements in various tasks. However, the geometric implications of softmax-attention remain underexplored. In this study, we demonstrate limitations arising from constraining attention weights to the probability simplex and its impact on the convex hull of value vectors. We reveal sequence length-dependent biases in Transformers towards token isolation at initialization and compare them with max- and sum-pooling, which are strong but often overlooked baselines. To address these issues, we propose a novel approach of replacing softmax with normalization in self-attention, resulting in a robust and widely applicable architecture. Our findings are supported by empirical results from over 25,000 trained models, and all results and implementations are publicly available."
    }
  ],
  "unstable": [
    {
      "title": "On Layer Normalization in the Transformer Architecture",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architecture.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is widely used in natural language processing. Training it typically requires a carefully designed learning rate warm-up stage, crucial for performance but slowing optimization and requiring more hyperparameter tuning. We provide theoretical insights into the necessity of the warm-up stage, demonstrating that the location of layer normalization affects gradient behavior. For Post-LN Transformers, with layer normalization between residual blocks, large gradients near the output layer at initialization make training unstable without warm-up. Conversely, for Pre-LN Transformers, with layer normalization inside residual blocks, gradients are well-behaved, suggesting the removal of the warm-up stage. Experimental results show that Pre-LN Transformers without warm-up achieve comparable performance with less training time and hyperparameter tuning."
    }
  ],
  "removal": [
    {
      "title": "On Layer Normalization in the Transformer Architecture",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architecture.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is widely used in natural language processing. Training it typically requires a carefully designed learning rate warm-up stage, crucial for performance but slowing optimization and requiring more hyperparameter tuning. We provide theoretical insights into the necessity of the warm-up stage, demonstrating that the location of layer normalization affects gradient behavior. For Post-LN Transformers, with layer normalization between residual blocks, large gradients near the output layer at initialization make training unstable without warm-up. Conversely, for Pre-LN Transformers, with layer normalization inside residual blocks, gradients are well-behaved, suggesting the removal of the warm-up stage. Experimental results show that Pre-LN Transformers without warm-up achieve comparable performance with less training time and hyperparameter tuning."
    }
  ],
  "22bparameter": [
    {
      "title": "Scaling Vision Transformers to 22 Billion Parameters",
      "url": "/research/2023/02/10/Scaling-Vision-Transformers-to-22-Billion-Parameters.html",
      "date": "February 10, 2023",
      "summary": "We introduce a method for efficiently training a 22B-parameter Vision Transformer (ViT-22B) and conduct various experiments to assess its performance. Compared to existing models, ViT-22B shows improved scalability and benefits such as enhanced fairness-performance tradeoff, alignment with human visual perception, and increased robustness. Our findings suggest promising avenues for achieving large-scale language model-like capabilities in vision tasks."
    }
  ],
  "vit22b": [
    {
      "title": "Scaling Vision Transformers to 22 Billion Parameters",
      "url": "/research/2023/02/10/Scaling-Vision-Transformers-to-22-Billion-Parameters.html",
      "date": "February 10, 2023",
      "summary": "We introduce a method for efficiently training a 22B-parameter Vision Transformer (ViT-22B) and conduct various experiments to assess its performance. Compared to existing models, ViT-22B shows improved scalability and benefits such as enhanced fairness-performance tradeoff, alignment with human visual perception, and increased robustness. Our findings suggest promising avenues for achieving large-scale language model-like capabilities in vision tasks."
    }
  ],
  "fairness": [
    {
      "title": "Scaling Vision Transformers to 22 Billion Parameters",
      "url": "/research/2023/02/10/Scaling-Vision-Transformers-to-22-Billion-Parameters.html",
      "date": "February 10, 2023",
      "summary": "We introduce a method for efficiently training a 22B-parameter Vision Transformer (ViT-22B) and conduct various experiments to assess its performance. Compared to existing models, ViT-22B shows improved scalability and benefits such as enhanced fairness-performance tradeoff, alignment with human visual perception, and increased robustness. Our findings suggest promising avenues for achieving large-scale language model-like capabilities in vision tasks."
    },
    {
      "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum",
      "url": "/research/2023/09/30/When-to-Learn-What:-Model-Adaptive-Data-Augmentation-Curriculum.html",
      "date": "September 30, 2023",
      "summary": "Data augmentation (DA) improves neural network generalization by enforcing invariances and symmetries to predefined transformations applied to input data. However, fixed augmentation policies affect samples differently at various training stages, and existing approaches cannot adapt policies to individual samples and the training model. This paper proposes Model-Adaptive Data Augmentation (MADAug) which trains an augmentation policy network to determine when to learn what. Unlike previous work, MADAug selects augmentation operators for each input image with a model-adaptive policy that varies between training stages, creating an optimized data augmentation curriculum. The policy is trained using a bi-level optimization scheme to minimize validation-set loss of a model trained with policy-produced augmentations. Extensive evaluations on multiple image classification tasks and network architectures show that MADAug outperforms or matches existing DA approaches, enhances fairness by improving all classes, particularly the difficult ones, and performs better when transferred to fine-grained datasets. Additionally, the auto-optimized policy in MADAug gradually increases perturbations, forming an easy-to-hard curriculum."
    }
  ],
  "tradeoff": [
    {
      "title": "Scaling Vision Transformers to 22 Billion Parameters",
      "url": "/research/2023/02/10/Scaling-Vision-Transformers-to-22-Billion-Parameters.html",
      "date": "February 10, 2023",
      "summary": "We introduce a method for efficiently training a 22B-parameter Vision Transformer (ViT-22B) and conduct various experiments to assess its performance. Compared to existing models, ViT-22B shows improved scalability and benefits such as enhanced fairness-performance tradeoff, alignment with human visual perception, and increased robustness. Our findings suggest promising avenues for achieving large-scale language model-like capabilities in vision tasks."
    }
  ],
  "alignment": [
    {
      "title": "Scaling Vision Transformers to 22 Billion Parameters",
      "url": "/research/2023/02/10/Scaling-Vision-Transformers-to-22-Billion-Parameters.html",
      "date": "February 10, 2023",
      "summary": "We introduce a method for efficiently training a 22B-parameter Vision Transformer (ViT-22B) and conduct various experiments to assess its performance. Compared to existing models, ViT-22B shows improved scalability and benefits such as enhanced fairness-performance tradeoff, alignment with human visual perception, and increased robustness. Our findings suggest promising avenues for achieving large-scale language model-like capabilities in vision tasks."
    },
    {
      "title": "How Does Sharpness-Aware Minimization Minimize Sharpness?",
      "url": "/research/2023/01/05/How-Does-Sharpness-Aware-Minimization-Minimize-Sharpness.html",
      "date": "January 5, 2023",
      "summary": "Sharpness-Aware Minimization (SAM) enhances deep neural networks' generalization across various settings. Although SAM aims to penalize model sharpness using a computationally efficient approach, its exact operational mechanism and the sharpness notion it regularizes remain unclear, partly due to differing sharpness concepts used in its theoretical framework and empirical validation. This study identifies the precise sharpness concept SAM regulates and explains its mechanism. It reveals that the combined effect of SAM's two-step approximations, despite being individually misleading, correctly improves generalization when using full-batch gradients. Moreover, we demonstrate that the stochastic SAM version indeed regularizes a third sharpness notion, aligning more closely with practical performance. This effectiveness is attributed to the gradient's alignment with the Hessian's top eigenvector under SAM."
    }
  ],
  "promise": [
    {
      "title": "Scaling Vision Transformers to 22 Billion Parameters",
      "url": "/research/2023/02/10/Scaling-Vision-Transformers-to-22-Billion-Parameters.html",
      "date": "February 10, 2023",
      "summary": "We introduce a method for efficiently training a 22B-parameter Vision Transformer (ViT-22B) and conduct various experiments to assess its performance. Compared to existing models, ViT-22B shows improved scalability and benefits such as enhanced fairness-performance tradeoff, alignment with human visual perception, and increased robustness. Our findings suggest promising avenues for achieving large-scale language model-like capabilities in vision tasks."
    }
  ],
  "modellike": [
    {
      "title": "Scaling Vision Transformers to 22 Billion Parameters",
      "url": "/research/2023/02/10/Scaling-Vision-Transformers-to-22-Billion-Parameters.html",
      "date": "February 10, 2023",
      "summary": "We introduce a method for efficiently training a 22B-parameter Vision Transformer (ViT-22B) and conduct various experiments to assess its performance. Compared to existing models, ViT-22B shows improved scalability and benefits such as enhanced fairness-performance tradeoff, alignment with human visual perception, and increased robustness. Our findings suggest promising avenues for achieving large-scale language model-like capabilities in vision tasks."
    }
  ],
  "multilayer": [
    {
      "title": "Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks",
      "url": "/research/2017/10/23/Cosine-Normalization-Using-Cosine-Similarity-Instead-of-Dot-Product-in-Neural-Networks.html",
      "date": "October 23, 2017",
      "summary": "In traditional multi-layer neural networks, the dot product between the output and weight vectors of preceding layers serves as input to the activation function, resulting in unbounded outputs and increased variance. This variance can lead to poor generalization and hinder training by exacerbating internal covariate shift. To address this, we propose cosine normalization, which replaces the dot product with cosine similarity or centered cosine similarity (Pearson Correlation Coefficient). We evaluate cosine normalization against batch, weight, and layer normalization in fully-connected and convolutional neural networks across various datasets. Our experiments demonstrate that cosine normalization outperforms other normalization techniques."
    },
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "unbound": [
    {
      "title": "Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks",
      "url": "/research/2017/10/23/Cosine-Normalization-Using-Cosine-Similarity-Instead-of-Dot-Product-in-Neural-Networks.html",
      "date": "October 23, 2017",
      "summary": "In traditional multi-layer neural networks, the dot product between the output and weight vectors of preceding layers serves as input to the activation function, resulting in unbounded outputs and increased variance. This variance can lead to poor generalization and hinder training by exacerbating internal covariate shift. To address this, we propose cosine normalization, which replaces the dot product with cosine similarity or centered cosine similarity (Pearson Correlation Coefficient). We evaluate cosine normalization against batch, weight, and layer normalization in fully-connected and convolutional neural networks across various datasets. Our experiments demonstrate that cosine normalization outperforms other normalization techniques."
    }
  ],
  "pearson": [
    {
      "title": "Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks",
      "url": "/research/2017/10/23/Cosine-Normalization-Using-Cosine-Similarity-Instead-of-Dot-Product-in-Neural-Networks.html",
      "date": "October 23, 2017",
      "summary": "In traditional multi-layer neural networks, the dot product between the output and weight vectors of preceding layers serves as input to the activation function, resulting in unbounded outputs and increased variance. This variance can lead to poor generalization and hinder training by exacerbating internal covariate shift. To address this, we propose cosine normalization, which replaces the dot product with cosine similarity or centered cosine similarity (Pearson Correlation Coefficient). We evaluate cosine normalization against batch, weight, and layer normalization in fully-connected and convolutional neural networks across various datasets. Our experiments demonstrate that cosine normalization outperforms other normalization techniques."
    }
  ],
  "seek": [
    {
      "title": "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization",
      "url": "/research/2023/07/04/Gradient-Norm-Aware-Minimization-Seeks-First-Order-Flatness-and-Improves-Generalization.html",
      "date": "July 4, 2023",
      "summary": "Recent advancements have highlighted the effectiveness of flat minima in enhancing generalization, particularly through Sharpness-Aware Minimization (SAM). However, existing definitions of flatness, such as zeroth-order flatness, have limitations in discerning between minima with low and high generalization errors. To address this, we propose first-order flatness, which considers maximal gradient norm within a perturbation radius. We introduce Gradient norm Aware Minimization (GAM) as a novel training approach to achieve uniformly small curvature across all directions. Experimental results demonstrate GAM's ability to enhance generalization compared to standard optimizers like SGD and AdamW across various datasets and networks. Moreover, GAM facilitates SAM in identifying flatter minima, leading to improved generalization."
    }
  ],
  "flatness": [
    {
      "title": "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization",
      "url": "/research/2023/07/04/Gradient-Norm-Aware-Minimization-Seeks-First-Order-Flatness-and-Improves-Generalization.html",
      "date": "July 4, 2023",
      "summary": "Recent advancements have highlighted the effectiveness of flat minima in enhancing generalization, particularly through Sharpness-Aware Minimization (SAM). However, existing definitions of flatness, such as zeroth-order flatness, have limitations in discerning between minima with low and high generalization errors. To address this, we propose first-order flatness, which considers maximal gradient norm within a perturbation radius. We introduce Gradient norm Aware Minimization (GAM) as a novel training approach to achieve uniformly small curvature across all directions. Experimental results demonstrate GAM's ability to enhance generalization compared to standard optimizers like SGD and AdamW across various datasets and networks. Moreover, GAM facilitates SAM in identifying flatter minima, leading to improved generalization."
    }
  ],
  "definition": [
    {
      "title": "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization",
      "url": "/research/2023/07/04/Gradient-Norm-Aware-Minimization-Seeks-First-Order-Flatness-and-Improves-Generalization.html",
      "date": "July 4, 2023",
      "summary": "Recent advancements have highlighted the effectiveness of flat minima in enhancing generalization, particularly through Sharpness-Aware Minimization (SAM). However, existing definitions of flatness, such as zeroth-order flatness, have limitations in discerning between minima with low and high generalization errors. To address this, we propose first-order flatness, which considers maximal gradient norm within a perturbation radius. We introduce Gradient norm Aware Minimization (GAM) as a novel training approach to achieve uniformly small curvature across all directions. Experimental results demonstrate GAM's ability to enhance generalization compared to standard optimizers like SGD and AdamW across various datasets and networks. Moreover, GAM facilitates SAM in identifying flatter minima, leading to improved generalization."
    }
  ],
  "discern": [
    {
      "title": "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization",
      "url": "/research/2023/07/04/Gradient-Norm-Aware-Minimization-Seeks-First-Order-Flatness-and-Improves-Generalization.html",
      "date": "July 4, 2023",
      "summary": "Recent advancements have highlighted the effectiveness of flat minima in enhancing generalization, particularly through Sharpness-Aware Minimization (SAM). However, existing definitions of flatness, such as zeroth-order flatness, have limitations in discerning between minima with low and high generalization errors. To address this, we propose first-order flatness, which considers maximal gradient norm within a perturbation radius. We introduce Gradient norm Aware Minimization (GAM) as a novel training approach to achieve uniformly small curvature across all directions. Experimental results demonstrate GAM's ability to enhance generalization compared to standard optimizers like SGD and AdamW across various datasets and networks. Moreover, GAM facilitates SAM in identifying flatter minima, leading to improved generalization."
    }
  ],
  "gam": [
    {
      "title": "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization",
      "url": "/research/2023/07/04/Gradient-Norm-Aware-Minimization-Seeks-First-Order-Flatness-and-Improves-Generalization.html",
      "date": "July 4, 2023",
      "summary": "Recent advancements have highlighted the effectiveness of flat minima in enhancing generalization, particularly through Sharpness-Aware Minimization (SAM). However, existing definitions of flatness, such as zeroth-order flatness, have limitations in discerning between minima with low and high generalization errors. To address this, we propose first-order flatness, which considers maximal gradient norm within a perturbation radius. We introduce Gradient norm Aware Minimization (GAM) as a novel training approach to achieve uniformly small curvature across all directions. Experimental results demonstrate GAM's ability to enhance generalization compared to standard optimizers like SGD and AdamW across various datasets and networks. Moreover, GAM facilitates SAM in identifying flatter minima, leading to improved generalization."
    }
  ],
  "identifying": [
    {
      "title": "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization",
      "url": "/research/2023/07/04/Gradient-Norm-Aware-Minimization-Seeks-First-Order-Flatness-and-Improves-Generalization.html",
      "date": "July 4, 2023",
      "summary": "Recent advancements have highlighted the effectiveness of flat minima in enhancing generalization, particularly through Sharpness-Aware Minimization (SAM). However, existing definitions of flatness, such as zeroth-order flatness, have limitations in discerning between minima with low and high generalization errors. To address this, we propose first-order flatness, which considers maximal gradient norm within a perturbation radius. We introduce Gradient norm Aware Minimization (GAM) as a novel training approach to achieve uniformly small curvature across all directions. Experimental results demonstrate GAM's ability to enhance generalization compared to standard optimizers like SGD and AdamW across various datasets and networks. Moreover, GAM facilitates SAM in identifying flatter minima, leading to improved generalization."
    }
  ],
  "sharpness": [
    {
      "title": "Model Generalization: A Sharpness Aware Optimization Perspective",
      "url": "/research/2022/08/14/Model-Generalization-A-Sharpness-Aware-Optimization-Perspective.html",
      "date": "August 14, 2022",
      "summary": "This study investigates the effectiveness of Sharpness-Aware Minimization (SAM) and adaptive Sharpness-Aware Minimization (ASAM) in enhancing model generalization. Through three experiments, we assess their impact from a sharpness-aware perspective. Results demonstrate that optimization techniques based on sharpness awareness can bolster model generalization. Furthermore, ASAM exhibits potential for enhancing generalization performance on un-normalized data, though additional research is required for validation."
    },
    {
      "title": "Sharpness-Aware Training for Free",
      "url": "/research/2023/03/02/Sharpness-Aware-Training-for-Free.html",
      "date": "March 2, 2023",
      "summary": "Modern deep neural networks (DNNs) excel in performance but often suffer from over-parameterization, leading to increased generalization error without tailored training strategies. Sharpness-Aware Minimization (SAM) has proven effective in reducing generalization error by minimizing sharpness in the loss landscape. However, SAM incurs a significant computational overhead. This paper introduces Sharpness-Aware Training for Free (SAF), which mitigates sharpness at nearly zero additional computational cost. SAF achieves this by preventing sudden drops in loss within sharp local minima during weight updates. A novel trajectory loss, based on KL-divergence between current and past DNN outputs, replaces SAM's sharpness measure, guiding convergence towards flat minima for enhanced generalization. Empirical results demonstrate SAF's effectiveness on ImageNet with comparable computational efficiency to the base optimizer."
    },
    {
      "title": "AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks",
      "url": "/research/2023/03/01/AdaSAM-Boosting-Sharpness-Aware-Minimization-with-Adaptive-Learning-Rate-and-Momentum-for-Training-Deep-Neural-Networks.html",
      "date": "March 1, 2023",
      "summary": "The Sharpness Aware Minimization (SAM) optimizer, known for enhancing the generalization of deep neural networks by introducing extra perturbation steps, is further developed into AdaSAM by integrating adaptive learning rates and momentum acceleration. Although AdaSAM has been empirically applied to large-scale networks, a theoretical understanding of its performance, considering the complexity of its components, was lacking. This study presents a theoretical analysis of AdaSAM's convergence in stochastic non-convex settings, demonstrating a convergence rate of O(1/√bT) that scales linearly with mini-batch size. By introducing a delayed second-order momentum term, the study successfully decouples and analyzes the intertwined effects of stochastic gradients, adaptive learning rates, and perturbations. This is the first work to offer a detailed convergence rate for SAM with adaptive mechanisms. Experimental results on various NLP tasks indicate AdaSAM's superior performance over SGD, AMSGrad, and SAM optimizers."
    },
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    },
    {
      "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
      "url": "/research/2022/03/19/Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training.html",
      "date": "March 19, 2022",
      "summary": "Sharpness-Aware Minimization (SAM) enhances generalization by optimizing a neighborhood-based perturbed loss, but it doesn't always favor flat minima due to both sharp and flat minima having low perturbed losses. We introduce a new measure, the surrogate gap, reflecting the dominant Hessian eigenvalue at small neighborhood radii, which is simple to compute and can be minimized during training. We propose the Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), an advancement over SAM with minimal additional computational cost. GSAM employs a dual-step approach: first, minimizing the perturbed loss similar to SAM, and second, reducing the surrogate gap without affecting the perturbed loss to target regions with low loss and sharpness, thus achieving superior generalization. GSAM is theoretically robust, showing better convergence and empirical generalization improvements, notably a +3.2% gain over SAM and +5.4% over AdamW in ImageNet accuracy for ViT-B/32."
    },
    {
      "title": "ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Network",
      "url": "/research/2021/06/29/ASAM-Adaptive-Sharpness-Aware-Minimization-for-Scale-Invariant-Learning-of-Deep-Neural-Network.html",
      "date": "June 29, 2021",
      "summary": "Recent advances in learning algorithms have highlighted the importance of the sharpness of the loss surface as a key indicator of the generalization gap, achieving state-of-the-art results. However, traditional sharpness measures, defined within a fixed region, suffer from sensitivity to parameter scaling, weakening their predictive value for the generalization gap. This paper introduces a scale-invariant measure, adaptive sharpness, along with a new generalization bound. We present a new learning method, adaptive sharpness-aware minimization (ASAM), which leverages this bound. Our experiments across various benchmark datasets demonstrate that ASAM significantly enhances model generalization."
    },
    {
      "title": "How Does Sharpness-Aware Minimization Minimize Sharpness?",
      "url": "/research/2023/01/05/How-Does-Sharpness-Aware-Minimization-Minimize-Sharpness.html",
      "date": "January 5, 2023",
      "summary": "Sharpness-Aware Minimization (SAM) enhances deep neural networks' generalization across various settings. Although SAM aims to penalize model sharpness using a computationally efficient approach, its exact operational mechanism and the sharpness notion it regularizes remain unclear, partly due to differing sharpness concepts used in its theoretical framework and empirical validation. This study identifies the precise sharpness concept SAM regulates and explains its mechanism. It reveals that the combined effect of SAM's two-step approximations, despite being individually misleading, correctly improves generalization when using full-batch gradients. Moreover, we demonstrate that the stochastic SAM version indeed regularizes a third sharpness notion, aligning more closely with practical performance. This effectiveness is attributed to the gradient's alignment with the Hessian's top eigenvector under SAM."
    }
  ],
  "asam": [
    {
      "title": "Model Generalization: A Sharpness Aware Optimization Perspective",
      "url": "/research/2022/08/14/Model-Generalization-A-Sharpness-Aware-Optimization-Perspective.html",
      "date": "August 14, 2022",
      "summary": "This study investigates the effectiveness of Sharpness-Aware Minimization (SAM) and adaptive Sharpness-Aware Minimization (ASAM) in enhancing model generalization. Through three experiments, we assess their impact from a sharpness-aware perspective. Results demonstrate that optimization techniques based on sharpness awareness can bolster model generalization. Furthermore, ASAM exhibits potential for enhancing generalization performance on un-normalized data, though additional research is required for validation."
    },
    {
      "title": "ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Network",
      "url": "/research/2021/06/29/ASAM-Adaptive-Sharpness-Aware-Minimization-for-Scale-Invariant-Learning-of-Deep-Neural-Network.html",
      "date": "June 29, 2021",
      "summary": "Recent advances in learning algorithms have highlighted the importance of the sharpness of the loss surface as a key indicator of the generalization gap, achieving state-of-the-art results. However, traditional sharpness measures, defined within a fixed region, suffer from sensitivity to parameter scaling, weakening their predictive value for the generalization gap. This paper introduces a scale-invariant measure, adaptive sharpness, along with a new generalization bound. We present a new learning method, adaptive sharpness-aware minimization (ASAM), which leverages this bound. Our experiments across various benchmark datasets demonstrate that ASAM significantly enhances model generalization."
    }
  ],
  "awareness": [
    {
      "title": "Model Generalization: A Sharpness Aware Optimization Perspective",
      "url": "/research/2022/08/14/Model-Generalization-A-Sharpness-Aware-Optimization-Perspective.html",
      "date": "August 14, 2022",
      "summary": "This study investigates the effectiveness of Sharpness-Aware Minimization (SAM) and adaptive Sharpness-Aware Minimization (ASAM) in enhancing model generalization. Through three experiments, we assess their impact from a sharpness-aware perspective. Results demonstrate that optimization techniques based on sharpness awareness can bolster model generalization. Furthermore, ASAM exhibits potential for enhancing generalization performance on un-normalized data, though additional research is required for validation."
    }
  ],
  "bolster": [
    {
      "title": "Model Generalization: A Sharpness Aware Optimization Perspective",
      "url": "/research/2022/08/14/Model-Generalization-A-Sharpness-Aware-Optimization-Perspective.html",
      "date": "August 14, 2022",
      "summary": "This study investigates the effectiveness of Sharpness-Aware Minimization (SAM) and adaptive Sharpness-Aware Minimization (ASAM) in enhancing model generalization. Through three experiments, we assess their impact from a sharpness-aware perspective. Results demonstrate that optimization techniques based on sharpness awareness can bolster model generalization. Furthermore, ASAM exhibits potential for enhancing generalization performance on un-normalized data, though additional research is required for validation."
    }
  ],
  "penalize": [
    {
      "title": "Penalizing Gradient Norm for Efficiently Improving Generalization in Deep Learning",
      "url": "/research/2022/06/26/Penalizing-Gradient-Norm-for-Efficiently-Improving-Generalization-in-Deep-Learning.html",
      "date": "June 26, 2022",
      "summary": "This paper proposes a method to enhance the generalization of deep neural networks (DNNs) by penalizing the gradient norm of the loss function during optimization. By constraining the gradient norm, the optimizers tend to find flat minima, improving generalization. We efficiently implement this method using first-order approximation within the gradient descent framework. Experimental results demonstrate improved generalization across various models and datasets. Additionally, we show that a recent method, sharpness-aware minimization, is a special case of our approach, with our method achieving new state-of-the-art performance on tested tasks."
    },
    {
      "title": "How Does Sharpness-Aware Minimization Minimize Sharpness?",
      "url": "/research/2023/01/05/How-Does-Sharpness-Aware-Minimization-Minimize-Sharpness.html",
      "date": "January 5, 2023",
      "summary": "Sharpness-Aware Minimization (SAM) enhances deep neural networks' generalization across various settings. Although SAM aims to penalize model sharpness using a computationally efficient approach, its exact operational mechanism and the sharpness notion it regularizes remain unclear, partly due to differing sharpness concepts used in its theoretical framework and empirical validation. This study identifies the precise sharpness concept SAM regulates and explains its mechanism. It reveals that the combined effect of SAM's two-step approximations, despite being individually misleading, correctly improves generalization when using full-batch gradients. Moreover, we demonstrate that the stochastic SAM version indeed regularizes a third sharpness notion, aligning more closely with practical performance. This effectiveness is attributed to the gradient's alignment with the Hessian's top eigenvector under SAM."
    }
  ],
  "tend": [
    {
      "title": "Penalizing Gradient Norm for Efficiently Improving Generalization in Deep Learning",
      "url": "/research/2022/06/26/Penalizing-Gradient-Norm-for-Efficiently-Improving-Generalization-in-Deep-Learning.html",
      "date": "June 26, 2022",
      "summary": "This paper proposes a method to enhance the generalization of deep neural networks (DNNs) by penalizing the gradient norm of the loss function during optimization. By constraining the gradient norm, the optimizers tend to find flat minima, improving generalization. We efficiently implement this method using first-order approximation within the gradient descent framework. Experimental results demonstrate improved generalization across various models and datasets. Additionally, we show that a recent method, sharpness-aware minimization, is a special case of our approach, with our method achieving new state-of-the-art performance on tested tasks."
    },
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    }
  ],
  "case": [
    {
      "title": "Penalizing Gradient Norm for Efficiently Improving Generalization in Deep Learning",
      "url": "/research/2022/06/26/Penalizing-Gradient-Norm-for-Efficiently-Improving-Generalization-in-Deep-Learning.html",
      "date": "June 26, 2022",
      "summary": "This paper proposes a method to enhance the generalization of deep neural networks (DNNs) by penalizing the gradient norm of the loss function during optimization. By constraining the gradient norm, the optimizers tend to find flat minima, improving generalization. We efficiently implement this method using first-order approximation within the gradient descent framework. Experimental results demonstrate improved generalization across various models and datasets. Additionally, we show that a recent method, sharpness-aware minimization, is a special case of our approach, with our method achieving new state-of-the-art performance on tested tasks."
    },
    {
      "title": "Self-Paced Absolute Learning Progress as a Regularized Approach to Curriculum Learning",
      "url": "/research/2023/06/09/Self-Paced-Absolute-Learning-Progress-as-a-Regularized-Approach-to-Curriculum-Learning.html",
      "date": "June 9, 2023",
      "summary": "Reinforcement Learning's usability is limited by its high computation times. Curriculum Reinforcement Learning accelerates learning by ordering tasks from simple to hard. Curricula based on Absolute Learning Progress (ALP) have shown success in various environments but waste computation on redundant tasks. This issue is addressed by introducing Self-Paced Absolute Learning Progress (SPALP), a regularization method inspired by Self-Paced Learning. Evaluated in three environments, SPALP achieves performance comparable to ALP in all cases and reaches it faster in two. Further improvements in SPALP's efficiency and performance are also discussed."
    }
  ],
  "relaxation": [
    {
      "title": "SAM as an Optimal Relaxation of Bayes",
      "url": "/research/2023/12/10/SAM-as-an-Optimal-Relaxation-of-Bayes.html",
      "date": "December 10, 2023",
      "summary": "We introduce SAM, a method enhancing generalization in deep learning, as a relaxation of the Bayes objective. SAM replaces the expected negative-loss with an optimal convex lower bound derived using the Fenchel biconjugate. This connection enables an Adam-like extension of SAM, offering automatic uncertainty estimates and potential accuracy improvements. Bridging adversarial and Bayesian methods, our work paves the way for robustness enhancement."
    }
  ],
  "baye": [
    {
      "title": "SAM as an Optimal Relaxation of Bayes",
      "url": "/research/2023/12/10/SAM-as-an-Optimal-Relaxation-of-Bayes.html",
      "date": "December 10, 2023",
      "summary": "We introduce SAM, a method enhancing generalization in deep learning, as a relaxation of the Bayes objective. SAM replaces the expected negative-loss with an optimal convex lower bound derived using the Fenchel biconjugate. This connection enables an Adam-like extension of SAM, offering automatic uncertainty estimates and potential accuracy improvements. Bridging adversarial and Bayesian methods, our work paves the way for robustness enhancement."
    }
  ],
  "fenchel": [
    {
      "title": "SAM as an Optimal Relaxation of Bayes",
      "url": "/research/2023/12/10/SAM-as-an-Optimal-Relaxation-of-Bayes.html",
      "date": "December 10, 2023",
      "summary": "We introduce SAM, a method enhancing generalization in deep learning, as a relaxation of the Bayes objective. SAM replaces the expected negative-loss with an optimal convex lower bound derived using the Fenchel biconjugate. This connection enables an Adam-like extension of SAM, offering automatic uncertainty estimates and potential accuracy improvements. Bridging adversarial and Bayesian methods, our work paves the way for robustness enhancement."
    }
  ],
  "biconjugate": [
    {
      "title": "SAM as an Optimal Relaxation of Bayes",
      "url": "/research/2023/12/10/SAM-as-an-Optimal-Relaxation-of-Bayes.html",
      "date": "December 10, 2023",
      "summary": "We introduce SAM, a method enhancing generalization in deep learning, as a relaxation of the Bayes objective. SAM replaces the expected negative-loss with an optimal convex lower bound derived using the Fenchel biconjugate. This connection enables an Adam-like extension of SAM, offering automatic uncertainty estimates and potential accuracy improvements. Bridging adversarial and Bayesian methods, our work paves the way for robustness enhancement."
    }
  ],
  "adamlike": [
    {
      "title": "SAM as an Optimal Relaxation of Bayes",
      "url": "/research/2023/12/10/SAM-as-an-Optimal-Relaxation-of-Bayes.html",
      "date": "December 10, 2023",
      "summary": "We introduce SAM, a method enhancing generalization in deep learning, as a relaxation of the Bayes objective. SAM replaces the expected negative-loss with an optimal convex lower bound derived using the Fenchel biconjugate. This connection enables an Adam-like extension of SAM, offering automatic uncertainty estimates and potential accuracy improvements. Bridging adversarial and Bayesian methods, our work paves the way for robustness enhancement."
    }
  ],
  "bayesian": [
    {
      "title": "SAM as an Optimal Relaxation of Bayes",
      "url": "/research/2023/12/10/SAM-as-an-Optimal-Relaxation-of-Bayes.html",
      "date": "December 10, 2023",
      "summary": "We introduce SAM, a method enhancing generalization in deep learning, as a relaxation of the Bayes objective. SAM replaces the expected negative-loss with an optimal convex lower bound derived using the Fenchel biconjugate. This connection enables an Adam-like extension of SAM, offering automatic uncertainty estimates and potential accuracy improvements. Bridging adversarial and Bayesian methods, our work paves the way for robustness enhancement."
    }
  ],
  "pave": [
    {
      "title": "SAM as an Optimal Relaxation of Bayes",
      "url": "/research/2023/12/10/SAM-as-an-Optimal-Relaxation-of-Bayes.html",
      "date": "December 10, 2023",
      "summary": "We introduce SAM, a method enhancing generalization in deep learning, as a relaxation of the Bayes objective. SAM replaces the expected negative-loss with an optimal convex lower bound derived using the Fenchel biconjugate. This connection enables an Adam-like extension of SAM, offering automatic uncertainty estimates and potential accuracy improvements. Bridging adversarial and Bayesian methods, our work paves the way for robustness enhancement."
    }
  ],
  "incur": [
    {
      "title": "Sharpness-Aware Training for Free",
      "url": "/research/2023/03/02/Sharpness-Aware-Training-for-Free.html",
      "date": "March 2, 2023",
      "summary": "Modern deep neural networks (DNNs) excel in performance but often suffer from over-parameterization, leading to increased generalization error without tailored training strategies. Sharpness-Aware Minimization (SAM) has proven effective in reducing generalization error by minimizing sharpness in the loss landscape. However, SAM incurs a significant computational overhead. This paper introduces Sharpness-Aware Training for Free (SAF), which mitigates sharpness at nearly zero additional computational cost. SAF achieves this by preventing sudden drops in loss within sharp local minima during weight updates. A novel trajectory loss, based on KL-divergence between current and past DNN outputs, replaces SAM's sharpness measure, guiding convergence towards flat minima for enhanced generalization. Empirical results demonstrate SAF's effectiveness on ImageNet with comparable computational efficiency to the base optimizer."
    }
  ],
  "saf": [
    {
      "title": "Sharpness-Aware Training for Free",
      "url": "/research/2023/03/02/Sharpness-Aware-Training-for-Free.html",
      "date": "March 2, 2023",
      "summary": "Modern deep neural networks (DNNs) excel in performance but often suffer from over-parameterization, leading to increased generalization error without tailored training strategies. Sharpness-Aware Minimization (SAM) has proven effective in reducing generalization error by minimizing sharpness in the loss landscape. However, SAM incurs a significant computational overhead. This paper introduces Sharpness-Aware Training for Free (SAF), which mitigates sharpness at nearly zero additional computational cost. SAF achieves this by preventing sudden drops in loss within sharp local minima during weight updates. A novel trajectory loss, based on KL-divergence between current and past DNN outputs, replaces SAM's sharpness measure, guiding convergence towards flat minima for enhanced generalization. Empirical results demonstrate SAF's effectiveness on ImageNet with comparable computational efficiency to the base optimizer."
    }
  ],
  "trajectory": [
    {
      "title": "Sharpness-Aware Training for Free",
      "url": "/research/2023/03/02/Sharpness-Aware-Training-for-Free.html",
      "date": "March 2, 2023",
      "summary": "Modern deep neural networks (DNNs) excel in performance but often suffer from over-parameterization, leading to increased generalization error without tailored training strategies. Sharpness-Aware Minimization (SAM) has proven effective in reducing generalization error by minimizing sharpness in the loss landscape. However, SAM incurs a significant computational overhead. This paper introduces Sharpness-Aware Training for Free (SAF), which mitigates sharpness at nearly zero additional computational cost. SAF achieves this by preventing sudden drops in loss within sharp local minima during weight updates. A novel trajectory loss, based on KL-divergence between current and past DNN outputs, replaces SAM's sharpness measure, guiding convergence towards flat minima for enhanced generalization. Empirical results demonstrate SAF's effectiveness on ImageNet with comparable computational efficiency to the base optimizer."
    }
  ],
  "kl": [
    {
      "title": "Sharpness-Aware Training for Free",
      "url": "/research/2023/03/02/Sharpness-Aware-Training-for-Free.html",
      "date": "March 2, 2023",
      "summary": "Modern deep neural networks (DNNs) excel in performance but often suffer from over-parameterization, leading to increased generalization error without tailored training strategies. Sharpness-Aware Minimization (SAM) has proven effective in reducing generalization error by minimizing sharpness in the loss landscape. However, SAM incurs a significant computational overhead. This paper introduces Sharpness-Aware Training for Free (SAF), which mitigates sharpness at nearly zero additional computational cost. SAF achieves this by preventing sudden drops in loss within sharp local minima during weight updates. A novel trajectory loss, based on KL-divergence between current and past DNN outputs, replaces SAM's sharpness measure, guiding convergence towards flat minima for enhanced generalization. Empirical results demonstrate SAF's effectiveness on ImageNet with comparable computational efficiency to the base optimizer."
    }
  ],
  "lion": [
    {
      "title": "Symbolic Discovery of Optimization Algorithms",
      "url": "/research/2023/05/08/Symbolic-Discovery-of-Optimization-Algorithms.html",
      "date": "May 8, 2023",
      "summary": "We propose a method for algorithm discovery via program search, focusing on optimizing deep neural network training. Our approach, Lion (EvoLved Sign Momentum), is memory-efficient and achieves comparable or superior performance to widely-used optimizers such as Adam and Adafactor across various tasks. Specifically, Lion enhances accuracy on tasks like image classification and vision-language contrastive learning, while reducing training compute. Notably, Lion exhibits improved performance with larger batch sizes and requires smaller learning rates compared to Adam. Despite its effectiveness, Lion has limitations, which we analyze, and provide insights into its deployment and performance. Our implementation is publicly available and has been successfully utilized in Google's search ads CTR model."
    }
  ],
  "evolve": [
    {
      "title": "Symbolic Discovery of Optimization Algorithms",
      "url": "/research/2023/05/08/Symbolic-Discovery-of-Optimization-Algorithms.html",
      "date": "May 8, 2023",
      "summary": "We propose a method for algorithm discovery via program search, focusing on optimizing deep neural network training. Our approach, Lion (EvoLved Sign Momentum), is memory-efficient and achieves comparable or superior performance to widely-used optimizers such as Adam and Adafactor across various tasks. Specifically, Lion enhances accuracy on tasks like image classification and vision-language contrastive learning, while reducing training compute. Notably, Lion exhibits improved performance with larger batch sizes and requires smaller learning rates compared to Adam. Despite its effectiveness, Lion has limitations, which we analyze, and provide insights into its deployment and performance. Our implementation is publicly available and has been successfully utilized in Google's search ads CTR model."
    }
  ],
  "adafactor": [
    {
      "title": "Symbolic Discovery of Optimization Algorithms",
      "url": "/research/2023/05/08/Symbolic-Discovery-of-Optimization-Algorithms.html",
      "date": "May 8, 2023",
      "summary": "We propose a method for algorithm discovery via program search, focusing on optimizing deep neural network training. Our approach, Lion (EvoLved Sign Momentum), is memory-efficient and achieves comparable or superior performance to widely-used optimizers such as Adam and Adafactor across various tasks. Specifically, Lion enhances accuracy on tasks like image classification and vision-language contrastive learning, while reducing training compute. Notably, Lion exhibits improved performance with larger batch sizes and requires smaller learning rates compared to Adam. Despite its effectiveness, Lion has limitations, which we analyze, and provide insights into its deployment and performance. Our implementation is publicly available and has been successfully utilized in Google's search ads CTR model."
    }
  ],
  "ads": [
    {
      "title": "Symbolic Discovery of Optimization Algorithms",
      "url": "/research/2023/05/08/Symbolic-Discovery-of-Optimization-Algorithms.html",
      "date": "May 8, 2023",
      "summary": "We propose a method for algorithm discovery via program search, focusing on optimizing deep neural network training. Our approach, Lion (EvoLved Sign Momentum), is memory-efficient and achieves comparable or superior performance to widely-used optimizers such as Adam and Adafactor across various tasks. Specifically, Lion enhances accuracy on tasks like image classification and vision-language contrastive learning, while reducing training compute. Notably, Lion exhibits improved performance with larger batch sizes and requires smaller learning rates compared to Adam. Despite its effectiveness, Lion has limitations, which we analyze, and provide insights into its deployment and performance. Our implementation is publicly available and has been successfully utilized in Google's search ads CTR model."
    }
  ],
  "ctr": [
    {
      "title": "Symbolic Discovery of Optimization Algorithms",
      "url": "/research/2023/05/08/Symbolic-Discovery-of-Optimization-Algorithms.html",
      "date": "May 8, 2023",
      "summary": "We propose a method for algorithm discovery via program search, focusing on optimizing deep neural network training. Our approach, Lion (EvoLved Sign Momentum), is memory-efficient and achieves comparable or superior performance to widely-used optimizers such as Adam and Adafactor across various tasks. Specifically, Lion enhances accuracy on tasks like image classification and vision-language contrastive learning, while reducing training compute. Notably, Lion exhibits improved performance with larger batch sizes and requires smaller learning rates compared to Adam. Despite its effectiveness, Lion has limitations, which we analyze, and provide insights into its deployment and performance. Our implementation is publicly available and has been successfully utilized in Google's search ads CTR model."
    }
  ],
  "adasam": [
    {
      "title": "AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks",
      "url": "/research/2023/03/01/AdaSAM-Boosting-Sharpness-Aware-Minimization-with-Adaptive-Learning-Rate-and-Momentum-for-Training-Deep-Neural-Networks.html",
      "date": "March 1, 2023",
      "summary": "The Sharpness Aware Minimization (SAM) optimizer, known for enhancing the generalization of deep neural networks by introducing extra perturbation steps, is further developed into AdaSAM by integrating adaptive learning rates and momentum acceleration. Although AdaSAM has been empirically applied to large-scale networks, a theoretical understanding of its performance, considering the complexity of its components, was lacking. This study presents a theoretical analysis of AdaSAM's convergence in stochastic non-convex settings, demonstrating a convergence rate of O(1/√bT) that scales linearly with mini-batch size. By introducing a delayed second-order momentum term, the study successfully decouples and analyzes the intertwined effects of stochastic gradients, adaptive learning rates, and perturbations. This is the first work to offer a detailed convergence rate for SAM with adaptive mechanisms. Experimental results on various NLP tasks indicate AdaSAM's superior performance over SGD, AMSGrad, and SAM optimizers."
    }
  ],
  "acceleration": [
    {
      "title": "AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks",
      "url": "/research/2023/03/01/AdaSAM-Boosting-Sharpness-Aware-Minimization-with-Adaptive-Learning-Rate-and-Momentum-for-Training-Deep-Neural-Networks.html",
      "date": "March 1, 2023",
      "summary": "The Sharpness Aware Minimization (SAM) optimizer, known for enhancing the generalization of deep neural networks by introducing extra perturbation steps, is further developed into AdaSAM by integrating adaptive learning rates and momentum acceleration. Although AdaSAM has been empirically applied to large-scale networks, a theoretical understanding of its performance, considering the complexity of its components, was lacking. This study presents a theoretical analysis of AdaSAM's convergence in stochastic non-convex settings, demonstrating a convergence rate of O(1/√bT) that scales linearly with mini-batch size. By introducing a delayed second-order momentum term, the study successfully decouples and analyzes the intertwined effects of stochastic gradients, adaptive learning rates, and perturbations. This is the first work to offer a detailed convergence rate for SAM with adaptive mechanisms. Experimental results on various NLP tasks indicate AdaSAM's superior performance over SGD, AMSGrad, and SAM optimizers."
    }
  ],
  "o1bt": [
    {
      "title": "AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks",
      "url": "/research/2023/03/01/AdaSAM-Boosting-Sharpness-Aware-Minimization-with-Adaptive-Learning-Rate-and-Momentum-for-Training-Deep-Neural-Networks.html",
      "date": "March 1, 2023",
      "summary": "The Sharpness Aware Minimization (SAM) optimizer, known for enhancing the generalization of deep neural networks by introducing extra perturbation steps, is further developed into AdaSAM by integrating adaptive learning rates and momentum acceleration. Although AdaSAM has been empirically applied to large-scale networks, a theoretical understanding of its performance, considering the complexity of its components, was lacking. This study presents a theoretical analysis of AdaSAM's convergence in stochastic non-convex settings, demonstrating a convergence rate of O(1/√bT) that scales linearly with mini-batch size. By introducing a delayed second-order momentum term, the study successfully decouples and analyzes the intertwined effects of stochastic gradients, adaptive learning rates, and perturbations. This is the first work to offer a detailed convergence rate for SAM with adaptive mechanisms. Experimental results on various NLP tasks indicate AdaSAM's superior performance over SGD, AMSGrad, and SAM optimizers."
    }
  ],
  "intertwine": [
    {
      "title": "AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks",
      "url": "/research/2023/03/01/AdaSAM-Boosting-Sharpness-Aware-Minimization-with-Adaptive-Learning-Rate-and-Momentum-for-Training-Deep-Neural-Networks.html",
      "date": "March 1, 2023",
      "summary": "The Sharpness Aware Minimization (SAM) optimizer, known for enhancing the generalization of deep neural networks by introducing extra perturbation steps, is further developed into AdaSAM by integrating adaptive learning rates and momentum acceleration. Although AdaSAM has been empirically applied to large-scale networks, a theoretical understanding of its performance, considering the complexity of its components, was lacking. This study presents a theoretical analysis of AdaSAM's convergence in stochastic non-convex settings, demonstrating a convergence rate of O(1/√bT) that scales linearly with mini-batch size. By introducing a delayed second-order momentum term, the study successfully decouples and analyzes the intertwined effects of stochastic gradients, adaptive learning rates, and perturbations. This is the first work to offer a detailed convergence rate for SAM with adaptive mechanisms. Experimental results on various NLP tasks indicate AdaSAM's superior performance over SGD, AMSGrad, and SAM optimizers."
    }
  ],
  "amsgrad": [
    {
      "title": "AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks",
      "url": "/research/2023/03/01/AdaSAM-Boosting-Sharpness-Aware-Minimization-with-Adaptive-Learning-Rate-and-Momentum-for-Training-Deep-Neural-Networks.html",
      "date": "March 1, 2023",
      "summary": "The Sharpness Aware Minimization (SAM) optimizer, known for enhancing the generalization of deep neural networks by introducing extra perturbation steps, is further developed into AdaSAM by integrating adaptive learning rates and momentum acceleration. Although AdaSAM has been empirically applied to large-scale networks, a theoretical understanding of its performance, considering the complexity of its components, was lacking. This study presents a theoretical analysis of AdaSAM's convergence in stochastic non-convex settings, demonstrating a convergence rate of O(1/√bT) that scales linearly with mini-batch size. By introducing a delayed second-order momentum term, the study successfully decouples and analyzes the intertwined effects of stochastic gradients, adaptive learning rates, and perturbations. This is the first work to offer a detailed convergence rate for SAM with adaptive mechanisms. Experimental results on various NLP tasks indicate AdaSAM's superior performance over SGD, AMSGrad, and SAM optimizers."
    }
  ],
  "optimizers": [
    {
      "title": "AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks",
      "url": "/research/2023/03/01/AdaSAM-Boosting-Sharpness-Aware-Minimization-with-Adaptive-Learning-Rate-and-Momentum-for-Training-Deep-Neural-Networks.html",
      "date": "March 1, 2023",
      "summary": "The Sharpness Aware Minimization (SAM) optimizer, known for enhancing the generalization of deep neural networks by introducing extra perturbation steps, is further developed into AdaSAM by integrating adaptive learning rates and momentum acceleration. Although AdaSAM has been empirically applied to large-scale networks, a theoretical understanding of its performance, considering the complexity of its components, was lacking. This study presents a theoretical analysis of AdaSAM's convergence in stochastic non-convex settings, demonstrating a convergence rate of O(1/√bT) that scales linearly with mini-batch size. By introducing a delayed second-order momentum term, the study successfully decouples and analyzes the intertwined effects of stochastic gradients, adaptive learning rates, and perturbations. This is the first work to offer a detailed convergence rate for SAM with adaptive mechanisms. Experimental results on various NLP tasks indicate AdaSAM's superior performance over SGD, AMSGrad, and SAM optimizers."
    }
  ],
  "deepnet": [
    {
      "title": "DeepNet: Scaling Transformers to 1,000 Layers",
      "url": "/research/2022/03/01/DeepNet-Scaling-Transformers-to-1,000-Layers.html",
      "date": "March 1, 2022",
      "summary": "This paper introduces DEEPNORM, a novel method to stabilize extremely deep Transformers through a new normalization function and a theoretically derived initialization for the residual connection. DEEPNORM offers a stable and efficient training regime, blending the benefits of Post-LN's performance with Pre-LN's stability, allowing for the scaling of Transformers up to 1,000 layers without difficulty. This represents a significant advancement over previous models, with a 200-layer, 3.2B parameter model outperforming a 48-layer, 12B parameter state-of-the-art model by 5 BLEU points on a multilingual benchmark with 7,482 translation directions, suggesting a promising direction for scaling."
    }
  ],
  "deepnorm": [
    {
      "title": "DeepNet: Scaling Transformers to 1,000 Layers",
      "url": "/research/2022/03/01/DeepNet-Scaling-Transformers-to-1,000-Layers.html",
      "date": "March 1, 2022",
      "summary": "This paper introduces DEEPNORM, a novel method to stabilize extremely deep Transformers through a new normalization function and a theoretically derived initialization for the residual connection. DEEPNORM offers a stable and efficient training regime, blending the benefits of Post-LN's performance with Pre-LN's stability, allowing for the scaling of Transformers up to 1,000 layers without difficulty. This represents a significant advancement over previous models, with a 200-layer, 3.2B parameter model outperforming a 48-layer, 12B parameter state-of-the-art model by 5 BLEU points on a multilingual benchmark with 7,482 translation directions, suggesting a promising direction for scaling."
    }
  ],
  "regime": [
    {
      "title": "DeepNet: Scaling Transformers to 1,000 Layers",
      "url": "/research/2022/03/01/DeepNet-Scaling-Transformers-to-1,000-Layers.html",
      "date": "March 1, 2022",
      "summary": "This paper introduces DEEPNORM, a novel method to stabilize extremely deep Transformers through a new normalization function and a theoretically derived initialization for the residual connection. DEEPNORM offers a stable and efficient training regime, blending the benefits of Post-LN's performance with Pre-LN's stability, allowing for the scaling of Transformers up to 1,000 layers without difficulty. This represents a significant advancement over previous models, with a 200-layer, 3.2B parameter model outperforming a 48-layer, 12B parameter state-of-the-art model by 5 BLEU points on a multilingual benchmark with 7,482 translation directions, suggesting a promising direction for scaling."
    }
  ],
  "200layer": [
    {
      "title": "DeepNet: Scaling Transformers to 1,000 Layers",
      "url": "/research/2022/03/01/DeepNet-Scaling-Transformers-to-1,000-Layers.html",
      "date": "March 1, 2022",
      "summary": "This paper introduces DEEPNORM, a novel method to stabilize extremely deep Transformers through a new normalization function and a theoretically derived initialization for the residual connection. DEEPNORM offers a stable and efficient training regime, blending the benefits of Post-LN's performance with Pre-LN's stability, allowing for the scaling of Transformers up to 1,000 layers without difficulty. This represents a significant advancement over previous models, with a 200-layer, 3.2B parameter model outperforming a 48-layer, 12B parameter state-of-the-art model by 5 BLEU points on a multilingual benchmark with 7,482 translation directions, suggesting a promising direction for scaling."
    }
  ],
  "48layer": [
    {
      "title": "DeepNet: Scaling Transformers to 1,000 Layers",
      "url": "/research/2022/03/01/DeepNet-Scaling-Transformers-to-1,000-Layers.html",
      "date": "March 1, 2022",
      "summary": "This paper introduces DEEPNORM, a novel method to stabilize extremely deep Transformers through a new normalization function and a theoretically derived initialization for the residual connection. DEEPNORM offers a stable and efficient training regime, blending the benefits of Post-LN's performance with Pre-LN's stability, allowing for the scaling of Transformers up to 1,000 layers without difficulty. This represents a significant advancement over previous models, with a 200-layer, 3.2B parameter model outperforming a 48-layer, 12B parameter state-of-the-art model by 5 BLEU points on a multilingual benchmark with 7,482 translation directions, suggesting a promising direction for scaling."
    }
  ],
  "uniformity": [
    {
      "title": "Do Vision Transformers See Like Convolutional Neural Networks?",
      "url": "/research/2022/03/03/Do-Vision-Transformers-See-Like-Convolutional-Neural-Networks.html",
      "date": "March 3, 2022",
      "summary": "Convolutional Neural Networks (CNNs) have been prominent in processing visual data, but recent studies show Vision Transformers (ViTs) can match or outperform CNNs in image classification. This work examines how ViTs solve classification tasks, discovering significant differences from CNNs, including uniformity in ViT representations across layers due to self-attention and residual connections that enhance feature propagation. ViTs also maintain spatial information effectively, influenced by classification methods. The study further explores how dataset scale affects ViT features and their transferability, linking these findings to new architectures like MLP-Mixer."
    }
  ],
  "mlp": [
    {
      "title": "Do Vision Transformers See Like Convolutional Neural Networks?",
      "url": "/research/2022/03/03/Do-Vision-Transformers-See-Like-Convolutional-Neural-Networks.html",
      "date": "March 3, 2022",
      "summary": "Convolutional Neural Networks (CNNs) have been prominent in processing visual data, but recent studies show Vision Transformers (ViTs) can match or outperform CNNs in image classification. This work examines how ViTs solve classification tasks, discovering significant differences from CNNs, including uniformity in ViT representations across layers due to self-attention and residual connections that enhance feature propagation. ViTs also maintain spatial information effectively, influenced by classification methods. The study further explores how dataset scale affects ViT features and their transferability, linking these findings to new architectures like MLP-Mixer."
    },
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    },
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "mixer": [
    {
      "title": "Do Vision Transformers See Like Convolutional Neural Networks?",
      "url": "/research/2022/03/03/Do-Vision-Transformers-See-Like-Convolutional-Neural-Networks.html",
      "date": "March 3, 2022",
      "summary": "Convolutional Neural Networks (CNNs) have been prominent in processing visual data, but recent studies show Vision Transformers (ViTs) can match or outperform CNNs in image classification. This work examines how ViTs solve classification tasks, discovering significant differences from CNNs, including uniformity in ViT representations across layers due to self-attention and residual connections that enhance feature propagation. ViTs also maintain spatial information effectively, influenced by classification methods. The study further explores how dataset scale affects ViT features and their transferability, linking these findings to new architectures like MLP-Mixer."
    },
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    }
  ],
  "doubling": [
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    }
  ],
  "esam": [
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    }
  ],
  "sacrifice": [
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    }
  ],
  "justification": [
    {
      "title": "Efficient Sharpness-Aware Minimization For Improved Training of Neural Networks",
      "url": "/research/2022/05/28/Efficient-Sharpness-Aware-Minimization-For-Improved-Training-of-Neural-Networks.html",
      "date": "May 28, 2022",
      "summary": "Overparametrized Deep Neural Networks (DNNs) can lead to severe generalization errors despite their impressive performances. It's been shown that the sharpness of the loss landscape is related to generalization error, leading to the development of the Sharpness Aware Minimizer (SAM) to improve generalization. However, SAM is computationally costly, doubling the time required compared to basic optimizers like Stochastic Gradient Descent (SGD). This paper introduces the Efficient Sharpness Aware Minimizer (ESAM), enhancing SAM's efficiency without sacrificing its generalization benefits. ESAM incorporates Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection strategies for more efficient training. These methods approximate sharpness by perturbing selected weights and optimize the SAM loss with a carefully chosen subset of data, respectively. Theoretical justifications for these strategies are provided, and extensive testing on CIFAR and ImageNet shows that ESAM reduces the computational overhead of SAM from 100% to 40% while maintaining or improving test accuracies."
    },
    {
      "title": "HYPO: Hyperspherical Out-of-Distribution Generalization",
      "url": "/research/2024/03/19/HYPO-Hyperspherical-Out-of-Distribution-Generalization.html",
      "date": "March 19, 2024",
      "summary": "We propose HYPO, a novel framework for out-of-distribution (OOD) generalization in machine learning, which learns domain-invariant features in a hyperspherical space. Our method focuses on aligning features of the same class across different domains close to their class prototypes and separating different class prototypes maximally. We offer theoretical justifications for its improvement on OOD generalization and show through experiments on OOD benchmarks that HYPO outperforms existing baselines with superior performance."
    }
  ],
  "fnet": [
    {
      "title": "FNet: Mixing Tokens with Fourier Transforms",
      "url": "/research/2022/05/26/FNet-Mixing-Tokens-with-Fourier-Transforms.html",
      "date": "May 26, 2022",
      "summary": "We demonstrate that Transformer encoder architectures can be accelerated with minimal impact on accuracy by substituting self-attention sublayers with simple linear transformations. Remarkably, using a standard Fourier Transform instead of the self-attention sublayer in a Transformer encoder achieves 92-97% of BERT models' accuracy on the GLUE benchmark, while training 80% faster on GPUs and 70% faster on TPUs for standard 512 input lengths. Our FNet model significantly outperforms in speed at longer input lengths, matching the accuracy of the most accurate models on the Long Range Arena benchmark and surpassing the fastest models in speed across all sequence lengths on GPUs and shorter lengths on TPUs. Additionally, FNet is lightweight in memory use and exceptionally efficient in smaller sizes, outperforming Transformer models under the same speed and accuracy constraints."
    }
  ],
  "bert": [
    {
      "title": "FNet: Mixing Tokens with Fourier Transforms",
      "url": "/research/2022/05/26/FNet-Mixing-Tokens-with-Fourier-Transforms.html",
      "date": "May 26, 2022",
      "summary": "We demonstrate that Transformer encoder architectures can be accelerated with minimal impact on accuracy by substituting self-attention sublayers with simple linear transformations. Remarkably, using a standard Fourier Transform instead of the self-attention sublayer in a Transformer encoder achieves 92-97% of BERT models' accuracy on the GLUE benchmark, while training 80% faster on GPUs and 70% faster on TPUs for standard 512 input lengths. Our FNet model significantly outperforms in speed at longer input lengths, matching the accuracy of the most accurate models on the Long Range Arena benchmark and surpassing the fastest models in speed across all sequence lengths on GPUs and shorter lengths on TPUs. Additionally, FNet is lightweight in memory use and exceptionally efficient in smaller sizes, outperforming Transformer models under the same speed and accuracy constraints."
    },
    {
      "title": "Exponentially Faster Language Modeling",
      "url": "/research/2023/11/21/Exponentially-Faster-Language-Modeling.html",
      "date": "November 21, 2023",
      "summary": "UltraFastBERT, a BERT variant, operates with just 0.3% of its neurons—selectively using 12 out of 4095 neurons per layer—for inference, matching the performance of similar models. It replaces conventional feedforward networks with fast feedforward networks (FFFs) to achieve this efficiency. Although fully efficient conditional neural execution isn't yet practical, we offer a high-level CPU code that achieves a 78x speedup and a PyTorch implementation with a 40x speedup over standard batched feedforward inference."
    }
  ],
  "tpus": [
    {
      "title": "FNet: Mixing Tokens with Fourier Transforms",
      "url": "/research/2022/05/26/FNet-Mixing-Tokens-with-Fourier-Transforms.html",
      "date": "May 26, 2022",
      "summary": "We demonstrate that Transformer encoder architectures can be accelerated with minimal impact on accuracy by substituting self-attention sublayers with simple linear transformations. Remarkably, using a standard Fourier Transform instead of the self-attention sublayer in a Transformer encoder achieves 92-97% of BERT models' accuracy on the GLUE benchmark, while training 80% faster on GPUs and 70% faster on TPUs for standard 512 input lengths. Our FNet model significantly outperforms in speed at longer input lengths, matching the accuracy of the most accurate models on the Long Range Arena benchmark and surpassing the fastest models in speed across all sequence lengths on GPUs and shorter lengths on TPUs. Additionally, FNet is lightweight in memory use and exceptionally efficient in smaller sizes, outperforming Transformer models under the same speed and accuracy constraints."
    }
  ],
  "arena": [
    {
      "title": "FNet: Mixing Tokens with Fourier Transforms",
      "url": "/research/2022/05/26/FNet-Mixing-Tokens-with-Fourier-Transforms.html",
      "date": "May 26, 2022",
      "summary": "We demonstrate that Transformer encoder architectures can be accelerated with minimal impact on accuracy by substituting self-attention sublayers with simple linear transformations. Remarkably, using a standard Fourier Transform instead of the self-attention sublayer in a Transformer encoder achieves 92-97% of BERT models' accuracy on the GLUE benchmark, while training 80% faster on GPUs and 70% faster on TPUs for standard 512 input lengths. Our FNet model significantly outperforms in speed at longer input lengths, matching the accuracy of the most accurate models on the Long Range Arena benchmark and surpassing the fastest models in speed across all sequence lengths on GPUs and shorter lengths on TPUs. Additionally, FNet is lightweight in memory use and exceptionally efficient in smaller sizes, outperforming Transformer models under the same speed and accuracy constraints."
    }
  ],
  "constraint": [
    {
      "title": "FNet: Mixing Tokens with Fourier Transforms",
      "url": "/research/2022/05/26/FNet-Mixing-Tokens-with-Fourier-Transforms.html",
      "date": "May 26, 2022",
      "summary": "We demonstrate that Transformer encoder architectures can be accelerated with minimal impact on accuracy by substituting self-attention sublayers with simple linear transformations. Remarkably, using a standard Fourier Transform instead of the self-attention sublayer in a Transformer encoder achieves 92-97% of BERT models' accuracy on the GLUE benchmark, while training 80% faster on GPUs and 70% faster on TPUs for standard 512 input lengths. Our FNet model significantly outperforms in speed at longer input lengths, matching the accuracy of the most accurate models on the Long Range Arena benchmark and surpassing the fastest models in speed across all sequence lengths on GPUs and shorter lengths on TPUs. Additionally, FNet is lightweight in memory use and exceptionally efficient in smaller sizes, outperforming Transformer models under the same speed and accuracy constraints."
    }
  ],
  "surrogate": [
    {
      "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
      "url": "/research/2022/03/19/Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training.html",
      "date": "March 19, 2022",
      "summary": "Sharpness-Aware Minimization (SAM) enhances generalization by optimizing a neighborhood-based perturbed loss, but it doesn't always favor flat minima due to both sharp and flat minima having low perturbed losses. We introduce a new measure, the surrogate gap, reflecting the dominant Hessian eigenvalue at small neighborhood radii, which is simple to compute and can be minimized during training. We propose the Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), an advancement over SAM with minimal additional computational cost. GSAM employs a dual-step approach: first, minimizing the perturbed loss similar to SAM, and second, reducing the surrogate gap without affecting the perturbed loss to target regions with low loss and sharpness, thus achieving superior generalization. GSAM is theoretically robust, showing better convergence and empirical generalization improvements, notably a +3.2% gain over SAM and +5.4% over AdamW in ImageNet accuracy for ViT-B/32."
    }
  ],
  "neighborhood": [
    {
      "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
      "url": "/research/2022/03/19/Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training.html",
      "date": "March 19, 2022",
      "summary": "Sharpness-Aware Minimization (SAM) enhances generalization by optimizing a neighborhood-based perturbed loss, but it doesn't always favor flat minima due to both sharp and flat minima having low perturbed losses. We introduce a new measure, the surrogate gap, reflecting the dominant Hessian eigenvalue at small neighborhood radii, which is simple to compute and can be minimized during training. We propose the Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), an advancement over SAM with minimal additional computational cost. GSAM employs a dual-step approach: first, minimizing the perturbed loss similar to SAM, and second, reducing the surrogate gap without affecting the perturbed loss to target regions with low loss and sharpness, thus achieving superior generalization. GSAM is theoretically robust, showing better convergence and empirical generalization improvements, notably a +3.2% gain over SAM and +5.4% over AdamW in ImageNet accuracy for ViT-B/32."
    }
  ],
  "reflect": [
    {
      "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
      "url": "/research/2022/03/19/Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training.html",
      "date": "March 19, 2022",
      "summary": "Sharpness-Aware Minimization (SAM) enhances generalization by optimizing a neighborhood-based perturbed loss, but it doesn't always favor flat minima due to both sharp and flat minima having low perturbed losses. We introduce a new measure, the surrogate gap, reflecting the dominant Hessian eigenvalue at small neighborhood radii, which is simple to compute and can be minimized during training. We propose the Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), an advancement over SAM with minimal additional computational cost. GSAM employs a dual-step approach: first, minimizing the perturbed loss similar to SAM, and second, reducing the surrogate gap without affecting the perturbed loss to target regions with low loss and sharpness, thus achieving superior generalization. GSAM is theoretically robust, showing better convergence and empirical generalization improvements, notably a +3.2% gain over SAM and +5.4% over AdamW in ImageNet accuracy for ViT-B/32."
    },
    {
      "title": "From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning",
      "url": "/research/2023/03/16/From-MNIST-to-ImageNet-and-Back:-Benchmarking-Continual-Curriculum-Learning.html",
      "date": "March 16, 2023",
      "summary": "Continual learning (CL) is a promising trend in machine learning, aiming to develop robust models and strategies for dynamic environments by incorporating new knowledge while retaining past knowledge. CL research is fragmented, with various protocols, tasks, datasets, and metrics, often not reflecting real-world complexity and tailored to specific strategies. This work addresses this gap by introducing two novel CL benchmarks with heterogeneous tasks from six image datasets, varying in complexity and quality. These benchmarks fairly evaluate state-of-the-art CL strategies in scenarios closer to real-world conditions. The benchmarks present tasks in both increasing and decreasing complexity, assessing models' ability to exploit task structure. The work emphasizes a rigorous, reproducible evaluation protocol for measuring generalization and memory retention in models. Experimental results show that popular CL strategies perform poorly on these benchmarks, exhibit high forgetting, and struggle with curriculum task ordering. These findings underscore the need for rigorous comparisons and the development of new CL strategies for complex scenarios."
    }
  ],
  "gsam": [
    {
      "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
      "url": "/research/2022/03/19/Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training.html",
      "date": "March 19, 2022",
      "summary": "Sharpness-Aware Minimization (SAM) enhances generalization by optimizing a neighborhood-based perturbed loss, but it doesn't always favor flat minima due to both sharp and flat minima having low perturbed losses. We introduce a new measure, the surrogate gap, reflecting the dominant Hessian eigenvalue at small neighborhood radii, which is simple to compute and can be minimized during training. We propose the Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), an advancement over SAM with minimal additional computational cost. GSAM employs a dual-step approach: first, minimizing the perturbed loss similar to SAM, and second, reducing the surrogate gap without affecting the perturbed loss to target regions with low loss and sharpness, thus achieving superior generalization. GSAM is theoretically robust, showing better convergence and empirical generalization improvements, notably a +3.2% gain over SAM and +5.4% over AdamW in ImageNet accuracy for ViT-B/32."
    }
  ],
  "showing": [
    {
      "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
      "url": "/research/2022/03/19/Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training.html",
      "date": "March 19, 2022",
      "summary": "Sharpness-Aware Minimization (SAM) enhances generalization by optimizing a neighborhood-based perturbed loss, but it doesn't always favor flat minima due to both sharp and flat minima having low perturbed losses. We introduce a new measure, the surrogate gap, reflecting the dominant Hessian eigenvalue at small neighborhood radii, which is simple to compute and can be minimized during training. We propose the Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), an advancement over SAM with minimal additional computational cost. GSAM employs a dual-step approach: first, minimizing the perturbed loss similar to SAM, and second, reducing the surrogate gap without affecting the perturbed loss to target regions with low loss and sharpness, thus achieving superior generalization. GSAM is theoretically robust, showing better convergence and empirical generalization improvements, notably a +3.2% gain over SAM and +5.4% over AdamW in ImageNet accuracy for ViT-B/32."
    }
  ],
  "b32": [
    {
      "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
      "url": "/research/2022/03/19/Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training.html",
      "date": "March 19, 2022",
      "summary": "Sharpness-Aware Minimization (SAM) enhances generalization by optimizing a neighborhood-based perturbed loss, but it doesn't always favor flat minima due to both sharp and flat minima having low perturbed losses. We introduce a new measure, the surrogate gap, reflecting the dominant Hessian eigenvalue at small neighborhood radii, which is simple to compute and can be minimized during training. We propose the Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), an advancement over SAM with minimal additional computational cost. GSAM employs a dual-step approach: first, minimizing the perturbed loss similar to SAM, and second, reducing the surrogate gap without affecting the perturbed loss to target regions with low loss and sharpness, thus achieving superior generalization. GSAM is theoretically robust, showing better convergence and empirical generalization improvements, notably a +3.2% gain over SAM and +5.4% over AdamW in ImageNet accuracy for ViT-B/32."
    }
  ],
  "including": [
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    }
  ],
  "preprocessing": [
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    }
  ],
  "sparser": [
    {
      "title": "When Vision Transformers Outperform Resnets Without Pre-training or Strong Data Augmentations",
      "url": "/research/2020/03/13/When-Vision-Transformers-Outperform-Resnets-Without-Pre-training-or-Strong-Data-Augmentations.html",
      "date": "March 13, 2020",
      "summary": "This paper explores the potential of Vision Transformers (ViTs) and MLP-Mixers to replace traditional neural architectures, which rely heavily on hand-wired features, by using a general-purpose approach. Despite previous models requiring massive datasets and strong data augmentations, they still faced optimization issues, such as sensitivity to initialization and learning rates. Through examining loss geometry, the study aims to enhance data efficiency and generalization of these models. Findings reveal that the models tend to converge to extremely sharp local minima. The application of a sharpness-aware optimizer significantly boosts the accuracy and robustness of ViTs and MLP-Mixers across a range of tasks, including supervised, adversarial, contrastive, and transfer learning, achieving substantial improvements in accuracy on ImageNet with simple preprocessing techniques. The improved performance is attributed to sparser active neurons in the initial layers, allowing ViTs to surpass the performance of similarly sized ResNets on ImageNet without the need for large-scale pre-training or intensive data augmentations."
    }
  ],
  "attend": [
    {
      "title": "A Phase Transition between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention",
      "url": "/research/2024/02/06/A-Phase-Transition-between-Positional-and-Semantic-Learning-in-a-Solvable-Model-of-Dot-Product-Attention.html",
      "date": "February 6, 2024",
      "summary": "We explore the learning process of a dot-product attention layer, which learns both positional and semantic attention matrices, enabling tokens to attend based on position or meaning. Through experiments on an algorithmic task, we demonstrate that this architecture can use either mechanism for solving the task. Theoretically, we examine a non-linear self-attention layer with special query and key matrices, offering a closed-form solution for its non-convex loss landscape in high-dimensional data, which reveals a phase transition from positional to semantic mechanisms as sample complexity increases. We also show that the dot-product attention layer surpasses a linear positional baseline through the semantic mechanism with adequate data."
    }
  ],
  "mean": [
    {
      "title": "A Phase Transition between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention",
      "url": "/research/2024/02/06/A-Phase-Transition-between-Positional-and-Semantic-Learning-in-a-Solvable-Model-of-Dot-Product-Attention.html",
      "date": "February 6, 2024",
      "summary": "We explore the learning process of a dot-product attention layer, which learns both positional and semantic attention matrices, enabling tokens to attend based on position or meaning. Through experiments on an algorithmic task, we demonstrate that this architecture can use either mechanism for solving the task. Theoretically, we examine a non-linear self-attention layer with special query and key matrices, offering a closed-form solution for its non-convex loss landscape in high-dimensional data, which reveals a phase transition from positional to semantic mechanisms as sample complexity increases. We also show that the dot-product attention layer surpasses a linear positional baseline through the semantic mechanism with adequate data."
    }
  ],
  "form": [
    {
      "title": "A Phase Transition between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention",
      "url": "/research/2024/02/06/A-Phase-Transition-between-Positional-and-Semantic-Learning-in-a-Solvable-Model-of-Dot-Product-Attention.html",
      "date": "February 6, 2024",
      "summary": "We explore the learning process of a dot-product attention layer, which learns both positional and semantic attention matrices, enabling tokens to attend based on position or meaning. Through experiments on an algorithmic task, we demonstrate that this architecture can use either mechanism for solving the task. Theoretically, we examine a non-linear self-attention layer with special query and key matrices, offering a closed-form solution for its non-convex loss landscape in high-dimensional data, which reveals a phase transition from positional to semantic mechanisms as sample complexity increases. We also show that the dot-product attention layer surpasses a linear positional baseline through the semantic mechanism with adequate data."
    },
    {
      "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum",
      "url": "/research/2023/09/30/When-to-Learn-What:-Model-Adaptive-Data-Augmentation-Curriculum.html",
      "date": "September 30, 2023",
      "summary": "Data augmentation (DA) improves neural network generalization by enforcing invariances and symmetries to predefined transformations applied to input data. However, fixed augmentation policies affect samples differently at various training stages, and existing approaches cannot adapt policies to individual samples and the training model. This paper proposes Model-Adaptive Data Augmentation (MADAug) which trains an augmentation policy network to determine when to learn what. Unlike previous work, MADAug selects augmentation operators for each input image with a model-adaptive policy that varies between training stages, creating an optimized data augmentation curriculum. The policy is trained using a bi-level optimization scheme to minimize validation-set loss of a model trained with policy-produced augmentations. Extensive evaluations on multiple image classification tasks and network architectures show that MADAug outperforms or matches existing DA approaches, enhances fairness by improving all classes, particularly the difficult ones, and performs better when transferred to fine-grained datasets. Additionally, the auto-optimized policy in MADAug gradually increases perturbations, forming an easy-to-hard curriculum."
    },
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "define": [
    {
      "title": "ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Network",
      "url": "/research/2021/06/29/ASAM-Adaptive-Sharpness-Aware-Minimization-for-Scale-Invariant-Learning-of-Deep-Neural-Network.html",
      "date": "June 29, 2021",
      "summary": "Recent advances in learning algorithms have highlighted the importance of the sharpness of the loss surface as a key indicator of the generalization gap, achieving state-of-the-art results. However, traditional sharpness measures, defined within a fixed region, suffer from sensitivity to parameter scaling, weakening their predictive value for the generalization gap. This paper introduces a scale-invariant measure, adaptive sharpness, along with a new generalization bound. We present a new learning method, adaptive sharpness-aware minimization (ASAM), which leverages this bound. Our experiments across various benchmark datasets demonstrate that ASAM significantly enhances model generalization."
    }
  ],
  "weaken": [
    {
      "title": "ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Network",
      "url": "/research/2021/06/29/ASAM-Adaptive-Sharpness-Aware-Minimization-for-Scale-Invariant-Learning-of-Deep-Neural-Network.html",
      "date": "June 29, 2021",
      "summary": "Recent advances in learning algorithms have highlighted the importance of the sharpness of the loss surface as a key indicator of the generalization gap, achieving state-of-the-art results. However, traditional sharpness measures, defined within a fixed region, suffer from sensitivity to parameter scaling, weakening their predictive value for the generalization gap. This paper introduces a scale-invariant measure, adaptive sharpness, along with a new generalization bound. We present a new learning method, adaptive sharpness-aware minimization (ASAM), which leverages this bound. Our experiments across various benchmark datasets demonstrate that ASAM significantly enhances model generalization."
    }
  ],
  "artprompt": [
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    }
  ],
  "ascii": [
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    }
  ],
  "jailbreak": [
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    }
  ],
  "llms": [
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    },
    {
      "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
      "url": "/research/2024/01/24/Better-Call-GPT,-Comparing-Large-Language-Models-Against-Lawyers.html",
      "date": "January 24, 2024",
      "summary": "This paper compares Large Language Models (LLMs) with traditional legal contract reviewers—Junior Lawyers and Legal Process Outsourcers (LPOs). It evaluates whether LLMs outperform humans in accuracy, speed, and cost-efficiency during contract review. Empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, revealing that advanced models match or exceed human accuracy in identifying legal issues. LLMs complete reviews in seconds, vastly outpacing the hours required by humans. Cost-wise, LLMs achieve a 99.97 percent reduction compared to traditional methods. These findings indicate a transformative shift in legal practice, with LLMs enhancing the accessibility and efficiency of legal services. The research suggests that LLM dominance in legal contract review challenges the status quo, necessitating a reimagined future for legal workflows."
    }
  ],
  "safety": [
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    }
  ],
  "forum": [
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    }
  ],
  "bypass": [
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    }
  ],
  "vitc": [
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    }
  ],
  "gpt35": [
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    }
  ],
  "gemini": [
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    }
  ],
  "llama2": [
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    }
  ],
  "exploit": [
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    },
    {
      "title": "From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning",
      "url": "/research/2023/03/16/From-MNIST-to-ImageNet-and-Back:-Benchmarking-Continual-Curriculum-Learning.html",
      "date": "March 16, 2023",
      "summary": "Continual learning (CL) is a promising trend in machine learning, aiming to develop robust models and strategies for dynamic environments by incorporating new knowledge while retaining past knowledge. CL research is fragmented, with various protocols, tasks, datasets, and metrics, often not reflecting real-world complexity and tailored to specific strategies. This work addresses this gap by introducing two novel CL benchmarks with heterogeneous tasks from six image datasets, varying in complexity and quality. These benchmarks fairly evaluate state-of-the-art CL strategies in scenarios closer to real-world conditions. The benchmarks present tasks in both increasing and decreasing complexity, assessing models' ability to exploit task structure. The work emphasizes a rigorous, reproducible evaluation protocol for measuring generalization and memory retention in models. Experimental results show that popular CL strategies perform poorly on these benchmarks, exhibit high forgetting, and struggle with curriculum task ordering. These findings underscore the need for rigorous comparisons and the development of new CL strategies for complex scenarios."
    }
  ],
  "vulnerability": [
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    }
  ],
  "access": [
    {
      "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "url": "/research/2024/02/22/ArtPrompt-ASCII-Art-based-Jailbreak-Attacks-against-Aligned-LLMs.html",
      "date": "February 22, 2024",
      "summary": "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
    }
  ],
  "novo": [
    {
      "title": "Atomically Accurate De Novo Design of Single-domain Antibodies",
      "url": "/research/2024/03/18/Atomically-Accurate-De-Novo-Design-of-Single-domain-Antibodies.html",
      "date": "March 18, 2024",
      "summary": "Despite the critical role antibodies play in medicine, current methods for designing new antibodies targeting specific epitopes are time-consuming. Here, we showcase the effectiveness of a refined RFdiffusion network in designing novel antibody variable heavy chains (VHH’s) to target user-specified epitopes. Through experiments, we validate the binding capability of these designed VHH's to four disease-relevant epitopes. Furthermore, the cryo-EM structure analysis reveals that a designed VHH bound to influenza hemagglutinin closely matches the design model, affirming the accuracy of our approach."
    }
  ],
  "antibody": [
    {
      "title": "Atomically Accurate De Novo Design of Single-domain Antibodies",
      "url": "/research/2024/03/18/Atomically-Accurate-De-Novo-Design-of-Single-domain-Antibodies.html",
      "date": "March 18, 2024",
      "summary": "Despite the critical role antibodies play in medicine, current methods for designing new antibodies targeting specific epitopes are time-consuming. Here, we showcase the effectiveness of a refined RFdiffusion network in designing novel antibody variable heavy chains (VHH’s) to target user-specified epitopes. Through experiments, we validate the binding capability of these designed VHH's to four disease-relevant epitopes. Furthermore, the cryo-EM structure analysis reveals that a designed VHH bound to influenza hemagglutinin closely matches the design model, affirming the accuracy of our approach."
    }
  ],
  "medicine": [
    {
      "title": "Atomically Accurate De Novo Design of Single-domain Antibodies",
      "url": "/research/2024/03/18/Atomically-Accurate-De-Novo-Design-of-Single-domain-Antibodies.html",
      "date": "March 18, 2024",
      "summary": "Despite the critical role antibodies play in medicine, current methods for designing new antibodies targeting specific epitopes are time-consuming. Here, we showcase the effectiveness of a refined RFdiffusion network in designing novel antibody variable heavy chains (VHH’s) to target user-specified epitopes. Through experiments, we validate the binding capability of these designed VHH's to four disease-relevant epitopes. Furthermore, the cryo-EM structure analysis reveals that a designed VHH bound to influenza hemagglutinin closely matches the design model, affirming the accuracy of our approach."
    }
  ],
  "epitope": [
    {
      "title": "Atomically Accurate De Novo Design of Single-domain Antibodies",
      "url": "/research/2024/03/18/Atomically-Accurate-De-Novo-Design-of-Single-domain-Antibodies.html",
      "date": "March 18, 2024",
      "summary": "Despite the critical role antibodies play in medicine, current methods for designing new antibodies targeting specific epitopes are time-consuming. Here, we showcase the effectiveness of a refined RFdiffusion network in designing novel antibody variable heavy chains (VHH’s) to target user-specified epitopes. Through experiments, we validate the binding capability of these designed VHH's to four disease-relevant epitopes. Furthermore, the cryo-EM structure analysis reveals that a designed VHH bound to influenza hemagglutinin closely matches the design model, affirming the accuracy of our approach."
    }
  ],
  "consuming": [
    {
      "title": "Atomically Accurate De Novo Design of Single-domain Antibodies",
      "url": "/research/2024/03/18/Atomically-Accurate-De-Novo-Design-of-Single-domain-Antibodies.html",
      "date": "March 18, 2024",
      "summary": "Despite the critical role antibodies play in medicine, current methods for designing new antibodies targeting specific epitopes are time-consuming. Here, we showcase the effectiveness of a refined RFdiffusion network in designing novel antibody variable heavy chains (VHH’s) to target user-specified epitopes. Through experiments, we validate the binding capability of these designed VHH's to four disease-relevant epitopes. Furthermore, the cryo-EM structure analysis reveals that a designed VHH bound to influenza hemagglutinin closely matches the design model, affirming the accuracy of our approach."
    }
  ],
  "showcase": [
    {
      "title": "Atomically Accurate De Novo Design of Single-domain Antibodies",
      "url": "/research/2024/03/18/Atomically-Accurate-De-Novo-Design-of-Single-domain-Antibodies.html",
      "date": "March 18, 2024",
      "summary": "Despite the critical role antibodies play in medicine, current methods for designing new antibodies targeting specific epitopes are time-consuming. Here, we showcase the effectiveness of a refined RFdiffusion network in designing novel antibody variable heavy chains (VHH’s) to target user-specified epitopes. Through experiments, we validate the binding capability of these designed VHH's to four disease-relevant epitopes. Furthermore, the cryo-EM structure analysis reveals that a designed VHH bound to influenza hemagglutinin closely matches the design model, affirming the accuracy of our approach."
    },
    {
      "title": "SGDR: Stochastic Gradient Descent With Warm Restarts",
      "url": "/research/2017/05/02/SGDR-Stochastic-Gradient-Descent-With-Warm-Restarts.html",
      "date": "May 2, 2017",
      "summary": "This paper introduces a warm restart technique for stochastic gradient descent aimed at enhancing anytime performance in deep neural network training. It showcases empirical performance improvements on CIFAR-10 and CIFAR-100 datasets, achieving state-of-the-art results with 3.14% and 16.21% error rates, respectively. Additionally, the technique's benefits are demonstrated on an EEG dataset and a downsampled ImageNet dataset."
    }
  ],
  "rfdiffusion": [
    {
      "title": "Atomically Accurate De Novo Design of Single-domain Antibodies",
      "url": "/research/2024/03/18/Atomically-Accurate-De-Novo-Design-of-Single-domain-Antibodies.html",
      "date": "March 18, 2024",
      "summary": "Despite the critical role antibodies play in medicine, current methods for designing new antibodies targeting specific epitopes are time-consuming. Here, we showcase the effectiveness of a refined RFdiffusion network in designing novel antibody variable heavy chains (VHH’s) to target user-specified epitopes. Through experiments, we validate the binding capability of these designed VHH's to four disease-relevant epitopes. Furthermore, the cryo-EM structure analysis reveals that a designed VHH bound to influenza hemagglutinin closely matches the design model, affirming the accuracy of our approach."
    }
  ],
  "variable": [
    {
      "title": "Atomically Accurate De Novo Design of Single-domain Antibodies",
      "url": "/research/2024/03/18/Atomically-Accurate-De-Novo-Design-of-Single-domain-Antibodies.html",
      "date": "March 18, 2024",
      "summary": "Despite the critical role antibodies play in medicine, current methods for designing new antibodies targeting specific epitopes are time-consuming. Here, we showcase the effectiveness of a refined RFdiffusion network in designing novel antibody variable heavy chains (VHH’s) to target user-specified epitopes. Through experiments, we validate the binding capability of these designed VHH's to four disease-relevant epitopes. Furthermore, the cryo-EM structure analysis reveals that a designed VHH bound to influenza hemagglutinin closely matches the design model, affirming the accuracy of our approach."
    }
  ],
  "vhh": [
    {
      "title": "Atomically Accurate De Novo Design of Single-domain Antibodies",
      "url": "/research/2024/03/18/Atomically-Accurate-De-Novo-Design-of-Single-domain-Antibodies.html",
      "date": "March 18, 2024",
      "summary": "Despite the critical role antibodies play in medicine, current methods for designing new antibodies targeting specific epitopes are time-consuming. Here, we showcase the effectiveness of a refined RFdiffusion network in designing novel antibody variable heavy chains (VHH’s) to target user-specified epitopes. Through experiments, we validate the binding capability of these designed VHH's to four disease-relevant epitopes. Furthermore, the cryo-EM structure analysis reveals that a designed VHH bound to influenza hemagglutinin closely matches the design model, affirming the accuracy of our approach."
    }
  ],
  "cryo": [
    {
      "title": "Atomically Accurate De Novo Design of Single-domain Antibodies",
      "url": "/research/2024/03/18/Atomically-Accurate-De-Novo-Design-of-Single-domain-Antibodies.html",
      "date": "March 18, 2024",
      "summary": "Despite the critical role antibodies play in medicine, current methods for designing new antibodies targeting specific epitopes are time-consuming. Here, we showcase the effectiveness of a refined RFdiffusion network in designing novel antibody variable heavy chains (VHH’s) to target user-specified epitopes. Through experiments, we validate the binding capability of these designed VHH's to four disease-relevant epitopes. Furthermore, the cryo-EM structure analysis reveals that a designed VHH bound to influenza hemagglutinin closely matches the design model, affirming the accuracy of our approach."
    }
  ],
  "em": [
    {
      "title": "Atomically Accurate De Novo Design of Single-domain Antibodies",
      "url": "/research/2024/03/18/Atomically-Accurate-De-Novo-Design-of-Single-domain-Antibodies.html",
      "date": "March 18, 2024",
      "summary": "Despite the critical role antibodies play in medicine, current methods for designing new antibodies targeting specific epitopes are time-consuming. Here, we showcase the effectiveness of a refined RFdiffusion network in designing novel antibody variable heavy chains (VHH’s) to target user-specified epitopes. Through experiments, we validate the binding capability of these designed VHH's to four disease-relevant epitopes. Furthermore, the cryo-EM structure analysis reveals that a designed VHH bound to influenza hemagglutinin closely matches the design model, affirming the accuracy of our approach."
    }
  ],
  "influenza": [
    {
      "title": "Atomically Accurate De Novo Design of Single-domain Antibodies",
      "url": "/research/2024/03/18/Atomically-Accurate-De-Novo-Design-of-Single-domain-Antibodies.html",
      "date": "March 18, 2024",
      "summary": "Despite the critical role antibodies play in medicine, current methods for designing new antibodies targeting specific epitopes are time-consuming. Here, we showcase the effectiveness of a refined RFdiffusion network in designing novel antibody variable heavy chains (VHH’s) to target user-specified epitopes. Through experiments, we validate the binding capability of these designed VHH's to four disease-relevant epitopes. Furthermore, the cryo-EM structure analysis reveals that a designed VHH bound to influenza hemagglutinin closely matches the design model, affirming the accuracy of our approach."
    }
  ],
  "hemagglutinin": [
    {
      "title": "Atomically Accurate De Novo Design of Single-domain Antibodies",
      "url": "/research/2024/03/18/Atomically-Accurate-De-Novo-Design-of-Single-domain-Antibodies.html",
      "date": "March 18, 2024",
      "summary": "Despite the critical role antibodies play in medicine, current methods for designing new antibodies targeting specific epitopes are time-consuming. Here, we showcase the effectiveness of a refined RFdiffusion network in designing novel antibody variable heavy chains (VHH’s) to target user-specified epitopes. Through experiments, we validate the binding capability of these designed VHH's to four disease-relevant epitopes. Furthermore, the cryo-EM structure analysis reveals that a designed VHH bound to influenza hemagglutinin closely matches the design model, affirming the accuracy of our approach."
    }
  ],
  "affirm": [
    {
      "title": "Atomically Accurate De Novo Design of Single-domain Antibodies",
      "url": "/research/2024/03/18/Atomically-Accurate-De-Novo-Design-of-Single-domain-Antibodies.html",
      "date": "March 18, 2024",
      "summary": "Despite the critical role antibodies play in medicine, current methods for designing new antibodies targeting specific epitopes are time-consuming. Here, we showcase the effectiveness of a refined RFdiffusion network in designing novel antibody variable heavy chains (VHH’s) to target user-specified epitopes. Through experiments, we validate the binding capability of these designed VHH's to four disease-relevant epitopes. Furthermore, the cryo-EM structure analysis reveals that a designed VHH bound to influenza hemagglutinin closely matches the design model, affirming the accuracy of our approach."
    }
  ],
  "partner": [
    {
      "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
      "url": "/research/2024/02/28/CL-MAE:-Curriculum-Learned-Masked-Autoencoders.html",
      "date": "February 28, 2024",
      "summary": "This paper proposes a curriculum learning approach that updates the masking strategy of Masked Auto Encoders (MAE) to progressively increase the complexity of the self-supervised reconstruction task. To achieve this, a novel learnable masking module is introduced, capable of generating masks of varying complexities, and integrated into masked autoencoders (MAE). This module is jointly trained with the MAE, adjusting its behavior from partner (optimizing the same reconstruction loss) to adversary (optimizing the opposite loss), with a smooth transition regulated by a factor multiplied with the reconstruction loss. This training procedure creates an easy-to-hard curriculum. The Curriculum-Learned Masked Autoencoder (CL-MAE) is trained on ImageNet and demonstrates superior representation learning compared to MAE. Empirical results on five downstream tasks confirm that curriculum learning can successfully self-supervise masked autoencoders."
    }
  ],
  "adversary": [
    {
      "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
      "url": "/research/2024/02/28/CL-MAE:-Curriculum-Learned-Masked-Autoencoders.html",
      "date": "February 28, 2024",
      "summary": "This paper proposes a curriculum learning approach that updates the masking strategy of Masked Auto Encoders (MAE) to progressively increase the complexity of the self-supervised reconstruction task. To achieve this, a novel learnable masking module is introduced, capable of generating masks of varying complexities, and integrated into masked autoencoders (MAE). This module is jointly trained with the MAE, adjusting its behavior from partner (optimizing the same reconstruction loss) to adversary (optimizing the opposite loss), with a smooth transition regulated by a factor multiplied with the reconstruction loss. This training procedure creates an easy-to-hard curriculum. The Curriculum-Learned Masked Autoencoder (CL-MAE) is trained on ImageNet and demonstrates superior representation learning compared to MAE. Empirical results on five downstream tasks confirm that curriculum learning can successfully self-supervise masked autoencoders."
    }
  ],
  "regulate": [
    {
      "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
      "url": "/research/2024/02/28/CL-MAE:-Curriculum-Learned-Masked-Autoencoders.html",
      "date": "February 28, 2024",
      "summary": "This paper proposes a curriculum learning approach that updates the masking strategy of Masked Auto Encoders (MAE) to progressively increase the complexity of the self-supervised reconstruction task. To achieve this, a novel learnable masking module is introduced, capable of generating masks of varying complexities, and integrated into masked autoencoders (MAE). This module is jointly trained with the MAE, adjusting its behavior from partner (optimizing the same reconstruction loss) to adversary (optimizing the opposite loss), with a smooth transition regulated by a factor multiplied with the reconstruction loss. This training procedure creates an easy-to-hard curriculum. The Curriculum-Learned Masked Autoencoder (CL-MAE) is trained on ImageNet and demonstrates superior representation learning compared to MAE. Empirical results on five downstream tasks confirm that curriculum learning can successfully self-supervise masked autoencoders."
    },
    {
      "title": "How Does Sharpness-Aware Minimization Minimize Sharpness?",
      "url": "/research/2023/01/05/How-Does-Sharpness-Aware-Minimization-Minimize-Sharpness.html",
      "date": "January 5, 2023",
      "summary": "Sharpness-Aware Minimization (SAM) enhances deep neural networks' generalization across various settings. Although SAM aims to penalize model sharpness using a computationally efficient approach, its exact operational mechanism and the sharpness notion it regularizes remain unclear, partly due to differing sharpness concepts used in its theoretical framework and empirical validation. This study identifies the precise sharpness concept SAM regulates and explains its mechanism. It reveals that the combined effect of SAM's two-step approximations, despite being individually misleading, correctly improves generalization when using full-batch gradients. Moreover, we demonstrate that the stochastic SAM version indeed regularizes a third sharpness notion, aligning more closely with practical performance. This effectiveness is attributed to the gradient's alignment with the Hessian's top eigenvector under SAM."
    }
  ],
  "multiply": [
    {
      "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
      "url": "/research/2024/02/28/CL-MAE:-Curriculum-Learned-Masked-Autoencoders.html",
      "date": "February 28, 2024",
      "summary": "This paper proposes a curriculum learning approach that updates the masking strategy of Masked Auto Encoders (MAE) to progressively increase the complexity of the self-supervised reconstruction task. To achieve this, a novel learnable masking module is introduced, capable of generating masks of varying complexities, and integrated into masked autoencoders (MAE). This module is jointly trained with the MAE, adjusting its behavior from partner (optimizing the same reconstruction loss) to adversary (optimizing the opposite loss), with a smooth transition regulated by a factor multiplied with the reconstruction loss. This training procedure creates an easy-to-hard curriculum. The Curriculum-Learned Masked Autoencoder (CL-MAE) is trained on ImageNet and demonstrates superior representation learning compared to MAE. Empirical results on five downstream tasks confirm that curriculum learning can successfully self-supervise masked autoencoders."
    }
  ],
  "clmae": [
    {
      "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
      "url": "/research/2024/02/28/CL-MAE:-Curriculum-Learned-Masked-Autoencoders.html",
      "date": "February 28, 2024",
      "summary": "This paper proposes a curriculum learning approach that updates the masking strategy of Masked Auto Encoders (MAE) to progressively increase the complexity of the self-supervised reconstruction task. To achieve this, a novel learnable masking module is introduced, capable of generating masks of varying complexities, and integrated into masked autoencoders (MAE). This module is jointly trained with the MAE, adjusting its behavior from partner (optimizing the same reconstruction loss) to adversary (optimizing the opposite loss), with a smooth transition regulated by a factor multiplied with the reconstruction loss. This training procedure creates an easy-to-hard curriculum. The Curriculum-Learned Masked Autoencoder (CL-MAE) is trained on ImageNet and demonstrates superior representation learning compared to MAE. Empirical results on five downstream tasks confirm that curriculum learning can successfully self-supervise masked autoencoders."
    }
  ],
  "successfully": [
    {
      "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
      "url": "/research/2024/02/28/CL-MAE:-Curriculum-Learned-Masked-Autoencoders.html",
      "date": "February 28, 2024",
      "summary": "This paper proposes a curriculum learning approach that updates the masking strategy of Masked Auto Encoders (MAE) to progressively increase the complexity of the self-supervised reconstruction task. To achieve this, a novel learnable masking module is introduced, capable of generating masks of varying complexities, and integrated into masked autoencoders (MAE). This module is jointly trained with the MAE, adjusting its behavior from partner (optimizing the same reconstruction loss) to adversary (optimizing the opposite loss), with a smooth transition regulated by a factor multiplied with the reconstruction loss. This training procedure creates an easy-to-hard curriculum. The Curriculum-Learned Masked Autoencoder (CL-MAE) is trained on ImageNet and demonstrates superior representation learning compared to MAE. Empirical results on five downstream tasks confirm that curriculum learning can successfully self-supervise masked autoencoders."
    }
  ],
  "chronos": [
    {
      "title": "Chronos: Learning the Language of Time Series",
      "url": "/research/2024/03/12/Chronos-Learning-the-Language-of-Time-Series.html",
      "date": "March 12, 2024",
      "summary": "Chronos is a framework that enhances pretrained probabilistic time series models by tokenizing time series data for training with transformer-based architectures, notably the T5 family. Pretrained on a mix of public and synthetic datasets created via Gaussian processes, Chronos models outshine others in 42 benchmark datasets by showing superior performance on familiar datasets and competitive or better zero-shot capabilities on new datasets. This illustrates Chronos's ability to generalize across different domains, offering a simpler approach to forecasting tasks."
    }
  ],
  "chrono": [
    {
      "title": "Chronos: Learning the Language of Time Series",
      "url": "/research/2024/03/12/Chronos-Learning-the-Language-of-Time-Series.html",
      "date": "March 12, 2024",
      "summary": "Chronos is a framework that enhances pretrained probabilistic time series models by tokenizing time series data for training with transformer-based architectures, notably the T5 family. Pretrained on a mix of public and synthetic datasets created via Gaussian processes, Chronos models outshine others in 42 benchmark datasets by showing superior performance on familiar datasets and competitive or better zero-shot capabilities on new datasets. This illustrates Chronos's ability to generalize across different domains, offering a simpler approach to forecasting tasks."
    }
  ],
  "tokenize": [
    {
      "title": "Chronos: Learning the Language of Time Series",
      "url": "/research/2024/03/12/Chronos-Learning-the-Language-of-Time-Series.html",
      "date": "March 12, 2024",
      "summary": "Chronos is a framework that enhances pretrained probabilistic time series models by tokenizing time series data for training with transformer-based architectures, notably the T5 family. Pretrained on a mix of public and synthetic datasets created via Gaussian processes, Chronos models outshine others in 42 benchmark datasets by showing superior performance on familiar datasets and competitive or better zero-shot capabilities on new datasets. This illustrates Chronos's ability to generalize across different domains, offering a simpler approach to forecasting tasks."
    }
  ],
  "t5": [
    {
      "title": "Chronos: Learning the Language of Time Series",
      "url": "/research/2024/03/12/Chronos-Learning-the-Language-of-Time-Series.html",
      "date": "March 12, 2024",
      "summary": "Chronos is a framework that enhances pretrained probabilistic time series models by tokenizing time series data for training with transformer-based architectures, notably the T5 family. Pretrained on a mix of public and synthetic datasets created via Gaussian processes, Chronos models outshine others in 42 benchmark datasets by showing superior performance on familiar datasets and competitive or better zero-shot capabilities on new datasets. This illustrates Chronos's ability to generalize across different domains, offering a simpler approach to forecasting tasks."
    }
  ],
  "outshine": [
    {
      "title": "Chronos: Learning the Language of Time Series",
      "url": "/research/2024/03/12/Chronos-Learning-the-Language-of-Time-Series.html",
      "date": "March 12, 2024",
      "summary": "Chronos is a framework that enhances pretrained probabilistic time series models by tokenizing time series data for training with transformer-based architectures, notably the T5 family. Pretrained on a mix of public and synthetic datasets created via Gaussian processes, Chronos models outshine others in 42 benchmark datasets by showing superior performance on familiar datasets and competitive or better zero-shot capabilities on new datasets. This illustrates Chronos's ability to generalize across different domains, offering a simpler approach to forecasting tasks."
    }
  ],
  "forecast": [
    {
      "title": "Chronos: Learning the Language of Time Series",
      "url": "/research/2024/03/12/Chronos-Learning-the-Language-of-Time-Series.html",
      "date": "March 12, 2024",
      "summary": "Chronos is a framework that enhances pretrained probabilistic time series models by tokenizing time series data for training with transformer-based architectures, notably the T5 family. Pretrained on a mix of public and synthetic datasets created via Gaussian processes, Chronos models outshine others in 42 benchmark datasets by showing superior performance on familiar datasets and competitive or better zero-shot capabilities on new datasets. This illustrates Chronos's ability to generalize across different domains, offering a simpler approach to forecasting tasks."
    }
  ],
  "miscalibration": [
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    }
  ],
  "soften": [
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    }
  ],
  "redistribute": [
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    }
  ],
  "mass": [
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    }
  ],
  "onehot": [
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    }
  ],
  "confuse": [
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    }
  ],
  "ls": [
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    }
  ],
  "multirater": [
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    }
  ],
  "httpsgithubcomaoshuang92confidencecalibrationcl": [
    {
      "title": "Confidence-Aware Calibration and Scoring Functions for Curriculum Learning",
      "url": "/research/2023/01/29/Confidence-Aware-Calibration-and-Scoring-Functions-for-Curriculum-Learning.html",
      "date": "January 29, 2023",
      "summary": "State-of-the-art deep neural networks often exhibit over-confidence in predictions, indicating miscalibration. Label Smoothing has been proposed to address this by softening hard targets during training, redistributing part of the probability mass from a ‘one-hot’ label uniformly to all other labels. However, neither model nor human confidence in a label is likely uniformly distributed, as some labels are more likely to be confused than others. This paper integrates model and human confidence with label smoothing, termed Model Confidence LS and Human Confidence LS, to improve model calibration and generalization. The study demonstrates how these confidence scores enhance curriculum learning, a strategy inspired by progressing from easier to harder tasks. Higher confidence scores indicate more recognizable and easier samples, serving as a scoring function to rank samples in curriculum learning. Evaluations using four state-of-the-art architectures for image and text classification, with multi-rater label annotations, show that integrating confidence information in label smoothing and curriculum learning improves both model performance and calibration. The code is available at https://github.com/AoShuang92/Confidence-Calibration-CL."
    }
  ],
  "dichotomy": [
    {
      "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
      "url": "/research/2023/11/30/Dichotomy-of-Early-and-Late-Phase-Implicit-Biases-Can-Provably-Induce-Grokking.html",
      "date": "November 30, 2023",
      "summary": "Power et al. (2022) discovered that neural networks initially memorize arithmetic tasks, achieving perfect training accuracy but poor test accuracy, then suddenly achieve perfect test accuracy after extended training. This study explains this grokking phenomenon with the theory that early and late phase implicit biases cause this shift. Training homogeneous neural networks with large initialization and small weight decay on classification and regression tasks results in a prolonged period where the network acts like a kernel predictor, followed by a sudden shift to min-norm/max-margin predictors, significantly improving test accuracy."
    }
  ],
  "induce": [
    {
      "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
      "url": "/research/2023/11/30/Dichotomy-of-Early-and-Late-Phase-Implicit-Biases-Can-Provably-Induce-Grokking.html",
      "date": "November 30, 2023",
      "summary": "Power et al. (2022) discovered that neural networks initially memorize arithmetic tasks, achieving perfect training accuracy but poor test accuracy, then suddenly achieve perfect test accuracy after extended training. This study explains this grokking phenomenon with the theory that early and late phase implicit biases cause this shift. Training homogeneous neural networks with large initialization and small weight decay on classification and regression tasks results in a prolonged period where the network acts like a kernel predictor, followed by a sudden shift to min-norm/max-margin predictors, significantly improving test accuracy."
    },
    {
      "title": "Fine-tuning with Very Large Dropout",
      "url": "/research/2024/03/01/Fine-tuning-with-Very-Large-Dropout.html",
      "date": "March 1, 2024",
      "summary": "This study challenges the notion that machine learning practices assume training and testing data have the same distribution. It explores the effectiveness of high dropout rates, as opposed to ensemble techniques, in developing rich data representations suitable for multiple distribution scenarios. These representations surpass those achieved by traditional in-distribution performance regularization and the implicit sparsity induced by common stochastic gradient methods. While training deep networks from scratch with high dropout rates is impractical, fine-tuning pre-trained models under these conditions is feasible and yields better out-of-distribution performance than ensembles and model averaging techniques like model soups. This finding is significant due to the growing relevance of fine-tuning with large pre-trained models, offering insights into the nature of rich representations and the linear characteristics of fine-tuning large networks with small datasets."
    }
  ],
  "et": [
    {
      "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
      "url": "/research/2023/11/30/Dichotomy-of-Early-and-Late-Phase-Implicit-Biases-Can-Provably-Induce-Grokking.html",
      "date": "November 30, 2023",
      "summary": "Power et al. (2022) discovered that neural networks initially memorize arithmetic tasks, achieving perfect training accuracy but poor test accuracy, then suddenly achieve perfect test accuracy after extended training. This study explains this grokking phenomenon with the theory that early and late phase implicit biases cause this shift. Training homogeneous neural networks with large initialization and small weight decay on classification and regression tasks results in a prolonged period where the network acts like a kernel predictor, followed by a sudden shift to min-norm/max-margin predictors, significantly improving test accuracy."
    },
    {
      "title": "On the Relationship Between Self-Attention And Convolutional Layers",
      "url": "/research/2020/01/10/On-the-Relationship-Between-Self-Attention-And-Convolutional-Layers.html",
      "date": "January 10, 2020",
      "summary": "Recent studies have shown that attention mechanisms can rival or even outperform convolutional layers in vision tasks, challenging their dominance. Ramachandran et al. (2019) demonstrated that attention could entirely replace convolution, achieving top results. This work investigates whether attention layers function like convolutional layers, finding that multi-head self-attention layers, with enough heads, match the expressiveness of convolutional layers. Numerical experiments confirm that self-attention layers learn to focus on pixel-grid patterns akin to convolutional layers, supporting our findings."
    }
  ],
  "al": [
    {
      "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
      "url": "/research/2023/11/30/Dichotomy-of-Early-and-Late-Phase-Implicit-Biases-Can-Provably-Induce-Grokking.html",
      "date": "November 30, 2023",
      "summary": "Power et al. (2022) discovered that neural networks initially memorize arithmetic tasks, achieving perfect training accuracy but poor test accuracy, then suddenly achieve perfect test accuracy after extended training. This study explains this grokking phenomenon with the theory that early and late phase implicit biases cause this shift. Training homogeneous neural networks with large initialization and small weight decay on classification and regression tasks results in a prolonged period where the network acts like a kernel predictor, followed by a sudden shift to min-norm/max-margin predictors, significantly improving test accuracy."
    },
    {
      "title": "On the Relationship Between Self-Attention And Convolutional Layers",
      "url": "/research/2020/01/10/On-the-Relationship-Between-Self-Attention-And-Convolutional-Layers.html",
      "date": "January 10, 2020",
      "summary": "Recent studies have shown that attention mechanisms can rival or even outperform convolutional layers in vision tasks, challenging their dominance. Ramachandran et al. (2019) demonstrated that attention could entirely replace convolution, achieving top results. This work investigates whether attention layers function like convolutional layers, finding that multi-head self-attention layers, with enough heads, match the expressiveness of convolutional layers. Numerical experiments confirm that self-attention layers learn to focus on pixel-grid patterns akin to convolutional layers, supporting our findings."
    }
  ],
  "arithmetic": [
    {
      "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
      "url": "/research/2023/11/30/Dichotomy-of-Early-and-Late-Phase-Implicit-Biases-Can-Provably-Induce-Grokking.html",
      "date": "November 30, 2023",
      "summary": "Power et al. (2022) discovered that neural networks initially memorize arithmetic tasks, achieving perfect training accuracy but poor test accuracy, then suddenly achieve perfect test accuracy after extended training. This study explains this grokking phenomenon with the theory that early and late phase implicit biases cause this shift. Training homogeneous neural networks with large initialization and small weight decay on classification and regression tasks results in a prolonged period where the network acts like a kernel predictor, followed by a sudden shift to min-norm/max-margin predictors, significantly improving test accuracy."
    }
  ],
  "min": [
    {
      "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
      "url": "/research/2023/11/30/Dichotomy-of-Early-and-Late-Phase-Implicit-Biases-Can-Provably-Induce-Grokking.html",
      "date": "November 30, 2023",
      "summary": "Power et al. (2022) discovered that neural networks initially memorize arithmetic tasks, achieving perfect training accuracy but poor test accuracy, then suddenly achieve perfect test accuracy after extended training. This study explains this grokking phenomenon with the theory that early and late phase implicit biases cause this shift. Training homogeneous neural networks with large initialization and small weight decay on classification and regression tasks results in a prolonged period where the network acts like a kernel predictor, followed by a sudden shift to min-norm/max-margin predictors, significantly improving test accuracy."
    }
  ],
  "normmax": [
    {
      "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
      "url": "/research/2023/11/30/Dichotomy-of-Early-and-Late-Phase-Implicit-Biases-Can-Provably-Induce-Grokking.html",
      "date": "November 30, 2023",
      "summary": "Power et al. (2022) discovered that neural networks initially memorize arithmetic tasks, achieving perfect training accuracy but poor test accuracy, then suddenly achieve perfect test accuracy after extended training. This study explains this grokking phenomenon with the theory that early and late phase implicit biases cause this shift. Training homogeneous neural networks with large initialization and small weight decay on classification and regression tasks results in a prolonged period where the network acts like a kernel predictor, followed by a sudden shift to min-norm/max-margin predictors, significantly improving test accuracy."
    }
  ],
  "margin": [
    {
      "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
      "url": "/research/2023/11/30/Dichotomy-of-Early-and-Late-Phase-Implicit-Biases-Can-Provably-Induce-Grokking.html",
      "date": "November 30, 2023",
      "summary": "Power et al. (2022) discovered that neural networks initially memorize arithmetic tasks, achieving perfect training accuracy but poor test accuracy, then suddenly achieve perfect test accuracy after extended training. This study explains this grokking phenomenon with the theory that early and late phase implicit biases cause this shift. Training homogeneous neural networks with large initialization and small weight decay on classification and regression tasks results in a prolonged period where the network acts like a kernel predictor, followed by a sudden shift to min-norm/max-margin predictors, significantly improving test accuracy."
    }
  ],
  "ultrafastbert": [
    {
      "title": "Exponentially Faster Language Modeling",
      "url": "/research/2023/11/21/Exponentially-Faster-Language-Modeling.html",
      "date": "November 21, 2023",
      "summary": "UltraFastBERT, a BERT variant, operates with just 0.3% of its neurons—selectively using 12 out of 4095 neurons per layer—for inference, matching the performance of similar models. It replaces conventional feedforward networks with fast feedforward networks (FFFs) to achieve this efficiency. Although fully efficient conditional neural execution isn't yet practical, we offer a high-level CPU code that achieves a 78x speedup and a PyTorch implementation with a 40x speedup over standard batched feedforward inference."
    }
  ],
  "cpu": [
    {
      "title": "Exponentially Faster Language Modeling",
      "url": "/research/2023/11/21/Exponentially-Faster-Language-Modeling.html",
      "date": "November 21, 2023",
      "summary": "UltraFastBERT, a BERT variant, operates with just 0.3% of its neurons—selectively using 12 out of 4095 neurons per layer—for inference, matching the performance of similar models. It replaces conventional feedforward networks with fast feedforward networks (FFFs) to achieve this efficiency. Although fully efficient conditional neural execution isn't yet practical, we offer a high-level CPU code that achieves a 78x speedup and a PyTorch implementation with a 40x speedup over standard batched feedforward inference."
    }
  ],
  "sparsity": [
    {
      "title": "Fine-tuning with Very Large Dropout",
      "url": "/research/2024/03/01/Fine-tuning-with-Very-Large-Dropout.html",
      "date": "March 1, 2024",
      "summary": "This study challenges the notion that machine learning practices assume training and testing data have the same distribution. It explores the effectiveness of high dropout rates, as opposed to ensemble techniques, in developing rich data representations suitable for multiple distribution scenarios. These representations surpass those achieved by traditional in-distribution performance regularization and the implicit sparsity induced by common stochastic gradient methods. While training deep networks from scratch with high dropout rates is impractical, fine-tuning pre-trained models under these conditions is feasible and yields better out-of-distribution performance than ensembles and model averaging techniques like model soups. This finding is significant due to the growing relevance of fine-tuning with large pre-trained models, offering insights into the nature of rich representations and the linear characteristics of fine-tuning large networks with small datasets."
    }
  ],
  "soup": [
    {
      "title": "Fine-tuning with Very Large Dropout",
      "url": "/research/2024/03/01/Fine-tuning-with-Very-Large-Dropout.html",
      "date": "March 1, 2024",
      "summary": "This study challenges the notion that machine learning practices assume training and testing data have the same distribution. It explores the effectiveness of high dropout rates, as opposed to ensemble techniques, in developing rich data representations suitable for multiple distribution scenarios. These representations surpass those achieved by traditional in-distribution performance regularization and the implicit sparsity induced by common stochastic gradient methods. While training deep networks from scratch with high dropout rates is impractical, fine-tuning pre-trained models under these conditions is feasible and yields better out-of-distribution performance than ensembles and model averaging techniques like model soups. This finding is significant due to the growing relevance of fine-tuning with large pre-trained models, offering insights into the nature of rich representations and the linear characteristics of fine-tuning large networks with small datasets."
    }
  ],
  "relevance": [
    {
      "title": "Fine-tuning with Very Large Dropout",
      "url": "/research/2024/03/01/Fine-tuning-with-Very-Large-Dropout.html",
      "date": "March 1, 2024",
      "summary": "This study challenges the notion that machine learning practices assume training and testing data have the same distribution. It explores the effectiveness of high dropout rates, as opposed to ensemble techniques, in developing rich data representations suitable for multiple distribution scenarios. These representations surpass those achieved by traditional in-distribution performance regularization and the implicit sparsity induced by common stochastic gradient methods. While training deep networks from scratch with high dropout rates is impractical, fine-tuning pre-trained models under these conditions is feasible and yields better out-of-distribution performance than ensembles and model averaging techniques like model soups. This finding is significant due to the growing relevance of fine-tuning with large pre-trained models, offering insights into the nature of rich representations and the linear characteristics of fine-tuning large networks with small datasets."
    }
  ],
  "hypo": [
    {
      "title": "HYPO: Hyperspherical Out-of-Distribution Generalization",
      "url": "/research/2024/03/19/HYPO-Hyperspherical-Out-of-Distribution-Generalization.html",
      "date": "March 19, 2024",
      "summary": "We propose HYPO, a novel framework for out-of-distribution (OOD) generalization in machine learning, which learns domain-invariant features in a hyperspherical space. Our method focuses on aligning features of the same class across different domains close to their class prototypes and separating different class prototypes maximally. We offer theoretical justifications for its improvement on OOD generalization and show through experiments on OOD benchmarks that HYPO outperforms existing baselines with superior performance."
    }
  ],
  "ood": [
    {
      "title": "HYPO: Hyperspherical Out-of-Distribution Generalization",
      "url": "/research/2024/03/19/HYPO-Hyperspherical-Out-of-Distribution-Generalization.html",
      "date": "March 19, 2024",
      "summary": "We propose HYPO, a novel framework for out-of-distribution (OOD) generalization in machine learning, which learns domain-invariant features in a hyperspherical space. Our method focuses on aligning features of the same class across different domains close to their class prototypes and separating different class prototypes maximally. We offer theoretical justifications for its improvement on OOD generalization and show through experiments on OOD benchmarks that HYPO outperforms existing baselines with superior performance."
    }
  ],
  "regularizes": [
    {
      "title": "How Does Sharpness-Aware Minimization Minimize Sharpness?",
      "url": "/research/2023/01/05/How-Does-Sharpness-Aware-Minimization-Minimize-Sharpness.html",
      "date": "January 5, 2023",
      "summary": "Sharpness-Aware Minimization (SAM) enhances deep neural networks' generalization across various settings. Although SAM aims to penalize model sharpness using a computationally efficient approach, its exact operational mechanism and the sharpness notion it regularizes remain unclear, partly due to differing sharpness concepts used in its theoretical framework and empirical validation. This study identifies the precise sharpness concept SAM regulates and explains its mechanism. It reveals that the combined effect of SAM's two-step approximations, despite being individually misleading, correctly improves generalization when using full-batch gradients. Moreover, we demonstrate that the stochastic SAM version indeed regularizes a third sharpness notion, aligning more closely with practical performance. This effectiveness is attributed to the gradient's alignment with the Hessian's top eigenvector under SAM."
    }
  ],
  "differing": [
    {
      "title": "How Does Sharpness-Aware Minimization Minimize Sharpness?",
      "url": "/research/2023/01/05/How-Does-Sharpness-Aware-Minimization-Minimize-Sharpness.html",
      "date": "January 5, 2023",
      "summary": "Sharpness-Aware Minimization (SAM) enhances deep neural networks' generalization across various settings. Although SAM aims to penalize model sharpness using a computationally efficient approach, its exact operational mechanism and the sharpness notion it regularizes remain unclear, partly due to differing sharpness concepts used in its theoretical framework and empirical validation. This study identifies the precise sharpness concept SAM regulates and explains its mechanism. It reveals that the combined effect of SAM's two-step approximations, despite being individually misleading, correctly improves generalization when using full-batch gradients. Moreover, we demonstrate that the stochastic SAM version indeed regularizes a third sharpness notion, aligning more closely with practical performance. This effectiveness is attributed to the gradient's alignment with the Hessian's top eigenvector under SAM."
    }
  ],
  "jepa": [
    {
      "title": "Learning and Leveraging World Models in Visual Representation Learning",
      "url": "/research/2024/03/01/Learning-and-Leveraging-World-Models-in-Visual-Representation-Learning.html",
      "date": "March 1, 2024",
      "summary": "The Joint-Embedding Predictive Architecture (JEPA) is a self-supervised method previously used for predicting missing input parts. This study expands JEPA to predict a wider range of corruptions by introducing Image World Models (IWM), which learn the effects of global photometric transformations in latent space. Key to effective IWM learning are conditioning, prediction difficulty, and model capacity. The study demonstrates that IWM's predictive world model, when fine-tuned, can tackle diverse tasks and either matches or outperforms existing self-supervised techniques. Additionally, it enables control over the abstraction level of learned representations, achieving either invariant or equivariant representations, similar to contrastive methods or masked image modeling, respectively."
    }
  ],
  "iwm": [
    {
      "title": "Learning and Leveraging World Models in Visual Representation Learning",
      "url": "/research/2024/03/01/Learning-and-Leveraging-World-Models-in-Visual-Representation-Learning.html",
      "date": "March 1, 2024",
      "summary": "The Joint-Embedding Predictive Architecture (JEPA) is a self-supervised method previously used for predicting missing input parts. This study expands JEPA to predict a wider range of corruptions by introducing Image World Models (IWM), which learn the effects of global photometric transformations in latent space. Key to effective IWM learning are conditioning, prediction difficulty, and model capacity. The study demonstrates that IWM's predictive world model, when fine-tuned, can tackle diverse tasks and either matches or outperforms existing self-supervised techniques. Additionally, it enables control over the abstraction level of learned representations, achieving either invariant or equivariant representations, similar to contrastive methods or masked image modeling, respectively."
    }
  ],
  "fine": [
    {
      "title": "Learning and Leveraging World Models in Visual Representation Learning",
      "url": "/research/2024/03/01/Learning-and-Leveraging-World-Models-in-Visual-Representation-Learning.html",
      "date": "March 1, 2024",
      "summary": "The Joint-Embedding Predictive Architecture (JEPA) is a self-supervised method previously used for predicting missing input parts. This study expands JEPA to predict a wider range of corruptions by introducing Image World Models (IWM), which learn the effects of global photometric transformations in latent space. Key to effective IWM learning are conditioning, prediction difficulty, and model capacity. The study demonstrates that IWM's predictive world model, when fine-tuned, can tackle diverse tasks and either matches or outperforms existing self-supervised techniques. Additionally, it enables control over the abstraction level of learned representations, achieving either invariant or equivariant representations, similar to contrastive methods or masked image modeling, respectively."
    }
  ],
  "abstraction": [
    {
      "title": "Learning and Leveraging World Models in Visual Representation Learning",
      "url": "/research/2024/03/01/Learning-and-Leveraging-World-Models-in-Visual-Representation-Learning.html",
      "date": "March 1, 2024",
      "summary": "The Joint-Embedding Predictive Architecture (JEPA) is a self-supervised method previously used for predicting missing input parts. This study expands JEPA to predict a wider range of corruptions by introducing Image World Models (IWM), which learn the effects of global photometric transformations in latent space. Key to effective IWM learning are conditioning, prediction difficulty, and model capacity. The study demonstrates that IWM's predictive world model, when fine-tuned, can tackle diverse tasks and either matches or outperforms existing self-supervised techniques. Additionally, it enables control over the abstraction level of learned representations, achieving either invariant or equivariant representations, similar to contrastive methods or masked image modeling, respectively."
    }
  ],
  "moe": [
    {
      "title": "MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts",
      "url": "/research/2024/02/26/MoE-Mamba-Efficient-Selective-State-Space-Models-with-Mixture-of-Experts.html",
      "date": "February 26, 2024",
      "summary": "State Space Models (SSMs) are emerging as strong competitors in sequential modeling, challenging Transformers. Integrating Mixture of Experts (MoE) has enhanced Transformer-based models, including cutting-edge open models. We suggest combining SSMs with MoE to further unlock their scaling potential. Our model, MoE-Mamba, based on the SSM model Mamba, exceeds the performance of both Mamba and traditional Transformer-MoE models. Notably, MoE-Mamba achieves Mamba's performance with 2.35 times fewer training steps, maintaining Mamba's inference advantages over Transformers."
    }
  ],
  "competitor": [
    {
      "title": "MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts",
      "url": "/research/2024/02/26/MoE-Mamba-Efficient-Selective-State-Space-Models-with-Mixture-of-Experts.html",
      "date": "February 26, 2024",
      "summary": "State Space Models (SSMs) are emerging as strong competitors in sequential modeling, challenging Transformers. Integrating Mixture of Experts (MoE) has enhanced Transformer-based models, including cutting-edge open models. We suggest combining SSMs with MoE to further unlock their scaling potential. Our model, MoE-Mamba, based on the SSM model Mamba, exceeds the performance of both Mamba and traditional Transformer-MoE models. Notably, MoE-Mamba achieves Mamba's performance with 2.35 times fewer training steps, maintaining Mamba's inference advantages over Transformers."
    }
  ],
  "unlock": [
    {
      "title": "MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts",
      "url": "/research/2024/02/26/MoE-Mamba-Efficient-Selective-State-Space-Models-with-Mixture-of-Experts.html",
      "date": "February 26, 2024",
      "summary": "State Space Models (SSMs) are emerging as strong competitors in sequential modeling, challenging Transformers. Integrating Mixture of Experts (MoE) has enhanced Transformer-based models, including cutting-edge open models. We suggest combining SSMs with MoE to further unlock their scaling potential. Our model, MoE-Mamba, based on the SSM model Mamba, exceeds the performance of both Mamba and traditional Transformer-MoE models. Notably, MoE-Mamba achieves Mamba's performance with 2.35 times fewer training steps, maintaining Mamba's inference advantages over Transformers."
    }
  ],
  "extract": [
    {
      "title": "Neural Network Diffusion",
      "url": "/research/2024/02/20/Neural-Network-Diffusion.html",
      "date": "February 20, 2024",
      "summary": "Diffusion models, known for their success in image and video generation, can also generate high-performing neural network parameters, as demonstrated in this study. By employing a simple combination of an autoencoder and a standard latent diffusion model, our method involves extracting latent representations of network parameters, which are then synthesized from random noise by the diffusion model. These new representations, processed through the autoencoder's decoder, serve as fresh network parameters. Tested across various architectures and datasets, this diffusion approach consistently produces models with comparable or superior performance to traditionally trained networks at minimal extra cost. Importantly, the generated models show distinct performance differences from the trained ones, suggesting further exploration into the versatile applications of diffusion models."
    }
  ],
  "dsb": [
    {
      "title": "Neural Networks Learn Statistics of Increasing Complexity",
      "url": "/research/2024/02/13/Neural-Networks-Learn-Statistics-of-Increasing-Complexity.html",
      "date": "February 13, 2024",
      "summary": "The distributional simplicity bias (DSB) theory suggests neural networks first learn basic patterns in data before understanding more complex correlations. We provide new evidence supporting DSB, showing networks initially excel with data matching training set's simple statistics but this ability diminishes later. Extending DSB to discrete domains, we demonstrate an equivalence between n-gram frequencies and vector moments, also observing this bias in large language models (LLMs). Additionally, by adjusting low-level statistics of images to resemble another class, we reveal that networks in early training phases misclassify these edited images as if they belonged to the target class."
    }
  ],
  "equivalence": [
    {
      "title": "Neural Networks Learn Statistics of Increasing Complexity",
      "url": "/research/2024/02/13/Neural-Networks-Learn-Statistics-of-Increasing-Complexity.html",
      "date": "February 13, 2024",
      "summary": "The distributional simplicity bias (DSB) theory suggests neural networks first learn basic patterns in data before understanding more complex correlations. We provide new evidence supporting DSB, showing networks initially excel with data matching training set's simple statistics but this ability diminishes later. Extending DSB to discrete domains, we demonstrate an equivalence between n-gram frequencies and vector moments, also observing this bias in large language models (LLMs). Additionally, by adjusting low-level statistics of images to resemble another class, we reveal that networks in early training phases misclassify these edited images as if they belonged to the target class."
    }
  ],
  "ngram": [
    {
      "title": "Neural Networks Learn Statistics of Increasing Complexity",
      "url": "/research/2024/02/13/Neural-Networks-Learn-Statistics-of-Increasing-Complexity.html",
      "date": "February 13, 2024",
      "summary": "The distributional simplicity bias (DSB) theory suggests neural networks first learn basic patterns in data before understanding more complex correlations. We provide new evidence supporting DSB, showing networks initially excel with data matching training set's simple statistics but this ability diminishes later. Extending DSB to discrete domains, we demonstrate an equivalence between n-gram frequencies and vector moments, also observing this bias in large language models (LLMs). Additionally, by adjusting low-level statistics of images to resemble another class, we reveal that networks in early training phases misclassify these edited images as if they belonged to the target class."
    }
  ],
  "moment": [
    {
      "title": "Neural Networks Learn Statistics of Increasing Complexity",
      "url": "/research/2024/02/13/Neural-Networks-Learn-Statistics-of-Increasing-Complexity.html",
      "date": "February 13, 2024",
      "summary": "The distributional simplicity bias (DSB) theory suggests neural networks first learn basic patterns in data before understanding more complex correlations. We provide new evidence supporting DSB, showing networks initially excel with data matching training set's simple statistics but this ability diminishes later. Extending DSB to discrete domains, we demonstrate an equivalence between n-gram frequencies and vector moments, also observing this bias in large language models (LLMs). Additionally, by adjusting low-level statistics of images to resemble another class, we reveal that networks in early training phases misclassify these edited images as if they belonged to the target class."
    }
  ],
  "statistic": [
    {
      "title": "Neural Networks Learn Statistics of Increasing Complexity",
      "url": "/research/2024/02/13/Neural-Networks-Learn-Statistics-of-Increasing-Complexity.html",
      "date": "February 13, 2024",
      "summary": "The distributional simplicity bias (DSB) theory suggests neural networks first learn basic patterns in data before understanding more complex correlations. We provide new evidence supporting DSB, showing networks initially excel with data matching training set's simple statistics but this ability diminishes later. Extending DSB to discrete domains, we demonstrate an equivalence between n-gram frequencies and vector moments, also observing this bias in large language models (LLMs). Additionally, by adjusting low-level statistics of images to resemble another class, we reveal that networks in early training phases misclassify these edited images as if they belonged to the target class."
    }
  ],
  "misclassify": [
    {
      "title": "Neural Networks Learn Statistics of Increasing Complexity",
      "url": "/research/2024/02/13/Neural-Networks-Learn-Statistics-of-Increasing-Complexity.html",
      "date": "February 13, 2024",
      "summary": "The distributional simplicity bias (DSB) theory suggests neural networks first learn basic patterns in data before understanding more complex correlations. We provide new evidence supporting DSB, showing networks initially excel with data matching training set's simple statistics but this ability diminishes later. Extending DSB to discrete domains, we demonstrate an equivalence between n-gram frequencies and vector moments, also observing this bias in large language models (LLMs). Additionally, by adjusting low-level statistics of images to resemble another class, we reveal that networks in early training phases misclassify these edited images as if they belonged to the target class."
    }
  ],
  "edit": [
    {
      "title": "Neural Networks Learn Statistics of Increasing Complexity",
      "url": "/research/2024/02/13/Neural-Networks-Learn-Statistics-of-Increasing-Complexity.html",
      "date": "February 13, 2024",
      "summary": "The distributional simplicity bias (DSB) theory suggests neural networks first learn basic patterns in data before understanding more complex correlations. We provide new evidence supporting DSB, showing networks initially excel with data matching training set's simple statistics but this ability diminishes later. Extending DSB to discrete domains, we demonstrate an equivalence between n-gram frequencies and vector moments, also observing this bias in large language models (LLMs). Additionally, by adjusting low-level statistics of images to resemble another class, we reveal that networks in early training phases misclassify these edited images as if they belonged to the target class."
    }
  ],
  "belong": [
    {
      "title": "Neural Networks Learn Statistics of Increasing Complexity",
      "url": "/research/2024/02/13/Neural-Networks-Learn-Statistics-of-Increasing-Complexity.html",
      "date": "February 13, 2024",
      "summary": "The distributional simplicity bias (DSB) theory suggests neural networks first learn basic patterns in data before understanding more complex correlations. We provide new evidence supporting DSB, showing networks initially excel with data matching training set's simple statistics but this ability diminishes later. Extending DSB to discrete domains, we demonstrate an equivalence between n-gram frequencies and vector moments, also observing this bias in large language models (LLMs). Additionally, by adjusting low-level statistics of images to resemble another class, we reveal that networks in early training phases misclassify these edited images as if they belonged to the target class."
    }
  ],
  "ncl": [
    {
      "title": "Non-negative Contrastive Learning",
      "url": "/research/2024/03/19/Non-negative-Contrastive-Learning.html",
      "date": "March 19, 2024",
      "summary": "Deep representations offer promising performance for downstream tasks but lack interpretability, posing a significant challenge. In this paper, we introduce Non-negative Contrastive Learning (NCL), a refinement of Non-negative Matrix Factorization (NMF) aimed at generating interpretable features. NCL enforces non-negativity constraints, akin to NMF, resulting in sparse and disentangled representations, unlike standard contrastive learning (CL). We establish theoretical guarantees on NCL's identifiability and downstream generalization. Empirically, NCL outperforms CL in feature disentanglement, selection, and downstream classification tasks. Moreover, NCL can be extended to other learning scenarios, benefiting supervised learning."
    }
  ],
  "refinement": [
    {
      "title": "Non-negative Contrastive Learning",
      "url": "/research/2024/03/19/Non-negative-Contrastive-Learning.html",
      "date": "March 19, 2024",
      "summary": "Deep representations offer promising performance for downstream tasks but lack interpretability, posing a significant challenge. In this paper, we introduce Non-negative Contrastive Learning (NCL), a refinement of Non-negative Matrix Factorization (NMF) aimed at generating interpretable features. NCL enforces non-negativity constraints, akin to NMF, resulting in sparse and disentangled representations, unlike standard contrastive learning (CL). We establish theoretical guarantees on NCL's identifiability and downstream generalization. Empirically, NCL outperforms CL in feature disentanglement, selection, and downstream classification tasks. Moreover, NCL can be extended to other learning scenarios, benefiting supervised learning."
    }
  ],
  "factorization": [
    {
      "title": "Non-negative Contrastive Learning",
      "url": "/research/2024/03/19/Non-negative-Contrastive-Learning.html",
      "date": "March 19, 2024",
      "summary": "Deep representations offer promising performance for downstream tasks but lack interpretability, posing a significant challenge. In this paper, we introduce Non-negative Contrastive Learning (NCL), a refinement of Non-negative Matrix Factorization (NMF) aimed at generating interpretable features. NCL enforces non-negativity constraints, akin to NMF, resulting in sparse and disentangled representations, unlike standard contrastive learning (CL). We establish theoretical guarantees on NCL's identifiability and downstream generalization. Empirically, NCL outperforms CL in feature disentanglement, selection, and downstream classification tasks. Moreover, NCL can be extended to other learning scenarios, benefiting supervised learning."
    }
  ],
  "nmf": [
    {
      "title": "Non-negative Contrastive Learning",
      "url": "/research/2024/03/19/Non-negative-Contrastive-Learning.html",
      "date": "March 19, 2024",
      "summary": "Deep representations offer promising performance for downstream tasks but lack interpretability, posing a significant challenge. In this paper, we introduce Non-negative Contrastive Learning (NCL), a refinement of Non-negative Matrix Factorization (NMF) aimed at generating interpretable features. NCL enforces non-negativity constraints, akin to NMF, resulting in sparse and disentangled representations, unlike standard contrastive learning (CL). We establish theoretical guarantees on NCL's identifiability and downstream generalization. Empirically, NCL outperforms CL in feature disentanglement, selection, and downstream classification tasks. Moreover, NCL can be extended to other learning scenarios, benefiting supervised learning."
    }
  ],
  "enforce": [
    {
      "title": "Non-negative Contrastive Learning",
      "url": "/research/2024/03/19/Non-negative-Contrastive-Learning.html",
      "date": "March 19, 2024",
      "summary": "Deep representations offer promising performance for downstream tasks but lack interpretability, posing a significant challenge. In this paper, we introduce Non-negative Contrastive Learning (NCL), a refinement of Non-negative Matrix Factorization (NMF) aimed at generating interpretable features. NCL enforces non-negativity constraints, akin to NMF, resulting in sparse and disentangled representations, unlike standard contrastive learning (CL). We establish theoretical guarantees on NCL's identifiability and downstream generalization. Empirically, NCL outperforms CL in feature disentanglement, selection, and downstream classification tasks. Moreover, NCL can be extended to other learning scenarios, benefiting supervised learning."
    },
    {
      "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum",
      "url": "/research/2023/09/30/When-to-Learn-What:-Model-Adaptive-Data-Augmentation-Curriculum.html",
      "date": "September 30, 2023",
      "summary": "Data augmentation (DA) improves neural network generalization by enforcing invariances and symmetries to predefined transformations applied to input data. However, fixed augmentation policies affect samples differently at various training stages, and existing approaches cannot adapt policies to individual samples and the training model. This paper proposes Model-Adaptive Data Augmentation (MADAug) which trains an augmentation policy network to determine when to learn what. Unlike previous work, MADAug selects augmentation operators for each input image with a model-adaptive policy that varies between training stages, creating an optimized data augmentation curriculum. The policy is trained using a bi-level optimization scheme to minimize validation-set loss of a model trained with policy-produced augmentations. Extensive evaluations on multiple image classification tasks and network architectures show that MADAug outperforms or matches existing DA approaches, enhances fairness by improving all classes, particularly the difficult ones, and performs better when transferred to fine-grained datasets. Additionally, the auto-optimized policy in MADAug gradually increases perturbations, forming an easy-to-hard curriculum."
    }
  ],
  "negativity": [
    {
      "title": "Non-negative Contrastive Learning",
      "url": "/research/2024/03/19/Non-negative-Contrastive-Learning.html",
      "date": "March 19, 2024",
      "summary": "Deep representations offer promising performance for downstream tasks but lack interpretability, posing a significant challenge. In this paper, we introduce Non-negative Contrastive Learning (NCL), a refinement of Non-negative Matrix Factorization (NMF) aimed at generating interpretable features. NCL enforces non-negativity constraints, akin to NMF, resulting in sparse and disentangled representations, unlike standard contrastive learning (CL). We establish theoretical guarantees on NCL's identifiability and downstream generalization. Empirically, NCL outperforms CL in feature disentanglement, selection, and downstream classification tasks. Moreover, NCL can be extended to other learning scenarios, benefiting supervised learning."
    }
  ],
  "constraints": [
    {
      "title": "Non-negative Contrastive Learning",
      "url": "/research/2024/03/19/Non-negative-Contrastive-Learning.html",
      "date": "March 19, 2024",
      "summary": "Deep representations offer promising performance for downstream tasks but lack interpretability, posing a significant challenge. In this paper, we introduce Non-negative Contrastive Learning (NCL), a refinement of Non-negative Matrix Factorization (NMF) aimed at generating interpretable features. NCL enforces non-negativity constraints, akin to NMF, resulting in sparse and disentangled representations, unlike standard contrastive learning (CL). We establish theoretical guarantees on NCL's identifiability and downstream generalization. Empirically, NCL outperforms CL in feature disentanglement, selection, and downstream classification tasks. Moreover, NCL can be extended to other learning scenarios, benefiting supervised learning."
    }
  ],
  "disentangle": [
    {
      "title": "Non-negative Contrastive Learning",
      "url": "/research/2024/03/19/Non-negative-Contrastive-Learning.html",
      "date": "March 19, 2024",
      "summary": "Deep representations offer promising performance for downstream tasks but lack interpretability, posing a significant challenge. In this paper, we introduce Non-negative Contrastive Learning (NCL), a refinement of Non-negative Matrix Factorization (NMF) aimed at generating interpretable features. NCL enforces non-negativity constraints, akin to NMF, resulting in sparse and disentangled representations, unlike standard contrastive learning (CL). We establish theoretical guarantees on NCL's identifiability and downstream generalization. Empirically, NCL outperforms CL in feature disentanglement, selection, and downstream classification tasks. Moreover, NCL can be extended to other learning scenarios, benefiting supervised learning."
    }
  ],
  "guarantee": [
    {
      "title": "Non-negative Contrastive Learning",
      "url": "/research/2024/03/19/Non-negative-Contrastive-Learning.html",
      "date": "March 19, 2024",
      "summary": "Deep representations offer promising performance for downstream tasks but lack interpretability, posing a significant challenge. In this paper, we introduce Non-negative Contrastive Learning (NCL), a refinement of Non-negative Matrix Factorization (NMF) aimed at generating interpretable features. NCL enforces non-negativity constraints, akin to NMF, resulting in sparse and disentangled representations, unlike standard contrastive learning (CL). We establish theoretical guarantees on NCL's identifiability and downstream generalization. Empirically, NCL outperforms CL in feature disentanglement, selection, and downstream classification tasks. Moreover, NCL can be extended to other learning scenarios, benefiting supervised learning."
    }
  ],
  "identifiability": [
    {
      "title": "Non-negative Contrastive Learning",
      "url": "/research/2024/03/19/Non-negative-Contrastive-Learning.html",
      "date": "March 19, 2024",
      "summary": "Deep representations offer promising performance for downstream tasks but lack interpretability, posing a significant challenge. In this paper, we introduce Non-negative Contrastive Learning (NCL), a refinement of Non-negative Matrix Factorization (NMF) aimed at generating interpretable features. NCL enforces non-negativity constraints, akin to NMF, resulting in sparse and disentangled representations, unlike standard contrastive learning (CL). We establish theoretical guarantees on NCL's identifiability and downstream generalization. Empirically, NCL outperforms CL in feature disentanglement, selection, and downstream classification tasks. Moreover, NCL can be extended to other learning scenarios, benefiting supervised learning."
    }
  ],
  "disentanglement": [
    {
      "title": "Non-negative Contrastive Learning",
      "url": "/research/2024/03/19/Non-negative-Contrastive-Learning.html",
      "date": "March 19, 2024",
      "summary": "Deep representations offer promising performance for downstream tasks but lack interpretability, posing a significant challenge. In this paper, we introduce Non-negative Contrastive Learning (NCL), a refinement of Non-negative Matrix Factorization (NMF) aimed at generating interpretable features. NCL enforces non-negativity constraints, akin to NMF, resulting in sparse and disentangled representations, unlike standard contrastive learning (CL). We establish theoretical guarantees on NCL's identifiability and downstream generalization. Empirically, NCL outperforms CL in feature disentanglement, selection, and downstream classification tasks. Moreover, NCL can be extended to other learning scenarios, benefiting supervised learning."
    }
  ],
  "benefiting": [
    {
      "title": "Non-negative Contrastive Learning",
      "url": "/research/2024/03/19/Non-negative-Contrastive-Learning.html",
      "date": "March 19, 2024",
      "summary": "Deep representations offer promising performance for downstream tasks but lack interpretability, posing a significant challenge. In this paper, we introduce Non-negative Contrastive Learning (NCL), a refinement of Non-negative Matrix Factorization (NMF) aimed at generating interpretable features. NCL enforces non-negativity constraints, akin to NMF, resulting in sparse and disentangled representations, unlike standard contrastive learning (CL). We establish theoretical guarantees on NCL's identifiability and downstream generalization. Empirically, NCL outperforms CL in feature disentanglement, selection, and downstream classification tasks. Moreover, NCL can be extended to other learning scenarios, benefiting supervised learning."
    }
  ],
  "ramachandran": [
    {
      "title": "On the Relationship Between Self-Attention And Convolutional Layers",
      "url": "/research/2020/01/10/On-the-Relationship-Between-Self-Attention-And-Convolutional-Layers.html",
      "date": "January 10, 2020",
      "summary": "Recent studies have shown that attention mechanisms can rival or even outperform convolutional layers in vision tasks, challenging their dominance. Ramachandran et al. (2019) demonstrated that attention could entirely replace convolution, achieving top results. This work investigates whether attention layers function like convolutional layers, finding that multi-head self-attention layers, with enough heads, match the expressiveness of convolutional layers. Numerical experiments confirm that self-attention layers learn to focus on pixel-grid patterns akin to convolutional layers, supporting our findings."
    }
  ],
  "expressiveness": [
    {
      "title": "On the Relationship Between Self-Attention And Convolutional Layers",
      "url": "/research/2020/01/10/On-the-Relationship-Between-Self-Attention-And-Convolutional-Layers.html",
      "date": "January 10, 2020",
      "summary": "Recent studies have shown that attention mechanisms can rival or even outperform convolutional layers in vision tasks, challenging their dominance. Ramachandran et al. (2019) demonstrated that attention could entirely replace convolution, achieving top results. This work investigates whether attention layers function like convolutional layers, finding that multi-head self-attention layers, with enough heads, match the expressiveness of convolutional layers. Numerical experiments confirm that self-attention layers learn to focus on pixel-grid patterns akin to convolutional layers, supporting our findings."
    }
  ],
  "grid": [
    {
      "title": "On the Relationship Between Self-Attention And Convolutional Layers",
      "url": "/research/2020/01/10/On-the-Relationship-Between-Self-Attention-And-Convolutional-Layers.html",
      "date": "January 10, 2020",
      "summary": "Recent studies have shown that attention mechanisms can rival or even outperform convolutional layers in vision tasks, challenging their dominance. Ramachandran et al. (2019) demonstrated that attention could entirely replace convolution, achieving top results. This work investigates whether attention layers function like convolutional layers, finding that multi-head self-attention layers, with enough heads, match the expressiveness of convolutional layers. Numerical experiments confirm that self-attention layers learn to focus on pixel-grid patterns akin to convolutional layers, supporting our findings."
    }
  ],
  "patterns": [
    {
      "title": "On the Relationship Between Self-Attention And Convolutional Layers",
      "url": "/research/2020/01/10/On-the-Relationship-Between-Self-Attention-And-Convolutional-Layers.html",
      "date": "January 10, 2020",
      "summary": "Recent studies have shown that attention mechanisms can rival or even outperform convolutional layers in vision tasks, challenging their dominance. Ramachandran et al. (2019) demonstrated that attention could entirely replace convolution, achieving top results. This work investigates whether attention layers function like convolutional layers, finding that multi-head self-attention layers, with enough heads, match the expressiveness of convolutional layers. Numerical experiments confirm that self-attention layers learn to focus on pixel-grid patterns akin to convolutional layers, supporting our findings."
    }
  ],
  "sgdr": [
    {
      "title": "SGDR: Stochastic Gradient Descent With Warm Restarts",
      "url": "/research/2017/05/02/SGDR-Stochastic-Gradient-Descent-With-Warm-Restarts.html",
      "date": "May 2, 2017",
      "summary": "This paper introduces a warm restart technique for stochastic gradient descent aimed at enhancing anytime performance in deep neural network training. It showcases empirical performance improvements on CIFAR-10 and CIFAR-100 datasets, achieving state-of-the-art results with 3.14% and 16.21% error rates, respectively. Additionally, the technique's benefits are demonstrated on an EEG dataset and a downsampled ImageNet dataset."
    }
  ],
  "restart": [
    {
      "title": "SGDR: Stochastic Gradient Descent With Warm Restarts",
      "url": "/research/2017/05/02/SGDR-Stochastic-Gradient-Descent-With-Warm-Restarts.html",
      "date": "May 2, 2017",
      "summary": "This paper introduces a warm restart technique for stochastic gradient descent aimed at enhancing anytime performance in deep neural network training. It showcases empirical performance improvements on CIFAR-10 and CIFAR-100 datasets, achieving state-of-the-art results with 3.14% and 16.21% error rates, respectively. Additionally, the technique's benefits are demonstrated on an EEG dataset and a downsampled ImageNet dataset."
    }
  ],
  "anytime": [
    {
      "title": "SGDR: Stochastic Gradient Descent With Warm Restarts",
      "url": "/research/2017/05/02/SGDR-Stochastic-Gradient-Descent-With-Warm-Restarts.html",
      "date": "May 2, 2017",
      "summary": "This paper introduces a warm restart technique for stochastic gradient descent aimed at enhancing anytime performance in deep neural network training. It showcases empirical performance improvements on CIFAR-10 and CIFAR-100 datasets, achieving state-of-the-art results with 3.14% and 16.21% error rates, respectively. Additionally, the technique's benefits are demonstrated on an EEG dataset and a downsampled ImageNet dataset."
    }
  ],
  "eeg": [
    {
      "title": "SGDR: Stochastic Gradient Descent With Warm Restarts",
      "url": "/research/2017/05/02/SGDR-Stochastic-Gradient-Descent-With-Warm-Restarts.html",
      "date": "May 2, 2017",
      "summary": "This paper introduces a warm restart technique for stochastic gradient descent aimed at enhancing anytime performance in deep neural network training. It showcases empirical performance improvements on CIFAR-10 and CIFAR-100 datasets, achieving state-of-the-art results with 3.14% and 16.21% error rates, respectively. Additionally, the technique's benefits are demonstrated on an EEG dataset and a downsampled ImageNet dataset."
    }
  ],
  "usability": [
    {
      "title": "Self-Paced Absolute Learning Progress as a Regularized Approach to Curriculum Learning",
      "url": "/research/2023/06/09/Self-Paced-Absolute-Learning-Progress-as-a-Regularized-Approach-to-Curriculum-Learning.html",
      "date": "June 9, 2023",
      "summary": "Reinforcement Learning's usability is limited by its high computation times. Curriculum Reinforcement Learning accelerates learning by ordering tasks from simple to hard. Curricula based on Absolute Learning Progress (ALP) have shown success in various environments but waste computation on redundant tasks. This issue is addressed by introducing Self-Paced Absolute Learning Progress (SPALP), a regularization method inspired by Self-Paced Learning. Evaluated in three environments, SPALP achieves performance comparable to ALP in all cases and reaches it faster in two. Further improvements in SPALP's efficiency and performance are also discussed."
    }
  ],
  "alp": [
    {
      "title": "Self-Paced Absolute Learning Progress as a Regularized Approach to Curriculum Learning",
      "url": "/research/2023/06/09/Self-Paced-Absolute-Learning-Progress-as-a-Regularized-Approach-to-Curriculum-Learning.html",
      "date": "June 9, 2023",
      "summary": "Reinforcement Learning's usability is limited by its high computation times. Curriculum Reinforcement Learning accelerates learning by ordering tasks from simple to hard. Curricula based on Absolute Learning Progress (ALP) have shown success in various environments but waste computation on redundant tasks. This issue is addressed by introducing Self-Paced Absolute Learning Progress (SPALP), a regularization method inspired by Self-Paced Learning. Evaluated in three environments, SPALP achieves performance comparable to ALP in all cases and reaches it faster in two. Further improvements in SPALP's efficiency and performance are also discussed."
    }
  ],
  "waste": [
    {
      "title": "Self-Paced Absolute Learning Progress as a Regularized Approach to Curriculum Learning",
      "url": "/research/2023/06/09/Self-Paced-Absolute-Learning-Progress-as-a-Regularized-Approach-to-Curriculum-Learning.html",
      "date": "June 9, 2023",
      "summary": "Reinforcement Learning's usability is limited by its high computation times. Curriculum Reinforcement Learning accelerates learning by ordering tasks from simple to hard. Curricula based on Absolute Learning Progress (ALP) have shown success in various environments but waste computation on redundant tasks. This issue is addressed by introducing Self-Paced Absolute Learning Progress (SPALP), a regularization method inspired by Self-Paced Learning. Evaluated in three environments, SPALP achieves performance comparable to ALP in all cases and reaches it faster in two. Further improvements in SPALP's efficiency and performance are also discussed."
    }
  ],
  "spalp": [
    {
      "title": "Self-Paced Absolute Learning Progress as a Regularized Approach to Curriculum Learning",
      "url": "/research/2023/06/09/Self-Paced-Absolute-Learning-Progress-as-a-Regularized-Approach-to-Curriculum-Learning.html",
      "date": "June 9, 2023",
      "summary": "Reinforcement Learning's usability is limited by its high computation times. Curriculum Reinforcement Learning accelerates learning by ordering tasks from simple to hard. Curricula based on Absolute Learning Progress (ALP) have shown success in various environments but waste computation on redundant tasks. This issue is addressed by introducing Self-Paced Absolute Learning Progress (SPALP), a regularization method inspired by Self-Paced Learning. Evaluated in three environments, SPALP achieves performance comparable to ALP in all cases and reaches it faster in two. Further improvements in SPALP's efficiency and performance are also discussed."
    }
  ],
  "environments": [
    {
      "title": "Self-Paced Absolute Learning Progress as a Regularized Approach to Curriculum Learning",
      "url": "/research/2023/06/09/Self-Paced-Absolute-Learning-Progress-as-a-Regularized-Approach-to-Curriculum-Learning.html",
      "date": "June 9, 2023",
      "summary": "Reinforcement Learning's usability is limited by its high computation times. Curriculum Reinforcement Learning accelerates learning by ordering tasks from simple to hard. Curricula based on Absolute Learning Progress (ALP) have shown success in various environments but waste computation on redundant tasks. This issue is addressed by introducing Self-Paced Absolute Learning Progress (SPALP), a regularization method inspired by Self-Paced Learning. Evaluated in three environments, SPALP achieves performance comparable to ALP in all cases and reaches it faster in two. Further improvements in SPALP's efficiency and performance are also discussed."
    }
  ],
  "consume": [
    {
      "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
      "url": "/research/2024/03/13/Simple-and-Scalable-Strategies-to-Continually-Pre-train-Large-Language-Models.html",
      "date": "March 13, 2024",
      "summary": "Large language models (LLMs) often require re-training on new data, consuming extensive computational resources. We present an efficient method that combines learning rate re-warming, re-decaying, and data replay, effectively maintaining performance without full re-training. This approach works well across different data distributions, including minor shifts (English→English) and significant shifts (English→German), tested up to 405M and 10B parameter models. Our findings suggest that continual learning strategies can update LLMs with minimal computational cost, rivaling traditional re-training methods. Additionally, we propose alternatives to the cosine learning rate schedule to reduce forgetting, offering more flexibility in learning without a fixed token budget."
    }
  ],
  "redecaying": [
    {
      "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
      "url": "/research/2024/03/13/Simple-and-Scalable-Strategies-to-Continually-Pre-train-Large-Language-Models.html",
      "date": "March 13, 2024",
      "summary": "Large language models (LLMs) often require re-training on new data, consuming extensive computational resources. We present an efficient method that combines learning rate re-warming, re-decaying, and data replay, effectively maintaining performance without full re-training. This approach works well across different data distributions, including minor shifts (English→English) and significant shifts (English→German), tested up to 405M and 10B parameter models. Our findings suggest that continual learning strategies can update LLMs with minimal computational cost, rivaling traditional re-training methods. Additionally, we propose alternatives to the cosine learning rate schedule to reduce forgetting, offering more flexibility in learning without a fixed token budget."
    }
  ],
  "shifts": [
    {
      "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
      "url": "/research/2024/03/13/Simple-and-Scalable-Strategies-to-Continually-Pre-train-Large-Language-Models.html",
      "date": "March 13, 2024",
      "summary": "Large language models (LLMs) often require re-training on new data, consuming extensive computational resources. We present an efficient method that combines learning rate re-warming, re-decaying, and data replay, effectively maintaining performance without full re-training. This approach works well across different data distributions, including minor shifts (English→English) and significant shifts (English→German), tested up to 405M and 10B parameter models. Our findings suggest that continual learning strategies can update LLMs with minimal computational cost, rivaling traditional re-training methods. Additionally, we propose alternatives to the cosine learning rate schedule to reduce forgetting, offering more flexibility in learning without a fixed token budget."
    }
  ],
  "englishgerman": [
    {
      "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
      "url": "/research/2024/03/13/Simple-and-Scalable-Strategies-to-Continually-Pre-train-Large-Language-Models.html",
      "date": "March 13, 2024",
      "summary": "Large language models (LLMs) often require re-training on new data, consuming extensive computational resources. We present an efficient method that combines learning rate re-warming, re-decaying, and data replay, effectively maintaining performance without full re-training. This approach works well across different data distributions, including minor shifts (English→English) and significant shifts (English→German), tested up to 405M and 10B parameter models. Our findings suggest that continual learning strategies can update LLMs with minimal computational cost, rivaling traditional re-training methods. Additionally, we propose alternatives to the cosine learning rate schedule to reduce forgetting, offering more flexibility in learning without a fixed token budget."
    }
  ],
  "rival": [
    {
      "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
      "url": "/research/2024/03/13/Simple-and-Scalable-Strategies-to-Continually-Pre-train-Large-Language-Models.html",
      "date": "March 13, 2024",
      "summary": "Large language models (LLMs) often require re-training on new data, consuming extensive computational resources. We present an efficient method that combines learning rate re-warming, re-decaying, and data replay, effectively maintaining performance without full re-training. This approach works well across different data distributions, including minor shifts (English→English) and significant shifts (English→German), tested up to 405M and 10B parameter models. Our findings suggest that continual learning strategies can update LLMs with minimal computational cost, rivaling traditional re-training methods. Additionally, we propose alternatives to the cosine learning rate schedule to reduce forgetting, offering more flexibility in learning without a fixed token budget."
    }
  ],
  "forget": [
    {
      "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
      "url": "/research/2024/03/13/Simple-and-Scalable-Strategies-to-Continually-Pre-train-Large-Language-Models.html",
      "date": "March 13, 2024",
      "summary": "Large language models (LLMs) often require re-training on new data, consuming extensive computational resources. We present an efficient method that combines learning rate re-warming, re-decaying, and data replay, effectively maintaining performance without full re-training. This approach works well across different data distributions, including minor shifts (English→English) and significant shifts (English→German), tested up to 405M and 10B parameter models. Our findings suggest that continual learning strategies can update LLMs with minimal computational cost, rivaling traditional re-training methods. Additionally, we propose alternatives to the cosine learning rate schedule to reduce forgetting, offering more flexibility in learning without a fixed token budget."
    }
  ],
  "offering": [
    {
      "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
      "url": "/research/2024/03/13/Simple-and-Scalable-Strategies-to-Continually-Pre-train-Large-Language-Models.html",
      "date": "March 13, 2024",
      "summary": "Large language models (LLMs) often require re-training on new data, consuming extensive computational resources. We present an efficient method that combines learning rate re-warming, re-decaying, and data replay, effectively maintaining performance without full re-training. This approach works well across different data distributions, including minor shifts (English→English) and significant shifts (English→German), tested up to 405M and 10B parameter models. Our findings suggest that continual learning strategies can update LLMs with minimal computational cost, rivaling traditional re-training methods. Additionally, we propose alternatives to the cosine learning rate schedule to reduce forgetting, offering more flexibility in learning without a fixed token budget."
    }
  ],
  "suppress": [
    {
      "title": "Suppressing Pink Elephants with Direct Principle Feedback",
      "url": "/research/2024/02/13/Suppressing-Pink-Elephants-with-Direct-Principle-Feedback.html",
      "date": "February 13, 2024",
      "summary": "Existing methods for controlling language models focus on training desired behaviors, but often lack the flexibility needed for diverse applications. The authors address this with the Pink Elephant Problem, demonstrating the need for language models to adapt to different contexts by avoiding certain topics (the Pink Elephant) in favor of others (Grey Elephant). The authors introduce Direct Principle Feedback (DPF), an adaptation of Constitutional AI that improves control by directly applying critiques and revisions without ranking responses. Our study shows that a 13B LLaMA 2 model fine-tuned with DPF on a synthetic dataset outperforms existing models and matches GPT-4 in managing the Pink Elephant Problem."
    }
  ],
  "elephant": [
    {
      "title": "Suppressing Pink Elephants with Direct Principle Feedback",
      "url": "/research/2024/02/13/Suppressing-Pink-Elephants-with-Direct-Principle-Feedback.html",
      "date": "February 13, 2024",
      "summary": "Existing methods for controlling language models focus on training desired behaviors, but often lack the flexibility needed for diverse applications. The authors address this with the Pink Elephant Problem, demonstrating the need for language models to adapt to different contexts by avoiding certain topics (the Pink Elephant) in favor of others (Grey Elephant). The authors introduce Direct Principle Feedback (DPF), an adaptation of Constitutional AI that improves control by directly applying critiques and revisions without ranking responses. Our study shows that a 13B LLaMA 2 model fine-tuned with DPF on a synthetic dataset outperforms existing models and matches GPT-4 in managing the Pink Elephant Problem."
    }
  ],
  "feedback": [
    {
      "title": "Suppressing Pink Elephants with Direct Principle Feedback",
      "url": "/research/2024/02/13/Suppressing-Pink-Elephants-with-Direct-Principle-Feedback.html",
      "date": "February 13, 2024",
      "summary": "Existing methods for controlling language models focus on training desired behaviors, but often lack the flexibility needed for diverse applications. The authors address this with the Pink Elephant Problem, demonstrating the need for language models to adapt to different contexts by avoiding certain topics (the Pink Elephant) in favor of others (Grey Elephant). The authors introduce Direct Principle Feedback (DPF), an adaptation of Constitutional AI that improves control by directly applying critiques and revisions without ranking responses. Our study shows that a 13B LLaMA 2 model fine-tuned with DPF on a synthetic dataset outperforms existing models and matches GPT-4 in managing the Pink Elephant Problem."
    }
  ],
  "topic": [
    {
      "title": "Suppressing Pink Elephants with Direct Principle Feedback",
      "url": "/research/2024/02/13/Suppressing-Pink-Elephants-with-Direct-Principle-Feedback.html",
      "date": "February 13, 2024",
      "summary": "Existing methods for controlling language models focus on training desired behaviors, but often lack the flexibility needed for diverse applications. The authors address this with the Pink Elephant Problem, demonstrating the need for language models to adapt to different contexts by avoiding certain topics (the Pink Elephant) in favor of others (Grey Elephant). The authors introduce Direct Principle Feedback (DPF), an adaptation of Constitutional AI that improves control by directly applying critiques and revisions without ranking responses. Our study shows that a 13B LLaMA 2 model fine-tuned with DPF on a synthetic dataset outperforms existing models and matches GPT-4 in managing the Pink Elephant Problem."
    }
  ],
  "dpf": [
    {
      "title": "Suppressing Pink Elephants with Direct Principle Feedback",
      "url": "/research/2024/02/13/Suppressing-Pink-Elephants-with-Direct-Principle-Feedback.html",
      "date": "February 13, 2024",
      "summary": "Existing methods for controlling language models focus on training desired behaviors, but often lack the flexibility needed for diverse applications. The authors address this with the Pink Elephant Problem, demonstrating the need for language models to adapt to different contexts by avoiding certain topics (the Pink Elephant) in favor of others (Grey Elephant). The authors introduce Direct Principle Feedback (DPF), an adaptation of Constitutional AI that improves control by directly applying critiques and revisions without ranking responses. Our study shows that a 13B LLaMA 2 model fine-tuned with DPF on a synthetic dataset outperforms existing models and matches GPT-4 in managing the Pink Elephant Problem."
    }
  ],
  "ai": [
    {
      "title": "Suppressing Pink Elephants with Direct Principle Feedback",
      "url": "/research/2024/02/13/Suppressing-Pink-Elephants-with-Direct-Principle-Feedback.html",
      "date": "February 13, 2024",
      "summary": "Existing methods for controlling language models focus on training desired behaviors, but often lack the flexibility needed for diverse applications. The authors address this with the Pink Elephant Problem, demonstrating the need for language models to adapt to different contexts by avoiding certain topics (the Pink Elephant) in favor of others (Grey Elephant). The authors introduce Direct Principle Feedback (DPF), an adaptation of Constitutional AI that improves control by directly applying critiques and revisions without ranking responses. Our study shows that a 13B LLaMA 2 model fine-tuned with DPF on a synthetic dataset outperforms existing models and matches GPT-4 in managing the Pink Elephant Problem."
    },
    {
      "title": "The Platonic Representation Hypothesis",
      "url": "/research/2024/05/13/The-Platonic-Representation-Hypothesis.html",
      "date": "May 13, 2024",
      "summary": "This paper argues that AI model representations, especially in deep networks, are converging. It surveys examples in the literature showing how neural network representations align over time and across domains. It demonstrates convergence across data modalities, noting that as vision and language models grow, they measure distances between data points similarly. The authors hypothesize that this convergence leads to a shared statistical model of reality, termed the platonic representation and discuss possible selective pressures toward it. The paper also addresses the implications, limitations, and counterexamples to this trend."
    }
  ],
  "critique": [
    {
      "title": "Suppressing Pink Elephants with Direct Principle Feedback",
      "url": "/research/2024/02/13/Suppressing-Pink-Elephants-with-Direct-Principle-Feedback.html",
      "date": "February 13, 2024",
      "summary": "Existing methods for controlling language models focus on training desired behaviors, but often lack the flexibility needed for diverse applications. The authors address this with the Pink Elephant Problem, demonstrating the need for language models to adapt to different contexts by avoiding certain topics (the Pink Elephant) in favor of others (Grey Elephant). The authors introduce Direct Principle Feedback (DPF), an adaptation of Constitutional AI that improves control by directly applying critiques and revisions without ranking responses. Our study shows that a 13B LLaMA 2 model fine-tuned with DPF on a synthetic dataset outperforms existing models and matches GPT-4 in managing the Pink Elephant Problem."
    }
  ],
  "revision": [
    {
      "title": "Suppressing Pink Elephants with Direct Principle Feedback",
      "url": "/research/2024/02/13/Suppressing-Pink-Elephants-with-Direct-Principle-Feedback.html",
      "date": "February 13, 2024",
      "summary": "Existing methods for controlling language models focus on training desired behaviors, but often lack the flexibility needed for diverse applications. The authors address this with the Pink Elephant Problem, demonstrating the need for language models to adapt to different contexts by avoiding certain topics (the Pink Elephant) in favor of others (Grey Elephant). The authors introduce Direct Principle Feedback (DPF), an adaptation of Constitutional AI that improves control by directly applying critiques and revisions without ranking responses. Our study shows that a 13B LLaMA 2 model fine-tuned with DPF on a synthetic dataset outperforms existing models and matches GPT-4 in managing the Pink Elephant Problem."
    }
  ],
  "response": [
    {
      "title": "Suppressing Pink Elephants with Direct Principle Feedback",
      "url": "/research/2024/02/13/Suppressing-Pink-Elephants-with-Direct-Principle-Feedback.html",
      "date": "February 13, 2024",
      "summary": "Existing methods for controlling language models focus on training desired behaviors, but often lack the flexibility needed for diverse applications. The authors address this with the Pink Elephant Problem, demonstrating the need for language models to adapt to different contexts by avoiding certain topics (the Pink Elephant) in favor of others (Grey Elephant). The authors introduce Direct Principle Feedback (DPF), an adaptation of Constitutional AI that improves control by directly applying critiques and revisions without ranking responses. Our study shows that a 13B LLaMA 2 model fine-tuned with DPF on a synthetic dataset outperforms existing models and matches GPT-4 in managing the Pink Elephant Problem."
    }
  ],
  "era": [
    {
      "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
      "url": "/research/2024/02/27/The-Era-of-1-bit-LLMs-All-Large-Language-Models-are-in-1.58-Bits.html",
      "date": "February 27, 2024",
      "summary": "Recent research introduces BitNet b1.58, a ternary (values of -1, 0, 1) 1-bit Large Language Model (LLM) that matches traditional full-precision LLMs in performance and perplexity but is significantly more efficient in terms of latency, memory, throughput, and energy. This work establishes a new scaling law and training approach for future high-performance, cost-effective LLMs, while also facilitating the design of specialized hardware optimized for 1-bit LLMs."
    }
  ],
  "bitnet": [
    {
      "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
      "url": "/research/2024/02/27/The-Era-of-1-bit-LLMs-All-Large-Language-Models-are-in-1.58-Bits.html",
      "date": "February 27, 2024",
      "summary": "Recent research introduces BitNet b1.58, a ternary (values of -1, 0, 1) 1-bit Large Language Model (LLM) that matches traditional full-precision LLMs in performance and perplexity but is significantly more efficient in terms of latency, memory, throughput, and energy. This work establishes a new scaling law and training approach for future high-performance, cost-effective LLMs, while also facilitating the design of specialized hardware optimized for 1-bit LLMs."
    }
  ],
  "b158": [
    {
      "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
      "url": "/research/2024/02/27/The-Era-of-1-bit-LLMs-All-Large-Language-Models-are-in-1.58-Bits.html",
      "date": "February 27, 2024",
      "summary": "Recent research introduces BitNet b1.58, a ternary (values of -1, 0, 1) 1-bit Large Language Model (LLM) that matches traditional full-precision LLMs in performance and perplexity but is significantly more efficient in terms of latency, memory, throughput, and energy. This work establishes a new scaling law and training approach for future high-performance, cost-effective LLMs, while also facilitating the design of specialized hardware optimized for 1-bit LLMs."
    }
  ],
  "perplexity": [
    {
      "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
      "url": "/research/2024/02/27/The-Era-of-1-bit-LLMs-All-Large-Language-Models-are-in-1.58-Bits.html",
      "date": "February 27, 2024",
      "summary": "Recent research introduces BitNet b1.58, a ternary (values of -1, 0, 1) 1-bit Large Language Model (LLM) that matches traditional full-precision LLMs in performance and perplexity but is significantly more efficient in terms of latency, memory, throughput, and energy. This work establishes a new scaling law and training approach for future high-performance, cost-effective LLMs, while also facilitating the design of specialized hardware optimized for 1-bit LLMs."
    }
  ],
  "throughput": [
    {
      "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
      "url": "/research/2024/02/27/The-Era-of-1-bit-LLMs-All-Large-Language-Models-are-in-1.58-Bits.html",
      "date": "February 27, 2024",
      "summary": "Recent research introduces BitNet b1.58, a ternary (values of -1, 0, 1) 1-bit Large Language Model (LLM) that matches traditional full-precision LLMs in performance and perplexity but is significantly more efficient in terms of latency, memory, throughput, and energy. This work establishes a new scaling law and training approach for future high-performance, cost-effective LLMs, while also facilitating the design of specialized hardware optimized for 1-bit LLMs."
    }
  ],
  "law": [
    {
      "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
      "url": "/research/2024/02/27/The-Era-of-1-bit-LLMs-All-Large-Language-Models-are-in-1.58-Bits.html",
      "date": "February 27, 2024",
      "summary": "Recent research introduces BitNet b1.58, a ternary (values of -1, 0, 1) 1-bit Large Language Model (LLM) that matches traditional full-precision LLMs in performance and perplexity but is significantly more efficient in terms of latency, memory, throughput, and energy. This work establishes a new scaling law and training approach for future high-performance, cost-effective LLMs, while also facilitating the design of specialized hardware optimized for 1-bit LLMs."
    },
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "future": [
    {
      "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
      "url": "/research/2024/02/27/The-Era-of-1-bit-LLMs-All-Large-Language-Models-are-in-1.58-Bits.html",
      "date": "February 27, 2024",
      "summary": "Recent research introduces BitNet b1.58, a ternary (values of -1, 0, 1) 1-bit Large Language Model (LLM) that matches traditional full-precision LLMs in performance and perplexity but is significantly more efficient in terms of latency, memory, throughput, and energy. This work establishes a new scaling law and training approach for future high-performance, cost-effective LLMs, while also facilitating the design of specialized hardware optimized for 1-bit LLMs."
    },
    {
      "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
      "url": "/research/2024/01/24/Better-Call-GPT,-Comparing-Large-Language-Models-Against-Lawyers.html",
      "date": "January 24, 2024",
      "summary": "This paper compares Large Language Models (LLMs) with traditional legal contract reviewers—Junior Lawyers and Legal Process Outsourcers (LPOs). It evaluates whether LLMs outperform humans in accuracy, speed, and cost-efficiency during contract review. Empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, revealing that advanced models match or exceed human accuracy in identifying legal issues. LLMs complete reviews in seconds, vastly outpacing the hours required by humans. Cost-wise, LLMs achieve a 99.97 percent reduction compared to traditional methods. These findings indicate a transformative shift in legal practice, with LLMs enhancing the accessibility and efficiency of legal services. The research suggests that LLM dominance in legal contract review challenges the status quo, necessitating a reimagined future for legal workflows."
    }
  ],
  "understudy": [
    {
      "title": "The loss landscape of deep linear neural networks: a second-order analysis",
      "url": "/research/2022/03/11/The-loss-landscape-of-deep-linear-neural-networks-a-second-order-analysis.html",
      "date": "March 11, 2022",
      "summary": "This study investigates the optimization landscape of deep linear neural networks with square loss, focusing on the role of non-strict saddle points in algorithm dynamics, which has been previously understudied. the authors conduct a comprehensive second-order analysis, identifying global minimizers, strict, and non-strict saddle points among all critical points, alongside their critical values. Our findings, based on conditions related to the ranks of partial matrix products, contribute insights into global convergence and implicit regularization observed in optimizing linear neural networks. Additionally, the authors offer an explicit parameterization of global minimizers and identify extensive sets of strict and non-strict saddle points."
    }
  ],
  "contribute": [
    {
      "title": "The loss landscape of deep linear neural networks: a second-order analysis",
      "url": "/research/2022/03/11/The-loss-landscape-of-deep-linear-neural-networks-a-second-order-analysis.html",
      "date": "March 11, 2022",
      "summary": "This study investigates the optimization landscape of deep linear neural networks with square loss, focusing on the role of non-strict saddle points in algorithm dynamics, which has been previously understudied. the authors conduct a comprehensive second-order analysis, identifying global minimizers, strict, and non-strict saddle points among all critical points, alongside their critical values. Our findings, based on conditions related to the ranks of partial matrix products, contribute insights into global convergence and implicit regularization observed in optimizing linear neural networks. Additionally, the authors offer an explicit parameterization of global minimizers and identify extensive sets of strict and non-strict saddle points."
    }
  ],
  "why": [
    {
      "title": "Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes",
      "url": "/research/2017/11/28/Towards-Understanding-Generalization-of-Deep-Learning-Perspective-of-Loss-Landscapes.html",
      "date": "November 28, 2017",
      "summary": "This study examines why deep learning models, despite having more parameters than training samples, generalize well. It highlights the importance of the loss function's landscape, showing that areas leading to better generalization (good minima) are more prevalent than those leading to poor outcomes. This predominance facilitates the convergence of optimization methods to these beneficial minima. The research provides theoretical backing by analyzing 2-layer neural networks, noting that solutions with better generalization have a smaller Hessian matrix norm. For deeper networks, extensive numerical data supports these conclusions."
    }
  ],
  "areave": [
    {
      "title": "Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes",
      "url": "/research/2017/11/28/Towards-Understanding-Generalization-of-Deep-Learning-Perspective-of-Loss-Landscapes.html",
      "date": "November 28, 2017",
      "summary": "This study examines why deep learning models, despite having more parameters than training samples, generalize well. It highlights the importance of the loss function's landscape, showing that areas leading to better generalization (good minima) are more prevalent than those leading to poor outcomes. This predominance facilitates the convergence of optimization methods to these beneficial minima. The research provides theoretical backing by analyzing 2-layer neural networks, noting that solutions with better generalization have a smaller Hessian matrix norm. For deeper networks, extensive numerical data supports these conclusions."
    }
  ],
  "predominance": [
    {
      "title": "Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes",
      "url": "/research/2017/11/28/Towards-Understanding-Generalization-of-Deep-Learning-Perspective-of-Loss-Landscapes.html",
      "date": "November 28, 2017",
      "summary": "This study examines why deep learning models, despite having more parameters than training samples, generalize well. It highlights the importance of the loss function's landscape, showing that areas leading to better generalization (good minima) are more prevalent than those leading to poor outcomes. This predominance facilitates the convergence of optimization methods to these beneficial minima. The research provides theoretical backing by analyzing 2-layer neural networks, noting that solutions with better generalization have a smaller Hessian matrix norm. For deeper networks, extensive numerical data supports these conclusions."
    }
  ],
  "simsiam": [
    {
      "title": "Towards a Unified Theoretical Understanding of Non-contrastive Learning via Rank Differential Mechanism",
      "url": "/research/2023/03/04/Towards-a-Unified-Theoretical-Understanding-of-Non-contrastive-Learning-via-Rank-Differential-Mechanism.html",
      "date": "March 4, 2023",
      "summary": "Recent non-contrastive learning methods like BYOL, SimSiam, SwAV, and DINO have shown that asymmetric architectural designs can achieve good self-supervised visual learning performance by aligning positive pairs alone, despite a lack of unified theoretical understanding on how these designs prevent feature collapse. This study introduces the Rank Differential Mechanism (RDM) as a unified theoretical framework for non-contrastive learning. RDM demonstrates that these asymmetric designs maintain a consistent rank difference in output features, enhancing effective dimensionality and preventing feature collapse. Unlike previous theories, RDM applies to designs both with and without a predictor, offering a comprehensive understanding of non-contrastive learning methods and guiding the development of new variants. Experiments confirm that these variants perform comparably, if not better than, existing methods on benchmark datasets."
    }
  ],
  "swav": [
    {
      "title": "Towards a Unified Theoretical Understanding of Non-contrastive Learning via Rank Differential Mechanism",
      "url": "/research/2023/03/04/Towards-a-Unified-Theoretical-Understanding-of-Non-contrastive-Learning-via-Rank-Differential-Mechanism.html",
      "date": "March 4, 2023",
      "summary": "Recent non-contrastive learning methods like BYOL, SimSiam, SwAV, and DINO have shown that asymmetric architectural designs can achieve good self-supervised visual learning performance by aligning positive pairs alone, despite a lack of unified theoretical understanding on how these designs prevent feature collapse. This study introduces the Rank Differential Mechanism (RDM) as a unified theoretical framework for non-contrastive learning. RDM demonstrates that these asymmetric designs maintain a consistent rank difference in output features, enhancing effective dimensionality and preventing feature collapse. Unlike previous theories, RDM applies to designs both with and without a predictor, offering a comprehensive understanding of non-contrastive learning methods and guiding the development of new variants. Experiments confirm that these variants perform comparably, if not better than, existing methods on benchmark datasets."
    }
  ],
  "rdm": [
    {
      "title": "Towards a Unified Theoretical Understanding of Non-contrastive Learning via Rank Differential Mechanism",
      "url": "/research/2023/03/04/Towards-a-Unified-Theoretical-Understanding-of-Non-contrastive-Learning-via-Rank-Differential-Mechanism.html",
      "date": "March 4, 2023",
      "summary": "Recent non-contrastive learning methods like BYOL, SimSiam, SwAV, and DINO have shown that asymmetric architectural designs can achieve good self-supervised visual learning performance by aligning positive pairs alone, despite a lack of unified theoretical understanding on how these designs prevent feature collapse. This study introduces the Rank Differential Mechanism (RDM) as a unified theoretical framework for non-contrastive learning. RDM demonstrates that these asymmetric designs maintain a consistent rank difference in output features, enhancing effective dimensionality and preventing feature collapse. Unlike previous theories, RDM applies to designs both with and without a predictor, offering a comprehensive understanding of non-contrastive learning methods and guiding the development of new variants. Experiments confirm that these variants perform comparably, if not better than, existing methods on benchmark datasets."
    }
  ],
  "theories": [
    {
      "title": "Towards a Unified Theoretical Understanding of Non-contrastive Learning via Rank Differential Mechanism",
      "url": "/research/2023/03/04/Towards-a-Unified-Theoretical-Understanding-of-Non-contrastive-Learning-via-Rank-Differential-Mechanism.html",
      "date": "March 4, 2023",
      "summary": "Recent non-contrastive learning methods like BYOL, SimSiam, SwAV, and DINO have shown that asymmetric architectural designs can achieve good self-supervised visual learning performance by aligning positive pairs alone, despite a lack of unified theoretical understanding on how these designs prevent feature collapse. This study introduces the Rank Differential Mechanism (RDM) as a unified theoretical framework for non-contrastive learning. RDM demonstrates that these asymmetric designs maintain a consistent rank difference in output features, enhancing effective dimensionality and preventing feature collapse. Unlike previous theories, RDM applies to designs both with and without a predictor, offering a comprehensive understanding of non-contrastive learning methods and guiding the development of new variants. Experiments confirm that these variants perform comparably, if not better than, existing methods on benchmark datasets."
    }
  ],
  "raf": [
    {
      "title": "Transformers with Learnable Activation Functions",
      "url": "/research/2023/02/14/Transformers-with-Learnable-Activation-Functions.html",
      "date": "February 14, 2023",
      "summary": "Activation functions significantly affect model performance by reducing data complexity, yet their selection in Transformer-based language models is often overlooked. This paper explores the impact of using rational activation functions (RAFs), which unlike fixed activation functions (FAFs), can learn optimal functions from data. Our experiments demonstrate that Transformer models with RAFs (RAFT) outperform those with FAFs (FAFT), achieving a 5.71 point higher score on the GLUE benchmark with only 100 training examples and a 2.05 point increase on SQuAD with full data. The varied shapes of learned RAFs across layers and tasks suggest a new method for analyzing and understanding large pre-trained language models."
    }
  ],
  "faf": [
    {
      "title": "Transformers with Learnable Activation Functions",
      "url": "/research/2023/02/14/Transformers-with-Learnable-Activation-Functions.html",
      "date": "February 14, 2023",
      "summary": "Activation functions significantly affect model performance by reducing data complexity, yet their selection in Transformer-based language models is often overlooked. This paper explores the impact of using rational activation functions (RAFs), which unlike fixed activation functions (FAFs), can learn optimal functions from data. Our experiments demonstrate that Transformer models with RAFs (RAFT) outperform those with FAFs (FAFT), achieving a 5.71 point higher score on the GLUE benchmark with only 100 training examples and a 2.05 point increase on SQuAD with full data. The varied shapes of learned RAFs across layers and tasks suggest a new method for analyzing and understanding large pre-trained language models."
    }
  ],
  "rafs": [
    {
      "title": "Transformers with Learnable Activation Functions",
      "url": "/research/2023/02/14/Transformers-with-Learnable-Activation-Functions.html",
      "date": "February 14, 2023",
      "summary": "Activation functions significantly affect model performance by reducing data complexity, yet their selection in Transformer-based language models is often overlooked. This paper explores the impact of using rational activation functions (RAFs), which unlike fixed activation functions (FAFs), can learn optimal functions from data. Our experiments demonstrate that Transformer models with RAFs (RAFT) outperform those with FAFs (FAFT), achieving a 5.71 point higher score on the GLUE benchmark with only 100 training examples and a 2.05 point increase on SQuAD with full data. The varied shapes of learned RAFs across layers and tasks suggest a new method for analyzing and understanding large pre-trained language models."
    }
  ],
  "raft": [
    {
      "title": "Transformers with Learnable Activation Functions",
      "url": "/research/2023/02/14/Transformers-with-Learnable-Activation-Functions.html",
      "date": "February 14, 2023",
      "summary": "Activation functions significantly affect model performance by reducing data complexity, yet their selection in Transformer-based language models is often overlooked. This paper explores the impact of using rational activation functions (RAFs), which unlike fixed activation functions (FAFs), can learn optimal functions from data. Our experiments demonstrate that Transformer models with RAFs (RAFT) outperform those with FAFs (FAFT), achieving a 5.71 point higher score on the GLUE benchmark with only 100 training examples and a 2.05 point increase on SQuAD with full data. The varied shapes of learned RAFs across layers and tasks suggest a new method for analyzing and understanding large pre-trained language models."
    }
  ],
  "fafs": [
    {
      "title": "Transformers with Learnable Activation Functions",
      "url": "/research/2023/02/14/Transformers-with-Learnable-Activation-Functions.html",
      "date": "February 14, 2023",
      "summary": "Activation functions significantly affect model performance by reducing data complexity, yet their selection in Transformer-based language models is often overlooked. This paper explores the impact of using rational activation functions (RAFs), which unlike fixed activation functions (FAFs), can learn optimal functions from data. Our experiments demonstrate that Transformer models with RAFs (RAFT) outperform those with FAFs (FAFT), achieving a 5.71 point higher score on the GLUE benchmark with only 100 training examples and a 2.05 point increase on SQuAD with full data. The varied shapes of learned RAFs across layers and tasks suggest a new method for analyzing and understanding large pre-trained language models."
    }
  ],
  "faft": [
    {
      "title": "Transformers with Learnable Activation Functions",
      "url": "/research/2023/02/14/Transformers-with-Learnable-Activation-Functions.html",
      "date": "February 14, 2023",
      "summary": "Activation functions significantly affect model performance by reducing data complexity, yet their selection in Transformer-based language models is often overlooked. This paper explores the impact of using rational activation functions (RAFs), which unlike fixed activation functions (FAFs), can learn optimal functions from data. Our experiments demonstrate that Transformer models with RAFs (RAFT) outperform those with FAFs (FAFT), achieving a 5.71 point higher score on the GLUE benchmark with only 100 training examples and a 2.05 point increase on SQuAD with full data. The varied shapes of learned RAFs across layers and tasks suggest a new method for analyzing and understanding large pre-trained language models."
    }
  ],
  "squad": [
    {
      "title": "Transformers with Learnable Activation Functions",
      "url": "/research/2023/02/14/Transformers-with-Learnable-Activation-Functions.html",
      "date": "February 14, 2023",
      "summary": "Activation functions significantly affect model performance by reducing data complexity, yet their selection in Transformer-based language models is often overlooked. This paper explores the impact of using rational activation functions (RAFs), which unlike fixed activation functions (FAFs), can learn optimal functions from data. Our experiments demonstrate that Transformer models with RAFs (RAFT) outperform those with FAFs (FAFT), achieving a 5.71 point higher score on the GLUE benchmark with only 100 training examples and a 2.05 point increase on SQuAD with full data. The varied shapes of learned RAFs across layers and tasks suggest a new method for analyzing and understanding large pre-trained language models."
    }
  ],
  "vjepa": [
    {
      "title": "V-Jepa: Latent Video Prediction For Visual Representation Learning",
      "url": "/research/2023/02/15/V-Jepa-Latent-Video-Prediction-For-Visual-Representation-Learning.html",
      "date": "February 15, 2023",
      "summary": "This paper introduces V-JEPA, a self-supervised video learning method that predicts masked spatio-temporal regions in latent space, effectively applying the masked-modelling principle of large language models to video. This approach generates visual features useful across various image and video tasks without needing model adjustment, achieving significant improvements on Kinetics-400 (82.1%) and Something-Something-v2 (71.2%) benchmarks, outperforming prior video models. V-JEPA also excels in motion understanding tasks, surpassing leading image models like DINOv2 and OpenCLIP, and achieves 77.9% on ImageNet classification with video training alone, setting a new standard for video models."
    }
  ],
  "predicts": [
    {
      "title": "V-Jepa: Latent Video Prediction For Visual Representation Learning",
      "url": "/research/2023/02/15/V-Jepa-Latent-Video-Prediction-For-Visual-Representation-Learning.html",
      "date": "February 15, 2023",
      "summary": "This paper introduces V-JEPA, a self-supervised video learning method that predicts masked spatio-temporal regions in latent space, effectively applying the masked-modelling principle of large language models to video. This approach generates visual features useful across various image and video tasks without needing model adjustment, achieving significant improvements on Kinetics-400 (82.1%) and Something-Something-v2 (71.2%) benchmarks, outperforming prior video models. V-JEPA also excels in motion understanding tasks, surpassing leading image models like DINOv2 and OpenCLIP, and achieves 77.9% on ImageNet classification with video training alone, setting a new standard for video models."
    }
  ],
  "openclip": [
    {
      "title": "V-Jepa: Latent Video Prediction For Visual Representation Learning",
      "url": "/research/2023/02/15/V-Jepa-Latent-Video-Prediction-For-Visual-Representation-Learning.html",
      "date": "February 15, 2023",
      "summary": "This paper introduces V-JEPA, a self-supervised video learning method that predicts masked spatio-temporal regions in latent space, effectively applying the masked-modelling principle of large language models to video. This approach generates visual features useful across various image and video tasks without needing model adjustment, achieving significant improvements on Kinetics-400 (82.1%) and Something-Something-v2 (71.2%) benchmarks, outperforming prior video models. V-JEPA also excels in motion understanding tasks, surpassing leading image models like DINOv2 and OpenCLIP, and achieves 77.9% on ImageNet classification with video training alone, setting a new standard for video models."
    }
  ],
  "videomamba": [
    {
      "title": "VideoMamba: State Space Model for Efficient Video Understanding",
      "url": "/research/2024/03/12/VideoMamba-State-Space-Model-for-Efficient-Video-Understanding.html",
      "date": "March 12, 2024",
      "summary": "This work introduces VideoMamba, an innovative approach that addresses local redundancy and global dependencies in video understanding by adapting the Mamba framework to video analysis. VideoMamba outperforms existing 3D convolutional neural networks and video transformers through its linear-complexity operator, which facilitates efficient long-term modeling for high-resolution, long-duration videos. Its effectiveness is demonstrated across four main areas: scalability in the visual domain without needing extensive dataset pretraining, thanks to a novel self-distillation technique; the ability to recognize short-term actions with fine-grained motion differences; superior performance in long-term video understanding compared to traditional feature-based models; and robust compatibility with multiple modalities, enhancing multi-modal video analysis. VideoMamba establishes a new standard for comprehensive and efficient video understanding."
    }
  ],
  "through": [
    {
      "title": "VideoMamba: State Space Model for Efficient Video Understanding",
      "url": "/research/2024/03/12/VideoMamba-State-Space-Model-for-Efficient-Video-Understanding.html",
      "date": "March 12, 2024",
      "summary": "This work introduces VideoMamba, an innovative approach that addresses local redundancy and global dependencies in video understanding by adapting the Mamba framework to video analysis. VideoMamba outperforms existing 3D convolutional neural networks and video transformers through its linear-complexity operator, which facilitates efficient long-term modeling for high-resolution, long-duration videos. Its effectiveness is demonstrated across four main areas: scalability in the visual domain without needing extensive dataset pretraining, thanks to a novel self-distillation technique; the ability to recognize short-term actions with fine-grained motion differences; superior performance in long-term video understanding compared to traditional feature-based models; and robust compatibility with multiple modalities, enhancing multi-modal video analysis. VideoMamba establishes a new standard for comprehensive and efficient video understanding."
    }
  ],
  "operator": [
    {
      "title": "VideoMamba: State Space Model for Efficient Video Understanding",
      "url": "/research/2024/03/12/VideoMamba-State-Space-Model-for-Efficient-Video-Understanding.html",
      "date": "March 12, 2024",
      "summary": "This work introduces VideoMamba, an innovative approach that addresses local redundancy and global dependencies in video understanding by adapting the Mamba framework to video analysis. VideoMamba outperforms existing 3D convolutional neural networks and video transformers through its linear-complexity operator, which facilitates efficient long-term modeling for high-resolution, long-duration videos. Its effectiveness is demonstrated across four main areas: scalability in the visual domain without needing extensive dataset pretraining, thanks to a novel self-distillation technique; the ability to recognize short-term actions with fine-grained motion differences; superior performance in long-term video understanding compared to traditional feature-based models; and robust compatibility with multiple modalities, enhancing multi-modal video analysis. VideoMamba establishes a new standard for comprehensive and efficient video understanding."
    },
    {
      "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum",
      "url": "/research/2023/09/30/When-to-Learn-What:-Model-Adaptive-Data-Augmentation-Curriculum.html",
      "date": "September 30, 2023",
      "summary": "Data augmentation (DA) improves neural network generalization by enforcing invariances and symmetries to predefined transformations applied to input data. However, fixed augmentation policies affect samples differently at various training stages, and existing approaches cannot adapt policies to individual samples and the training model. This paper proposes Model-Adaptive Data Augmentation (MADAug) which trains an augmentation policy network to determine when to learn what. Unlike previous work, MADAug selects augmentation operators for each input image with a model-adaptive policy that varies between training stages, creating an optimized data augmentation curriculum. The policy is trained using a bi-level optimization scheme to minimize validation-set loss of a model trained with policy-produced augmentations. Extensive evaluations on multiple image classification tasks and network architectures show that MADAug outperforms or matches existing DA approaches, enhances fairness by improving all classes, particularly the difficult ones, and performs better when transferred to fine-grained datasets. Additionally, the auto-optimized policy in MADAug gradually increases perturbations, forming an easy-to-hard curriculum."
    }
  ],
  "duration": [
    {
      "title": "VideoMamba: State Space Model for Efficient Video Understanding",
      "url": "/research/2024/03/12/VideoMamba-State-Space-Model-for-Efficient-Video-Understanding.html",
      "date": "March 12, 2024",
      "summary": "This work introduces VideoMamba, an innovative approach that addresses local redundancy and global dependencies in video understanding by adapting the Mamba framework to video analysis. VideoMamba outperforms existing 3D convolutional neural networks and video transformers through its linear-complexity operator, which facilitates efficient long-term modeling for high-resolution, long-duration videos. Its effectiveness is demonstrated across four main areas: scalability in the visual domain without needing extensive dataset pretraining, thanks to a novel self-distillation technique; the ability to recognize short-term actions with fine-grained motion differences; superior performance in long-term video understanding compared to traditional feature-based models; and robust compatibility with multiple modalities, enhancing multi-modal video analysis. VideoMamba establishes a new standard for comprehensive and efficient video understanding."
    }
  ],
  "areas": [
    {
      "title": "VideoMamba: State Space Model for Efficient Video Understanding",
      "url": "/research/2024/03/12/VideoMamba-State-Space-Model-for-Efficient-Video-Understanding.html",
      "date": "March 12, 2024",
      "summary": "This work introduces VideoMamba, an innovative approach that addresses local redundancy and global dependencies in video understanding by adapting the Mamba framework to video analysis. VideoMamba outperforms existing 3D convolutional neural networks and video transformers through its linear-complexity operator, which facilitates efficient long-term modeling for high-resolution, long-duration videos. Its effectiveness is demonstrated across four main areas: scalability in the visual domain without needing extensive dataset pretraining, thanks to a novel self-distillation technique; the ability to recognize short-term actions with fine-grained motion differences; superior performance in long-term video understanding compared to traditional feature-based models; and robust compatibility with multiple modalities, enhancing multi-modal video analysis. VideoMamba establishes a new standard for comprehensive and efficient video understanding."
    }
  ],
  "lawyer": [
    {
      "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
      "url": "/research/2024/01/24/Better-Call-GPT,-Comparing-Large-Language-Models-Against-Lawyers.html",
      "date": "January 24, 2024",
      "summary": "This paper compares Large Language Models (LLMs) with traditional legal contract reviewers—Junior Lawyers and Legal Process Outsourcers (LPOs). It evaluates whether LLMs outperform humans in accuracy, speed, and cost-efficiency during contract review. Empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, revealing that advanced models match or exceed human accuracy in identifying legal issues. LLMs complete reviews in seconds, vastly outpacing the hours required by humans. Cost-wise, LLMs achieve a 99.97 percent reduction compared to traditional methods. These findings indicate a transformative shift in legal practice, with LLMs enhancing the accessibility and efficiency of legal services. The research suggests that LLM dominance in legal contract review challenges the status quo, necessitating a reimagined future for legal workflows."
    }
  ],
  "reviewers": [
    {
      "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
      "url": "/research/2024/01/24/Better-Call-GPT,-Comparing-Large-Language-Models-Against-Lawyers.html",
      "date": "January 24, 2024",
      "summary": "This paper compares Large Language Models (LLMs) with traditional legal contract reviewers—Junior Lawyers and Legal Process Outsourcers (LPOs). It evaluates whether LLMs outperform humans in accuracy, speed, and cost-efficiency during contract review. Empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, revealing that advanced models match or exceed human accuracy in identifying legal issues. LLMs complete reviews in seconds, vastly outpacing the hours required by humans. Cost-wise, LLMs achieve a 99.97 percent reduction compared to traditional methods. These findings indicate a transformative shift in legal practice, with LLMs enhancing the accessibility and efficiency of legal services. The research suggests that LLM dominance in legal contract review challenges the status quo, necessitating a reimagined future for legal workflows."
    }
  ],
  "junior": [
    {
      "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
      "url": "/research/2024/01/24/Better-Call-GPT,-Comparing-Large-Language-Models-Against-Lawyers.html",
      "date": "January 24, 2024",
      "summary": "This paper compares Large Language Models (LLMs) with traditional legal contract reviewers—Junior Lawyers and Legal Process Outsourcers (LPOs). It evaluates whether LLMs outperform humans in accuracy, speed, and cost-efficiency during contract review. Empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, revealing that advanced models match or exceed human accuracy in identifying legal issues. LLMs complete reviews in seconds, vastly outpacing the hours required by humans. Cost-wise, LLMs achieve a 99.97 percent reduction compared to traditional methods. These findings indicate a transformative shift in legal practice, with LLMs enhancing the accessibility and efficiency of legal services. The research suggests that LLM dominance in legal contract review challenges the status quo, necessitating a reimagined future for legal workflows."
    }
  ],
  "outsourcers": [
    {
      "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
      "url": "/research/2024/01/24/Better-Call-GPT,-Comparing-Large-Language-Models-Against-Lawyers.html",
      "date": "January 24, 2024",
      "summary": "This paper compares Large Language Models (LLMs) with traditional legal contract reviewers—Junior Lawyers and Legal Process Outsourcers (LPOs). It evaluates whether LLMs outperform humans in accuracy, speed, and cost-efficiency during contract review. Empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, revealing that advanced models match or exceed human accuracy in identifying legal issues. LLMs complete reviews in seconds, vastly outpacing the hours required by humans. Cost-wise, LLMs achieve a 99.97 percent reduction compared to traditional methods. These findings indicate a transformative shift in legal practice, with LLMs enhancing the accessibility and efficiency of legal services. The research suggests that LLM dominance in legal contract review challenges the status quo, necessitating a reimagined future for legal workflows."
    }
  ],
  "lpo": [
    {
      "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
      "url": "/research/2024/01/24/Better-Call-GPT,-Comparing-Large-Language-Models-Against-Lawyers.html",
      "date": "January 24, 2024",
      "summary": "This paper compares Large Language Models (LLMs) with traditional legal contract reviewers—Junior Lawyers and Legal Process Outsourcers (LPOs). It evaluates whether LLMs outperform humans in accuracy, speed, and cost-efficiency during contract review. Empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, revealing that advanced models match or exceed human accuracy in identifying legal issues. LLMs complete reviews in seconds, vastly outpacing the hours required by humans. Cost-wise, LLMs achieve a 99.97 percent reduction compared to traditional methods. These findings indicate a transformative shift in legal practice, with LLMs enhancing the accessibility and efficiency of legal services. The research suggests that LLM dominance in legal contract review challenges the status quo, necessitating a reimagined future for legal workflows."
    }
  ],
  "benchmarks": [
    {
      "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
      "url": "/research/2024/01/24/Better-Call-GPT,-Comparing-Large-Language-Models-Against-Lawyers.html",
      "date": "January 24, 2024",
      "summary": "This paper compares Large Language Models (LLMs) with traditional legal contract reviewers—Junior Lawyers and Legal Process Outsourcers (LPOs). It evaluates whether LLMs outperform humans in accuracy, speed, and cost-efficiency during contract review. Empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, revealing that advanced models match or exceed human accuracy in identifying legal issues. LLMs complete reviews in seconds, vastly outpacing the hours required by humans. Cost-wise, LLMs achieve a 99.97 percent reduction compared to traditional methods. These findings indicate a transformative shift in legal practice, with LLMs enhancing the accessibility and efficiency of legal services. The research suggests that LLM dominance in legal contract review challenges the status quo, necessitating a reimagined future for legal workflows."
    }
  ],
  "truth": [
    {
      "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
      "url": "/research/2024/01/24/Better-Call-GPT,-Comparing-Large-Language-Models-Against-Lawyers.html",
      "date": "January 24, 2024",
      "summary": "This paper compares Large Language Models (LLMs) with traditional legal contract reviewers—Junior Lawyers and Legal Process Outsourcers (LPOs). It evaluates whether LLMs outperform humans in accuracy, speed, and cost-efficiency during contract review. Empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, revealing that advanced models match or exceed human accuracy in identifying legal issues. LLMs complete reviews in seconds, vastly outpacing the hours required by humans. Cost-wise, LLMs achieve a 99.97 percent reduction compared to traditional methods. These findings indicate a transformative shift in legal practice, with LLMs enhancing the accessibility and efficiency of legal services. The research suggests that LLM dominance in legal contract review challenges the status quo, necessitating a reimagined future for legal workflows."
    }
  ],
  "senior": [
    {
      "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
      "url": "/research/2024/01/24/Better-Call-GPT,-Comparing-Large-Language-Models-Against-Lawyers.html",
      "date": "January 24, 2024",
      "summary": "This paper compares Large Language Models (LLMs) with traditional legal contract reviewers—Junior Lawyers and Legal Process Outsourcers (LPOs). It evaluates whether LLMs outperform humans in accuracy, speed, and cost-efficiency during contract review. Empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, revealing that advanced models match or exceed human accuracy in identifying legal issues. LLMs complete reviews in seconds, vastly outpacing the hours required by humans. Cost-wise, LLMs achieve a 99.97 percent reduction compared to traditional methods. These findings indicate a transformative shift in legal practice, with LLMs enhancing the accessibility and efficiency of legal services. The research suggests that LLM dominance in legal contract review challenges the status quo, necessitating a reimagined future for legal workflows."
    }
  ],
  "outpace": [
    {
      "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
      "url": "/research/2024/01/24/Better-Call-GPT,-Comparing-Large-Language-Models-Against-Lawyers.html",
      "date": "January 24, 2024",
      "summary": "This paper compares Large Language Models (LLMs) with traditional legal contract reviewers—Junior Lawyers and Legal Process Outsourcers (LPOs). It evaluates whether LLMs outperform humans in accuracy, speed, and cost-efficiency during contract review. Empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, revealing that advanced models match or exceed human accuracy in identifying legal issues. LLMs complete reviews in seconds, vastly outpacing the hours required by humans. Cost-wise, LLMs achieve a 99.97 percent reduction compared to traditional methods. These findings indicate a transformative shift in legal practice, with LLMs enhancing the accessibility and efficiency of legal services. The research suggests that LLM dominance in legal contract review challenges the status quo, necessitating a reimagined future for legal workflows."
    }
  ],
  "percent": [
    {
      "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
      "url": "/research/2024/01/24/Better-Call-GPT,-Comparing-Large-Language-Models-Against-Lawyers.html",
      "date": "January 24, 2024",
      "summary": "This paper compares Large Language Models (LLMs) with traditional legal contract reviewers—Junior Lawyers and Legal Process Outsourcers (LPOs). It evaluates whether LLMs outperform humans in accuracy, speed, and cost-efficiency during contract review. Empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, revealing that advanced models match or exceed human accuracy in identifying legal issues. LLMs complete reviews in seconds, vastly outpacing the hours required by humans. Cost-wise, LLMs achieve a 99.97 percent reduction compared to traditional methods. These findings indicate a transformative shift in legal practice, with LLMs enhancing the accessibility and efficiency of legal services. The research suggests that LLM dominance in legal contract review challenges the status quo, necessitating a reimagined future for legal workflows."
    }
  ],
  "accessibility": [
    {
      "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
      "url": "/research/2024/01/24/Better-Call-GPT,-Comparing-Large-Language-Models-Against-Lawyers.html",
      "date": "January 24, 2024",
      "summary": "This paper compares Large Language Models (LLMs) with traditional legal contract reviewers—Junior Lawyers and Legal Process Outsourcers (LPOs). It evaluates whether LLMs outperform humans in accuracy, speed, and cost-efficiency during contract review. Empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, revealing that advanced models match or exceed human accuracy in identifying legal issues. LLMs complete reviews in seconds, vastly outpacing the hours required by humans. Cost-wise, LLMs achieve a 99.97 percent reduction compared to traditional methods. These findings indicate a transformative shift in legal practice, with LLMs enhancing the accessibility and efficiency of legal services. The research suggests that LLM dominance in legal contract review challenges the status quo, necessitating a reimagined future for legal workflows."
    }
  ],
  "service": [
    {
      "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
      "url": "/research/2024/01/24/Better-Call-GPT,-Comparing-Large-Language-Models-Against-Lawyers.html",
      "date": "January 24, 2024",
      "summary": "This paper compares Large Language Models (LLMs) with traditional legal contract reviewers—Junior Lawyers and Legal Process Outsourcers (LPOs). It evaluates whether LLMs outperform humans in accuracy, speed, and cost-efficiency during contract review. Empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, revealing that advanced models match or exceed human accuracy in identifying legal issues. LLMs complete reviews in seconds, vastly outpacing the hours required by humans. Cost-wise, LLMs achieve a 99.97 percent reduction compared to traditional methods. These findings indicate a transformative shift in legal practice, with LLMs enhancing the accessibility and efficiency of legal services. The research suggests that LLM dominance in legal contract review challenges the status quo, necessitating a reimagined future for legal workflows."
    }
  ],
  "status": [
    {
      "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
      "url": "/research/2024/01/24/Better-Call-GPT,-Comparing-Large-Language-Models-Against-Lawyers.html",
      "date": "January 24, 2024",
      "summary": "This paper compares Large Language Models (LLMs) with traditional legal contract reviewers—Junior Lawyers and Legal Process Outsourcers (LPOs). It evaluates whether LLMs outperform humans in accuracy, speed, and cost-efficiency during contract review. Empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, revealing that advanced models match or exceed human accuracy in identifying legal issues. LLMs complete reviews in seconds, vastly outpacing the hours required by humans. Cost-wise, LLMs achieve a 99.97 percent reduction compared to traditional methods. These findings indicate a transformative shift in legal practice, with LLMs enhancing the accessibility and efficiency of legal services. The research suggests that LLM dominance in legal contract review challenges the status quo, necessitating a reimagined future for legal workflows."
    }
  ],
  "reimagine": [
    {
      "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
      "url": "/research/2024/01/24/Better-Call-GPT,-Comparing-Large-Language-Models-Against-Lawyers.html",
      "date": "January 24, 2024",
      "summary": "This paper compares Large Language Models (LLMs) with traditional legal contract reviewers—Junior Lawyers and Legal Process Outsourcers (LPOs). It evaluates whether LLMs outperform humans in accuracy, speed, and cost-efficiency during contract review. Empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, revealing that advanced models match or exceed human accuracy in identifying legal issues. LLMs complete reviews in seconds, vastly outpacing the hours required by humans. Cost-wise, LLMs achieve a 99.97 percent reduction compared to traditional methods. These findings indicate a transformative shift in legal practice, with LLMs enhancing the accessibility and efficiency of legal services. The research suggests that LLM dominance in legal contract review challenges the status quo, necessitating a reimagined future for legal workflows."
    }
  ],
  "workflow": [
    {
      "title": "Better Call GPT, Comparing Large Language Models Against Lawyers",
      "url": "/research/2024/01/24/Better-Call-GPT,-Comparing-Large-Language-Models-Against-Lawyers.html",
      "date": "January 24, 2024",
      "summary": "This paper compares Large Language Models (LLMs) with traditional legal contract reviewers—Junior Lawyers and Legal Process Outsourcers (LPOs). It evaluates whether LLMs outperform humans in accuracy, speed, and cost-efficiency during contract review. Empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, revealing that advanced models match or exceed human accuracy in identifying legal issues. LLMs complete reviews in seconds, vastly outpacing the hours required by humans. Cost-wise, LLMs achieve a 99.97 percent reduction compared to traditional methods. These findings indicate a transformative shift in legal practice, with LLMs enhancing the accessibility and efficiency of legal services. The research suggests that LLM dominance in legal contract review challenges the status quo, necessitating a reimagined future for legal workflows."
    }
  ],
  "kan": [
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "kolmogorov": [
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "arnold": [
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "theorem": [
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "kans": [
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "perceptrons": [
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "nodes": [
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "univariate": [
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "parametrize": [
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "spline": [
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "pde": [
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "allowing": [
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "visualization": [
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "mathematic": [
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "physics": [
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "helping": [
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "scientist": [
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "rediscover": [
    {
      "title": "KAN: Kolmogorov–Arnold Networks",
      "url": "/research/2024/05/02/KAN:-Kolmogorov–Arnold-Networks.html",
      "date": "May 2, 2024",
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, Kolmogorov-Arnold Networks (KANs) are proposed as alternatives to Multi-Layer Perceptrons (MLPs). Unlike MLPs with fixed activation functions on nodes, KANs have learnable activation functions on edges. KANs replace linear weights with univariate functions parametrized as splines. This modification enhances KANs' accuracy and interpretability compared to MLPs. Smaller KANs achieve comparable or superior accuracy in data fitting and PDE solving. KANs also exhibit faster neural scaling laws and are more interpretable, allowing intuitive visualization and user interaction. Examples in mathematics and physics demonstrate KANs' utility in helping scientists (re)discover laws. KANs present promising alternatives to MLPs, potentially advancing current deep learning models."
    }
  ],
  "grow": [
    {
      "title": "The Platonic Representation Hypothesis",
      "url": "/research/2024/05/13/The-Platonic-Representation-Hypothesis.html",
      "date": "May 13, 2024",
      "summary": "This paper argues that AI model representations, especially in deep networks, are converging. It surveys examples in the literature showing how neural network representations align over time and across domains. It demonstrates convergence across data modalities, noting that as vision and language models grow, they measure distances between data points similarly. The authors hypothesize that this convergence leads to a shared statistical model of reality, termed the platonic representation and discuss possible selective pressures toward it. The paper also addresses the implications, limitations, and counterexamples to this trend."
    }
  ],
  "pressure": [
    {
      "title": "The Platonic Representation Hypothesis",
      "url": "/research/2024/05/13/The-Platonic-Representation-Hypothesis.html",
      "date": "May 13, 2024",
      "summary": "This paper argues that AI model representations, especially in deep networks, are converging. It surveys examples in the literature showing how neural network representations align over time and across domains. It demonstrates convergence across data modalities, noting that as vision and language models grow, they measure distances between data points similarly. The authors hypothesize that this convergence leads to a shared statistical model of reality, termed the platonic representation and discuss possible selective pressures toward it. The paper also addresses the implications, limitations, and counterexamples to this trend."
    }
  ],
  "implications": [
    {
      "title": "The Platonic Representation Hypothesis",
      "url": "/research/2024/05/13/The-Platonic-Representation-Hypothesis.html",
      "date": "May 13, 2024",
      "summary": "This paper argues that AI model representations, especially in deep networks, are converging. It surveys examples in the literature showing how neural network representations align over time and across domains. It demonstrates convergence across data modalities, noting that as vision and language models grow, they measure distances between data points similarly. The authors hypothesize that this convergence leads to a shared statistical model of reality, termed the platonic representation and discuss possible selective pressures toward it. The paper also addresses the implications, limitations, and counterexamples to this trend."
    }
  ],
  "counterexample": [
    {
      "title": "The Platonic Representation Hypothesis",
      "url": "/research/2024/05/13/The-Platonic-Representation-Hypothesis.html",
      "date": "May 13, 2024",
      "summary": "This paper argues that AI model representations, especially in deep networks, are converging. It surveys examples in the literature showing how neural network representations align over time and across domains. It demonstrates convergence across data modalities, noting that as vision and language models grow, they measure distances between data points similarly. The authors hypothesize that this convergence leads to a shared statistical model of reality, termed the platonic representation and discuss possible selective pressures toward it. The paper also addresses the implications, limitations, and counterexamples to this trend."
    }
  ],
  "da": [
    {
      "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum",
      "url": "/research/2023/09/30/When-to-Learn-What:-Model-Adaptive-Data-Augmentation-Curriculum.html",
      "date": "September 30, 2023",
      "summary": "Data augmentation (DA) improves neural network generalization by enforcing invariances and symmetries to predefined transformations applied to input data. However, fixed augmentation policies affect samples differently at various training stages, and existing approaches cannot adapt policies to individual samples and the training model. This paper proposes Model-Adaptive Data Augmentation (MADAug) which trains an augmentation policy network to determine when to learn what. Unlike previous work, MADAug selects augmentation operators for each input image with a model-adaptive policy that varies between training stages, creating an optimized data augmentation curriculum. The policy is trained using a bi-level optimization scheme to minimize validation-set loss of a model trained with policy-produced augmentations. Extensive evaluations on multiple image classification tasks and network architectures show that MADAug outperforms or matches existing DA approaches, enhances fairness by improving all classes, particularly the difficult ones, and performs better when transferred to fine-grained datasets. Additionally, the auto-optimized policy in MADAug gradually increases perturbations, forming an easy-to-hard curriculum."
    }
  ],
  "symmetry": [
    {
      "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum",
      "url": "/research/2023/09/30/When-to-Learn-What:-Model-Adaptive-Data-Augmentation-Curriculum.html",
      "date": "September 30, 2023",
      "summary": "Data augmentation (DA) improves neural network generalization by enforcing invariances and symmetries to predefined transformations applied to input data. However, fixed augmentation policies affect samples differently at various training stages, and existing approaches cannot adapt policies to individual samples and the training model. This paper proposes Model-Adaptive Data Augmentation (MADAug) which trains an augmentation policy network to determine when to learn what. Unlike previous work, MADAug selects augmentation operators for each input image with a model-adaptive policy that varies between training stages, creating an optimized data augmentation curriculum. The policy is trained using a bi-level optimization scheme to minimize validation-set loss of a model trained with policy-produced augmentations. Extensive evaluations on multiple image classification tasks and network architectures show that MADAug outperforms or matches existing DA approaches, enhances fairness by improving all classes, particularly the difficult ones, and performs better when transferred to fine-grained datasets. Additionally, the auto-optimized policy in MADAug gradually increases perturbations, forming an easy-to-hard curriculum."
    }
  ],
  "madaug": [
    {
      "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum",
      "url": "/research/2023/09/30/When-to-Learn-What:-Model-Adaptive-Data-Augmentation-Curriculum.html",
      "date": "September 30, 2023",
      "summary": "Data augmentation (DA) improves neural network generalization by enforcing invariances and symmetries to predefined transformations applied to input data. However, fixed augmentation policies affect samples differently at various training stages, and existing approaches cannot adapt policies to individual samples and the training model. This paper proposes Model-Adaptive Data Augmentation (MADAug) which trains an augmentation policy network to determine when to learn what. Unlike previous work, MADAug selects augmentation operators for each input image with a model-adaptive policy that varies between training stages, creating an optimized data augmentation curriculum. The policy is trained using a bi-level optimization scheme to minimize validation-set loss of a model trained with policy-produced augmentations. Extensive evaluations on multiple image classification tasks and network architectures show that MADAug outperforms or matches existing DA approaches, enhances fairness by improving all classes, particularly the difficult ones, and performs better when transferred to fine-grained datasets. Additionally, the auto-optimized policy in MADAug gradually increases perturbations, forming an easy-to-hard curriculum."
    }
  ],
  "bilevel": [
    {
      "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum",
      "url": "/research/2023/09/30/When-to-Learn-What:-Model-Adaptive-Data-Augmentation-Curriculum.html",
      "date": "September 30, 2023",
      "summary": "Data augmentation (DA) improves neural network generalization by enforcing invariances and symmetries to predefined transformations applied to input data. However, fixed augmentation policies affect samples differently at various training stages, and existing approaches cannot adapt policies to individual samples and the training model. This paper proposes Model-Adaptive Data Augmentation (MADAug) which trains an augmentation policy network to determine when to learn what. Unlike previous work, MADAug selects augmentation operators for each input image with a model-adaptive policy that varies between training stages, creating an optimized data augmentation curriculum. The policy is trained using a bi-level optimization scheme to minimize validation-set loss of a model trained with policy-produced augmentations. Extensive evaluations on multiple image classification tasks and network architectures show that MADAug outperforms or matches existing DA approaches, enhances fairness by improving all classes, particularly the difficult ones, and performs better when transferred to fine-grained datasets. Additionally, the auto-optimized policy in MADAug gradually increases perturbations, forming an easy-to-hard curriculum."
    }
  ],
  "xlstm": [
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "carousel": [
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "idea": [
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "lstm": [
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "stabilization": [
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "slstm": [
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "ii": [
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ],
  "mlstm": [
    {
      "title": "xLSTM: Extended Long Short-Term Memory",
      "url": "/research/2024/05/07/xLSTM:-Extended-Long-Short-Term-Memory.html",
      "date": "May 7, 2024",
      "summary": "In the 1990s, the constant error carousel and gating were introduced as central ideas of the Long Short-Term Memory (LSTM). LSTMs have since proven crucial to many deep learning successes, including the first Large Language Models (LLMs). However, Transformer technology with parallelizable self-attention has surpassed LSTMs at scale. This study explores scaling LSTMs to billions of parameters using modern LLM techniques while addressing LSTM limitations. It introduces exponential gating with normalization and stabilization techniques and modifies the LSTM memory structure, creating (i) sLSTM with scalar memory, scalar update, and new memory mixing, and (ii) mLSTM with fully parallelizable matrix memory and covariance update. These extensions form xLSTM blocks, residually stacked into xLSTM architectures, enabling xLSTMs to perform comparably to state-of-the-art Transformers and State Space Models in performance and scaling."
    }
  ]
}
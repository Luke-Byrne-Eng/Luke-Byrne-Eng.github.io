---
layout: paper
title:  "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs"
date:   22 Feb 2024
categories: research
paper_url: https://arxiv.org/pdf/2402.11753.pdf
code_url: 
summary: "Safety in large language models (LLMs) is crucial, but current safety techniques, such as data filtering and supervised fine-tuning, overlook the complexity of real-world applications, like the use of ASCII art in forums, which can bypass these safety measures. We introduce an ASCII art-based jailbreak attack, ArtPrompt, and a benchmark, Vision-in-Text Challenge (VITC), to test LLMs' abilities to recognize non-semantic prompts. Our findings reveal that state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle with ASCII art recognition. ArtPrompt exploits this vulnerability, demonstrating that it can effectively compromise the safety mechanisms of these models with just black-box access."
---


---
layout: paper
title:  "On the Relationship Between Self-Attention And Convolutional Layers"
date:   10 Jan 2020
categories: research
paper_url: https://arxiv.org/pdf/1911.03584.pdf
code_url: 
summary: "Recent studies have shown that attention mechanisms can rival or even outperform convolutional layers in vision tasks, challenging their dominance. Ramachandran et al. (2019) demonstrated that attention could entirely replace convolution, achieving top results. This work investigates whether attention layers function like convolutional layers, finding that multi-head self-attention layers, with enough heads, match the expressiveness of convolutional layers. Numerical experiments confirm that self-attention layers learn to focus on pixel-grid patterns akin to convolutional layers, supporting our findings."
---


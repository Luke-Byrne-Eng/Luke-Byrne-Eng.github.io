---
layout: paper
title:  "Three things everyone should know about Vision Transformers"
date:   18 Mar 2022
categories: research
paper_url: https://arxiv.org/pdf/2203.09795
code_url: 
summary: "This research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. The authors validate these approaches using the ImageNet-1k dataset and confirm their findings with the ImageNet-v2 test set, evaluating transfer performance across six additional datasets."
---


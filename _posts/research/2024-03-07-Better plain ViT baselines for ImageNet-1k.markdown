---
layout: paper
title:  "Better plain ViT baselines for ImageNet-1k"
date:   2 May 2022
categories: research
paper_url: https://arxiv.org/pdf/1606.08415.pdf
code_url: 
summary: "This paper challenges the common belief that the Vision Transformer (ViT) model requires sophisticated regularization techniques to excel on ImageNet-1k scale data. The authors present minor modifications to the original ViT vanilla training setting that significantly improve the performance of plain ViT models. They find that standard data augmentation is sufficient, with 90 epochs of training surpassing 76% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reaching 80% in less than one day."
---


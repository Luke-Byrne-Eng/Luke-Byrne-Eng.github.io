---
layout: paper
title:  "Hungry Hungry Hippos: Towards Language Modeling with State Space Models"
date:   29 Apr 2023
categories: research
paper_url: https://arxiv.org/pdf/2212.14052.pdf
code_url: 
summary: "State Space Models (SSM), despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. To improve SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing."
---


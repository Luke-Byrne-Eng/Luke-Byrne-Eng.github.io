---
layout: paper
title:  "Delving Deep into Rectifiers:
Surpassing Human-Level Performance on ImageNet Classificatio"
date:   6 Feb 2015
categories: research
paper_url: https://arxiv.org/pdf/1502.01852.pdf
code_url: 
summary: "This paper intriduces the Kaiming He initialisation strategy for RELU like activation functions. This the default init in pytorch, improving model stability and decreasing training time for RELU networks. This study also introduces a Parametric Rectified Linear Unit (PReLU) activation function that extends traditional units, offering improved model fitting with negligible additional computational cost."
---


{
  "convolution": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    },
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/research/2022/08/05/Efficiently-Modeling-Long-Sequences-with-Structured-State-Spaces.html",
      "date": "August 5, 2022",
      "summary": "he quest for a universal sequence model capable of handling data across different tasks and modalities, especially long-range dependencies, faces challenges with conventional models like RNNs, CNNs, and Transformers, which struggle with very long sequences. A promising approach using state space models (SSM) showed potential but had high computational demands. We introduce the Structured State Space sequence model (S4), a more efficient parameterization of SSM, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/research/2021/05/03/Robust-and-Generalizable-Visual-Representation-Learning-via-Random-Convolutions.html",
      "date": "May 3, 2021",
      "summary": "This study demonstrates that deep neural networks' vulnerability to texture shifts and perturbations can be mitigated by using random convolutions for data augmentation. Random convolutions, which approximately preserve shape while distorting local textures, create an infinite variety of new domains. We tested using these convolutions as new images or in combination with original images during training. Our method significantly enhances performance on unseen domains and domain generalization benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods. Additionally, it provides a more robust pretrained visual representation for downstream tasks."
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "Training data-efficient image transformers & distillation through attention",
      "url": "/research/2021/01/15/Training-data-efficient-image-transformers-&-distillation-through-attention.html",
      "date": "January 15, 2021",
      "summary": "This study introduces DeiT, competitive vision transformers trained solely on ImageNet without convolutions, achieving up to 83.1% top-1 accuracy on ImageNet, using a single computer in under 3 days. Notably, it presents a unique teacher-student strategy for transformers, leveraging a distillation token for efficient learning from a convolutional network teacher. This method results in performance on par with convolutional networks, achieving up to 85.2% accuracy on ImageNet and demonstrating effective transferability to other tasks. The authors provide access to their code and models."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "Recent studies have shown that Convolutional Neural Networks (CNNs), which were thought to recognize objects by learning complex shapes, are actually more biased towards recognizing textures. This finding, based on a series of tests comparing CNNs with human observers using images with conflicting texture and shape cues, contradicts the traditional understanding of how these networks process images. We discovered that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, when we trained a standard CNN architecture (ResNet-50) on a stylized version of ImageNet, designed to emphasize shape, the network's performance aligned more closely with human behavior. This training not only matched human performance in controlled experiments (involving 97 observers across 48,560 trials) but also enhanced object detection and robustness against image distortions, underscoring the benefits of shape-based representations in visual recognition systems."
    },
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "The limited number of videos in current action classification datasets like UCF-101 and HMDB-51 hinders the identification of effective video architectures due to similar performance across small-scale benchmarks. This study re-examines top architectures using the new, significantly larger Kinetics Human Action Video dataset, featuring 400 classes and over 400 clips per class from challenging YouTube videos. It analyzes the impact of this dataset on the performance of existing architectures and the benefits of pre-training on Kinetics. Additionally, the paper introduces the Two-Stream Inflated 3D ConvNet (I3D), an advancement that expands 2D ConvNet designs into 3D for enhanced video feature extraction. This model, pre-trained on Kinetics, sets new benchmarks in action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    },
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "Recent advancements in image transformers, notably in models such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, have narrowed the performance gap with traditional CNNs. These models are typically trained on large datasets like ImageNet-21k and then finetuned on ImageNet-1k. However, assessments often overlook Tiny ImageNet when evaluating transfer learning performance. This paper updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. The code is available at: https://github.com/ehuynh1106/TinyImageNet-Transformers"
    },
    {
      "title": "Are Convolutional Neural Networks or Transformers more like human vision?",
      "url": "/research/2021/07/01/Are-Convolutional-Neural-Networks-or-Transformers-more-like-human-vision.html",
      "date": "July 1, 2021",
      "summary": "This study investigates the performance of Convolutional Neural Networks (CNNs) and attention-based Vision Transformers (ViTs) in computer vision tasks. While CNNs excel in accuracy, ViTs offer a different approach with weaker inductive biases. By analyzing error patterns, we find that ViTs exhibit consistency with human errors, suggesting potential for more human-like vision models and insights into human object recognition."
    },
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    }
  ],
  "roi": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    }
  ],
  "pool": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    }
  ],
  "module": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "We introduce DiffBIR, a framework using pretrained text-to-image diffusion models for blind image restoration. It consists of a two-stage pipeline: initially, we pretrain a restoration module on various degradations to enhance real-world applicability. Then, we use latent diffusion models for realistic restoration, including a novel injective modulation sub-network, LAControlNet, for fine-tuning, and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, we provide a controllable module for users to adjust quality and fidelity during the denoising process. Our extensive tests show DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets. The code is available online."
    }
  ],
  "transformation": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    },
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/On-Interaction-Between-Augmentations-and-Corruptions-in-Natural-Corruption-Robustness.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "Deep neural networks, while effective in complex vision tasks, suffer from overfitting. Current image augmentation techniques, largely linear and hand-crafted like scale, flip, and color jitter, fail to produce sufficiently challenging examples. We introduce a new augmentation approach, Mask-Reconstruct Augmentation (MRA), leveraging self-supervised masked autoencoders to generate distorted inputs. This method, inspired by masked image modeling's success in self-supervised learning, uses nonlinear transformations for regularization. Our extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks. The code is available at https://github.com/haohang96/MRA."
    }
  ],
  "augmentation": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    },
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/research/2023/03/08/Extreme-Masking-for-Learning-Instance-and-Distributed-Visual-Representations.html",
      "date": "March 8, 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/research/2021/05/03/Robust-and-Generalizable-Visual-Representation-Learning-via-Random-Convolutions.html",
      "date": "May 3, 2021",
      "summary": "This study demonstrates that deep neural networks' vulnerability to texture shifts and perturbations can be mitigated by using random convolutions for data augmentation. Random convolutions, which approximately preserve shape while distorting local textures, create an infinite variety of new domains. We tested using these convolutions as new images or in combination with original images during training. Our method significantly enhances performance on unseen domains and domain generalization benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods. Additionally, it provides a more robust pretrained visual representation for downstream tasks."
    },
    {
      "title": "Dropout as Data Augmentation",
      "url": "/research/2016/01/08/Dropout-as-Data-Augmentation.html",
      "date": "January 8, 2016",
      "summary": "Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/research/2023/06/30/Impact-of-Noise-on-Calibration-and-Generalisation-of-Neural-Networks.html",
      "date": "June 30, 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. We examine different types of noise, such as activation and input augmentation noise, in both in-distribution and out-of-distribution scenarios. Activation noise significantly improves generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/On-Interaction-Between-Augmentations-and-Corruptions-in-Natural-Corruption-Robustness.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "Vision Transformers (ViT) excel in various vision tasks, like image classification, object detection, and semantic segmentation, but require more model regularization or data augmentation (AugReg) than convolutional neural networks, especially with smaller datasets. Through a comprehensive study, we explore how training data amount, AugReg, model size, and compute budget interact. Our findings indicate that using more compute and AugReg can achieve the same performance as training with significantly more data. Specifically, we demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "Deep neural networks, while effective in complex vision tasks, suffer from overfitting. Current image augmentation techniques, largely linear and hand-crafted like scale, flip, and color jitter, fail to produce sufficiently challenging examples. We introduce a new augmentation approach, Mask-Reconstruct Augmentation (MRA), leveraging self-supervised masked autoencoders to generate distorted inputs. This method, inspired by masked image modeling's success in self-supervised learning, uses nonlinear transformations for regularization. Our extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks. The code is available at https://github.com/haohang96/MRA."
    },
    {
      "title": "Better plain ViT baselines for ImageNet-1k",
      "url": "/research/2022/05/02/Better-plain-ViT-baselines-for-ImageNet-1k.html",
      "date": "May 2, 2022",
      "summary": "It is commonly accepted that the Vision Transformer model requires sophisticated regularization techniques to excel at ImageNet-1k scale data. Surprisingly, we find this is not the case and standard data augmentation is sufficient. This note presents a few minor modifications to the original Vision Transformer (ViT) vanilla training setting that dramatically improve the performance of plain ViT models. Notably, 90 epochs of training surpass 76% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reach 80% in less than one day."
    },
    {
      "title": "DeiT III: Revenge of the ViT",
      "url": "/research/2023/01/18/DeiT-III-Revenge-of-the-ViT.html",
      "date": "January 18, 2023",
      "summary": "This paper explores the supervised training of Vision Transformers (ViTs), using a simplified training approach adapted from ResNet-50 that includes a novel data-augmentation method with just 3 augmentations. The study demonstrates that this method significantly improves ViTs' performance in image classification, transfer learning, and semantic segmentation over previous supervised training techniques. Moreover, it shows ViTs' performance can match newer architectures, providing a new benchmark for evaluating self-supervised methods on ViTs."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "Semi-supervised learning (SSL) utilizes unlabeled data to boost model performance but often at the expense of increased complexity. This paper introduces FixMatch, a streamlined SSL algorithm that simplifies existing methods. FixMatch uses a model's predictions on weakly-augmented unlabeled images to generate pseudo-labels, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. Remarkably, FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness. The code is publicly available."
    },
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This study investigates the use of high-quality, physically-based rendering and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle, enhancing image classification and semantic segmentation models. Synthetic images proved to be effective for augmenting limited real training data. However, models trained solely on synthetic images showed low accuracy on real validation images, which significantly improved when even small amounts of real data were included. Augmenting real data with synthetic images outperformed training on only real images. Moreover, pretraining models on synthetic data before transfer learning significantly reduced training costs, allowing most of the training to be completed upfront."
    },
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method, applies a single augmentation to each image and outperforms existing methods with minimal complexity and cost. We conducted extensive experiments to validate its performance against state-of-the-art methods across various image classification scenarios and explored its effectiveness through multiple ablation studies involving different augmentation spaces and methods. Our work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Highlighting a stagnation in automatic augmentation research, we conclude with proposed best practices for future advancements in the field."
    }
  ],
  "sampling": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    },
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "Self-paced curriculum learning (SCL) has been effective in various fields, employing easy-to-hard sampling based on data difficulty. However, in medical image analysis tasks like universal lesion detection, it faces challenges due to inaccurate difficulty estimation and the under-utilization of hard samples. This paper introduces a novel approach, mixed-order self-paced curriculum learning (Mo-SCL), which combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches to address these issues. Through theoretical analysis and experiments on the DeepLesion dataset, Mo-SCL is shown to enhance lesion detection accuracy in state-of-the-art methods without additional network modifications."
    }
  ],
  "offset": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    }
  ],
  "supervision": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    },
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    },
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/research/2020/09/10/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning.html",
      "date": "September 10, 2020",
      "summary": "We introduce BYOL, a novel self-supervised learning method for image representation without using negative pairs. Utilizing two networks, BYOL trains one to predict the representation of an image as seen by the other under a different view, updating the latter with the former's slow-moving average. Surpassing state-of-the-art methods, it achieves up to 79.6% accuracy on ImageNet with ResNet models, performing equally or better on transfer and semi-supervised tasks. Code and models are available on GitHub."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/research/2021/08/16/An-Empirical-Study-of-Training-Self-Supervised-Vision-Transformers.html",
      "date": "August 16, 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/research/2020/08/03/Are-Labels-Necessary-for-Neural-Architecture-Search.html",
      "date": "August 3, 2020",
      "summary": "This paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning if effective neural architectures can be discovered using only images without human-annotated labels. Through two experimental setups—sample-based and search-based—we investigate this possibility. In the sample-based approach, we evaluate 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. In the search-based experiments, we employ a recognized NAS algorithm, DARTS, with various unsupervised objectives, finding that architectures identified without labels perform competitively compared to those found with labels. These findings suggest that human-annotated labels may not be necessary for identifying efficient neural architectures, as image statistics alone could be sufficient."
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "Deep neural networks, while effective in complex vision tasks, suffer from overfitting. Current image augmentation techniques, largely linear and hand-crafted like scale, flip, and color jitter, fail to produce sufficiently challenging examples. We introduce a new augmentation approach, Mask-Reconstruct Augmentation (MRA), leveraging self-supervised masked autoencoders to generate distorted inputs. This method, inspired by masked image modeling's success in self-supervised learning, uses nonlinear transformations for regularization. Our extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks. The code is available at https://github.com/haohang96/MRA."
    },
    {
      "title": "DeiT III: Revenge of the ViT",
      "url": "/research/2023/01/18/DeiT-III-Revenge-of-the-ViT.html",
      "date": "January 18, 2023",
      "summary": "This paper explores the supervised training of Vision Transformers (ViTs), using a simplified training approach adapted from ResNet-50 that includes a novel data-augmentation method with just 3 augmentations. The study demonstrates that this method significantly improves ViTs' performance in image classification, transfer learning, and semantic segmentation over previous supervised training techniques. Moreover, it shows ViTs' performance can match newer architectures, providing a new benchmark for evaluating self-supervised methods on ViTs."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "Semi-supervised learning (SSL) utilizes unlabeled data to boost model performance but often at the expense of increased complexity. This paper introduces FixMatch, a streamlined SSL algorithm that simplifies existing methods. FixMatch uses a model's predictions on weakly-augmented unlabeled images to generate pseudo-labels, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. Remarkably, FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness. The code is publicly available."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    }
  ],
  "replace": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    }
  ],
  "vision": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    },
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "Fast Feedforward Networks",
      "url": "/research/2023/09/18/Fast-Feedforward-Networks.html",
      "date": "September 18, 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/research/2014/01/01/Dropout-A-Simple-Way-to-Prevent-Neural-Networks-from-Overfitting.html",
      "date": "January 1, 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    },
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/On-Interaction-Between-Augmentations-and-Corruptions-in-Natural-Corruption-Robustness.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/research/2021/08/16/An-Empirical-Study-of-Training-Self-Supervised-Vision-Transformers.html",
      "date": "August 16, 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    },
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "Vision Transformers (ViT) excel in various vision tasks, like image classification, object detection, and semantic segmentation, but require more model regularization or data augmentation (AugReg) than convolutional neural networks, especially with smaller datasets. Through a comprehensive study, we explore how training data amount, AugReg, model size, and compute budget interact. Our findings indicate that using more compute and AugReg can achieve the same performance as training with significantly more data. Specifically, we demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    },
    {
      "title": "Training data-efficient image transformers & distillation through attention",
      "url": "/research/2021/01/15/Training-data-efficient-image-transformers-&-distillation-through-attention.html",
      "date": "January 15, 2021",
      "summary": "This study introduces DeiT, competitive vision transformers trained solely on ImageNet without convolutions, achieving up to 83.1% top-1 accuracy on ImageNet, using a single computer in under 3 days. Notably, it presents a unique teacher-student strategy for transformers, leveraging a distillation token for efficient learning from a convolutional network teacher. This method results in performance on par with convolutional networks, achieving up to 85.2% accuracy on ImageNet and demonstrating effective transferability to other tasks. The authors provide access to their code and models."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "Deep neural networks, while effective in complex vision tasks, suffer from overfitting. Current image augmentation techniques, largely linear and hand-crafted like scale, flip, and color jitter, fail to produce sufficiently challenging examples. We introduce a new augmentation approach, Mask-Reconstruct Augmentation (MRA), leveraging self-supervised masked autoencoders to generate distorted inputs. This method, inspired by masked image modeling's success in self-supervised learning, uses nonlinear transformations for regularization. Our extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks. The code is available at https://github.com/haohang96/MRA."
    },
    {
      "title": "Better plain ViT baselines for ImageNet-1k",
      "url": "/research/2022/05/02/Better-plain-ViT-baselines-for-ImageNet-1k.html",
      "date": "May 2, 2022",
      "summary": "It is commonly accepted that the Vision Transformer model requires sophisticated regularization techniques to excel at ImageNet-1k scale data. Surprisingly, we find this is not the case and standard data augmentation is sufficient. This note presents a few minor modifications to the original Vision Transformer (ViT) vanilla training setting that dramatically improve the performance of plain ViT models. Notably, 90 epochs of training surpass 76% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reach 80% in less than one day."
    },
    {
      "title": "DeiT III: Revenge of the ViT",
      "url": "/research/2023/01/18/DeiT-III-Revenge-of-the-ViT.html",
      "date": "January 18, 2023",
      "summary": "This paper explores the supervised training of Vision Transformers (ViTs), using a simplified training approach adapted from ResNet-50 that includes a novel data-augmentation method with just 3 augmentations. The study demonstrates that this method significantly improves ViTs' performance in image classification, transfer learning, and semantic segmentation over previous supervised training techniques. Moreover, it shows ViTs' performance can match newer architectures, providing a new benchmark for evaluating self-supervised methods on ViTs."
    },
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "We introduce the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. Our empirical evaluation across computer vision, natural language processing, and speech tasks shows that GELU offers performance improvements over ReLU and ELU activations."
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "The Vision Transformer (ViT) offers simplicity, robustness, and superior performance in vision tasks but struggles with small-scale datasets due to its lack of inherent inductive biases and dependence on large-scale pre-training datasets like ImageNet and JFT. This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of ViTs without large-scale pre-training or modifications to the architecture or loss functions. Experiments show improved performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, maintaining ViT's attention to relevant regions and robustness. The code and models are available online."
    },
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "This paper explores the effectiveness of using synthetic data, generated by graphics simulators, for pre-training models in computer vision, addressing the challenges posed by using real-image datasets like high costs and ethical concerns. It highlights that the performance of models on downstream tasks varies with different simulation parameters, suggesting a tailored approach to synthetic data generation for optimal results. The introduction of Task2Sim, a model that maps downstream task requirements to optimal simulation parameters, allows for the generation of synthetic pre-training data customized for specific tasks. Task2Sim demonstrates significant improvements in model performance on a wide range of tasks without additional training for new tasks, showing potential to rival pre-training with real images from Imagenet."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article offers the first extensive review of vision transformer techniques specifically applied to human action recognition, a field gaining interest for its wide applications. Termed action transformers, these methods are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also delves into optimizing spatio-temporal attention in transformers for longer sequences and examines different learning strategies, including self-supervised and zero-shot learning, with their respective loss functions. Additionally, the survey assesses progress in benchmark evaluation scores and discusses the challenges and future directions in this research area."
    },
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "Recent advancements in image transformers, notably in models such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, have narrowed the performance gap with traditional CNNs. These models are typically trained on large datasets like ImageNet-21k and then finetuned on ImageNet-1k. However, assessments often overlook Tiny ImageNet when evaluating transfer learning performance. This paper updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. The code is available at: https://github.com/ehuynh1106/TinyImageNet-Transformers"
    },
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    },
    {
      "title": "Are Convolutional Neural Networks or Transformers more like human vision?",
      "url": "/research/2021/07/01/Are-Convolutional-Neural-Networks-or-Transformers-more-like-human-vision.html",
      "date": "July 1, 2021",
      "summary": "This study investigates the performance of Convolutional Neural Networks (CNNs) and attention-based Vision Transformers (ViTs) in computer vision tasks. While CNNs excel in accuracy, ViTs offer a different approach with weaker inductive biases. By analyzing error patterns, we find that ViTs exhibit consistency with human errors, suggesting potential for more human-like vision models and insights into human object recognition."
    }
  ],
  "object": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    },
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "Vision Transformers (ViT) excel in various vision tasks, like image classification, object detection, and semantic segmentation, but require more model regularization or data augmentation (AugReg) than convolutional neural networks, especially with smaller datasets. Through a comprehensive study, we explore how training data amount, AugReg, model size, and compute budget interact. Our findings indicate that using more compute and AugReg can achieve the same performance as training with significantly more data. Specifically, we demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "Recent studies have shown that Convolutional Neural Networks (CNNs), which were thought to recognize objects by learning complex shapes, are actually more biased towards recognizing textures. This finding, based on a series of tests comparing CNNs with human observers using images with conflicting texture and shape cues, contradicts the traditional understanding of how these networks process images. We discovered that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, when we trained a standard CNN architecture (ResNet-50) on a stylized version of ImageNet, designed to emphasize shape, the network's performance aligned more closely with human behavior. This training not only matched human performance in controlled experiments (involving 97 observers across 48,560 trials) but also enhanced object detection and robustness against image distortions, underscoring the benefits of shape-based representations in visual recognition systems."
    },
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "Image-caption pretraining, crucial for tasks like zero-shot image classification and object detection, faces challenges in aligning multiple concepts from captions to objects in images. We introduce a curriculum learning framework inspired by children's language learning from cognitive science, starting with simple image-caption pairs and gradually increasing complexity by adding more concepts. This method, which leverages knowledge from each phase for subsequent learning, outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios. The code is available at: https://github.com/hayyubi/cur_vl.git."
    },
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "6D object pose estimation is crucial for robotic bin-picking and essential for many industrial applications. Annotating custom datasets for each bin-picking scenario is challenging. We introduce an iterative self-training framework for sim-to-real 6D object pose estimation, enhancing cost-effective robotic grasping. We create a photo-realistic simulator for synthesizing virtual data to train an initial pose estimation network. This network, acting as a teacher model, predicts poses on unlabeled real data. We develop an adaptive selection scheme to filter reliable predictions for updating a student model with pseudo labels for real data pose estimation. By iteratively refining the teacher model with the trained student model, we improve the quality of pseudo labels. Our method, tested on a public benchmark and a newly-released dataset, shows an 11.49% and 22.62% ADD(-S) improvement, respectively, and a 19.54% increase in robotic bin-picking success, demonstrating the effectiveness of iterative sim-to-real approaches. Project homepage: www.cse.cuhk.edu.hk/~kaichen/sim2real_pose.html"
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "We introduce a system that trains deep neural networks for object detection using synthetic images, leveraging domain randomization to manage real-world data variability. This involves randomizing simulator parameters—like lighting and object textures—in non-realistic ways, helping the network to identify essential object features. Our findings reveal that networks can achieve impressive performance with just synthetic data and further improve with real data fine-tuning. This suggests the potential of using cost-effective synthetic data for training, circumventing the challenges of acquiring vast amounts of real-world data or creating detailed synthetic environments. We validate our method through car bounding box detection on the KITTI dataset."
    },
    {
      "title": "Are Convolutional Neural Networks or Transformers more like human vision?",
      "url": "/research/2021/07/01/Are-Convolutional-Neural-Networks-or-Transformers-more-like-human-vision.html",
      "date": "July 1, 2021",
      "summary": "This study investigates the performance of Convolutional Neural Networks (CNNs) and attention-based Vision Transformers (ViTs) in computer vision tasks. While CNNs excel in accuracy, ViTs offer a different approach with weaker inductive biases. By analyzing error patterns, we find that ViTs exhibit consistency with human errors, suggesting potential for more human-like vision models and insights into human object recognition."
    }
  ],
  "detection": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    },
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "Vision Transformers (ViT) excel in various vision tasks, like image classification, object detection, and semantic segmentation, but require more model regularization or data augmentation (AugReg) than convolutional neural networks, especially with smaller datasets. Through a comprehensive study, we explore how training data amount, AugReg, model size, and compute budget interact. Our findings indicate that using more compute and AugReg can achieve the same performance as training with significantly more data. Specifically, we demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "Recent studies have shown that Convolutional Neural Networks (CNNs), which were thought to recognize objects by learning complex shapes, are actually more biased towards recognizing textures. This finding, based on a series of tests comparing CNNs with human observers using images with conflicting texture and shape cues, contradicts the traditional understanding of how these networks process images. We discovered that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, when we trained a standard CNN architecture (ResNet-50) on a stylized version of ImageNet, designed to emphasize shape, the network's performance aligned more closely with human behavior. This training not only matched human performance in controlled experiments (involving 97 observers across 48,560 trials) but also enhanced object detection and robustness against image distortions, underscoring the benefits of shape-based representations in visual recognition systems."
    },
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "Image-caption pretraining, crucial for tasks like zero-shot image classification and object detection, faces challenges in aligning multiple concepts from captions to objects in images. We introduce a curriculum learning framework inspired by children's language learning from cognitive science, starting with simple image-caption pairs and gradually increasing complexity by adding more concepts. This method, which leverages knowledge from each phase for subsequent learning, outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios. The code is available at: https://github.com/hayyubi/cur_vl.git."
    },
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "Self-paced curriculum learning (SCL) has been effective in various fields, employing easy-to-hard sampling based on data difficulty. However, in medical image analysis tasks like universal lesion detection, it faces challenges due to inaccurate difficulty estimation and the under-utilization of hard samples. This paper introduces a novel approach, mixed-order self-paced curriculum learning (Mo-SCL), which combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches to address these issues. Through theoretical analysis and experiments on the DeepLesion dataset, Mo-SCL is shown to enhance lesion detection accuracy in state-of-the-art methods without additional network modifications."
    },
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "We introduce SlowFast networks for video recognition, featuring a Slow pathway for spatial semantics at low frame rates and a Fast pathway for motion details at high frame rates. The Fast pathway's efficiency is achieved through reduced channel capacity, yet it effectively captures temporal information. Our models significantly enhance action classification and detection, achieving top accuracy on benchmarks like Kinetics, Charades, and AVA. Code is available at: https://github.com/facebookresearch/SlowFast."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "We introduce a system that trains deep neural networks for object detection using synthetic images, leveraging domain randomization to manage real-world data variability. This involves randomizing simulator parameters—like lighting and object textures—in non-realistic ways, helping the network to identify essential object features. Our findings reveal that networks can achieve impressive performance with just synthetic data and further improve with real data fine-tuning. This suggests the potential of using cost-effective synthetic data for training, circumventing the challenges of acquiring vast amounts of real-world data or creating detailed synthetic environments. We validate our method through car bounding box detection on the KITTI dataset."
    },
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    }
  ],
  "segmentation": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    },
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "Vision Transformers (ViT) excel in various vision tasks, like image classification, object detection, and semantic segmentation, but require more model regularization or data augmentation (AugReg) than convolutional neural networks, especially with smaller datasets. Through a comprehensive study, we explore how training data amount, AugReg, model size, and compute budget interact. Our findings indicate that using more compute and AugReg can achieve the same performance as training with significantly more data. Specifically, we demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    },
    {
      "title": "DeiT III: Revenge of the ViT",
      "url": "/research/2023/01/18/DeiT-III-Revenge-of-the-ViT.html",
      "date": "January 18, 2023",
      "summary": "This paper explores the supervised training of Vision Transformers (ViTs), using a simplified training approach adapted from ResNet-50 that includes a novel data-augmentation method with just 3 augmentations. The study demonstrates that this method significantly improves ViTs' performance in image classification, transfer learning, and semantic segmentation over previous supervised training techniques. Moreover, it shows ViTs' performance can match newer architectures, providing a new benchmark for evaluating self-supervised methods on ViTs."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "Online Normalization is a novel method for normalizing neural network hidden activations, offering a batch-independent alternative with comparable accuracy to Batch Normalization. It overcomes Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. Applicable to recurrent, fully connected networks, and those with high activation memory requirements, it demonstrates effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    },
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This study investigates the use of high-quality, physically-based rendering and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle, enhancing image classification and semantic segmentation models. Synthetic images proved to be effective for augmenting limited real training data. However, models trained solely on synthetic images showed low accuracy on real validation images, which significantly improved when even small amounts of real data were included. Augmenting real data with synthetic images outperformed training on only real images. Moreover, pretraining models on synthetic data before transfer learning significantly reduced training costs, allowing most of the training to be completed upfront."
    }
  ],
  "httpsgithubcommsracverdeformableconvnets": [
    {
      "title": "Deformable Convolutional Networks",
      "url": "/research/2017/06/05/Deformable-Convolutional-Network.html",
      "date": "June 5, 2017",
      "summary": "This work introduces deformable convolution and deformable RoI pooling modules to improve the geometric transformation capability of CNNs by augmenting spatial sampling with learned offsets, without extra supervision. These modules can replace standard ones in CNNs for end-to-end training. The approach, validated by extensive experiments, effectively learns dense spatial transformations for complex vision tasks like object detection and semantic segmentation. The code is available at https://github.com/msracver/Deformable-ConvNets."
    }
  ],
  "transformer": [
    {
      "title": "Do Transformer Modifications Transfer Across Implementations and Applications?",
      "url": "/research/2021/09/10/Do_Transformer_Modifications_Transfer_Across_Implementations_and_Applications.html",
      "date": "September 10, 2021",
      "summary": "This paper evaluates numerous Transformer architecture modifications in a unified experimental framework, focusing on common natural language processing applications. Surprisingly, it finds that most modifications do not significantly enhance performance. The beneficial variants are mostly minor or developed in the same code base used for testing. The study suggests that performance gains may largely depend on implementation details and offers recommendations for improving the generalizability of experimental results."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/research/2022/08/05/Efficiently-Modeling-Long-Sequences-with-Structured-State-Spaces.html",
      "date": "August 5, 2022",
      "summary": "he quest for a universal sequence model capable of handling data across different tasks and modalities, especially long-range dependencies, faces challenges with conventional models like RNNs, CNNs, and Transformers, which struggle with very long sequences. A promising approach using state space models (SSM) showed potential but had high computational demands. We introduce the Structured State Space sequence model (S4), a more efficient parameterization of SSM, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    },
    {
      "title": "Fast Feedforward Networks",
      "url": "/research/2023/09/18/Fast-Feedforward-Networks.html",
      "date": "September 18, 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    },
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/research/2019/07/26/DropAttention-A-Regularization-Method-for-Fully-Connected-Self-Attention-Networks.html",
      "date": "July 26, 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. Unlike existing dropout techniques for other neural network layers, DropAttention specifically targets the unique challenges of self-attention mechanisms. Our experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    },
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/research/2021/08/16/An-Empirical-Study-of-Training-Self-Supervised-Vision-Transformers.html",
      "date": "August 16, 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    },
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/research/2022/10/10/Turbo-Training-with-Token-Dropout.html",
      "date": "October 10, 2022",
      "summary": "This paper introduces Turbo training, an efficient training method for video-related tasks using Transformers. Turbo training offers three major contributions: Firstly, it presents a simple yet versatile training paradigm applicable to multiple video tasks. Secondly, it demonstrates Turbo training's effectiveness across action classification, video-language representation learning, and long-video activity classification, achieving competitive performance with up to 4× speed-up and reduced memory usage. Thirdly, it enables long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous methods that were resource-intensive."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "Vision Transformers (ViT) excel in various vision tasks, like image classification, object detection, and semantic segmentation, but require more model regularization or data augmentation (AugReg) than convolutional neural networks, especially with smaller datasets. Through a comprehensive study, we explore how training data amount, AugReg, model size, and compute budget interact. Our findings indicate that using more compute and AugReg can achieve the same performance as training with significantly more data. Specifically, we demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "Self-supervised transformer training methods have shown excellent results in various fields. Unlike previous models like masked autoencoders (MAE) which use a single normalization layer for the class token [CLS] and other tokens, we introduce a novel normalization technique that normalizes the [CLS] token and normal tokens separately. This approach aims to better capture their unique characteristics, improving performance in downstream tasks. Our findings indicate that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    },
    {
      "title": "Training data-efficient image transformers & distillation through attention",
      "url": "/research/2021/01/15/Training-data-efficient-image-transformers-&-distillation-through-attention.html",
      "date": "January 15, 2021",
      "summary": "This study introduces DeiT, competitive vision transformers trained solely on ImageNet without convolutions, achieving up to 83.1% top-1 accuracy on ImageNet, using a single computer in under 3 days. Notably, it presents a unique teacher-student strategy for transformers, leveraging a distillation token for efficient learning from a convolutional network teacher. This method results in performance on par with convolutional networks, achieving up to 85.2% accuracy on ImageNet and demonstrating effective transferability to other tasks. The authors provide access to their code and models."
    },
    {
      "title": "Better plain ViT baselines for ImageNet-1k",
      "url": "/research/2022/05/02/Better-plain-ViT-baselines-for-ImageNet-1k.html",
      "date": "May 2, 2022",
      "summary": "It is commonly accepted that the Vision Transformer model requires sophisticated regularization techniques to excel at ImageNet-1k scale data. Surprisingly, we find this is not the case and standard data augmentation is sufficient. This note presents a few minor modifications to the original Vision Transformer (ViT) vanilla training setting that dramatically improve the performance of plain ViT models. Notably, 90 epochs of training surpass 76% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reach 80% in less than one day."
    },
    {
      "title": "DeiT III: Revenge of the ViT",
      "url": "/research/2023/01/18/DeiT-III-Revenge-of-the-ViT.html",
      "date": "January 18, 2023",
      "summary": "This paper explores the supervised training of Vision Transformers (ViTs), using a simplified training approach adapted from ResNet-50 that includes a novel data-augmentation method with just 3 augmentations. The study demonstrates that this method significantly improves ViTs' performance in image classification, transfer learning, and semantic segmentation over previous supervised training techniques. Moreover, it shows ViTs' performance can match newer architectures, providing a new benchmark for evaluating self-supervised methods on ViTs."
    },
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "Gated Linear Units (GLUs) involve the component-wise product of two linear projections, with one undergoing a sigmoid function. Exploring GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers, we find certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "The Vision Transformer (ViT) offers simplicity, robustness, and superior performance in vision tasks but struggles with small-scale datasets due to its lack of inherent inductive biases and dependence on large-scale pre-training datasets like ImageNet and JFT. This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of ViTs without large-scale pre-training or modifications to the architecture or loss functions. Experiments show improved performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, maintaining ViT's attention to relevant regions and robustness. The code and models are available online."
    },
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is crucial for natural language processing tasks, but its training requires a learning rate warm-up stage, essential for performance yet slowing optimization and increasing hyper-parameter tuning needs. This paper investigates the importance of the warm-up stage and the impact of layer normalization placement. It uses mean field theory to demonstrate that the original Post-LN Transformer's design, with layer normalization between residual blocks, results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    },
    {
      "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "Diffusion models, effective for generating high-dimensional perceptual data like images, lack established standard practices. This study enhances noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Our large-scale study demonstrates superior performance over established diffusion methods in high-resolution text-to-image synthesis. We introduce a transformer-based architecture for text-to-image generation, improving text comprehension, typography, and human preference ratings. Our findings show predictable scaling trends, with lower validation loss correlating with improved synthesis. The largest models outperform state-of-the-art approaches, and our data, code, and model weights will be publicly accessible."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article offers the first extensive review of vision transformer techniques specifically applied to human action recognition, a field gaining interest for its wide applications. Termed action transformers, these methods are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also delves into optimizing spatio-temporal attention in transformers for longer sequences and examines different learning strategies, including self-supervised and zero-shot learning, with their respective loss functions. Additionally, the survey assesses progress in benchmark evaluation scores and discusses the challenges and future directions in this research area."
    },
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "Recent advancements in image transformers, notably in models such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, have narrowed the performance gap with traditional CNNs. These models are typically trained on large datasets like ImageNet-21k and then finetuned on ImageNet-1k. However, assessments often overlook Tiny ImageNet when evaluating transfer learning performance. This paper updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. The code is available at: https://github.com/ehuynh1106/TinyImageNet-Transformers"
    },
    {
      "title": "Are Convolutional Neural Networks or Transformers more like human vision?",
      "url": "/research/2021/07/01/Are-Convolutional-Neural-Networks-or-Transformers-more-like-human-vision.html",
      "date": "July 1, 2021",
      "summary": "This study investigates the performance of Convolutional Neural Networks (CNNs) and attention-based Vision Transformers (ViTs) in computer vision tasks. While CNNs excel in accuracy, ViTs offer a different approach with weaker inductive biases. By analyzing error patterns, we find that ViTs exhibit consistency with human errors, suggesting potential for more human-like vision models and insights into human object recognition."
    },
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-5.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-6.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    }
  ],
  "transfer": [
    {
      "title": "Do Transformer Modifications Transfer Across Implementations and Applications?",
      "url": "/research/2021/09/10/Do_Transformer_Modifications_Transfer_Across_Implementations_and_Applications.html",
      "date": "September 10, 2021",
      "summary": "This paper evaluates numerous Transformer architecture modifications in a unified experimental framework, focusing on common natural language processing applications. Surprisingly, it finds that most modifications do not significantly enhance performance. The beneficial variants are mostly minor or developed in the same code base used for testing. The study suggests that performance gains may largely depend on implementation details and offers recommendations for improving the generalizability of experimental results."
    },
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    },
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/research/2020/09/10/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning.html",
      "date": "September 10, 2020",
      "summary": "We introduce BYOL, a novel self-supervised learning method for image representation without using negative pairs. Utilizing two networks, BYOL trains one to predict the representation of an image as seen by the other under a different view, updating the latter with the former's slow-moving average. Surpassing state-of-the-art methods, it achieves up to 79.6% accuracy on ImageNet with ResNet models, performing equally or better on transfer and semi-supervised tasks. Code and models are available on GitHub."
    },
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/research/2023/03/08/Extreme-Masking-for-Learning-Instance-and-Distributed-Visual-Representations.html",
      "date": "March 8, 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    },
    {
      "title": "Training data-efficient image transformers & distillation through attention",
      "url": "/research/2021/01/15/Training-data-efficient-image-transformers-&-distillation-through-attention.html",
      "date": "January 15, 2021",
      "summary": "This study introduces DeiT, competitive vision transformers trained solely on ImageNet without convolutions, achieving up to 83.1% top-1 accuracy on ImageNet, using a single computer in under 3 days. Notably, it presents a unique teacher-student strategy for transformers, leveraging a distillation token for efficient learning from a convolutional network teacher. This method results in performance on par with convolutional networks, achieving up to 85.2% accuracy on ImageNet and demonstrating effective transferability to other tasks. The authors provide access to their code and models."
    },
    {
      "title": "DeiT III: Revenge of the ViT",
      "url": "/research/2023/01/18/DeiT-III-Revenge-of-the-ViT.html",
      "date": "January 18, 2023",
      "summary": "This paper explores the supervised training of Vision Transformers (ViTs), using a simplified training approach adapted from ResNet-50 that includes a novel data-augmentation method with just 3 augmentations. The study demonstrates that this method significantly improves ViTs' performance in image classification, transfer learning, and semantic segmentation over previous supervised training techniques. Moreover, it shows ViTs' performance can match newer architectures, providing a new benchmark for evaluating self-supervised methods on ViTs."
    },
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "This paper explores the effectiveness of using synthetic data, generated by graphics simulators, for pre-training models in computer vision, addressing the challenges posed by using real-image datasets like high costs and ethical concerns. It highlights that the performance of models on downstream tasks varies with different simulation parameters, suggesting a tailored approach to synthetic data generation for optimal results. The introduction of Task2Sim, a model that maps downstream task requirements to optimal simulation parameters, allows for the generation of synthetic pre-training data customized for specific tasks. Task2Sim demonstrates significant improvements in model performance on a wide range of tasks without additional training for new tasks, showing potential to rival pre-training with real images from Imagenet."
    },
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "Monocular depth estimation's success hinges on large, diverse training datasets. Acquiring dense ground-truth depth is challenging, leading to diverse datasets with unique biases. We introduce tools for training with multiple datasets, despite incompatible annotations. Our approach includes a robust training objective unaffected by depth range and scale variations, employs principled multi-objective learning for data integration from various sources, and emphasizes the importance of pretraining encoders on auxiliary tasks. We tested our methods using five datasets, including 3D films, a novel, extensive data source. Through zero-shot cross-dataset transfer, we demonstrate our approach's superior generalization, significantly outperforming existing methods and establishing a new benchmark in monocular depth estimation."
    },
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "Recent advancements in image transformers, notably in models such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, have narrowed the performance gap with traditional CNNs. These models are typically trained on large datasets like ImageNet-21k and then finetuned on ImageNet-1k. However, assessments often overlook Tiny ImageNet when evaluating transfer learning performance. This paper updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. The code is available at: https://github.com/ehuynh1106/TinyImageNet-Transformers"
    }
  ],
  "language": [
    {
      "title": "Do Transformer Modifications Transfer Across Implementations and Applications?",
      "url": "/research/2021/09/10/Do_Transformer_Modifications_Transfer_Across_Implementations_and_Applications.html",
      "date": "September 10, 2021",
      "summary": "This paper evaluates numerous Transformer architecture modifications in a unified experimental framework, focusing on common natural language processing applications. Surprisingly, it finds that most modifications do not significantly enhance performance. The beneficial variants are mostly minor or developed in the same code base used for testing. The study suggests that performance gains may largely depend on implementation details and offers recommendations for improving the generalizability of experimental results."
    },
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    },
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/research/2024/01/24/MambaByte-Token-free-Selective-State-Space-Model.html",
      "date": "January 24, 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    },
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    },
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/research/2022/10/10/Turbo-Training-with-Token-Dropout.html",
      "date": "October 10, 2022",
      "summary": "This paper introduces Turbo training, an efficient training method for video-related tasks using Transformers. Turbo training offers three major contributions: Firstly, it presents a simple yet versatile training paradigm applicable to multiple video tasks. Secondly, it demonstrates Turbo training's effectiveness across action classification, video-language representation learning, and long-video activity classification, achieving competitive performance with up to 4× speed-up and reduced memory usage. Thirdly, it enables long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous methods that were resource-intensive."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "Self-supervised transformer training methods have shown excellent results in various fields. Unlike previous models like masked autoencoders (MAE) which use a single normalization layer for the class token [CLS] and other tokens, we introduce a novel normalization technique that normalizes the [CLS] token and normal tokens separately. This approach aims to better capture their unique characteristics, improving performance in downstream tasks. Our findings indicate that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks."
    },
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "We introduce the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. Our empirical evaluation across computer vision, natural language processing, and speech tasks shows that GELU offers performance improvements over ReLU and ELU activations."
    },
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "Image-caption pretraining, crucial for tasks like zero-shot image classification and object detection, faces challenges in aligning multiple concepts from captions to objects in images. We introduce a curriculum learning framework inspired by children's language learning from cognitive science, starting with simple image-caption pairs and gradually increasing complexity by adding more concepts. This method, which leverages knowledge from each phase for subsequent learning, outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios. The code is available at: https://github.com/hayyubi/cur_vl.git."
    },
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is crucial for natural language processing tasks, but its training requires a learning rate warm-up stage, essential for performance yet slowing optimization and increasing hyper-parameter tuning needs. This paper investigates the importance of the warm-up stage and the impact of layer normalization placement. It uses mean field theory to demonstrate that the original Post-LN Transformer's design, with layer normalization between residual blocks, results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "Online Normalization is a novel method for normalizing neural network hidden activations, offering a batch-independent alternative with comparable accuracy to Batch Normalization. It overcomes Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. Applicable to recurrent, fully connected networks, and those with high activation memory requirements, it demonstrates effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    },
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    },
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    }
  ],
  "depend": [
    {
      "title": "Do Transformer Modifications Transfer Across Implementations and Applications?",
      "url": "/research/2021/09/10/Do_Transformer_Modifications_Transfer_Across_Implementations_and_Applications.html",
      "date": "September 10, 2021",
      "summary": "This paper evaluates numerous Transformer architecture modifications in a unified experimental framework, focusing on common natural language processing applications. Surprisingly, it finds that most modifications do not significantly enhance performance. The beneficial variants are mostly minor or developed in the same code base used for testing. The study suggests that performance gains may largely depend on implementation details and offers recommendations for improving the generalizability of experimental results."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "generalizability": [
    {
      "title": "Do Transformer Modifications Transfer Across Implementations and Applications?",
      "url": "/research/2021/09/10/Do_Transformer_Modifications_Transfer_Across_Implementations_and_Applications.html",
      "date": "September 10, 2021",
      "summary": "This paper evaluates numerous Transformer architecture modifications in a unified experimental framework, focusing on common natural language processing applications. Surprisingly, it finds that most modifications do not significantly enhance performance. The beneficial variants are mostly minor or developed in the same code base used for testing. The study suggests that performance gains may largely depend on implementation details and offers recommendations for improving the generalizability of experimental results."
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    }
  ],
  "simclr": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    }
  ],
  "specialize": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    }
  ],
  "memory": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/research/2022/10/10/Turbo-Training-with-Token-Dropout.html",
      "date": "October 10, 2022",
      "summary": "This paper introduces Turbo training, an efficient training method for video-related tasks using Transformers. Turbo training offers three major contributions: Firstly, it presents a simple yet versatile training paradigm applicable to multiple video tasks. Secondly, it demonstrates Turbo training's effectiveness across action classification, video-language representation learning, and long-video activity classification, achieving competitive performance with up to 4× speed-up and reduced memory usage. Thirdly, it enables long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous methods that were resource-intensive."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "Online Normalization is a novel method for normalizing neural network hidden activations, offering a batch-independent alternative with comparable accuracy to Batch Normalization. It overcomes Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. Applicable to recurrent, fully connected networks, and those with high activation memory requirements, it demonstrates effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    },
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "bank": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    }
  ],
  "composition": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    }
  ],
  "nonlinear": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "Deep neural networks, while effective in complex vision tasks, suffer from overfitting. Current image augmentation techniques, largely linear and hand-crafted like scale, flip, and color jitter, fail to produce sufficiently challenging examples. We introduce a new augmentation approach, Mask-Reconstruct Augmentation (MRA), leveraging self-supervised masked autoencoders to generate distorted inputs. This method, inspired by masked image modeling's success in self-supervised learning, uses nonlinear transformations for regularization. Our extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks. The code is available at https://github.com/haohang96/MRA."
    },
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "Gated Linear Units (GLUs) involve the component-wise product of two linear projections, with one undergoing a sigmoid function. Exploring GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers, we find certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    }
  ],
  "loss": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION",
      "url": "/research/2020/10/23/AN-INVESTIGATION-OF-HOW-LABEL-SMOOTHING-AFFECTS-GENERALIZATION.html",
      "date": "October 23, 2020",
      "summary": "Label smoothing is hypothesized to mitigate overfitting and enhance generalization, with supporting empirical evidence. However, its mathematical underpinnings remain unclear. This paper introduces a theoretical framework explaining label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. We identify an optimal label smoothing value that minimizes generalization loss and validate our theory with comprehensive experiments. Our results aim to clarify label smoothing's utility for both theoreticians and practitioners in real-world applications"
    },
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    },
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    },
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "We introduce a Fourier neural network (FNN) aligned with Fourier decomposition, utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design ensures easy integration with more complex networks for data processing tasks. The FNN's efficacy is demonstrated on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits include solution validity beyond the training scope, model interpretability, and ease of use."
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "The Vision Transformer (ViT) offers simplicity, robustness, and superior performance in vision tasks but struggles with small-scale datasets due to its lack of inherent inductive biases and dependence on large-scale pre-training datasets like ImageNet and JFT. This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of ViTs without large-scale pre-training or modifications to the architecture or loss functions. Experiments show improved performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, maintaining ViT's attention to relevant regions and robustness. The code and models are available online."
    },
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "Self-paced curriculum learning (SCL) has been effective in various fields, employing easy-to-hard sampling based on data difficulty. However, in medical image analysis tasks like universal lesion detection, it faces challenges due to inaccurate difficulty estimation and the under-utilization of hard samples. This paper introduces a novel approach, mixed-order self-paced curriculum learning (Mo-SCL), which combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches to address these issues. Through theoretical analysis and experiments on the DeepLesion dataset, Mo-SCL is shown to enhance lesion detection accuracy in state-of-the-art methods without additional network modifications."
    },
    {
      "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "Diffusion models, effective for generating high-dimensional perceptual data like images, lack established standard practices. This study enhances noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Our large-scale study demonstrates superior performance over established diffusion methods in high-resolution text-to-image synthesis. We introduce a transformer-based architecture for text-to-image generation, improving text comprehension, typography, and human preference ratings. Our findings show predictable scaling trends, with lower validation loss correlating with improved synthesis. The largest models outperform state-of-the-art approaches, and our data, code, and model weights will be publicly accessible."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article offers the first extensive review of vision transformer techniques specifically applied to human action recognition, a field gaining interest for its wide applications. Termed action transformers, these methods are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also delves into optimizing spatio-temporal attention in transformers for longer sequences and examines different learning strategies, including self-supervised and zero-shot learning, with their respective loss functions. Additionally, the survey assesses progress in benchmark evaluation scores and discusses the challenges and future directions in this research area."
    },
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    },
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    }
  ],
  "batch": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    },
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/research/2017/11/06/Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization.html",
      "date": "November 6, 2017",
      "summary": "In this study, we improve upon the fast stylization method by Ulyanov et al. (2016) through a minor yet impactful modification: substituting batch normalization with instance normalization during both training and testing. This adjustment significantly enhances the quality of generated images and enables the training of efficient real-time image generation architectures. The code and full paper are available online."
    },
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    },
    {
      "title": "Rethinking Batch in BatchNorm",
      "url": "/research/2021/05/17/Rethinking-Batch-in-BatchNorm.html",
      "date": "May 17, 2021",
      "summary": "BatchNorm, essential in convolutional neural networks, behaves uniquely due to its batch-based operation, leading to performance issues in visual recognition tasks. This paper identifies these issues and suggests reevaluating the concept of batch in BatchNorm for better performance, aiming to guide researchers in its effective use."
    },
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    },
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "Self-paced curriculum learning (SCL) has been effective in various fields, employing easy-to-hard sampling based on data difficulty. However, in medical image analysis tasks like universal lesion detection, it faces challenges due to inaccurate difficulty estimation and the under-utilization of hard samples. This paper introduces a novel approach, mixed-order self-paced curriculum learning (Mo-SCL), which combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches to address these issues. Through theoretical analysis and experiments on the DeepLesion dataset, Mo-SCL is shown to enhance lesion detection accuracy in state-of-the-art methods without additional network modifications."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "Online Normalization is a novel method for normalizing neural network hidden activations, offering a batch-independent alternative with comparable accuracy to Batch Normalization. It overcomes Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. Applicable to recurrent, fully connected networks, and those with high activation memory requirements, it demonstrates effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    }
  ],
  "imagenet": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/research/2020/09/10/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning.html",
      "date": "September 10, 2020",
      "summary": "We introduce BYOL, a novel self-supervised learning method for image representation without using negative pairs. Utilizing two networks, BYOL trains one to predict the representation of an image as seen by the other under a different view, updating the latter with the former's slow-moving average. Surpassing state-of-the-art methods, it achieves up to 79.6% accuracy on ImageNet with ResNet models, performing equally or better on transfer and semi-supervised tasks. Code and models are available on GitHub."
    },
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/research/2017/08/23/A-downsampled-variant-of-imagenet-as-an-alternative-to-the-cifar-datasets.html",
      "date": "August 23, 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/research/2019/04/27/Benchmarking-Neural-Network-Robustness-to-Common-Corruptions-and-Surface-Variations.html",
      "date": "April 27, 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    },
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/On-Interaction-Between-Augmentations-and-Corruptions-in-Natural-Corruption-Robustness.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    },
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "Vision Transformers (ViT) excel in various vision tasks, like image classification, object detection, and semantic segmentation, but require more model regularization or data augmentation (AugReg) than convolutional neural networks, especially with smaller datasets. Through a comprehensive study, we explore how training data amount, AugReg, model size, and compute budget interact. Our findings indicate that using more compute and AugReg can achieve the same performance as training with significantly more data. Specifically, we demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    },
    {
      "title": "Training data-efficient image transformers & distillation through attention",
      "url": "/research/2021/01/15/Training-data-efficient-image-transformers-&-distillation-through-attention.html",
      "date": "January 15, 2021",
      "summary": "This study introduces DeiT, competitive vision transformers trained solely on ImageNet without convolutions, achieving up to 83.1% top-1 accuracy on ImageNet, using a single computer in under 3 days. Notably, it presents a unique teacher-student strategy for transformers, leveraging a distillation token for efficient learning from a convolutional network teacher. This method results in performance on par with convolutional networks, achieving up to 85.2% accuracy on ImageNet and demonstrating effective transferability to other tasks. The authors provide access to their code and models."
    },
    {
      "title": "Better plain ViT baselines for ImageNet-1k",
      "url": "/research/2022/05/02/Better-plain-ViT-baselines-for-ImageNet-1k.html",
      "date": "May 2, 2022",
      "summary": "It is commonly accepted that the Vision Transformer model requires sophisticated regularization techniques to excel at ImageNet-1k scale data. Surprisingly, we find this is not the case and standard data augmentation is sufficient. This note presents a few minor modifications to the original Vision Transformer (ViT) vanilla training setting that dramatically improve the performance of plain ViT models. Notably, 90 epochs of training surpass 76% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reach 80% in less than one day."
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "The Vision Transformer (ViT) offers simplicity, robustness, and superior performance in vision tasks but struggles with small-scale datasets due to its lack of inherent inductive biases and dependence on large-scale pre-training datasets like ImageNet and JFT. This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of ViTs without large-scale pre-training or modifications to the architecture or loss functions. Experiments show improved performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, maintaining ViT's attention to relevant regions and robustness. The code and models are available online."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "Recent studies have shown that Convolutional Neural Networks (CNNs), which were thought to recognize objects by learning complex shapes, are actually more biased towards recognizing textures. This finding, based on a series of tests comparing CNNs with human observers using images with conflicting texture and shape cues, contradicts the traditional understanding of how these networks process images. We discovered that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, when we trained a standard CNN architecture (ResNet-50) on a stylized version of ImageNet, designed to emphasize shape, the network's performance aligned more closely with human behavior. This training not only matched human performance in controlled experiments (involving 97 observers across 48,560 trials) but also enhanced object detection and robustness against image distortions, underscoring the benefits of shape-based representations in visual recognition systems."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "Online Normalization is a novel method for normalizing neural network hidden activations, offering a batch-independent alternative with comparable accuracy to Batch Normalization. It overcomes Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. Applicable to recurrent, fully connected networks, and those with high activation memory requirements, it demonstrates effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    },
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "This paper explores the effectiveness of using synthetic data, generated by graphics simulators, for pre-training models in computer vision, addressing the challenges posed by using real-image datasets like high costs and ethical concerns. It highlights that the performance of models on downstream tasks varies with different simulation parameters, suggesting a tailored approach to synthetic data generation for optimal results. The introduction of Task2Sim, a model that maps downstream task requirements to optimal simulation parameters, allows for the generation of synthetic pre-training data customized for specific tasks. Task2Sim demonstrates significant improvements in model performance on a wide range of tasks without additional training for new tasks, showing potential to rival pre-training with real images from Imagenet."
    },
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "Recent advancements in image transformers, notably in models such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, have narrowed the performance gap with traditional CNNs. These models are typically trained on large datasets like ImageNet-21k and then finetuned on ImageNet-1k. However, assessments often overlook Tiny ImageNet when evaluating transfer learning performance. This paper updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. The code is available at: https://github.com/ehuynh1106/TinyImageNet-Transformers"
    }
  ],
  "linear": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/research/2023/03/08/Extreme-Masking-for-Learning-Instance-and-Distributed-Visual-Representations.html",
      "date": "March 8, 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    },
    {
      "title": "Fast Feedforward Networks",
      "url": "/research/2023/09/18/Fast-Feedforward-Networks.html",
      "date": "September 18, 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    },
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    },
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/research/2024/01/24/MambaByte-Token-free-Selective-State-Space-Model.html",
      "date": "January 24, 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "Deep neural networks, while effective in complex vision tasks, suffer from overfitting. Current image augmentation techniques, largely linear and hand-crafted like scale, flip, and color jitter, fail to produce sufficiently challenging examples. We introduce a new augmentation approach, Mask-Reconstruct Augmentation (MRA), leveraging self-supervised masked autoencoders to generate distorted inputs. This method, inspired by masked image modeling's success in self-supervised learning, uses nonlinear transformations for regularization. Our extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks. The code is available at https://github.com/haohang96/MRA."
    },
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "We introduce the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. Our empirical evaluation across computer vision, natural language processing, and speech tasks shows that GELU offers performance improvements over ReLU and ELU activations."
    },
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "Gated Linear Units (GLUs) involve the component-wise product of two linear projections, with one undergoing a sigmoid function. Exploring GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers, we find certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    },
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions for neural networks in reinforcement learning, the sigmoid-weighted linear unit (SiLU) and its derivative (dSiLU), and challenges the need for experience replay and separate target networks in deep reinforcement learning. By employing on-policy learning with eligibility traces and softmax action selection, the study achieves state-of-the-art results in stochastic SZ-Tetris and a small-board Tetris using TD(λ) learning and shallow dSiLU network agents. Furthermore, it outperforms the DQN algorithm in the Atari 2600 domain with a deep Sarsa(λ) agent utilizing SiLU and dSiLU hidden units, suggesting a competitive alternative to traditional DQN approaches."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    }
  ],
  "classification": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/research/2019/04/27/Benchmarking-Neural-Network-Robustness-to-Common-Corruptions-and-Surface-Variations.html",
      "date": "April 27, 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    },
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/research/2014/01/01/Dropout-A-Simple-Way-to-Prevent-Neural-Networks-from-Overfitting.html",
      "date": "January 1, 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    },
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/research/2022/10/10/Turbo-Training-with-Token-Dropout.html",
      "date": "October 10, 2022",
      "summary": "This paper introduces Turbo training, an efficient training method for video-related tasks using Transformers. Turbo training offers three major contributions: Firstly, it presents a simple yet versatile training paradigm applicable to multiple video tasks. Secondly, it demonstrates Turbo training's effectiveness across action classification, video-language representation learning, and long-video activity classification, achieving competitive performance with up to 4× speed-up and reduced memory usage. Thirdly, it enables long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous methods that were resource-intensive."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "Vision Transformers (ViT) excel in various vision tasks, like image classification, object detection, and semantic segmentation, but require more model regularization or data augmentation (AugReg) than convolutional neural networks, especially with smaller datasets. Through a comprehensive study, we explore how training data amount, AugReg, model size, and compute budget interact. Our findings indicate that using more compute and AugReg can achieve the same performance as training with significantly more data. Specifically, we demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "Deep neural networks, while effective in complex vision tasks, suffer from overfitting. Current image augmentation techniques, largely linear and hand-crafted like scale, flip, and color jitter, fail to produce sufficiently challenging examples. We introduce a new augmentation approach, Mask-Reconstruct Augmentation (MRA), leveraging self-supervised masked autoencoders to generate distorted inputs. This method, inspired by masked image modeling's success in self-supervised learning, uses nonlinear transformations for regularization. Our extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks. The code is available at https://github.com/haohang96/MRA."
    },
    {
      "title": "DeiT III: Revenge of the ViT",
      "url": "/research/2023/01/18/DeiT-III-Revenge-of-the-ViT.html",
      "date": "January 18, 2023",
      "summary": "This paper explores the supervised training of Vision Transformers (ViTs), using a simplified training approach adapted from ResNet-50 that includes a novel data-augmentation method with just 3 augmentations. The study demonstrates that this method significantly improves ViTs' performance in image classification, transfer learning, and semantic segmentation over previous supervised training techniques. Moreover, it shows ViTs' performance can match newer architectures, providing a new benchmark for evaluating self-supervised methods on ViTs."
    },
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "Image-caption pretraining, crucial for tasks like zero-shot image classification and object detection, faces challenges in aligning multiple concepts from captions to objects in images. We introduce a curriculum learning framework inspired by children's language learning from cognitive science, starting with simple image-caption pairs and gradually increasing complexity by adding more concepts. This method, which leverages knowledge from each phase for subsequent learning, outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios. The code is available at: https://github.com/hayyubi/cur_vl.git."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "Online Normalization is a novel method for normalizing neural network hidden activations, offering a batch-independent alternative with comparable accuracy to Batch Normalization. It overcomes Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. Applicable to recurrent, fully connected networks, and those with high activation memory requirements, it demonstrates effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    },
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "The limited number of videos in current action classification datasets like UCF-101 and HMDB-51 hinders the identification of effective video architectures due to similar performance across small-scale benchmarks. This study re-examines top architectures using the new, significantly larger Kinetics Human Action Video dataset, featuring 400 classes and over 400 clips per class from challenging YouTube videos. It analyzes the impact of this dataset on the performance of existing architectures and the benefits of pre-training on Kinetics. Additionally, the paper introduces the Two-Stream Inflated 3D ConvNet (I3D), an advancement that expands 2D ConvNet designs into 3D for enhanced video feature extraction. This model, pre-trained on Kinetics, sets new benchmarks in action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    },
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "We introduce SlowFast networks for video recognition, featuring a Slow pathway for spatial semantics at low frame rates and a Fast pathway for motion details at high frame rates. The Fast pathway's efficiency is achieved through reduced channel capacity, yet it effectively captures temporal information. Our models significantly enhance action classification and detection, achieving top accuracy on benchmarks like Kinetics, Charades, and AVA. Code is available at: https://github.com/facebookresearch/SlowFast."
    },
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This study investigates the use of high-quality, physically-based rendering and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle, enhancing image classification and semantic segmentation models. Synthetic images proved to be effective for augmenting limited real training data. However, models trained solely on synthetic images showed low accuracy on real validation images, which significantly improved when even small amounts of real data were included. Augmenting real data with synthetic images outperformed training on only real images. Moreover, pretraining models on synthetic data before transfer learning significantly reduced training costs, allowing most of the training to be completed upfront."
    },
    {
      "title": "Taming the Waves: Sine as Activation Function in Deep Neural Networks",
      "url": "/research/2016/11/04/Taming-the-Waves-Sine-as-Activation-Function-in-Deep-Neural-Networks.html",
      "date": "November 4, 2016",
      "summary": "This paper examines the challenges of training deep neural networks with sinusoidal activation functions, despite their successful use in specific applications. It explains the difficulty in training due to the emergence of numerous shallow local minima from the architecture. The study also reveals that successful learning in typical classification tasks occurs when the network effectively disregards the periodic cycles. Furthermore, it demonstrates that for certain non-trivial tasks, networks with sinusoidal activations can outperform those using traditional monotonic functions."
    },
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method, applies a single augmentation to each image and outperforms existing methods with minimal complexity and cost. We conducted extensive experiments to validate its performance against state-of-the-art methods across various image classification scenarios and explored its effectiveness through multiple ablation studies involving different augmentation spaces and methods. Our work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Highlighting a stagnation in automatic augmentation research, we conclude with proposed best practices for future advancements in the field."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-5.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-6.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    }
  ],
  "best": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    }
  ],
  "resnet": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/research/2020/09/10/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning.html",
      "date": "September 10, 2020",
      "summary": "We introduce BYOL, a novel self-supervised learning method for image representation without using negative pairs. Utilizing two networks, BYOL trains one to predict the representation of an image as seen by the other under a different view, updating the latter with the former's slow-moving average. Surpassing state-of-the-art methods, it achieves up to 79.6% accuracy on ImageNet with ResNet models, performing equally or better on transfer and semi-supervised tasks. Code and models are available on GitHub."
    },
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/research/2019/04/27/Benchmarking-Neural-Network-Robustness-to-Common-Corruptions-and-Surface-Variations.html",
      "date": "April 27, 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "DeiT III: Revenge of the ViT",
      "url": "/research/2023/01/18/DeiT-III-Revenge-of-the-ViT.html",
      "date": "January 18, 2023",
      "summary": "This paper explores the supervised training of Vision Transformers (ViTs), using a simplified training approach adapted from ResNet-50 that includes a novel data-augmentation method with just 3 augmentations. The study demonstrates that this method significantly improves ViTs' performance in image classification, transfer learning, and semantic segmentation over previous supervised training techniques. Moreover, it shows ViTs' performance can match newer architectures, providing a new benchmark for evaluating self-supervised methods on ViTs."
    }
  ],
  "label": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION",
      "url": "/research/2020/10/23/AN-INVESTIGATION-OF-HOW-LABEL-SMOOTHING-AFFECTS-GENERALIZATION.html",
      "date": "October 23, 2020",
      "summary": "Label smoothing is hypothesized to mitigate overfitting and enhance generalization, with supporting empirical evidence. However, its mathematical underpinnings remain unclear. This paper introduces a theoretical framework explaining label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. We identify an optimal label smoothing value that minimizes generalization loss and validate our theory with comprehensive experiments. Our results aim to clarify label smoothing's utility for both theoreticians and practitioners in real-world applications"
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/research/2020/08/03/Are-Labels-Necessary-for-Neural-Architecture-Search.html",
      "date": "August 3, 2020",
      "summary": "This paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning if effective neural architectures can be discovered using only images without human-annotated labels. Through two experimental setups—sample-based and search-based—we investigate this possibility. In the sample-based approach, we evaluate 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. In the search-based experiments, we employ a recognized NAS algorithm, DARTS, with various unsupervised objectives, finding that architectures identified without labels perform competitively compared to those found with labels. These findings suggest that human-annotated labels may not be necessary for identifying efficient neural architectures, as image statistics alone could be sufficient."
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper presents the AVA dataset, featuring 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets focused on composite actions in shorter clips with sparse annotations, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. It highlights the challenges in action recognition, introduces a novel localization approach surpassing existing benchmarks but shows modest performance on AVA (15.6% mAP), pointing out the necessity for advanced video understanding methods."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "Semi-supervised learning (SSL) utilizes unlabeled data to boost model performance but often at the expense of increased complexity. This paper introduces FixMatch, a streamlined SSL algorithm that simplifies existing methods. FixMatch uses a model's predictions on weakly-augmented unlabeled images to generate pseudo-labels, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. Remarkably, FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness. The code is publicly available."
    },
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "6D object pose estimation is crucial for robotic bin-picking and essential for many industrial applications. Annotating custom datasets for each bin-picking scenario is challenging. We introduce an iterative self-training framework for sim-to-real 6D object pose estimation, enhancing cost-effective robotic grasping. We create a photo-realistic simulator for synthesizing virtual data to train an initial pose estimation network. This network, acting as a teacher model, predicts poses on unlabeled real data. We develop an adaptive selection scheme to filter reliable predictions for updating a student model with pseudo labels for real data pose estimation. By iteratively refining the teacher model with the trained student model, we improve the quality of pseudo labels. Our method, tested on a public benchmark and a newly-released dataset, shows an 11.49% and 22.62% ADD(-S) improvement, respectively, and a 19.54% increase in robotic bin-picking success, demonstrating the effectiveness of iterative sim-to-real approaches. Project homepage: www.cse.cuhk.edu.hk/~kaichen/sim2real_pose.html"
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    }
  ],
  "outdo": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    }
  ],
  "alexnet": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/research/2019/04/27/Benchmarking-Neural-Network-Robustness-to-Common-Corruptions-and-Surface-Variations.html",
      "date": "April 27, 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    }
  ],
  "time": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "url": "/research/2020/07/01/A-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations.html",
      "date": "July 1, 2020",
      "summary": "This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations, simplifying recent algorithms without needing specialized architectures or a memory bank. It explores key framework components, finding that data augmentation composition, a learnable nonlinear transformation to the contrastive loss, and larger batch sizes with more training steps crucially enhance representation quality. SimCLR significantly surpasses previous self-supervised and semi-supervised learning methods on ImageNet. A linear classifier using SimCLR's self-supervised representations reaches 76.5% top-1 accuracy, a 7% improvement over the prior best and equal to supervised ResNet-50. With only 1% of labels, it achieves 85.8% top-5 accuracy, outdoing AlexNet with 100 times fewer labels."
    },
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "Fast Feedforward Networks",
      "url": "/research/2023/09/18/Fast-Feedforward-Networks.html",
      "date": "September 18, 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    },
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    },
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/research/2014/01/01/Dropout-A-Simple-Way-to-Prevent-Neural-Networks-from-Overfitting.html",
      "date": "January 1, 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    },
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/research/2017/11/06/Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization.html",
      "date": "November 6, 2017",
      "summary": "In this study, we improve upon the fast stylization method by Ulyanov et al. (2016) through a minor yet impactful modification: substituting batch normalization with instance normalization during both training and testing. This adjustment significantly enhances the quality of generated images and enables the training of efficient real-time image generation architectures. The code and full paper are available online."
    },
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    },
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    },
    {
      "title": "Curriculum Learning by Dynamic Instance Hardness",
      "url": "/research/2020/12/06/Curriculum-Learning-by-Dynamic-Instance-Hardness.html",
      "date": "December 6, 2020",
      "summary": "This paper explores the adaptation of deep neural network (DNN) training using a method called dynamic instance hardness (DIH), which measures a sample's learning difficulty over time. By tracking the exponential moving average of a sample's hardness, DIH provides a stable indicator of learning progress. Early predictions of a sample's DIH allow for prioritizing more challenging samples and deprioritizing easier ones, leading to a DIH guided curriculum learning (DIHCL) approach. DIHCL enhances learning efficiency and model accuracy without extra computational costs, as it leverages data from the training process itself. Tested on 11 datasets, DIHCL surpasses traditional training methods and recent curriculum learning techniques in both efficiency and effectiveness. The implementation is publicly available."
    },
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is crucial for natural language processing tasks, but its training requires a learning rate warm-up stage, essential for performance yet slowing optimization and increasing hyper-parameter tuning needs. This paper investigates the importance of the warm-up stage and the impact of layer normalization placement. It uses mean field theory to demonstrate that the original Post-LN Transformer's design, with layer normalization between residual blocks, results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    },
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    }
  ],
  "mask": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    },
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/research/2023/03/08/Extreme-Masking-for-Learning-Instance-and-Distributed-Visual-Representations.html",
      "date": "March 8, 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    },
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "Self-supervised transformer training methods have shown excellent results in various fields. Unlike previous models like masked autoencoders (MAE) which use a single normalization layer for the class token [CLS] and other tokens, we introduce a novel normalization technique that normalizes the [CLS] token and normal tokens separately. This approach aims to better capture their unique characteristics, improving performance in downstream tasks. Our findings indicate that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "Deep neural networks, while effective in complex vision tasks, suffer from overfitting. Current image augmentation techniques, largely linear and hand-crafted like scale, flip, and color jitter, fail to produce sufficiently challenging examples. We introduce a new augmentation approach, Mask-Reconstruct Augmentation (MRA), leveraging self-supervised masked autoencoders to generate distorted inputs. This method, inspired by masked image modeling's success in self-supervised learning, uses nonlinear transformations for regularization. Our extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks. The code is available at https://github.com/haohang96/MRA."
    },
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "This paper introduces VideoMAE, a data-efficient approach for self-supervised video pre-training (SSVP) that utilizes video masked autoencoders with a novel, high-ratio video tube masking technique. This method, inspired by ImageMAE, challenges the model with video reconstruction tasks to improve video representation learning. Key findings include: (1) High masking ratios (90% to 95%) are effective due to video's temporal redundancy, (2) VideoMAE performs well on small datasets (~3k-4k videos) without extra data, highlighting the importance of high-level structure learning, and (3) Data quality is more crucial than quantity for SSVP, with domain shift between pre-training and target datasets being significant. Remarkably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone, without extra data. The code is available online."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    }
  ],
  "autoencoder": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    },
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "Self-supervised transformer training methods have shown excellent results in various fields. Unlike previous models like masked autoencoders (MAE) which use a single normalization layer for the class token [CLS] and other tokens, we introduce a novel normalization technique that normalizes the [CLS] token and normal tokens separately. This approach aims to better capture their unique characteristics, improving performance in downstream tasks. Our findings indicate that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "Deep neural networks, while effective in complex vision tasks, suffer from overfitting. Current image augmentation techniques, largely linear and hand-crafted like scale, flip, and color jitter, fail to produce sufficiently challenging examples. We introduce a new augmentation approach, Mask-Reconstruct Augmentation (MRA), leveraging self-supervised masked autoencoders to generate distorted inputs. This method, inspired by masked image modeling's success in self-supervised learning, uses nonlinear transformations for regularization. Our extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks. The code is available at https://github.com/haohang96/MRA."
    },
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "This paper introduces VideoMAE, a data-efficient approach for self-supervised video pre-training (SSVP) that utilizes video masked autoencoders with a novel, high-ratio video tube masking technique. This method, inspired by ImageMAE, challenges the model with video reconstruction tasks to improve video representation learning. Key findings include: (1) High masking ratios (90% to 95%) are effective due to video's temporal redundancy, (2) VideoMAE performs well on small datasets (~3k-4k videos) without extra data, highlighting the importance of high-level structure learning, and (3) Data quality is more crucial than quantity for SSVP, with domain shift between pre-training and target datasets being significant. Remarkably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone, without extra data. The code is available online."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    }
  ],
  "representations": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    }
  ],
  "introuce": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    }
  ],
  "can": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    }
  ],
  "combination": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    },
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/research/2021/05/03/Robust-and-Generalizable-Visual-Representation-Learning-via-Random-Convolutions.html",
      "date": "May 3, 2021",
      "summary": "This study demonstrates that deep neural networks' vulnerability to texture shifts and perturbations can be mitigated by using random convolutions for data augmentation. Random convolutions, which approximately preserve shape while distorting local textures, create an infinite variety of new domains. We tested using these convolutions as new images or in combination with original images during training. Our method significantly enhances performance on unseen domains and domain generalization benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods. Additionally, it provides a more robust pretrained visual representation for downstream tasks."
    },
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/research/2021/08/31/Universal-Adversarial-Robustness-of-Texture-and-Shape-biased-Models.html",
      "date": "August 31, 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    },
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "Self-paced curriculum learning (SCL) has been effective in various fields, employing easy-to-hard sampling based on data difficulty. However, in medical image analysis tasks like universal lesion detection, it faces challenges due to inaccurate difficulty estimation and the under-utilization of hard samples. This paper introduces a novel approach, mixed-order self-paced curriculum learning (Mo-SCL), which combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches to address these issues. Through theoretical analysis and experiments on the DeepLesion dataset, Mo-SCL is shown to enhance lesion detection accuracy in state-of-the-art methods without additional network modifications."
    }
  ],
  "noise": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    },
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    },
    {
      "title": "Dropout as Data Augmentation",
      "url": "/research/2016/01/08/Dropout-as-Data-Augmentation.html",
      "date": "January 8, 2016",
      "summary": "Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/research/2023/06/30/Impact-of-Noise-on-Calibration-and-Generalisation-of-Neural-Networks.html",
      "date": "June 30, 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. We examine different types of noise, such as activation and input augmentation noise, in both in-distribution and out-of-distribution scenarios. Activation noise significantly improves generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/On-Interaction-Between-Augmentations-and-Corruptions-in-Natural-Corruption-Robustness.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    },
    {
      "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "Diffusion models, effective for generating high-dimensional perceptual data like images, lack established standard practices. This study enhances noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Our large-scale study demonstrates superior performance over established diffusion methods in high-resolution text-to-image synthesis. We introduce a transformer-based architecture for text-to-image generation, improving text comprehension, typography, and human preference ratings. Our findings show predictable scaling trends, with lower validation loss correlating with improved synthesis. The largest models outperform state-of-the-art approaches, and our data, code, and model weights will be publicly accessible."
    }
  ],
  "prediction": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "Curriculum Learning by Dynamic Instance Hardness",
      "url": "/research/2020/12/06/Curriculum-Learning-by-Dynamic-Instance-Hardness.html",
      "date": "December 6, 2020",
      "summary": "This paper explores the adaptation of deep neural network (DNN) training using a method called dynamic instance hardness (DIH), which measures a sample's learning difficulty over time. By tracking the exponential moving average of a sample's hardness, DIH provides a stable indicator of learning progress. Early predictions of a sample's DIH allow for prioritizing more challenging samples and deprioritizing easier ones, leading to a DIH guided curriculum learning (DIHCL) approach. DIHCL enhances learning efficiency and model accuracy without extra computational costs, as it leverages data from the training process itself. Tested on 11 datasets, DIHCL surpasses traditional training methods and recent curriculum learning techniques in both efficiency and effectiveness. The implementation is publicly available."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "Semi-supervised learning (SSL) utilizes unlabeled data to boost model performance but often at the expense of increased complexity. This paper introduces FixMatch, a streamlined SSL algorithm that simplifies existing methods. FixMatch uses a model's predictions on weakly-augmented unlabeled images to generate pseudo-labels, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. Remarkably, FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness. The code is publicly available."
    },
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "6D object pose estimation is crucial for robotic bin-picking and essential for many industrial applications. Annotating custom datasets for each bin-picking scenario is challenging. We introduce an iterative self-training framework for sim-to-real 6D object pose estimation, enhancing cost-effective robotic grasping. We create a photo-realistic simulator for synthesizing virtual data to train an initial pose estimation network. This network, acting as a teacher model, predicts poses on unlabeled real data. We develop an adaptive selection scheme to filter reliable predictions for updating a student model with pseudo labels for real data pose estimation. By iteratively refining the teacher model with the trained student model, we improve the quality of pseudo labels. Our method, tested on a public benchmark and a newly-released dataset, shows an 11.49% and 22.62% ADD(-S) improvement, respectively, and a 19.54% increase in robotic bin-picking success, demonstrating the effectiveness of iterative sim-to-real approaches. Project homepage: www.cse.cuhk.edu.hk/~kaichen/sim2real_pose.html"
    },
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "We introduce an algorithm designed to identify networks that are both simple and highly generalizable by searching for extensive areas of flat minima in the error function, where the error rate is relatively stable. These flat minima are associated with lower risks of overfitting according to MDL principles. Despite needing to calculate second-order derivatives, the algorithm maintains a complexity level comparable to backpropagation. We tested it on both feedforward and recurrent networks, and in stock market prediction tasks, it performed better than traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    }
  ],
  "robustness": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    },
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/research/2019/04/27/Benchmarking-Neural-Network-Robustness-to-Common-Corruptions-and-Surface-Variations.html",
      "date": "April 27, 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/research/2023/06/30/Impact-of-Noise-on-Calibration-and-Generalisation-of-Neural-Networks.html",
      "date": "June 30, 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. We examine different types of noise, such as activation and input augmentation noise, in both in-distribution and out-of-distribution scenarios. Activation noise significantly improves generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/On-Interaction-Between-Augmentations-and-Corruptions-in-Natural-Corruption-Robustness.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/research/2021/08/31/Universal-Adversarial-Robustness-of-Texture-and-Shape-biased-Models.html",
      "date": "August 31, 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "The Vision Transformer (ViT) offers simplicity, robustness, and superior performance in vision tasks but struggles with small-scale datasets due to its lack of inherent inductive biases and dependence on large-scale pre-training datasets like ImageNet and JFT. This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of ViTs without large-scale pre-training or modifications to the architecture or loss functions. Experiments show improved performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, maintaining ViT's attention to relevant regions and robustness. The code and models are available online."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "Recent studies have shown that Convolutional Neural Networks (CNNs), which were thought to recognize objects by learning complex shapes, are actually more biased towards recognizing textures. This finding, based on a series of tests comparing CNNs with human observers using images with conflicting texture and shape cues, contradicts the traditional understanding of how these networks process images. We discovered that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, when we trained a standard CNN architecture (ResNet-50) on a stylized version of ImageNet, designed to emphasize shape, the network's performance aligned more closely with human behavior. This training not only matched human performance in controlled experiments (involving 97 observers across 48,560 trials) but also enhanced object detection and robustness against image distortions, underscoring the benefits of shape-based representations in visual recognition systems."
    }
  ],
  "pretraining": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    },
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "We introduce DiffBIR, a framework using pretrained text-to-image diffusion models for blind image restoration. It consists of a two-stage pipeline: initially, we pretrain a restoration module on various degradations to enhance real-world applicability. Then, we use latent diffusion models for realistic restoration, including a novel injective modulation sub-network, LAControlNet, for fine-tuning, and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, we provide a controllable module for users to adjust quality and fidelity during the denoising process. Our extensive tests show DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets. The code is available online."
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "The Vision Transformer (ViT) offers simplicity, robustness, and superior performance in vision tasks but struggles with small-scale datasets due to its lack of inherent inductive biases and dependence on large-scale pre-training datasets like ImageNet and JFT. This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of ViTs without large-scale pre-training or modifications to the architecture or loss functions. Experiments show improved performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, maintaining ViT's attention to relevant regions and robustness. The code and models are available online."
    },
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "Image-caption pretraining, crucial for tasks like zero-shot image classification and object detection, faces challenges in aligning multiple concepts from captions to objects in images. We introduce a curriculum learning framework inspired by children's language learning from cognitive science, starting with simple image-caption pairs and gradually increasing complexity by adding more concepts. This method, which leverages knowledge from each phase for subsequent learning, outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios. The code is available at: https://github.com/hayyubi/cur_vl.git."
    },
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "The limited number of videos in current action classification datasets like UCF-101 and HMDB-51 hinders the identification of effective video architectures due to similar performance across small-scale benchmarks. This study re-examines top architectures using the new, significantly larger Kinetics Human Action Video dataset, featuring 400 classes and over 400 clips per class from challenging YouTube videos. It analyzes the impact of this dataset on the performance of existing architectures and the benefits of pre-training on Kinetics. Additionally, the paper introduces the Two-Stream Inflated 3D ConvNet (I3D), an advancement that expands 2D ConvNet designs into 3D for enhanced video feature extraction. This model, pre-trained on Kinetics, sets new benchmarks in action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    },
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This study investigates the use of high-quality, physically-based rendering and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle, enhancing image classification and semantic segmentation models. Synthetic images proved to be effective for augmenting limited real training data. However, models trained solely on synthetic images showed low accuracy on real validation images, which significantly improved when even small amounts of real data were included. Augmenting real data with synthetic images outperformed training on only real images. Moreover, pretraining models on synthetic data before transfer learning significantly reduced training costs, allowing most of the training to be completed upfront."
    },
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "This paper explores the effectiveness of using synthetic data, generated by graphics simulators, for pre-training models in computer vision, addressing the challenges posed by using real-image datasets like high costs and ethical concerns. It highlights that the performance of models on downstream tasks varies with different simulation parameters, suggesting a tailored approach to synthetic data generation for optimal results. The introduction of Task2Sim, a model that maps downstream task requirements to optimal simulation parameters, allows for the generation of synthetic pre-training data customized for specific tasks. Task2Sim demonstrates significant improvements in model performance on a wide range of tasks without additional training for new tasks, showing potential to rival pre-training with real images from Imagenet."
    },
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "Monocular depth estimation's success hinges on large, diverse training datasets. Acquiring dense ground-truth depth is challenging, leading to diverse datasets with unique biases. We introduce tools for training with multiple datasets, despite incompatible annotations. Our approach includes a robust training objective unaffected by depth range and scale variations, employs principled multi-objective learning for data integration from various sources, and emphasizes the importance of pretraining encoders on auxiliary tasks. We tested our methods using five datasets, including 3D films, a novel, extensive data source. Through zero-shot cross-dataset transfer, we demonstrate our approach's superior generalization, significantly outperforming existing methods and establishing a new benchmark in monocular depth estimation."
    },
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "This paper introduces VideoMAE, a data-efficient approach for self-supervised video pre-training (SSVP) that utilizes video masked autoencoders with a novel, high-ratio video tube masking technique. This method, inspired by ImageMAE, challenges the model with video reconstruction tasks to improve video representation learning. Key findings include: (1) High masking ratios (90% to 95%) are effective due to video's temporal redundancy, (2) VideoMAE performs well on small datasets (~3k-4k videos) without extra data, highlighting the importance of high-level structure learning, and (3) Data quality is more crucial than quantity for SSVP, with domain shift between pre-training and target datasets being significant. Remarkably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone, without extra data. The code is available online."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    }
  ],
  "uncurate": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    }
  ],
  "dataset": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    },
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/research/2017/08/23/A-downsampled-variant-of-imagenet-as-an-alternative-to-the-cifar-datasets.html",
      "date": "August 23, 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    },
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/research/2019/04/27/Benchmarking-Neural-Network-Robustness-to-Common-Corruptions-and-Surface-Variations.html",
      "date": "April 27, 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    },
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    },
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "Vision Transformers (ViT) excel in various vision tasks, like image classification, object detection, and semantic segmentation, but require more model regularization or data augmentation (AugReg) than convolutional neural networks, especially with smaller datasets. Through a comprehensive study, we explore how training data amount, AugReg, model size, and compute budget interact. Our findings indicate that using more compute and AugReg can achieve the same performance as training with significantly more data. Specifically, we demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    },
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper presents the AVA dataset, featuring 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets focused on composite actions in shorter clips with sparse annotations, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. It highlights the challenges in action recognition, introduces a novel localization approach surpassing existing benchmarks but shows modest performance on AVA (15.6% mAP), pointing out the necessity for advanced video understanding methods."
    },
    {
      "title": "Curriculum Learning by Dynamic Instance Hardness",
      "url": "/research/2020/12/06/Curriculum-Learning-by-Dynamic-Instance-Hardness.html",
      "date": "December 6, 2020",
      "summary": "This paper explores the adaptation of deep neural network (DNN) training using a method called dynamic instance hardness (DIH), which measures a sample's learning difficulty over time. By tracking the exponential moving average of a sample's hardness, DIH provides a stable indicator of learning progress. Early predictions of a sample's DIH allow for prioritizing more challenging samples and deprioritizing easier ones, leading to a DIH guided curriculum learning (DIHCL) approach. DIHCL enhances learning efficiency and model accuracy without extra computational costs, as it leverages data from the training process itself. Tested on 11 datasets, DIHCL surpasses traditional training methods and recent curriculum learning techniques in both efficiency and effectiveness. The implementation is publicly available."
    },
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "We introduce DiffBIR, a framework using pretrained text-to-image diffusion models for blind image restoration. It consists of a two-stage pipeline: initially, we pretrain a restoration module on various degradations to enhance real-world applicability. Then, we use latent diffusion models for realistic restoration, including a novel injective modulation sub-network, LAControlNet, for fine-tuning, and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, we provide a controllable module for users to adjust quality and fidelity during the denoising process. Our extensive tests show DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets. The code is available online."
    },
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "Temporal action localization (TAL) is crucial for video understanding but is hindered by benchmarks that rely on coarse action classes, leading to model overfitting and ambiguous annotations. Addressing these issues, we introduce FineAction, a large-scale, fine-grained video dataset with 103K instances across 106 action categories in 17K videos. FineAction offers a diverse, densely annotated dataset with co-occurring actions, posing new challenges for TAL. We evaluated popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and proposed a baseline method achieving a 13.17% mAP. FineAction aims to propel TAL research forward and is accessible online."
    },
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper investigates how neural networks generalize on small, algorithmically generated datasets, focusing on data efficiency, memorization, generalization, and learning speed. It highlights instances where neural networks grok patterns, significantly improving their generalization ability from mere chance to perfect accuracy, even beyond the point of overfitting. Additionally, the study finds that generalization on smaller datasets demands more optimization. It posits that such datasets are ideal for exploring the enigmatic phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "The Vision Transformer (ViT) offers simplicity, robustness, and superior performance in vision tasks but struggles with small-scale datasets due to its lack of inherent inductive biases and dependence on large-scale pre-training datasets like ImageNet and JFT. This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of ViTs without large-scale pre-training or modifications to the architecture or loss functions. Experiments show improved performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, maintaining ViT's attention to relevant regions and robustness. The code and models are available online."
    },
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "Self-paced curriculum learning (SCL) has been effective in various fields, employing easy-to-hard sampling based on data difficulty. However, in medical image analysis tasks like universal lesion detection, it faces challenges due to inaccurate difficulty estimation and the under-utilization of hard samples. This paper introduces a novel approach, mixed-order self-paced curriculum learning (Mo-SCL), which combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches to address these issues. Through theoretical analysis and experiments on the DeepLesion dataset, Mo-SCL is shown to enhance lesion detection accuracy in state-of-the-art methods without additional network modifications."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "Online Normalization is a novel method for normalizing neural network hidden activations, offering a batch-independent alternative with comparable accuracy to Batch Normalization. It overcomes Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. Applicable to recurrent, fully connected networks, and those with high activation memory requirements, it demonstrates effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    },
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    },
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "The limited number of videos in current action classification datasets like UCF-101 and HMDB-51 hinders the identification of effective video architectures due to similar performance across small-scale benchmarks. This study re-examines top architectures using the new, significantly larger Kinetics Human Action Video dataset, featuring 400 classes and over 400 clips per class from challenging YouTube videos. It analyzes the impact of this dataset on the performance of existing architectures and the benefits of pre-training on Kinetics. Additionally, the paper introduces the Two-Stream Inflated 3D ConvNet (I3D), an advancement that expands 2D ConvNet designs into 3D for enhanced video feature extraction. This model, pre-trained on Kinetics, sets new benchmarks in action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    },
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "6D object pose estimation is crucial for robotic bin-picking and essential for many industrial applications. Annotating custom datasets for each bin-picking scenario is challenging. We introduce an iterative self-training framework for sim-to-real 6D object pose estimation, enhancing cost-effective robotic grasping. We create a photo-realistic simulator for synthesizing virtual data to train an initial pose estimation network. This network, acting as a teacher model, predicts poses on unlabeled real data. We develop an adaptive selection scheme to filter reliable predictions for updating a student model with pseudo labels for real data pose estimation. By iteratively refining the teacher model with the trained student model, we improve the quality of pseudo labels. Our method, tested on a public benchmark and a newly-released dataset, shows an 11.49% and 22.62% ADD(-S) improvement, respectively, and a 19.54% increase in robotic bin-picking success, demonstrating the effectiveness of iterative sim-to-real approaches. Project homepage: www.cse.cuhk.edu.hk/~kaichen/sim2real_pose.html"
    },
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This study investigates the use of high-quality, physically-based rendering and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle, enhancing image classification and semantic segmentation models. Synthetic images proved to be effective for augmenting limited real training data. However, models trained solely on synthetic images showed low accuracy on real validation images, which significantly improved when even small amounts of real data were included. Augmenting real data with synthetic images outperformed training on only real images. Moreover, pretraining models on synthetic data before transfer learning significantly reduced training costs, allowing most of the training to be completed upfront."
    },
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "This paper explores the effectiveness of using synthetic data, generated by graphics simulators, for pre-training models in computer vision, addressing the challenges posed by using real-image datasets like high costs and ethical concerns. It highlights that the performance of models on downstream tasks varies with different simulation parameters, suggesting a tailored approach to synthetic data generation for optimal results. The introduction of Task2Sim, a model that maps downstream task requirements to optimal simulation parameters, allows for the generation of synthetic pre-training data customized for specific tasks. Task2Sim demonstrates significant improvements in model performance on a wide range of tasks without additional training for new tasks, showing potential to rival pre-training with real images from Imagenet."
    },
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "Monocular depth estimation's success hinges on large, diverse training datasets. Acquiring dense ground-truth depth is challenging, leading to diverse datasets with unique biases. We introduce tools for training with multiple datasets, despite incompatible annotations. Our approach includes a robust training objective unaffected by depth range and scale variations, employs principled multi-objective learning for data integration from various sources, and emphasizes the importance of pretraining encoders on auxiliary tasks. We tested our methods using five datasets, including 3D films, a novel, extensive data source. Through zero-shot cross-dataset transfer, we demonstrate our approach's superior generalization, significantly outperforming existing methods and establishing a new benchmark in monocular depth estimation."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "We introduce a system that trains deep neural networks for object detection using synthetic images, leveraging domain randomization to manage real-world data variability. This involves randomizing simulator parameters—like lighting and object textures—in non-realistic ways, helping the network to identify essential object features. Our findings reveal that networks can achieve impressive performance with just synthetic data and further improve with real data fine-tuning. This suggests the potential of using cost-effective synthetic data for training, circumventing the challenges of acquiring vast amounts of real-world data or creating detailed synthetic environments. We validate our method through car bounding box detection on the KITTI dataset."
    },
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "This paper introduces VideoMAE, a data-efficient approach for self-supervised video pre-training (SSVP) that utilizes video masked autoencoders with a novel, high-ratio video tube masking technique. This method, inspired by ImageMAE, challenges the model with video reconstruction tasks to improve video representation learning. Key findings include: (1) High masking ratios (90% to 95%) are effective due to video's temporal redundancy, (2) VideoMAE performs well on small datasets (~3k-4k videos) without extra data, highlighting the importance of high-level structure learning, and (3) Data quality is more crucial than quantity for SSVP, with domain shift between pre-training and target datasets being significant. Remarkably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone, without extra data. The code is available online."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    },
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "Recent advancements in image transformers, notably in models such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, have narrowed the performance gap with traditional CNNs. These models are typically trained on large datasets like ImageNet-21k and then finetuned on ImageNet-1k. However, assessments often overlook Tiny ImageNet when evaluating transfer learning performance. This paper updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. The code is available at: https://github.com/ehuynh1106/TinyImageNet-Transformers"
    },
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-5.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-6.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    }
  ],
  "load": [
    {
      "title": "A SIMPLE, EFFICIENT AND SCALABLE CONTRASTIVE MASKED AUTOENCODER FOR LEARNING VISUAL REPRESENTATIONS",
      "url": "/research/2022/10/30/A-SIMPLE,-EFFICIENT-AND-SCALABLE-CONTRASTIVE-MASKED-AUTOENCODER-FOR-LEARNING-VISUAL-REP--RESENTATIONS.html",
      "date": "October 30, 2022",
      "summary": "Introuces CAN, a method that combines contrastive learning, masked autoencoders, and noise prediction for efficient and scalable self-supervised visual learning. Its a straightforward framework that enhances image representation learning by leveraging the strengths of each approach. CAN outperforms existing methods in transfer learning and robustness tasks, showing particularly strong performance when pre-training on large, uncurated datasets. It offers a significant efficiency improvement and reduces the computational load compared to previous models."
    }
  ],
  "adam": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "optimization": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    },
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    },
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper investigates how neural networks generalize on small, algorithmically generated datasets, focusing on data efficiency, memorization, generalization, and learning speed. It highlights instances where neural networks grok patterns, significantly improving their generalization ability from mere chance to perfect accuracy, even beyond the point of overfitting. Additionally, the study finds that generalization on smaller datasets demands more optimization. It posits that such datasets are ideal for exploring the enigmatic phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    },
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is crucial for natural language processing tasks, but its training requires a learning rate warm-up stage, essential for performance yet slowing optimization and increasing hyper-parameter tuning needs. This paper investigates the importance of the warm-up stage and the impact of layer normalization placement. It uses mean field theory to demonstrate that the original Post-LN Transformer's design, with layer normalization between residual blocks, results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    },
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    },
    {
      "title": "Self-Paced Curriculum Learning",
      "url": "/research/2015/01/01/Self-Paced-Curriculum-Learning.html",
      "date": "January 1, 2015",
      "summary": "Curriculum learning (CL) and self-paced learning (SPL) are methods inspired by the incremental complexity of human and animal learning, with CL using a fixed curriculum based on prior knowledge and SPL adjusting dynamically to the learner's pace but struggling with prior knowledge and overfitting. This paper introduces a unified framework called self-paced curriculum learning (SPCL), blending both methods' strengths by considering prior knowledge and ongoing learning progress through a concise optimization problem. SPCL mimics a collaborative instructor-student learning mode, showing empirical advantages in two tasks."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article offers the first extensive review of vision transformer techniques specifically applied to human action recognition, a field gaining interest for its wide applications. Termed action transformers, these methods are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also delves into optimizing spatio-temporal attention in transformers for longer sequences and examines different learning strategies, including self-supervised and zero-shot learning, with their respective loss functions. Additionally, the survey assesses progress in benchmark evaluation scores and discusses the challenges and future directions in this research area."
    },
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    }
  ],
  "datum": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "Recent advancements in image transformers, notably in models such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, have narrowed the performance gap with traditional CNNs. These models are typically trained on large datasets like ImageNet-21k and then finetuned on ImageNet-1k. However, assessments often overlook Tiny ImageNet when evaluating transfer learning performance. This paper updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. The code is available at: https://github.com/ehuynh1106/TinyImageNet-Transformers"
    }
  ],
  "parameter": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Dropout as Data Augmentation",
      "url": "/research/2016/01/08/Dropout-as-Data-Augmentation.html",
      "date": "January 8, 2016",
      "summary": "Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    },
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is crucial for natural language processing tasks, but its training requires a learning rate warm-up stage, essential for performance yet slowing optimization and increasing hyper-parameter tuning needs. This paper investigates the importance of the warm-up stage and the impact of layer normalization placement. It uses mean field theory to demonstrate that the original Post-LN Transformer's design, with layer normalization between residual blocks, results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    },
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    },
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "This paper explores the effectiveness of using synthetic data, generated by graphics simulators, for pre-training models in computer vision, addressing the challenges posed by using real-image datasets like high costs and ethical concerns. It highlights that the performance of models on downstream tasks varies with different simulation parameters, suggesting a tailored approach to synthetic data generation for optimal results. The introduction of Task2Sim, a model that maps downstream task requirements to optimal simulation parameters, allows for the generation of synthetic pre-training data customized for specific tasks. Task2Sim demonstrates significant improvements in model performance on a wide range of tasks without additional training for new tasks, showing potential to rival pre-training with real images from Imagenet."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    },
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    }
  ],
  "handle": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    }
  ],
  "moment": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    }
  ],
  "estimation": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "Self-paced curriculum learning (SCL) has been effective in various fields, employing easy-to-hard sampling based on data difficulty. However, in medical image analysis tasks like universal lesion detection, it faces challenges due to inaccurate difficulty estimation and the under-utilization of hard samples. This paper introduces a novel approach, mixed-order self-paced curriculum learning (Mo-SCL), which combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches to address these issues. Through theoretical analysis and experiments on the DeepLesion dataset, Mo-SCL is shown to enhance lesion detection accuracy in state-of-the-art methods without additional network modifications."
    },
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "6D object pose estimation is crucial for robotic bin-picking and essential for many industrial applications. Annotating custom datasets for each bin-picking scenario is challenging. We introduce an iterative self-training framework for sim-to-real 6D object pose estimation, enhancing cost-effective robotic grasping. We create a photo-realistic simulator for synthesizing virtual data to train an initial pose estimation network. This network, acting as a teacher model, predicts poses on unlabeled real data. We develop an adaptive selection scheme to filter reliable predictions for updating a student model with pseudo labels for real data pose estimation. By iteratively refining the teacher model with the trained student model, we improve the quality of pseudo labels. Our method, tested on a public benchmark and a newly-released dataset, shows an 11.49% and 22.62% ADD(-S) improvement, respectively, and a 19.54% increase in robotic bin-picking success, demonstrating the effectiveness of iterative sim-to-real approaches. Project homepage: www.cse.cuhk.edu.hk/~kaichen/sim2real_pose.html"
    },
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "Monocular depth estimation's success hinges on large, diverse training datasets. Acquiring dense ground-truth depth is challenging, leading to diverse datasets with unique biases. We introduce tools for training with multiple datasets, despite incompatible annotations. Our approach includes a robust training objective unaffected by depth range and scale variations, employs principled multi-objective learning for data integration from various sources, and emphasizes the importance of pretraining encoders on auxiliary tasks. We tested our methods using five datasets, including 3D films, a novel, extensive data source. Through zero-shot cross-dataset transfer, we demonstrate our approach's superior generalization, significantly outperforming existing methods and establishing a new benchmark in monocular depth estimation."
    }
  ],
  "adapt": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    },
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    },
    {
      "title": "Curriculum Learning by Dynamic Instance Hardness",
      "url": "/research/2020/12/06/Curriculum-Learning-by-Dynamic-Instance-Hardness.html",
      "date": "December 6, 2020",
      "summary": "This paper explores the adaptation of deep neural network (DNN) training using a method called dynamic instance hardness (DIH), which measures a sample's learning difficulty over time. By tracking the exponential moving average of a sample's hardness, DIH provides a stable indicator of learning progress. Early predictions of a sample's DIH allow for prioritizing more challenging samples and deprioritizing easier ones, leading to a DIH guided curriculum learning (DIHCL) approach. DIHCL enhances learning efficiency and model accuracy without extra computational costs, as it leverages data from the training process itself. Tested on 11 datasets, DIHCL surpasses traditional training methods and recent curriculum learning techniques in both efficiency and effectiveness. The implementation is publicly available."
    },
    {
      "title": "DeiT III: Revenge of the ViT",
      "url": "/research/2023/01/18/DeiT-III-Revenge-of-the-ViT.html",
      "date": "January 18, 2023",
      "summary": "This paper explores the supervised training of Vision Transformers (ViTs), using a simplified training approach adapted from ResNet-50 that includes a novel data-augmentation method with just 3 augmentations. The study demonstrates that this method significantly improves ViTs' performance in image classification, transfer learning, and semantic segmentation over previous supervised training techniques. Moreover, it shows ViTs' performance can match newer architectures, providing a new benchmark for evaluating self-supervised methods on ViTs."
    }
  ],
  "stationary": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    }
  ],
  "gradient": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    },
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is crucial for natural language processing tasks, but its training requires a learning rate warm-up stage, essential for performance yet slowing optimization and increasing hyper-parameter tuning needs. This paper investigates the importance of the warm-up stage and the impact of layer normalization placement. It uses mean field theory to demonstrate that the original Post-LN Transformer's design, with layer normalization between residual blocks, results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "Online Normalization is a novel method for normalizing neural network hidden activations, offering a batch-independent alternative with comparable accuracy to Batch Normalization. It overcomes Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. Applicable to recurrent, fully connected networks, and those with high activation memory requirements, it demonstrates effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    }
  ],
  "hyper": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is crucial for natural language processing tasks, but its training requires a learning rate warm-up stage, essential for performance yet slowing optimization and increasing hyper-parameter tuning needs. This paper investigates the importance of the warm-up stage and the impact of layer normalization placement. It uses mean field theory to demonstrate that the original Post-LN Transformer's design, with layer normalization between residual blocks, results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    }
  ],
  "connection": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    }
  ],
  "convergence": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "regret": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    }
  ],
  "bind": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "We introduce a system that trains deep neural networks for object detection using synthetic images, leveraging domain randomization to manage real-world data variability. This involves randomizing simulator parameters—like lighting and object textures—in non-realistic ways, helping the network to identify essential object features. Our findings reveal that networks can achieve impressive performance with just synthetic data and further improve with real data fine-tuning. This suggests the potential of using cost-effective synthetic data for training, circumventing the challenges of acquiring vast amounts of real-world data or creating detailed synthetic environments. We validate our method through car bounding box detection on the KITTI dataset."
    }
  ],
  "adamax": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    }
  ],
  "infinity": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    }
  ],
  "normalization": [
    {
      "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
      "url": "/research/2017/01/30/ADAM-A-METHOD-FOR-STOCHASTIC-OPTIMIZATION.html",
      "date": "January 30, 2017",
      "summary": "Adam is an optimization algorithm designed for large-scale data and parameter problems, efficiently handling stochastic objective functions with adaptive moment estimation. It's easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. The paper also explores Adam's connections to related algorithms, its theoretical convergence properties, including a competitive regret bound, and its practical performance, which favorably compares with other methods. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
    },
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    },
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/research/2017/11/06/Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization.html",
      "date": "November 6, 2017",
      "summary": "In this study, we improve upon the fast stylization method by Ulyanov et al. (2016) through a minor yet impactful modification: substituting batch normalization with instance normalization during both training and testing. This adjustment significantly enhances the quality of generated images and enables the training of efficient real-time image generation architectures. The code and full paper are available online."
    },
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    },
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "Self-supervised transformer training methods have shown excellent results in various fields. Unlike previous models like masked autoencoders (MAE) which use a single normalization layer for the class token [CLS] and other tokens, we introduce a novel normalization technique that normalizes the [CLS] token and normal tokens separately. This approach aims to better capture their unique characteristics, improving performance in downstream tasks. Our findings indicate that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks."
    },
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is crucial for natural language processing tasks, but its training requires a learning rate warm-up stage, essential for performance yet slowing optimization and increasing hyper-parameter tuning needs. This paper investigates the importance of the warm-up stage and the impact of layer normalization placement. It uses mean field theory to demonstrate that the original Post-LN Transformer's design, with layer normalization between residual blocks, results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "Online Normalization is a novel method for normalizing neural network hidden activations, offering a batch-independent alternative with comparable accuracy to Batch Normalization. It overcomes Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. Applicable to recurrent, fully connected networks, and those with high activation memory requirements, it demonstrates effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    },
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    }
  ],
  "smooth": [
    {
      "title": "AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION",
      "url": "/research/2020/10/23/AN-INVESTIGATION-OF-HOW-LABEL-SMOOTHING-AFFECTS-GENERALIZATION.html",
      "date": "October 23, 2020",
      "summary": "Label smoothing is hypothesized to mitigate overfitting and enhance generalization, with supporting empirical evidence. However, its mathematical underpinnings remain unclear. This paper introduces a theoretical framework explaining label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. We identify an optimal label smoothing value that minimizes generalization loss and validate our theory with comprehensive experiments. Our results aim to clarify label smoothing's utility for both theoreticians and practitioners in real-world applications"
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    }
  ],
  "generalization": [
    {
      "title": "AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION",
      "url": "/research/2020/10/23/AN-INVESTIGATION-OF-HOW-LABEL-SMOOTHING-AFFECTS-GENERALIZATION.html",
      "date": "October 23, 2020",
      "summary": "Label smoothing is hypothesized to mitigate overfitting and enhance generalization, with supporting empirical evidence. However, its mathematical underpinnings remain unclear. This paper introduces a theoretical framework explaining label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. We identify an optimal label smoothing value that minimizes generalization loss and validate our theory with comprehensive experiments. Our results aim to clarify label smoothing's utility for both theoreticians and practitioners in real-world applications"
    },
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/research/2021/05/03/Robust-and-Generalizable-Visual-Representation-Learning-via-Random-Convolutions.html",
      "date": "May 3, 2021",
      "summary": "This study demonstrates that deep neural networks' vulnerability to texture shifts and perturbations can be mitigated by using random convolutions for data augmentation. Random convolutions, which approximately preserve shape while distorting local textures, create an infinite variety of new domains. We tested using these convolutions as new images or in combination with original images during training. Our method significantly enhances performance on unseen domains and domain generalization benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods. Additionally, it provides a more robust pretrained visual representation for downstream tasks."
    },
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    },
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/research/2023/06/30/Impact-of-Noise-on-Calibration-and-Generalisation-of-Neural-Networks.html",
      "date": "June 30, 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. We examine different types of noise, such as activation and input augmentation noise, in both in-distribution and out-of-distribution scenarios. Activation noise significantly improves generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    },
    {
      "title": "Learning RelU Networks To High Uniform Accuracy Is Intractable",
      "url": "/research/2023/02/28/Learning-Relu-Networks-To-High-Uniform-Accuracy-Is-Intractable.html",
      "date": "February 28, 2023",
      "summary": "Statistical learning theory provides guidelines on the number of training samples needed for achieving desired accuracy in learning problems, especially emphasizing generalization error. However, this is not always adequate, particularly in security-sensitive areas or computational sciences, where uniform accuracy across all inputs is necessary. This paper quantifies the training samples required for uniform accuracy in learning problems involving ReLU neural networks, revealing that the number of samples needed exponentially increases with the network's depth and input dimension."
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/On-Interaction-Between-Augmentations-and-Corruptions-in-Natural-Corruption-Robustness.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper investigates how neural networks generalize on small, algorithmically generated datasets, focusing on data efficiency, memorization, generalization, and learning speed. It highlights instances where neural networks grok patterns, significantly improving their generalization ability from mere chance to perfect accuracy, even beyond the point of overfitting. Additionally, the study finds that generalization on smaller datasets demands more optimization. It posits that such datasets are ideal for exploring the enigmatic phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    },
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "Monocular depth estimation's success hinges on large, diverse training datasets. Acquiring dense ground-truth depth is challenging, leading to diverse datasets with unique biases. We introduce tools for training with multiple datasets, despite incompatible annotations. Our approach includes a robust training objective unaffected by depth range and scale variations, employs principled multi-objective learning for data integration from various sources, and emphasizes the importance of pretraining encoders on auxiliary tasks. We tested our methods using five datasets, including 3D films, a novel, extensive data source. Through zero-shot cross-dataset transfer, we demonstrate our approach's superior generalization, significantly outperforming existing methods and establishing a new benchmark in monocular depth estimation."
    },
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    },
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-5.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-6.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    }
  ],
  "smoothing": [
    {
      "title": "AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION",
      "url": "/research/2020/10/23/AN-INVESTIGATION-OF-HOW-LABEL-SMOOTHING-AFFECTS-GENERALIZATION.html",
      "date": "October 23, 2020",
      "summary": "Label smoothing is hypothesized to mitigate overfitting and enhance generalization, with supporting empirical evidence. However, its mathematical underpinnings remain unclear. This paper introduces a theoretical framework explaining label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. We identify an optimal label smoothing value that minimizes generalization loss and validate our theory with comprehensive experiments. Our results aim to clarify label smoothing's utility for both theoreticians and practitioners in real-world applications"
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    }
  ],
  "underpinning": [
    {
      "title": "AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION",
      "url": "/research/2020/10/23/AN-INVESTIGATION-OF-HOW-LABEL-SMOOTHING-AFFECTS-GENERALIZATION.html",
      "date": "October 23, 2020",
      "summary": "Label smoothing is hypothesized to mitigate overfitting and enhance generalization, with supporting empirical evidence. However, its mathematical underpinnings remain unclear. This paper introduces a theoretical framework explaining label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. We identify an optimal label smoothing value that minimizes generalization loss and validate our theory with comprehensive experiments. Our results aim to clarify label smoothing's utility for both theoreticians and practitioners in real-world applications"
    }
  ],
  "control": [
    {
      "title": "AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION",
      "url": "/research/2020/10/23/AN-INVESTIGATION-OF-HOW-LABEL-SMOOTHING-AFFECTS-GENERALIZATION.html",
      "date": "October 23, 2020",
      "summary": "Label smoothing is hypothesized to mitigate overfitting and enhance generalization, with supporting empirical evidence. However, its mathematical underpinnings remain unclear. This paper introduces a theoretical framework explaining label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. We identify an optimal label smoothing value that minimizes generalization loss and validate our theory with comprehensive experiments. Our results aim to clarify label smoothing's utility for both theoreticians and practitioners in real-world applications"
    },
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "value": [
    {
      "title": "AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION",
      "url": "/research/2020/10/23/AN-INVESTIGATION-OF-HOW-LABEL-SMOOTHING-AFFECTS-GENERALIZATION.html",
      "date": "October 23, 2020",
      "summary": "Label smoothing is hypothesized to mitigate overfitting and enhance generalization, with supporting empirical evidence. However, its mathematical underpinnings remain unclear. This paper introduces a theoretical framework explaining label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. We identify an optimal label smoothing value that minimizes generalization loss and validate our theory with comprehensive experiments. Our results aim to clarify label smoothing's utility for both theoreticians and practitioners in real-world applications"
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "Neural networks parameterized for continuous, differentiable signal representations provide a promising paradigm but struggle with modeling fine detail and representing spatial and temporal derivatives critical for signals defined by partial differential equations. We introduce sinusoidal representation networks (SIRENs), utilizing periodic activation functions to effectively capture complex natural signals and their derivatives. Our analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. We also showcase SIRENs' application in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations, and extend their use with hypernetworks to learn priors for SIREN functions. A video overview and full applications are available on our project website."
    }
  ],
  "utility": [
    {
      "title": "AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION",
      "url": "/research/2020/10/23/AN-INVESTIGATION-OF-HOW-LABEL-SMOOTHING-AFFECTS-GENERALIZATION.html",
      "date": "October 23, 2020",
      "summary": "Label smoothing is hypothesized to mitigate overfitting and enhance generalization, with supporting empirical evidence. However, its mathematical underpinnings remain unclear. This paper introduces a theoretical framework explaining label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. We identify an optimal label smoothing value that minimizes generalization loss and validate our theory with comprehensive experiments. Our results aim to clarify label smoothing's utility for both theoreticians and practitioners in real-world applications"
    },
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    }
  ],
  "theoretician": [
    {
      "title": "AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION",
      "url": "/research/2020/10/23/AN-INVESTIGATION-OF-HOW-LABEL-SMOOTHING-AFFECTS-GENERALIZATION.html",
      "date": "October 23, 2020",
      "summary": "Label smoothing is hypothesized to mitigate overfitting and enhance generalization, with supporting empirical evidence. However, its mathematical underpinnings remain unclear. This paper introduces a theoretical framework explaining label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. We identify an optimal label smoothing value that minimizes generalization loss and validate our theory with comprehensive experiments. Our results aim to clarify label smoothing's utility for both theoreticians and practitioners in real-world applications"
    }
  ],
  "practitioner": [
    {
      "title": "AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION",
      "url": "/research/2020/10/23/AN-INVESTIGATION-OF-HOW-LABEL-SMOOTHING-AFFECTS-GENERALIZATION.html",
      "date": "October 23, 2020",
      "summary": "Label smoothing is hypothesized to mitigate overfitting and enhance generalization, with supporting empirical evidence. However, its mathematical underpinnings remain unclear. This paper introduces a theoretical framework explaining label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. We identify an optimal label smoothing value that minimizes generalization loss and validate our theory with comprehensive experiments. Our results aim to clarify label smoothing's utility for both theoreticians and practitioners in real-world applications"
    }
  ],
  "world": [
    {
      "title": "AN INVESTIGATION OF HOW LABEL SMOOTHING AFFECTS GENERALIZATION",
      "url": "/research/2020/10/23/AN-INVESTIGATION-OF-HOW-LABEL-SMOOTHING-AFFECTS-GENERALIZATION.html",
      "date": "October 23, 2020",
      "summary": "Label smoothing is hypothesized to mitigate overfitting and enhance generalization, with supporting empirical evidence. However, its mathematical underpinnings remain unclear. This paper introduces a theoretical framework explaining label smoothing's effectiveness in controlling generalization loss, especially in scenarios with partially incorrect training labels. We identify an optimal label smoothing value that minimizes generalization loss and validate our theory with comprehensive experiments. Our results aim to clarify label smoothing's utility for both theoreticians and practitioners in real-world applications"
    },
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "We introduce DiffBIR, a framework using pretrained text-to-image diffusion models for blind image restoration. It consists of a two-stage pipeline: initially, we pretrain a restoration module on various degradations to enhance real-world applicability. Then, we use latent diffusion models for realistic restoration, including a novel injective modulation sub-network, LAControlNet, for fine-tuning, and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, we provide a controllable module for users to adjust quality and fidelity during the denoising process. Our extensive tests show DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets. The code is available online."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "We introduce a system that trains deep neural networks for object detection using synthetic images, leveraging domain randomization to manage real-world data variability. This involves randomizing simulator parameters—like lighting and object textures—in non-realistic ways, helping the network to identify essential object features. Our findings reveal that networks can achieve impressive performance with just synthetic data and further improve with real data fine-tuning. This suggests the potential of using cost-effective synthetic data for training, circumventing the challenges of acquiring vast amounts of real-world data or creating detailed synthetic environments. We validate our method through car bounding box detection on the KITTI dataset."
    }
  ],
  "bootstrap": [
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/research/2020/09/10/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning.html",
      "date": "September 10, 2020",
      "summary": "We introduce BYOL, a novel self-supervised learning method for image representation without using negative pairs. Utilizing two networks, BYOL trains one to predict the representation of an image as seen by the other under a different view, updating the latter with the former's slow-moving average. Surpassing state-of-the-art methods, it achieves up to 79.6% accuracy on ImageNet with ResNet models, performing equally or better on transfer and semi-supervised tasks. Code and models are available on GitHub."
    }
  ],
  "byol": [
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/research/2020/09/10/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning.html",
      "date": "September 10, 2020",
      "summary": "We introduce BYOL, a novel self-supervised learning method for image representation without using negative pairs. Utilizing two networks, BYOL trains one to predict the representation of an image as seen by the other under a different view, updating the latter with the former's slow-moving average. Surpassing state-of-the-art methods, it achieves up to 79.6% accuracy on ImageNet with ResNet models, performing equally or better on transfer and semi-supervised tasks. Code and models are available on GitHub."
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    }
  ],
  "pair": [
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/research/2020/09/10/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning.html",
      "date": "September 10, 2020",
      "summary": "We introduce BYOL, a novel self-supervised learning method for image representation without using negative pairs. Utilizing two networks, BYOL trains one to predict the representation of an image as seen by the other under a different view, updating the latter with the former's slow-moving average. Surpassing state-of-the-art methods, it achieves up to 79.6% accuracy on ImageNet with ResNet models, performing equally or better on transfer and semi-supervised tasks. Code and models are available on GitHub."
    },
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    },
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "Image-caption pretraining, crucial for tasks like zero-shot image classification and object detection, faces challenges in aligning multiple concepts from captions to objects in images. We introduce a curriculum learning framework inspired by children's language learning from cognitive science, starting with simple image-caption pairs and gradually increasing complexity by adding more concepts. This method, which leverages knowledge from each phase for subsequent learning, outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios. The code is available at: https://github.com/hayyubi/cur_vl.git."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    }
  ],
  "see": [
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/research/2020/09/10/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning.html",
      "date": "September 10, 2020",
      "summary": "We introduce BYOL, a novel self-supervised learning method for image representation without using negative pairs. Utilizing two networks, BYOL trains one to predict the representation of an image as seen by the other under a different view, updating the latter with the former's slow-moving average. Surpassing state-of-the-art methods, it achieves up to 79.6% accuracy on ImageNet with ResNet models, performing equally or better on transfer and semi-supervised tasks. Code and models are available on GitHub."
    },
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is crucial for natural language processing tasks, but its training requires a learning rate warm-up stage, essential for performance yet slowing optimization and increasing hyper-parameter tuning needs. This paper investigates the importance of the warm-up stage and the impact of layer normalization placement. It uses mean field theory to demonstrate that the original Post-LN Transformer's design, with layer normalization between residual blocks, results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    }
  ],
  "update": [
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/research/2020/09/10/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning.html",
      "date": "September 10, 2020",
      "summary": "We introduce BYOL, a novel self-supervised learning method for image representation without using negative pairs. Utilizing two networks, BYOL trains one to predict the representation of an image as seen by the other under a different view, updating the latter with the former's slow-moving average. Surpassing state-of-the-art methods, it achieves up to 79.6% accuracy on ImageNet with ResNet models, performing equally or better on transfer and semi-supervised tasks. Code and models are available on GitHub."
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "6D object pose estimation is crucial for robotic bin-picking and essential for many industrial applications. Annotating custom datasets for each bin-picking scenario is challenging. We introduce an iterative self-training framework for sim-to-real 6D object pose estimation, enhancing cost-effective robotic grasping. We create a photo-realistic simulator for synthesizing virtual data to train an initial pose estimation network. This network, acting as a teacher model, predicts poses on unlabeled real data. We develop an adaptive selection scheme to filter reliable predictions for updating a student model with pseudo labels for real data pose estimation. By iteratively refining the teacher model with the trained student model, we improve the quality of pseudo labels. Our method, tested on a public benchmark and a newly-released dataset, shows an 11.49% and 22.62% ADD(-S) improvement, respectively, and a 19.54% increase in robotic bin-picking success, demonstrating the effectiveness of iterative sim-to-real approaches. Project homepage: www.cse.cuhk.edu.hk/~kaichen/sim2real_pose.html"
    },
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "Recent advancements in image transformers, notably in models such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, have narrowed the performance gap with traditional CNNs. These models are typically trained on large datasets like ImageNet-21k and then finetuned on ImageNet-1k. However, assessments often overlook Tiny ImageNet when evaluating transfer learning performance. This paper updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. The code is available at: https://github.com/ehuynh1106/TinyImageNet-Transformers"
    }
  ],
  "latter": [
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/research/2020/09/10/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning.html",
      "date": "September 10, 2020",
      "summary": "We introduce BYOL, a novel self-supervised learning method for image representation without using negative pairs. Utilizing two networks, BYOL trains one to predict the representation of an image as seen by the other under a different view, updating the latter with the former's slow-moving average. Surpassing state-of-the-art methods, it achieves up to 79.6% accuracy on ImageNet with ResNet models, performing equally or better on transfer and semi-supervised tasks. Code and models are available on GitHub."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "former": [
    {
      "title": "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning",
      "url": "/research/2020/09/10/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning.html",
      "date": "September 10, 2020",
      "summary": "We introduce BYOL, a novel self-supervised learning method for image representation without using negative pairs. Utilizing two networks, BYOL trains one to predict the representation of an image as seen by the other under a different view, updating the latter with the former's slow-moving average. Surpassing state-of-the-art methods, it achieves up to 79.6% accuracy on ImageNet with ResNet models, performing equally or better on transfer and semi-supervised tasks. Code and models are available on GitHub."
    }
  ],
  "cmae": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    }
  ],
  "modeling": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    },
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/research/2024/01/24/MambaByte-Token-free-Selective-State-Space-Model.html",
      "date": "January 24, 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "Deep neural networks, while effective in complex vision tasks, suffer from overfitting. Current image augmentation techniques, largely linear and hand-crafted like scale, flip, and color jitter, fail to produce sufficiently challenging examples. We introduce a new augmentation approach, Mask-Reconstruct Augmentation (MRA), leveraging self-supervised masked autoencoders to generate distorted inputs. This method, inspired by masked image modeling's success in self-supervised learning, uses nonlinear transformations for regularization. Our extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks. The code is available at https://github.com/haohang96/MRA."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "Neural networks parameterized for continuous, differentiable signal representations provide a promising paradigm but struggle with modeling fine detail and representing spatial and temporal derivatives critical for signals defined by partial differential equations. We introduce sinusoidal representation networks (SIRENs), utilizing periodic activation functions to effectively capture complex natural signals and their derivatives. Our analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. We also showcase SIRENs' application in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations, and extend their use with hypernetworks to learn priors for SIREN functions. A video overview and full applications are available on our project website."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "Online Normalization is a novel method for normalizing neural network hidden activations, offering a batch-independent alternative with comparable accuracy to Batch Normalization. It overcomes Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. Applicable to recurrent, fully connected networks, and those with high activation memory requirements, it demonstrates effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    }
  ],
  "mim": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "discriminability": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    }
  ],
  "perceptibility": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    }
  ],
  "branch": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    }
  ],
  "encoder": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "Image-caption pretraining, crucial for tasks like zero-shot image classification and object detection, faces challenges in aligning multiple concepts from captions to objects in images. We introduce a curriculum learning framework inspired by children's language learning from cognitive science, starting with simple image-caption pairs and gradually increasing complexity by adding more concepts. This method, which leverages knowledge from each phase for subsequent learning, outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios. The code is available at: https://github.com/hayyubi/cur_vl.git."
    },
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "Monocular depth estimation's success hinges on large, diverse training datasets. Acquiring dense ground-truth depth is challenging, leading to diverse datasets with unique biases. We introduce tools for training with multiple datasets, despite incompatible annotations. Our approach includes a robust training objective unaffected by depth range and scale variations, employs principled multi-objective learning for data integration from various sources, and emphasizes the importance of pretraining encoders on auxiliary tasks. We tested our methods using five datasets, including 3D films, a novel, extensive data source. Through zero-shot cross-dataset transfer, we demonstrate our approach's superior generalization, significantly outperforming existing methods and establishing a new benchmark in monocular depth estimation."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    }
  ],
  "decoder": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    }
  ],
  "momentum": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    }
  ],
  "pixel": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    }
  ],
  "ensure": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "We introduce a Fourier neural network (FNN) aligned with Fourier decomposition, utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design ensures easy integration with more complex networks for data processing tasks. The FNN's efficacy is demonstrated on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits include solution validity beyond the training scope, model interpretability, and ease of use."
    }
  ],
  "compatibility": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    }
  ],
  "miou": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    }
  ],
  "ade20k": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    }
  ],
  "httpsgithubcomzhichenghuangcmae": [
    {
      "title": "Contrastive Masked Autoencoders are Stronger Vision Learners",
      "url": "/research/2024/01/29/Contrastive-Masked-Autoencoders-are-Stronger-Vision-Learners.html",
      "date": "January 29, 2024",
      "summary": "Contrastive Masked Autoencoders (CMAE) is a new self-supervised method for enhancing vision representation learning by integrating contrastive learning with masked image modeling. CMAE improves upon traditional MIM by offering stronger discriminability and local perceptibility in representations. It features a dual-branch architecture, including an asymmetric encoder-decoder for holistic feature learning and a momentum encoder for boosting feature discriminability through contrastive learning. Innovations like pixel shifting and a feature decoder ensure compatibility between contrastive learning and MIM. CMAE has set new benchmarks in image classification, semantic segmentation, and object detection, with notable performances like 85.3% top-1 accuracy on ImageNet and 52.5% mIoU on ADE20k. The source code is available at https://github.com/ZhichengHuang/CMAE."
    }
  ],
  "adamw": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "decouple": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    }
  ],
  "weight": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/research/2019/07/26/DropAttention-A-Regularization-Method-for-Fully-Connected-Self-Attention-Networks.html",
      "date": "July 26, 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. Unlike existing dropout techniques for other neural network layers, DropAttention specifically targets the unique challenges of self-attention mechanisms. Our experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    },
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/research/2014/01/01/Dropout-A-Simple-Way-to-Prevent-Neural-Networks-from-Overfitting.html",
      "date": "January 1, 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    },
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "We introduce the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. Our empirical evaluation across computer vision, natural language processing, and speech tasks shows that GELU offers performance improvements over ReLU and ELU activations."
    },
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    },
    {
      "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "Diffusion models, effective for generating high-dimensional perceptual data like images, lack established standard practices. This study enhances noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Our large-scale study demonstrates superior performance over established diffusion methods in high-resolution text-to-image synthesis. We introduce a transformer-based architecture for text-to-image generation, improving text comprehension, typography, and human preference ratings. Our findings show predictable scaling trends, with lower validation loss correlating with improved synthesis. The largest models outperform state-of-the-art approaches, and our data, code, and model weights will be publicly accessible."
    },
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "We introduce an algorithm designed to identify networks that are both simple and highly generalizable by searching for extensive areas of flat minima in the error function, where the error rate is relatively stable. These flat minima are associated with lower risks of overfitting according to MDL principles. Despite needing to calculate second-order derivatives, the algorithm maintains a complexity level comparable to backpropagation. We tested it on both feedforward and recurrent networks, and in stock market prediction tasks, it performed better than traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    },
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    },
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    }
  ],
  "decay": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "We introduce an algorithm designed to identify networks that are both simple and highly generalizable by searching for extensive areas of flat minima in the error function, where the error rate is relatively stable. These flat minima are associated with lower risks of overfitting according to MDL principles. Despite needing to calculate second-order derivatives, the algorithm maintains a complexity level comparable to backpropagation. We tested it on both feedforward and recurrent networks, and in stock market prediction tasks, it performed better than traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    }
  ],
  "regularization": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/research/2019/07/26/DropAttention-A-Regularization-Method-for-Fully-Connected-Self-Attention-Networks.html",
      "date": "July 26, 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. Unlike existing dropout techniques for other neural network layers, DropAttention specifically targets the unique challenges of self-attention mechanisms. Our experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    },
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "Vision Transformers (ViT) excel in various vision tasks, like image classification, object detection, and semantic segmentation, but require more model regularization or data augmentation (AugReg) than convolutional neural networks, especially with smaller datasets. Through a comprehensive study, we explore how training data amount, AugReg, model size, and compute budget interact. Our findings indicate that using more compute and AugReg can achieve the same performance as training with significantly more data. Specifically, we demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "Deep neural networks, while effective in complex vision tasks, suffer from overfitting. Current image augmentation techniques, largely linear and hand-crafted like scale, flip, and color jitter, fail to produce sufficiently challenging examples. We introduce a new augmentation approach, Mask-Reconstruct Augmentation (MRA), leveraging self-supervised masked autoencoders to generate distorted inputs. This method, inspired by masked image modeling's success in self-supervised learning, uses nonlinear transformations for regularization. Our extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks. The code is available at https://github.com/haohang96/MRA."
    },
    {
      "title": "Better plain ViT baselines for ImageNet-1k",
      "url": "/research/2022/05/02/Better-plain-ViT-baselines-for-ImageNet-1k.html",
      "date": "May 2, 2022",
      "summary": "It is commonly accepted that the Vision Transformer model requires sophisticated regularization techniques to excel at ImageNet-1k scale data. Surprisingly, we find this is not the case and standard data augmentation is sufficient. This note presents a few minor modifications to the original Vision Transformer (ViT) vanilla training setting that dramatically improve the performance of plain ViT models. Notably, 90 epochs of training surpass 76% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reach 80% in less than one day."
    }
  ],
  "l2": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    }
  ],
  "sgd": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "inequivalence": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    }
  ],
  "align": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "We introduce a Fourier neural network (FNN) aligned with Fourier decomposition, utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design ensures easy integration with more complex networks for data processing tasks. The FNN's efficacy is demonstrated on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits include solution validity beyond the training scope, model interpretability, and ease of use."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "Recent studies have shown that Convolutional Neural Networks (CNNs), which were thought to recognize objects by learning complex shapes, are actually more biased towards recognizing textures. This finding, based on a series of tests comparing CNNs with human observers using images with conflicting texture and shape cues, contradicts the traditional understanding of how these networks process images. We discovered that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, when we trained a standard CNN architecture (ResNet-50) on a stylized version of ImageNet, designed to emphasize shape, the network's performance aligned more closely with human behavior. This training not only matched human performance in controlled experiments (involving 97 observers across 48,560 trials) but also enhanced object detection and robustness against image distortions, underscoring the benefits of shape-based representations in visual recognition systems."
    },
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "Image-caption pretraining, crucial for tasks like zero-shot image classification and object detection, faces challenges in aligning multiple concepts from captions to objects in images. We introduce a curriculum learning framework inspired by children's language learning from cognitive science, starting with simple image-caption pairs and gradually increasing complexity by adding more concepts. This method, which leverages knowledge from each phase for subsequent learning, outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios. The code is available at: https://github.com/hayyubi/cur_vl.git."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "separate": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    }
  ],
  "step": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    }
  ],
  "factor": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "Semi-supervised learning (SSL) utilizes unlabeled data to boost model performance but often at the expense of increased complexity. This paper introduces FixMatch, a streamlined SSL algorithm that simplifies existing methods. FixMatch uses a model's predictions on weakly-augmented unlabeled images to generate pseudo-labels, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. Remarkably, FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness. The code is publicly available."
    }
  ],
  "traction": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    }
  ],
  "tensorflow": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    }
  ],
  "pytorch": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    }
  ],
  "httpsgithubcomloshchiladamwandsgdw": [
    {
      "title": "ADAMW: DECOUPLED WEIGHT DECAY REGULARIZATION",
      "url": "/research/2019/01/04/DECOUPLED-WEIGHT-DECAY-REGULARIZATION.html",
      "date": "January 4, 2019",
      "summary": "L2 regularization and weight decay are equivalent for standard SGD when adjusted for learning rate, but not for adaptive gradient methods like Adam. We highlight the inequivalence and introduce a modification to align weight decay with its original concept, separate from loss optimization steps. This change allows for independent optimization of the weight decay factor and learning rate, enhancing Adam's performance to match or exceed SGD with momentum on image classification tasks. Our decoupled weight decay approach has gained traction, with implementations in TensorFlow and PyTorch, and the full code for our study is available at https://github.com/loshchil/AdamW-and-SGDW"
    }
  ],
  "grok": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper investigates how neural networks generalize on small, algorithmically generated datasets, focusing on data efficiency, memorization, generalization, and learning speed. It highlights instances where neural networks grok patterns, significantly improving their generalization ability from mere chance to perfect accuracy, even beyond the point of overfitting. Additionally, the study finds that generalization on smaller datasets demands more optimization. It posits that such datasets are ideal for exploring the enigmatic phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    }
  ],
  "here": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    }
  ],
  "why": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    }
  ],
  "phenomenon": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper investigates how neural networks generalize on small, algorithmically generated datasets, focusing on data efficiency, memorization, generalization, and learning speed. It highlights instances where neural networks grok patterns, significantly improving their generalization ability from mere chance to perfect accuracy, even beyond the point of overfitting. Additionally, the study finds that generalization on smaller datasets demands more optimization. It posits that such datasets are ideal for exploring the enigmatic phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    }
  ],
  "dnn": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/research/2021/08/31/Universal-Adversarial-Robustness-of-Texture-and-Shape-biased-Models.html",
      "date": "August 31, 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    },
    {
      "title": "Curriculum Learning by Dynamic Instance Hardness",
      "url": "/research/2020/12/06/Curriculum-Learning-by-Dynamic-Instance-Hardness.html",
      "date": "December 6, 2020",
      "summary": "This paper explores the adaptation of deep neural network (DNN) training using a method called dynamic instance hardness (DIH), which measures a sample's learning difficulty over time. By tracking the exponential moving average of a sample's hardness, DIH provides a stable indicator of learning progress. Early predictions of a sample's DIH allow for prioritizing more challenging samples and deprioritizing easier ones, leading to a DIH guided curriculum learning (DIHCL) approach. DIHCL enhances learning efficiency and model accuracy without extra computational costs, as it leverages data from the training process itself. Tested on 11 datasets, DIHCL surpasses traditional training methods and recent curriculum learning techniques in both efficiency and effectiveness. The implementation is publicly available."
    }
  ],
  "occur": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "Taming the Waves: Sine as Activation Function in Deep Neural Networks",
      "url": "/research/2016/11/04/Taming-the-Waves-Sine-as-Activation-Function-in-Deep-Neural-Networks.html",
      "date": "November 4, 2016",
      "summary": "This paper examines the challenges of training deep neural networks with sinusoidal activation functions, despite their successful use in specific applications. It explains the difficulty in training due to the emergence of numerous shallow local minima from the architecture. The study also reveals that successful learning in typical classification tasks occurs when the network effectively disregards the periodic cycles. Furthermore, it demonstrates that for certain non-trivial tasks, networks with sinusoidal activations can outperform those using traditional monotonic functions."
    }
  ],
  "setting": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "Image-caption pretraining, crucial for tasks like zero-shot image classification and object detection, faces challenges in aligning multiple concepts from captions to objects in images. We introduce a curriculum learning framework inspired by children's language learning from cognitive science, starting with simple image-caption pairs and gradually increasing complexity by adding more concepts. This method, which leverages knowledge from each phase for subsequent learning, outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios. The code is available at: https://github.com/hayyubi/cur_vl.git."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    },
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "cifar10": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "imagenette": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    }
  ],
  "become": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    }
  ],
  "emergence": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    },
    {
      "title": "Taming the Waves: Sine as Activation Function in Deep Neural Networks",
      "url": "/research/2016/11/04/Taming-the-Waves-Sine-as-Activation-Function-in-Deep-Neural-Networks.html",
      "date": "November 4, 2016",
      "summary": "This paper examines the challenges of training deep neural networks with sinusoidal activation functions, despite their successful use in specific applications. It explains the difficulty in training due to the emergence of numerous shallow local minima from the architecture. The study also reveals that successful learning in typical classification tasks occurs when the network effectively disregards the periodic cycles. Furthermore, it demonstrates that for certain non-trivial tasks, networks with sinusoidal activations can outperform those using traditional monotonic functions."
    }
  ],
  "complexity": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "Semi-supervised learning (SSL) utilizes unlabeled data to boost model performance but often at the expense of increased complexity. This paper introduces FixMatch, a streamlined SSL algorithm that simplifies existing methods. FixMatch uses a model's predictions on weakly-augmented unlabeled images to generate pseudo-labels, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. Remarkably, FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness. The code is publicly available."
    },
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "Image-caption pretraining, crucial for tasks like zero-shot image classification and object detection, faces challenges in aligning multiple concepts from captions to objects in images. We introduce a curriculum learning framework inspired by children's language learning from cognitive science, starting with simple image-caption pairs and gradually increasing complexity by adding more concepts. This method, which leverages knowledge from each phase for subsequent learning, outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios. The code is available at: https://github.com/hayyubi/cur_vl.git."
    },
    {
      "title": "Self-Paced Curriculum Learning",
      "url": "/research/2015/01/01/Self-Paced-Curriculum-Learning.html",
      "date": "January 1, 2015",
      "summary": "Curriculum learning (CL) and self-paced learning (SPL) are methods inspired by the incremental complexity of human and animal learning, with CL using a fixed curriculum based on prior knowledge and SPL adjusting dynamically to the learner's pace but struggling with prior knowledge and overfitting. This paper introduces a unified framework called self-paced curriculum learning (SPCL), blending both methods' strengths by considering prior knowledge and ongoing learning progress through a concise optimization problem. SPCL mimics a collaborative instructor-student learning mode, showing empirical advantages in two tasks."
    },
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "We introduce an algorithm designed to identify networks that are both simple and highly generalizable by searching for extensive areas of flat minima in the error function, where the error rate is relatively stable. These flat minima are associated with lower risks of overfitting according to MDL principles. Despite needing to calculate second-order derivatives, the algorithm maintains a complexity level comparable to backpropagation. We tested it on both feedforward and recurrent networks, and in stock market prediction tasks, it performed better than traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    },
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method, applies a single augmentation to each image and outperforms existing methods with minimal complexity and cost. We conducted extensive experiments to validate its performance against state-of-the-art methods across various image classification scenarios and explored its effectiveness through multiple ablation studies involving different augmentation spaces and methods. Our work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Highlighting a stagnation in automatic augmentation research, we conclude with proposed best practices for future advancements in the field."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    },
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    }
  ],
  "density": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "region": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "The Vision Transformer (ViT) offers simplicity, robustness, and superior performance in vision tasks but struggles with small-scale datasets due to its lack of inherent inductive biases and dependence on large-scale pre-training datasets like ImageNet and JFT. This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of ViTs without large-scale pre-training or modifications to the architecture or loss functions. Experiments show improved performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, maintaining ViT's attention to relevant regions and robustness. The code and models are available online."
    }
  ],
  "space": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/research/2022/08/05/Efficiently-Modeling-Long-Sequences-with-Structured-State-Spaces.html",
      "date": "August 5, 2022",
      "summary": "he quest for a universal sequence model capable of handling data across different tasks and modalities, especially long-range dependencies, faces challenges with conventional models like RNNs, CNNs, and Transformers, which struggle with very long sequences. A promising approach using state space models (SSM) showed potential but had high computational demands. We introduce the Structured State Space sequence model (S4), a more efficient parameterization of SSM, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    },
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    },
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/research/2024/01/24/MambaByte-Token-free-Selective-State-Space-Model.html",
      "date": "January 24, 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "Dropout as Data Augmentation",
      "url": "/research/2016/01/08/Dropout-as-Data-Augmentation.html",
      "date": "January 8, 2016",
      "summary": "Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/On-Interaction-Between-Augmentations-and-Corruptions-in-Natural-Corruption-Robustness.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "Self-supervised transformer training methods have shown excellent results in various fields. Unlike previous models like masked autoencoders (MAE) which use a single normalization layer for the class token [CLS] and other tokens, we introduce a novel normalization technique that normalizes the [CLS] token and normal tokens separately. This approach aims to better capture their unique characteristics, improving performance in downstream tasks. Our findings indicate that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    },
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method, applies a single augmentation to each image and outperforms existing methods with minimal complexity and cost. We conducted extensive experiments to validate its performance against state-of-the-art methods across various image classification scenarios and explored its effectiveness through multiple ablation studies involving different augmentation spaces and methods. Our work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Highlighting a stagnation in automatic augmentation research, we conclude with proposed best practices for future advancements in the field."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    }
  ],
  "smoothness": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    }
  ],
  "decision": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    }
  ],
  "boundary": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    },
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "We introduce a Fourier neural network (FNN) aligned with Fourier decomposition, utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design ensures easy integration with more complex networks for data processing tasks. The FNN's efficacy is demonstrated on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits include solution validity beyond the training scope, model interpretability, and ease of use."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "Neural networks parameterized for continuous, differentiable signal representations provide a promising paradigm but struggle with modeling fine detail and representing spatial and temporal derivatives critical for signals defined by partial differential equations. We introduce sinusoidal representation networks (SIRENs), utilizing periodic activation functions to effectively capture complex natural signals and their derivatives. Our analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. We also showcase SIRENs' application in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations, and extend their use with hypernetworks to learn priors for SIREN functions. A video overview and full applications are available on our project website."
    }
  ],
  "partitioning": [
    {
      "title": "Deep Networks Always Grok and Heres Why",
      "url": "/research/2024/02/23/Deep-Networks-Always-Grok-and-Heres-Why.html",
      "date": "February 23, 2024",
      "summary": "Grokking, or delayed generalization, is a widespread phenomenon in deep neural networks (DNNs) where generalization occurs well after achieving low training error, including in practical settings like CNNs on CIFAR10 or Resnets on Imagenette. We introduce delayed robustness to describe DNNs becoming robust to adversarial examples post-generalization. This emergence is explained through a new measure of local complexity, analyzing the density of linear regions in the DNN input space. We find that during training, these regions transition, enhancing the DNN's smoothness near training samples and its complexity near decision boundaries, facilitating grokking and robust partitioning of the input space."
    }
  ],
  "binarization": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    }
  ],
  "distortion": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "Recent studies have shown that Convolutional Neural Networks (CNNs), which were thought to recognize objects by learning complex shapes, are actually more biased towards recognizing textures. This finding, based on a series of tests comparing CNNs with human observers using images with conflicting texture and shape cues, contradicts the traditional understanding of how these networks process images. We discovered that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, when we trained a standard CNN architecture (ResNet-50) on a stylized version of ImageNet, designed to emphasize shape, the network's performance aligned more closely with human behavior. This training not only matched human performance in controlled experiments (involving 97 observers across 48,560 trials) but also enhanced object detection and robustness against image distortions, underscoring the benefits of shape-based representations in visual recognition systems."
    }
  ],
  "quantize": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    },
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "beginning": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    }
  ],
  "resilience": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    }
  ],
  "projection": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "Gated Linear Units (GLUs) involve the component-wise product of two linear projections, with one undergoing a sigmoid function. Exploring GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers, we find certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    }
  ],
  "quantization": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "instance": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/research/2023/03/08/Extreme-Masking-for-Learning-Instance-and-Distributed-Visual-Representations.html",
      "date": "March 8, 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/research/2017/11/06/Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization.html",
      "date": "November 6, 2017",
      "summary": "In this study, we improve upon the fast stylization method by Ulyanov et al. (2016) through a minor yet impactful modification: substituting batch normalization with instance normalization during both training and testing. This adjustment significantly enhances the quality of generated images and enables the training of efficient real-time image generation architectures. The code and full paper are available online."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "Curriculum Learning by Dynamic Instance Hardness",
      "url": "/research/2020/12/06/Curriculum-Learning-by-Dynamic-Instance-Hardness.html",
      "date": "December 6, 2020",
      "summary": "This paper explores the adaptation of deep neural network (DNN) training using a method called dynamic instance hardness (DIH), which measures a sample's learning difficulty over time. By tracking the exponential moving average of a sample's hardness, DIH provides a stable indicator of learning progress. Early predictions of a sample's DIH allow for prioritizing more challenging samples and deprioritizing easier ones, leading to a DIH guided curriculum learning (DIHCL) approach. DIHCL enhances learning efficiency and model accuracy without extra computational costs, as it leverages data from the training process itself. Tested on 11 datasets, DIHCL surpasses traditional training methods and recent curriculum learning techniques in both efficiency and effectiveness. The implementation is publicly available."
    },
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "Temporal action localization (TAL) is crucial for video understanding but is hindered by benchmarks that rely on coarse action classes, leading to model overfitting and ambiguous annotations. Addressing these issues, we introduce FineAction, a large-scale, fine-grained video dataset with 103K instances across 106 action categories in 17K videos. FineAction offers a diverse, densely annotated dataset with co-occurring actions, posing new challenges for TAL. We evaluated popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and proposed a baseline method achieving a 13.17% mAP. FineAction aims to propel TAL research forward and is accessible online."
    },
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper investigates how neural networks generalize on small, algorithmically generated datasets, focusing on data efficiency, memorization, generalization, and learning speed. It highlights instances where neural networks grok patterns, significantly improving their generalization ability from mere chance to perfect accuracy, even beyond the point of overfitting. Additionally, the study finds that generalization on smaller datasets demands more optimization. It posits that such datasets are ideal for exploring the enigmatic phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    }
  ],
  "cifar": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/research/2017/08/23/A-downsampled-variant-of-imagenet-as-an-alternative-to-the-cifar-datasets.html",
      "date": "August 23, 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "Semi-supervised learning (SSL) utilizes unlabeled data to boost model performance but often at the expense of increased complexity. This paper introduces FixMatch, a streamlined SSL algorithm that simplifies existing methods. FixMatch uses a model's predictions on weakly-augmented unlabeled images to generate pseudo-labels, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. Remarkably, FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness. The code is publicly available."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "Online Normalization is a novel method for normalizing neural network hidden activations, offering a batch-independent alternative with comparable accuracy to Batch Normalization. It overcomes Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. Applicable to recurrent, fully connected networks, and those with high activation memory requirements, it demonstrates effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-5.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-6.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    }
  ],
  "bit": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "backpropagation": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    },
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "We introduce an algorithm designed to identify networks that are both simple and highly generalizable by searching for extensive areas of flat minima in the error function, where the error rate is relatively stable. These flat minima are associated with lower risks of overfitting according to MDL principles. Despite needing to calculate second-order derivatives, the algorithm maintains a complexity level comparable to backpropagation. We tested it on both feedforward and recurrent networks, and in stock market prediction tasks, it performed better than traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    }
  ],
  "omit": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    },
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/research/2014/01/01/Dropout-A-Simple-Way-to-Prevent-Neural-Networks-from-Overfitting.html",
      "date": "January 1, 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    }
  ],
  "sacrificing": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    }
  ],
  "rule": [
    {
      "title": "Deep neural networks are robust to weight binarization and other non-linear distortions",
      "url": "/research/2016/06/07/Deep-neural-networks-are-robust-to-weight-binarization-and-other-non-linear-distortions.html",
      "date": "June 7, 2016",
      "summary": "Recent studies reveal that deep neural networks maintain high performance levels even when trained with binary quantized weights, but this is just a beginning. These networks also show significant resilience to various test-time distortions, including noise and non-linear projections, with robustness extending beyond binary quantization. For instance, a network demonstrated only 11% error on CIFAR-10 with less than one effective bit per weight. Surprisingly, some conventional training adjustments, like weight quantization during backpropagation, can be modified or omitted without sacrificing this robustness. Experiments confirmed these findings on CIFAR-10 and ImageNet, leading to the proposal of a stochastic projection rule that sets a new benchmark of 7.64% error on CIFAR-10 without data augmentation."
    }
  ],
  "dino": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "new": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "vit": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/research/2021/08/16/An-Empirical-Study-of-Training-Self-Supervised-Vision-Transformers.html",
      "date": "August 16, 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "Vision Transformers (ViT) excel in various vision tasks, like image classification, object detection, and semantic segmentation, but require more model regularization or data augmentation (AugReg) than convolutional neural networks, especially with smaller datasets. Through a comprehensive study, we explore how training data amount, AugReg, model size, and compute budget interact. Our findings indicate that using more compute and AugReg can achieve the same performance as training with significantly more data. Specifically, we demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    },
    {
      "title": "Better plain ViT baselines for ImageNet-1k",
      "url": "/research/2022/05/02/Better-plain-ViT-baselines-for-ImageNet-1k.html",
      "date": "May 2, 2022",
      "summary": "It is commonly accepted that the Vision Transformer model requires sophisticated regularization techniques to excel at ImageNet-1k scale data. Surprisingly, we find this is not the case and standard data augmentation is sufficient. This note presents a few minor modifications to the original Vision Transformer (ViT) vanilla training setting that dramatically improve the performance of plain ViT models. Notably, 90 epochs of training surpass 76% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reach 80% in less than one day."
    },
    {
      "title": "DeiT III: Revenge of the ViT",
      "url": "/research/2023/01/18/DeiT-III-Revenge-of-the-ViT.html",
      "date": "January 18, 2023",
      "summary": "This paper explores the supervised training of Vision Transformers (ViTs), using a simplified training approach adapted from ResNet-50 that includes a novel data-augmentation method with just 3 augmentations. The study demonstrates that this method significantly improves ViTs' performance in image classification, transfer learning, and semantic segmentation over previous supervised training techniques. Moreover, it shows ViTs' performance can match newer architectures, providing a new benchmark for evaluating self-supervised methods on ViTs."
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "The Vision Transformer (ViT) offers simplicity, robustness, and superior performance in vision tasks but struggles with small-scale datasets due to its lack of inherent inductive biases and dependence on large-scale pre-training datasets like ImageNet and JFT. This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of ViTs without large-scale pre-training or modifications to the architecture or loss functions. Experiments show improved performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, maintaining ViT's attention to relevant regions and robustness. The code and models are available online."
    },
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "This paper introduces VideoMAE, a data-efficient approach for self-supervised video pre-training (SSVP) that utilizes video masked autoencoders with a novel, high-ratio video tube masking technique. This method, inspired by ImageMAE, challenges the model with video reconstruction tasks to improve video representation learning. Key findings include: (1) High masking ratios (90% to 95%) are effective due to video's temporal redundancy, (2) VideoMAE performs well on small datasets (~3k-4k videos) without extra data, highlighting the importance of high-level structure learning, and (3) Data quality is more crucial than quantity for SSVP, with domain shift between pre-training and target datasets being significant. Remarkably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone, without extra data. The code is available online."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    },
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "Recent advancements in image transformers, notably in models such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, have narrowed the performance gap with traditional CNNs. These models are typically trained on large datasets like ImageNet-21k and then finetuned on ImageNet-1k. However, assessments often overlook Tiny ImageNet when evaluating transfer learning performance. This paper updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. The code is available at: https://github.com/ehuynh1106/TinyImageNet-Transformers"
    },
    {
      "title": "Are Convolutional Neural Networks or Transformers more like human vision?",
      "url": "/research/2021/07/01/Are-Convolutional-Neural-Networks-or-Transformers-more-like-human-vision.html",
      "date": "July 1, 2021",
      "summary": "This study investigates the performance of Convolutional Neural Networks (CNNs) and attention-based Vision Transformers (ViTs) in computer vision tasks. While CNNs excel in accuracy, ViTs offer a different approach with weaker inductive biases. By analyzing error patterns, we find that ViTs exhibit consistency with human errors, suggesting potential for more human-like vision models and insights into human object recognition."
    }
  ],
  "out": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "beyond": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "fact": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "following": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "contain": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "information": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    },
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "We introduce SlowFast networks for video recognition, featuring a Slow pathway for spatial semantics at low frame rates and a Fast pathway for motion details at high frame rates. The Fast pathway's efficiency is achieved through reduced channel capacity, yet it effectively captures temporal information. Our models significantly enhance action classification and detection, achieving top accuracy on benchmarks like Kinetics, Charades, and AVA. Code is available at: https://github.com/facebookresearch/SlowFast."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-5.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-6.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    }
  ],
  "emerge": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "second": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "knn": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "underline": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "multicrop": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "patch": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article offers the first extensive review of vision transformer techniques specifically applied to human action recognition, a field gaining interest for its wide applications. Termed action transformers, these methods are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also delves into optimizing spatio-temporal attention in transformers for longer sequences and examines different learning strategies, including self-supervised and zero-shot learning, with their respective loss functions. Additionally, the survey assesses progress in benchmark evaluation scores and discusses the challenges and future directions in this research area."
    }
  ],
  "form": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "distillation": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    },
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "Training data-efficient image transformers & distillation through attention",
      "url": "/research/2021/01/15/Training-data-efficient-image-transformers-&-distillation-through-attention.html",
      "date": "January 15, 2021",
      "summary": "This study introduces DeiT, competitive vision transformers trained solely on ImageNet without convolutions, achieving up to 83.1% top-1 accuracy on ImageNet, using a single computer in under 3 days. Notably, it presents a unique teacher-student strategy for transformers, leveraging a distillation token for efficient learning from a convolutional network teacher. This method results in performance on par with convolutional networks, achieving up to 85.2% accuracy on ImageNet and demonstrating effective transferability to other tasks. The authors provide access to their code and models."
    }
  ],
  "synergy": [
    {
      "title": "Dino: Emerging Properties in Self-Supervised Vision Transformers",
      "url": "/research/2021/05/24/Dino-Emerging-Properties-in-Self-Supervised-Vision-Transformers.html",
      "date": "May 24, 2021",
      "summary": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training , and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base"
    }
  ],
  "dinov2": [
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    }
  ],
  "creation": [
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    }
  ],
  "tuning": [
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    },
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "We introduce DiffBIR, a framework using pretrained text-to-image diffusion models for blind image restoration. It consists of a two-stage pipeline: initially, we pretrain a restoration module on various degradations to enhance real-world applicability. Then, we use latent diffusion models for realistic restoration, including a novel injective modulation sub-network, LAControlNet, for fine-tuning, and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, we provide a controllable module for users to adjust quality and fidelity during the denoising process. Our extensive tests show DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets. The code is available online."
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "The Vision Transformer (ViT) offers simplicity, robustness, and superior performance in vision tasks but struggles with small-scale datasets due to its lack of inherent inductive biases and dependence on large-scale pre-training datasets like ImageNet and JFT. This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of ViTs without large-scale pre-training or modifications to the architecture or loss functions. Experiments show improved performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, maintaining ViT's attention to relevant regions and robustness. The code and models are available online."
    },
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is crucial for natural language processing tasks, but its training requires a learning rate warm-up stage, essential for performance yet slowing optimization and increasing hyper-parameter tuning needs. This paper investigates the importance of the warm-up stage and the impact of layer normalization placement. It uses mean field theory to demonstrate that the original Post-LN Transformer's design, with layer normalization between residual blocks, results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "We introduce a system that trains deep neural networks for object detection using synthetic images, leveraging domain randomization to manage real-world data variability. This involves randomizing simulator parameters—like lighting and object textures—in non-realistic ways, helping the network to identify essential object features. Our findings reveal that networks can achieve impressive performance with just synthetic data and further improve with real data fine-tuning. This suggests the potential of using cost-effective synthetic data for training, circumventing the challenges of acquiring vast amounts of real-world data or creating detailed synthetic environments. We validate our method through car bounding box detection on the KITTI dataset."
    }
  ],
  "curate": [
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    }
  ],
  "includes": [
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    }
  ],
  "stability": [
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/research/2021/08/16/An-Empirical-Study-of-Training-Self-Supervised-Vision-Transformers.html",
      "date": "August 16, 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    },
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is crucial for natural language processing tasks, but its training requires a learning rate warm-up stage, essential for performance yet slowing optimization and increasing hyper-parameter tuning needs. This paper investigates the importance of the warm-up stage and the impact of layer normalization placement. It uses mean field theory to demonstrate that the original Post-LN Transformer's design, with layer normalization between residual blocks, results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    }
  ],
  "automate": [
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "pipeline": [
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "We introduce DiffBIR, a framework using pretrained text-to-image diffusion models for blind image restoration. It consists of a two-stage pipeline: initially, we pretrain a restoration module on various degradations to enhance real-world applicability. Then, we use latent diffusion models for realistic restoration, including a novel injective modulation sub-network, LAControlNet, for fine-tuning, and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, we provide a controllable module for users to adjust quality and fidelity during the denoising process. Our extensive tests show DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets. The code is available online."
    }
  ],
  "building": [
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "url": "/research/2024/02/02/DINOv2-Learning-Robust-Visual-Features-without-Supervision.html",
      "date": "February 2, 2024",
      "summary": "Recent advances in natural language processing have led to the development of foundational models for computer vision, enabling the creation of versatile visual features without task-specific fine-tuning. By leveraging diverse, curated datasets and combining various pretraining methods, especially self-supervised ones, these models achieve significant improvements. The approach includes scaling data and model sizes and refining training processes for efficiency and stability. An automated pipeline was developed for building high-quality, diverse image datasets. A large ViT model with 1 billion parameters was trained and distilled into smaller models, surpassing existing general-purpose features in several benchmarks."
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/On-Interaction-Between-Augmentations-and-Corruptions-in-Natural-Corruption-Robustness.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    }
  ],
  "sequence": [
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/research/2022/08/05/Efficiently-Modeling-Long-Sequences-with-Structured-State-Spaces.html",
      "date": "August 5, 2022",
      "summary": "he quest for a universal sequence model capable of handling data across different tasks and modalities, especially long-range dependencies, faces challenges with conventional models like RNNs, CNNs, and Transformers, which struggle with very long sequences. A promising approach using state space models (SSM) showed potential but had high computational demands. We introduce the Structured State Space sequence model (S4), a more efficient parameterization of SSM, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    },
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    },
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/research/2024/01/24/MambaByte-Token-free-Selective-State-Space-Model.html",
      "date": "January 24, 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    },
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article offers the first extensive review of vision transformer techniques specifically applied to human action recognition, a field gaining interest for its wide applications. Termed action transformers, these methods are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also delves into optimizing spatio-temporal attention in transformers for longer sequences and examines different learning strategies, including self-supervised and zero-shot learning, with their respective loss functions. Additionally, the survey assesses progress in benchmark evaluation scores and discusses the challenges and future directions in this research area."
    }
  ],
  "structure": [
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/research/2022/08/05/Efficiently-Modeling-Long-Sequences-with-Structured-State-Spaces.html",
      "date": "August 5, 2022",
      "summary": "he quest for a universal sequence model capable of handling data across different tasks and modalities, especially long-range dependencies, faces challenges with conventional models like RNNs, CNNs, and Transformers, which struggle with very long sequences. A promising approach using state space models (SSM) showed potential but had high computational demands. We introduce the Structured State Space sequence model (S4), a more efficient parameterization of SSM, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    },
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    },
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "This paper introduces VideoMAE, a data-efficient approach for self-supervised video pre-training (SSVP) that utilizes video masked autoencoders with a novel, high-ratio video tube masking technique. This method, inspired by ImageMAE, challenges the model with video reconstruction tasks to improve video representation learning. Key findings include: (1) High masking ratios (90% to 95%) are effective due to video's temporal redundancy, (2) VideoMAE performs well on small datasets (~3k-4k videos) without extra data, highlighting the importance of high-level structure learning, and (3) Data quality is more crucial than quantity for SSVP, with domain shift between pre-training and target datasets being significant. Remarkably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone, without extra data. The code is available online."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    }
  ],
  "quest": [
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/research/2022/08/05/Efficiently-Modeling-Long-Sequences-with-Structured-State-Spaces.html",
      "date": "August 5, 2022",
      "summary": "he quest for a universal sequence model capable of handling data across different tasks and modalities, especially long-range dependencies, faces challenges with conventional models like RNNs, CNNs, and Transformers, which struggle with very long sequences. A promising approach using state space models (SSM) showed potential but had high computational demands. We introduce the Structured State Space sequence model (S4), a more efficient parameterization of SSM, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    },
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    }
  ],
  "handling": [
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/research/2022/08/05/Efficiently-Modeling-Long-Sequences-with-Structured-State-Spaces.html",
      "date": "August 5, 2022",
      "summary": "he quest for a universal sequence model capable of handling data across different tasks and modalities, especially long-range dependencies, faces challenges with conventional models like RNNs, CNNs, and Transformers, which struggle with very long sequences. A promising approach using state space models (SSM) showed potential but had high computational demands. We introduce the Structured State Space sequence model (S4), a more efficient parameterization of SSM, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    }
  ],
  "modality": [
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/research/2022/08/05/Efficiently-Modeling-Long-Sequences-with-Structured-State-Spaces.html",
      "date": "August 5, 2022",
      "summary": "he quest for a universal sequence model capable of handling data across different tasks and modalities, especially long-range dependencies, faces challenges with conventional models like RNNs, CNNs, and Transformers, which struggle with very long sequences. A promising approach using state space models (SSM) showed potential but had high computational demands. We introduce the Structured State Space sequence model (S4), a more efficient parameterization of SSM, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article offers the first extensive review of vision transformer techniques specifically applied to human action recognition, a field gaining interest for its wide applications. Termed action transformers, these methods are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also delves into optimizing spatio-temporal attention in transformers for longer sequences and examines different learning strategies, including self-supervised and zero-shot learning, with their respective loss functions. Additionally, the survey assesses progress in benchmark evaluation scores and discusses the challenges and future directions in this research area."
    }
  ],
  "dependency": [
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/research/2022/08/05/Efficiently-Modeling-Long-Sequences-with-Structured-State-Spaces.html",
      "date": "August 5, 2022",
      "summary": "he quest for a universal sequence model capable of handling data across different tasks and modalities, especially long-range dependencies, faces challenges with conventional models like RNNs, CNNs, and Transformers, which struggle with very long sequences. A promising approach using state space models (SSM) showed potential but had high computational demands. We introduce the Structured State Space sequence model (S4), a more efficient parameterization of SSM, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    }
  ],
  "rnns": [
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/research/2022/08/05/Efficiently-Modeling-Long-Sequences-with-Structured-State-Spaces.html",
      "date": "August 5, 2022",
      "summary": "he quest for a universal sequence model capable of handling data across different tasks and modalities, especially long-range dependencies, faces challenges with conventional models like RNNs, CNNs, and Transformers, which struggle with very long sequences. A promising approach using state space models (SSM) showed potential but had high computational demands. We introduce the Structured State Space sequence model (S4), a more efficient parameterization of SSM, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    }
  ],
  "ssm": [
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/research/2022/08/05/Efficiently-Modeling-Long-Sequences-with-Structured-State-Spaces.html",
      "date": "August 5, 2022",
      "summary": "he quest for a universal sequence model capable of handling data across different tasks and modalities, especially long-range dependencies, faces challenges with conventional models like RNNs, CNNs, and Transformers, which struggle with very long sequences. A promising approach using state space models (SSM) showed potential but had high computational demands. We introduce the Structured State Space sequence model (S4), a more efficient parameterization of SSM, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    },
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    }
  ],
  "s4": [
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/research/2022/08/05/Efficiently-Modeling-Long-Sequences-with-Structured-State-Spaces.html",
      "date": "August 5, 2022",
      "summary": "he quest for a universal sequence model capable of handling data across different tasks and modalities, especially long-range dependencies, faces challenges with conventional models like RNNs, CNNs, and Transformers, which struggle with very long sequences. A promising approach using state space models (SSM) showed potential but had high computational demands. We introduce the Structured State Space sequence model (S4), a more efficient parameterization of SSM, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    }
  ],
  "parameterization": [
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "url": "/research/2022/08/05/Efficiently-Modeling-Long-Sequences-with-Structured-State-Spaces.html",
      "date": "August 5, 2022",
      "summary": "he quest for a universal sequence model capable of handling data across different tasks and modalities, especially long-range dependencies, faces challenges with conventional models like RNNs, CNNs, and Transformers, which struggle with very long sequences. A promising approach using state space models (SSM) showed potential but had high computational demands. We introduce the Structured State Space sequence model (S4), a more efficient parameterization of SSM, demonstrating strong empirical performance across various benchmarks, including achieving state-of-the-art results and significantly outperforming prior models in efficiency and speed."
    }
  ],
  "distribute": [
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/research/2023/03/08/Extreme-Masking-for-Learning-Instance-and-Distributed-Visual-Representations.html",
      "date": "March 8, 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    },
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "extrema": [
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/research/2023/03/08/Extreme-Masking-for-Learning-Instance-and-Distributed-Visual-Representations.html",
      "date": "March 8, 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    }
  ],
  "attention": [
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/research/2023/03/08/Extreme-Masking-for-Learning-Instance-and-Distributed-Visual-Representations.html",
      "date": "March 8, 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    },
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/research/2019/07/26/DropAttention-A-Regularization-Method-for-Fully-Connected-Self-Attention-Networks.html",
      "date": "July 26, 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. Unlike existing dropout techniques for other neural network layers, DropAttention specifically targets the unique challenges of self-attention mechanisms. Our experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    },
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    },
    {
      "title": "Training data-efficient image transformers & distillation through attention",
      "url": "/research/2021/01/15/Training-data-efficient-image-transformers-&-distillation-through-attention.html",
      "date": "January 15, 2021",
      "summary": "This study introduces DeiT, competitive vision transformers trained solely on ImageNet without convolutions, achieving up to 83.1% top-1 accuracy on ImageNet, using a single computer in under 3 days. Notably, it presents a unique teacher-student strategy for transformers, leveraging a distillation token for efficient learning from a convolutional network teacher. This method results in performance on par with convolutional networks, achieving up to 85.2% accuracy on ImageNet and demonstrating effective transferability to other tasks. The authors provide access to their code and models."
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "The Vision Transformer (ViT) offers simplicity, robustness, and superior performance in vision tasks but struggles with small-scale datasets due to its lack of inherent inductive biases and dependence on large-scale pre-training datasets like ImageNet and JFT. This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of ViTs without large-scale pre-training or modifications to the architecture or loss functions. Experiments show improved performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, maintaining ViT's attention to relevant regions and robustness. The code and models are available online."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article offers the first extensive review of vision transformer techniques specifically applied to human action recognition, a field gaining interest for its wide applications. Termed action transformers, these methods are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also delves into optimizing spatio-temporal attention in transformers for longer sequences and examines different learning strategies, including self-supervised and zero-shot learning, with their respective loss functions. Additionally, the survey assesses progress in benchmark evaluation scores and discusses the challenges and future directions in this research area."
    },
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "Recent advancements in image transformers, notably in models such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, have narrowed the performance gap with traditional CNNs. These models are typically trained on large datasets like ImageNet-21k and then finetuned on ImageNet-1k. However, assessments often overlook Tiny ImageNet when evaluating transfer learning performance. This paper updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. The code is available at: https://github.com/ehuynh1106/TinyImageNet-Transformers"
    },
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    }
  ],
  "cross": [
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/research/2023/03/08/Extreme-Masking-for-Learning-Instance-and-Distributed-Visual-Representations.html",
      "date": "March 8, 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "Monocular depth estimation's success hinges on large, diverse training datasets. Acquiring dense ground-truth depth is challenging, leading to diverse datasets with unique biases. We introduce tools for training with multiple datasets, despite incompatible annotations. Our approach includes a robust training objective unaffected by depth range and scale variations, employs principled multi-objective learning for data integration from various sources, and emphasizes the importance of pretraining encoders on auxiliary tasks. We tested our methods using five datasets, including 3D films, a novel, extensive data source. Through zero-shot cross-dataset transfer, we demonstrate our approach's superior generalization, significantly outperforming existing methods and establishing a new benchmark in monocular depth estimation."
    },
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    }
  ],
  "block": [
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/research/2023/03/08/Extreme-Masking-for-Learning-Instance-and-Distributed-Visual-Representations.html",
      "date": "March 8, 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    },
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is crucial for natural language processing tasks, but its training requires a learning rate warm-up stage, essential for performance yet slowing optimization and increasing hyper-parameter tuning needs. This paper investigates the importance of the warm-up stage and the impact of layer normalization placement. It uses mean field theory to demonstrate that the original Post-LN Transformer's design, with layer normalization between residual blocks, results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    }
  ],
  "seek": [
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/research/2023/03/08/Extreme-Masking-for-Learning-Instance-and-Distributed-Visual-Representations.html",
      "date": "March 8, 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    }
  ],
  "invariance": [
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/research/2023/03/08/Extreme-Masking-for-Learning-Instance-and-Distributed-Visual-Representations.html",
      "date": "March 8, 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/On-Interaction-Between-Augmentations-and-Corruptions-in-Natural-Corruption-Robustness.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    }
  ],
  "siamese": [
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/research/2023/03/08/Extreme-Masking-for-Learning-Instance-and-Distributed-Visual-Representations.html",
      "date": "March 8, 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    }
  ],
  "accelerates": [
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/research/2023/03/08/Extreme-Masking-for-Learning-Instance-and-Distributed-Visual-Representations.html",
      "date": "March 8, 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    }
  ],
  "probe": [
    {
      "title": "Extreme Masking for Learning Instance and Distributed Visual Representations",
      "url": "/research/2023/03/08/Extreme-Masking-for-Learning-Instance-and-Distributed-Visual-Representations.html",
      "date": "March 8, 2023",
      "summary": "The paper introduces ExtreMA, a method for learning visual representations by using high levels of token masking (75%-90%) for data augmentation. It employs self-attention and cross-attention blocks to learn spatial and holistic instance representations. Unlike methods that seek input invariance, ExtreMA focuses on capturing informative image variations. Its contributions include demonstrating the effectiveness of random masking for siamese learning, showing that extreme masking accelerates learning and enhances performance, and achieving superior linear probing and transfer results compared to previous model"
    }
  ],
  "feedforward": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/research/2023/09/18/Fast-Feedforward-Networks.html",
      "date": "September 18, 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    },
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "We introduce an algorithm designed to identify networks that are both simple and highly generalizable by searching for extensive areas of flat minima in the error function, where the error rate is relatively stable. These flat minima are associated with lower risks of overfitting according to MDL principles. Despite needing to calculate second-order derivatives, the algorithm maintains a complexity level comparable to backpropagation. We tested it on both feedforward and recurrent networks, and in stock market prediction tasks, it performed better than traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    }
  ],
  "break": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/research/2023/09/18/Fast-Feedforward-Networks.html",
      "date": "September 18, 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    }
  ],
  "link": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/research/2023/09/18/Fast-Feedforward-Networks.html",
      "date": "September 18, 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    }
  ],
  "layer": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/research/2023/09/18/Fast-Feedforward-Networks.html",
      "date": "September 18, 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    },
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/research/2019/07/26/DropAttention-A-Regularization-Method-for-Fully-Connected-Self-Attention-Networks.html",
      "date": "July 26, 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. Unlike existing dropout techniques for other neural network layers, DropAttention specifically targets the unique challenges of self-attention mechanisms. Our experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    },
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "Self-supervised transformer training methods have shown excellent results in various fields. Unlike previous models like masked autoencoders (MAE) which use a single normalization layer for the class token [CLS] and other tokens, we introduce a novel normalization technique that normalizes the [CLS] token and normal tokens separately. This approach aims to better capture their unique characteristics, improving performance in downstream tasks. Our findings indicate that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    },
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "We introduce a Fourier neural network (FNN) aligned with Fourier decomposition, utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design ensures easy integration with more complex networks for data processing tasks. The FNN's efficacy is demonstrated on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits include solution validity beyond the training scope, model interpretability, and ease of use."
    },
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is crucial for natural language processing tasks, but its training requires a learning rate warm-up stage, essential for performance yet slowing optimization and increasing hyper-parameter tuning needs. This paper investigates the importance of the warm-up stage and the impact of layer normalization placement. It uses mean field theory to demonstrate that the original Post-LN Transformer's design, with layer normalization between residual blocks, results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    }
  ],
  "cost": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/research/2023/09/18/Fast-Feedforward-Networks.html",
      "date": "September 18, 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    },
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/research/2017/08/23/A-downsampled-variant-of-imagenet-as-an-alternative-to-the-cifar-datasets.html",
      "date": "August 23, 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    },
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "Curriculum Learning by Dynamic Instance Hardness",
      "url": "/research/2020/12/06/Curriculum-Learning-by-Dynamic-Instance-Hardness.html",
      "date": "December 6, 2020",
      "summary": "This paper explores the adaptation of deep neural network (DNN) training using a method called dynamic instance hardness (DIH), which measures a sample's learning difficulty over time. By tracking the exponential moving average of a sample's hardness, DIH provides a stable indicator of learning progress. Early predictions of a sample's DIH allow for prioritizing more challenging samples and deprioritizing easier ones, leading to a DIH guided curriculum learning (DIHCL) approach. DIHCL enhances learning efficiency and model accuracy without extra computational costs, as it leverages data from the training process itself. Tested on 11 datasets, DIHCL surpasses traditional training methods and recent curriculum learning techniques in both efficiency and effectiveness. The implementation is publicly available."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    },
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This study investigates the use of high-quality, physically-based rendering and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle, enhancing image classification and semantic segmentation models. Synthetic images proved to be effective for augmenting limited real training data. However, models trained solely on synthetic images showed low accuracy on real validation images, which significantly improved when even small amounts of real data were included. Augmenting real data with synthetic images outperformed training on only real images. Moreover, pretraining models on synthetic data before transfer learning significantly reduced training costs, allowing most of the training to be completed upfront."
    },
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "This paper explores the effectiveness of using synthetic data, generated by graphics simulators, for pre-training models in computer vision, addressing the challenges posed by using real-image datasets like high costs and ethical concerns. It highlights that the performance of models on downstream tasks varies with different simulation parameters, suggesting a tailored approach to synthetic data generation for optimal results. The introduction of Task2Sim, a model that maps downstream task requirements to optimal simulation parameters, allows for the generation of synthetic pre-training data customized for specific tasks. Task2Sim demonstrates significant improvements in model performance on a wide range of tasks without additional training for new tasks, showing potential to rival pre-training with real images from Imagenet."
    },
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method, applies a single augmentation to each image and outperforms existing methods with minimal complexity and cost. We conducted extensive experiments to validate its performance against state-of-the-art methods across various image classification scenarios and explored its effectiveness through multiple ablation studies involving different augmentation spaces and methods. Our work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Highlighting a stagnation in automatic augmentation research, we conclude with proposed best practices for future advancements in the field."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    },
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-5.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-6.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    }
  ],
  "feedforward1": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/research/2023/09/18/Fast-Feedforward-Networks.html",
      "date": "September 18, 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    }
  ],
  "fff": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/research/2023/09/18/Fast-Feedforward-Networks.html",
      "date": "September 18, 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    }
  ],
  "log": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/research/2023/09/18/Fast-Feedforward-Networks.html",
      "date": "September 18, 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    }
  ],
  "x": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/research/2023/09/18/Fast-Feedforward-Networks.html",
      "date": "September 18, 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    }
  ],
  "mixture": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/research/2023/09/18/Fast-Feedforward-Networks.html",
      "date": "September 18, 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    }
  ],
  "expert": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/research/2023/09/18/Fast-Feedforward-Networks.html",
      "date": "September 18, 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    }
  ],
  "thank": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/research/2023/09/18/Fast-Feedforward-Networks.html",
      "date": "September 18, 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    }
  ],
  "push": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/research/2023/09/18/Fast-Feedforward-Networks.html",
      "date": "September 18, 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    },
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    }
  ],
  "neuron": [
    {
      "title": "Fast Feedforward Networks",
      "url": "/research/2023/09/18/Fast-Feedforward-Networks.html",
      "date": "September 18, 2023",
      "summary": "We break the linear link between the layer size and its inference cost by introducing the fast feedforward1 (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance"
    },
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    }
  ],
  "batch normalization": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "Rethinking Batch in BatchNorm",
      "url": "/research/2021/05/17/Rethinking-Batch-in-BatchNorm.html",
      "date": "May 17, 2021",
      "summary": "BatchNorm, essential in convolutional neural networks, behaves uniquely due to its batch-based operation, leading to performance issues in visual recognition tasks. This paper identifies these issues and suggests reevaluating the concept of batch in BatchNorm for better performance, aiming to guide researchers in its effective use."
    }
  ],
  "adopt": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    }
  ],
  "pervasiveness": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    }
  ],
  "reason": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    }
  ],
  "belief": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    }
  ],
  "stem": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    }
  ],
  "layers": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/research/2019/07/26/DropAttention-A-Regularization-Method-for-Fully-Connected-Self-Attention-Networks.html",
      "date": "July 26, 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. Unlike existing dropout techniques for other neural network layers, DropAttention specifically targets the unique challenges of self-attention mechanisms. Our experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    }
  ],
  "distribution": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/research/2023/06/30/Impact-of-Noise-on-Calibration-and-Generalisation-of-Neural-Networks.html",
      "date": "June 30, 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. We examine different types of noise, such as activation and input augmentation noise, in both in-distribution and out-of-distribution scenarios. Activation noise significantly improves generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "Self-supervised transformer training methods have shown excellent results in various fields. Unlike previous models like masked autoencoders (MAE) which use a single normalization layer for the class token [CLS] and other tokens, we introduce a novel normalization technique that normalizes the [CLS] token and normal tokens separately. This approach aims to better capture their unique characteristics, improving performance in downstream tasks. Our findings indicate that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks."
    },
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "We introduce the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. Our empirical evaluation across computer vision, natural language processing, and speech tasks shows that GELU offers performance improvements over ReLU and ELU activations."
    }
  ],
  "covariate": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    }
  ],
  "uncover": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    }
  ],
  "landscape": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    }
  ],
  "induce": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    }
  ],
  "behavior": [
    {
      "title": "How Does Batch Normalization Help Optimization?",
      "url": "/research/2019/04/15/How-Does-Batch-Normalization-Help-Optimization.html",
      "date": "April 15, 2019",
      "summary": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training"
    },
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "Recent studies have shown that Convolutional Neural Networks (CNNs), which were thought to recognize objects by learning complex shapes, are actually more biased towards recognizing textures. This finding, based on a series of tests comparing CNNs with human observers using images with conflicting texture and shape cues, contradicts the traditional understanding of how these networks process images. We discovered that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, when we trained a standard CNN architecture (ResNet-50) on a stylized version of ImageNet, designed to emphasize shape, the network's performance aligned more closely with human behavior. This training not only matched human performance in controlled experiments (involving 97 observers across 48,560 trials) but also enhanced object detection and robustness against image distortions, underscoring the benefits of shape-based representations in visual recognition systems."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "hippo": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    }
  ],
  "gap": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "We introduce a system that trains deep neural networks for object detection using synthetic images, leveraging domain randomization to manage real-world data variability. This involves randomizing simulator parameters—like lighting and object textures—in non-realistic ways, helping the network to identify essential object features. Our findings reveal that networks can achieve impressive performance with just synthetic data and further improve with real data fine-tuning. This suggests the potential of using cost-effective synthetic data for training, circumventing the challenges of acquiring vast amounts of real-world data or creating detailed synthetic environments. We validate our method through car bounding box detection on the KITTI dataset."
    },
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "Recent advancements in image transformers, notably in models such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, have narrowed the performance gap with traditional CNNs. These models are typically trained on large datasets like ImageNet-21k and then finetuned on ImageNet-1k. However, assessments often overlook Tiny ImageNet when evaluating transfer learning performance. This paper updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. The code is available at: https://github.com/ehuynh1106/TinyImageNet-Transformers"
    }
  ],
  "length": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    }
  ],
  "underperform": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    }
  ],
  "suffer": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "Deep neural networks, while effective in complex vision tasks, suffer from overfitting. Current image augmentation techniques, largely linear and hand-crafted like scale, flip, and color jitter, fail to produce sufficiently challenging examples. We introduce a new augmentation approach, Mask-Reconstruct Augmentation (MRA), leveraging self-supervised masked autoencoders to generate distorted inputs. This method, inspired by masked image modeling's success in self-supervised learning, uses nonlinear transformations for regularization. Our extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks. The code is available at https://github.com/haohang96/MRA."
    }
  ],
  "hardware": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    }
  ],
  "utilization": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "Self-paced curriculum learning (SCL) has been effective in various fields, employing easy-to-hard sampling based on data difficulty. However, in medical image analysis tasks like universal lesion detection, it faces challenges due to inaccurate difficulty estimation and the under-utilization of hard samples. This paper introduces a novel approach, mixed-order self-paced curriculum learning (Mo-SCL), which combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches to address these issues. Through theoretical analysis and experiments on the DeepLesion dataset, Mo-SCL is shown to enhance lesion detection accuracy in state-of-the-art methods without additional network modifications."
    }
  ],
  "h3": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    }
  ],
  "recall": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    }
  ],
  "comparison": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "narrow": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "Recent advancements in image transformers, notably in models such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, have narrowed the performance gap with traditional CNNs. These models are typically trained on large datasets like ImageNet-21k and then finetuned on ImageNet-1k. However, assessments often overlook Tiny ImageNet when evaluating transfer learning performance. This paper updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. The code is available at: https://github.com/ehuynh1106/TinyImageNet-Transformers"
    }
  ],
  "h3attention": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    }
  ],
  "openwebtext": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    }
  ],
  "flashconv": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    }
  ],
  "speeds": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "and": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "enables": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    }
  ],
  "perplexity": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "shot": [
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "url": "/research/2023/04/29/Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models.html",
      "date": "April 29, 2023",
      "summary": "This paper investigates the performance and efficiency gaps between State Space Models (SSMs) and attention mechanisms in language modeling. SSMs, despite scaling better with sequence length, underperform attention and suffer from poor hardware utilization. The study introduces a new SSM layer, H3, designed to improve recall and comparison across sequences, narrowing the performance gap with Transformers. Furthermore, a hybrid H3-attention model surpasses Transformer performance on OpenWebText. To enhance SSM training efficiency, the paper proposes FlashConv, a method that significantly speeds up processing and enables scaling of hybrid models, showing promising results against Transformers in both perplexity and few-shot learning tasks."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article offers the first extensive review of vision transformer techniques specifically applied to human action recognition, a field gaining interest for its wide applications. Termed action transformers, these methods are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also delves into optimizing spatio-temporal attention in transformers for longer sequences and examines different learning strategies, including self-supervised and zero-shot learning, with their respective loss functions. Additionally, the survey assesses progress in benchmark evaluation scores and discusses the challenges and future directions in this research area."
    }
  ],
  "ijepa": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    }
  ],
  "joint": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    }
  ],
  "embedding": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "Self-supervised transformer training methods have shown excellent results in various fields. Unlike previous models like masked autoencoders (MAE) which use a single normalization layer for the class token [CLS] and other tokens, we introduce a novel normalization technique that normalizes the [CLS] token and normal tokens separately. This approach aims to better capture their unique characteristics, improving performance in downstream tasks. Our findings indicate that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    }
  ],
  "generation": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "Dropout as Data Augmentation",
      "url": "/research/2016/01/08/Dropout-as-Data-Augmentation.html",
      "date": "January 8, 2016",
      "summary": "Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/research/2017/11/06/Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization.html",
      "date": "November 6, 2017",
      "summary": "In this study, we improve upon the fast stylization method by Ulyanov et al. (2016) through a minor yet impactful modification: substituting batch normalization with instance normalization during both training and testing. This adjustment significantly enhances the quality of generated images and enables the training of efficient real-time image generation architectures. The code and full paper are available online."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "Deep neural networks, while effective in complex vision tasks, suffer from overfitting. Current image augmentation techniques, largely linear and hand-crafted like scale, flip, and color jitter, fail to produce sufficiently challenging examples. We introduce a new augmentation approach, Mask-Reconstruct Augmentation (MRA), leveraging self-supervised masked autoencoders to generate distorted inputs. This method, inspired by masked image modeling's success in self-supervised learning, uses nonlinear transformations for regularization. Our extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks. The code is available at https://github.com/haohang96/MRA."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "Semi-supervised learning (SSL) utilizes unlabeled data to boost model performance but often at the expense of increased complexity. This paper introduces FixMatch, a streamlined SSL algorithm that simplifies existing methods. FixMatch uses a model's predictions on weakly-augmented unlabeled images to generate pseudo-labels, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. Remarkably, FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness. The code is publicly available."
    },
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper investigates how neural networks generalize on small, algorithmically generated datasets, focusing on data efficiency, memorization, generalization, and learning speed. It highlights instances where neural networks grok patterns, significantly improving their generalization ability from mere chance to perfect accuracy, even beyond the point of overfitting. Additionally, the study finds that generalization on smaller datasets demands more optimization. It posits that such datasets are ideal for exploring the enigmatic phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    },
    {
      "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "Diffusion models, effective for generating high-dimensional perceptual data like images, lack established standard practices. This study enhances noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Our large-scale study demonstrates superior performance over established diffusion methods in high-resolution text-to-image synthesis. We introduce a transformer-based architecture for text-to-image generation, improving text comprehension, typography, and human preference ratings. Our findings show predictable scaling trends, with lower validation loss correlating with improved synthesis. The largest models outperform state-of-the-art approaches, and our data, code, and model weights will be publicly accessible."
    },
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This study investigates the use of high-quality, physically-based rendering and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle, enhancing image classification and semantic segmentation models. Synthetic images proved to be effective for augmenting limited real training data. However, models trained solely on synthetic images showed low accuracy on real validation images, which significantly improved when even small amounts of real data were included. Augmenting real data with synthetic images outperformed training on only real images. Moreover, pretraining models on synthetic data before transfer learning significantly reduced training costs, allowing most of the training to be completed upfront."
    },
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "This paper explores the effectiveness of using synthetic data, generated by graphics simulators, for pre-training models in computer vision, addressing the challenges posed by using real-image datasets like high costs and ethical concerns. It highlights that the performance of models on downstream tasks varies with different simulation parameters, suggesting a tailored approach to synthetic data generation for optimal results. The introduction of Task2Sim, a model that maps downstream task requirements to optimal simulation parameters, allows for the generation of synthetic pre-training data customized for specific tasks. Task2Sim demonstrates significant improvements in model performance on a wide range of tasks without additional training for new tasks, showing potential to rival pre-training with real images from Imagenet."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "scalability": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    }
  ],
  "depth": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "Learning RelU Networks To High Uniform Accuracy Is Intractable",
      "url": "/research/2023/02/28/Learning-Relu-Networks-To-High-Uniform-Accuracy-Is-Intractable.html",
      "date": "February 28, 2023",
      "summary": "Statistical learning theory provides guidelines on the number of training samples needed for achieving desired accuracy in learning problems, especially emphasizing generalization error. However, this is not always adequate, particularly in security-sensitive areas or computational sciences, where uniform accuracy across all inputs is necessary. This paper quantifies the training samples required for uniform accuracy in learning problems involving ReLU neural networks, revealing that the number of samples needed exponentially increases with the network's depth and input dimension."
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "Semi-supervised learning (SSL) utilizes unlabeled data to boost model performance but often at the expense of increased complexity. This paper introduces FixMatch, a streamlined SSL algorithm that simplifies existing methods. FixMatch uses a model's predictions on weakly-augmented unlabeled images to generate pseudo-labels, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. Remarkably, FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness. The code is publicly available."
    },
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "Monocular depth estimation's success hinges on large, diverse training datasets. Acquiring dense ground-truth depth is challenging, leading to diverse datasets with unique biases. We introduce tools for training with multiple datasets, despite incompatible annotations. Our approach includes a robust training objective unaffected by depth range and scale variations, employs principled multi-objective learning for data integration from various sources, and emphasizes the importance of pretraining encoders on auxiliary tasks. We tested our methods using five datasets, including 3D films, a novel, extensive data source. Through zero-shot cross-dataset transfer, we demonstrate our approach's superior generalization, significantly outperforming existing methods and establishing a new benchmark in monocular depth estimation."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    }
  ],
  "huge14": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    }
  ],
  "a100": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    }
  ],
  "gpus": [
    {
      "title": "I-JEPA Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
      "url": "/research/2023/04/13/I-JEPA-Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture.html",
      "date": "April 13, 2023",
      "summary": "This paper introduces the Image-based Joint-Embedding Predictive Architecture (I-JEPA) for learning semantic image representations without hand-crafted data augmentations. By predicting the representations of different image blocks from a single context block, I-JEPA generates meaningful representations using a strategic masking approach. When applied with Vision Transformers, I-JEPA shows scalability and efficiency, achieving strong performance in various tasks like classification and depth prediction on ImageNet with a ViT-Huge/14 and 16 A100 GPUs in under 72 hours."
    },
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    }
  ],
  "plasticity": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    }
  ],
  "environment": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "We introduce a system that trains deep neural networks for object detection using synthetic images, leveraging domain randomization to manage real-world data variability. This involves randomizing simulator parameters—like lighting and object textures—in non-realistic ways, helping the network to identify essential object features. Our findings reveal that networks can achieve impressive performance with just synthetic data and further improve with real data fine-tuning. This suggests the potential of using cost-effective synthetic data for training, circumventing the challenges of acquiring vast amounts of real-world data or creating detailed synthetic environments. We validate our method through car bounding box detection on the KITTI dataset."
    }
  ],
  "forget": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    }
  ],
  "know": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    }
  ],
  "mnist": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    }
  ],
  "fall": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    }
  ],
  "l2regularization": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    }
  ],
  "perturbation": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    },
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/research/2021/05/03/Robust-and-Generalizable-Visual-Representation-Learning-via-Random-Convolutions.html",
      "date": "May 3, 2021",
      "summary": "This study demonstrates that deep neural networks' vulnerability to texture shifts and perturbations can be mitigated by using random convolutions for data augmentation. Random convolutions, which approximately preserve shape while distorting local textures, create an infinite variety of new domains. We tested using these convolutions as new images or in combination with original images during training. Our method significantly enhances performance on unseen domains and domain generalization benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods. Additionally, it provides a more robust pretrained visual representation for downstream tasks."
    },
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/research/2019/04/27/Benchmarking-Neural-Network-Robustness-to-Common-Corruptions-and-Surface-Variations.html",
      "date": "April 27, 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/research/2021/08/31/Universal-Adversarial-Robustness-of-Texture-and-Shape-biased-Models.html",
      "date": "August 31, 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    }
  ],
  "alter": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    }
  ],
  "reinitialize": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    }
  ],
  "promise": [
    {
      "title": "Loss of Plasticity in Deep Continual Learning",
      "url": "/research/2023/08/18/Loss-of-Plasticity-in-Deep-Continual-Learning.html",
      "date": "August 18, 2023",
      "summary": "Modern deep-learning systems, designed for one-time training, struggle in continual-learning environments where training is ongoing. They not only forget previous examples but also lose the ability to learn new ones, known as loss of plasticity. This was demonstrated using MNIST and ImageNet datasets adapted for continual learning. On the 2000th task in ImageNet, accuracy fell from 89% to 77%, similar to a linear network. Various architectures and techniques were tested, with L2-regularization and weight perturbation somewhat mitigating the issue. A new approach, continual backpropagation, which slightly alters backpropagation to periodically reinitialize less-used units, shows promise in maintaining learning ability indefinitely. "
    }
  ],
  "mamba": [
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    }
  ],
  "inefficiency": [
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    }
  ],
  "reasoning": [
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    }
  ],
  "improving": [
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    }
  ],
  "domain": [
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    },
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/research/2021/05/03/Robust-and-Generalizable-Visual-Representation-Learning-via-Random-Convolutions.html",
      "date": "May 3, 2021",
      "summary": "This study demonstrates that deep neural networks' vulnerability to texture shifts and perturbations can be mitigated by using random convolutions for data augmentation. Random convolutions, which approximately preserve shape while distorting local textures, create an infinite variety of new domains. We tested using these convolutions as new images or in combination with original images during training. Our method significantly enhances performance on unseen domains and domain generalization benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods. Additionally, it provides a more robust pretrained visual representation for downstream tasks."
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    },
    {
      "title": "Dropout as Data Augmentation",
      "url": "/research/2016/01/08/Dropout-as-Data-Augmentation.html",
      "date": "January 8, 2016",
      "summary": "Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions for neural networks in reinforcement learning, the sigmoid-weighted linear unit (SiLU) and its derivative (dSiLU), and challenges the need for experience replay and separate target networks in deep reinforcement learning. By employing on-policy learning with eligibility traces and softmax action selection, the study achieves state-of-the-art results in stochastic SZ-Tetris and a small-board Tetris using TD(λ) learning and shallow dSiLU network agents. Furthermore, it outperforms the DQN algorithm in the Atari 2600 domain with a deep Sarsa(λ) agent utilizing SiLU and dSiLU hidden units, suggesting a competitive alternative to traditional DQN approaches."
    },
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This study investigates the use of high-quality, physically-based rendering and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle, enhancing image classification and semantic segmentation models. Synthetic images proved to be effective for augmenting limited real training data. However, models trained solely on synthetic images showed low accuracy on real validation images, which significantly improved when even small amounts of real data were included. Augmenting real data with synthetic images outperformed training on only real images. Moreover, pretraining models on synthetic data before transfer learning significantly reduced training costs, allowing most of the training to be completed upfront."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "We introduce a system that trains deep neural networks for object detection using synthetic images, leveraging domain randomization to manage real-world data variability. This involves randomizing simulator parameters—like lighting and object textures—in non-realistic ways, helping the network to identify essential object features. Our findings reveal that networks can achieve impressive performance with just synthetic data and further improve with real data fine-tuning. This suggests the potential of using cost-effective synthetic data for training, circumventing the challenges of acquiring vast amounts of real-world data or creating detailed synthetic environments. We validate our method through car bounding box detection on the KITTI dataset."
    },
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "This paper introduces VideoMAE, a data-efficient approach for self-supervised video pre-training (SSVP) that utilizes video masked autoencoders with a novel, high-ratio video tube masking technique. This method, inspired by ImageMAE, challenges the model with video reconstruction tasks to improve video representation learning. Key findings include: (1) High masking ratios (90% to 95%) are effective due to video's temporal redundancy, (2) VideoMAE performs well on small datasets (~3k-4k videos) without extra data, highlighting the importance of high-level structure learning, and (3) Data quality is more crucial than quantity for SSVP, with domain shift between pre-training and target datasets being significant. Remarkably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone, without extra data. The code is available online."
    },
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    }
  ],
  "audio": [
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    },
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    }
  ],
  "genomic": [
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "url": "/research/2023/12/01/Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces.html",
      "date": "December 1, 2023",
      "summary": "Transformer-based models, crucial for advancements in deep learning, suffer from inefficiency in processing long sequences. Recent models attempting to address this issue haven't matched Transformers' success in key areas like language due to their poor content-based reasoning. Our work enhances these models by making selective state space models (SSMs) input-dependent, improving handling of sequences and designing a hardware-friendly algorithm for better performance. The new architecture, Mamba, achieves superior speed and scalability, outperforming traditional Transformers in various domains including language, audio, and genomics, especially in tasks involving extremely long sequences."
    }
  ],
  "mambabyte": [
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/research/2024/01/24/MambaByte-Token-free-Selective-State-Space-Model.html",
      "date": "January 24, 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    }
  ],
  "operate": [
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/research/2024/01/24/MambaByte-Token-free-Selective-State-Space-Model.html",
      "date": "January 24, 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "byte": [
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/research/2024/01/24/MambaByte-Token-free-Selective-State-Space-Model.html",
      "date": "January 24, 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    }
  ],
  "subword": [
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/research/2024/01/24/MambaByte-Token-free-Selective-State-Space-Model.html",
      "date": "January 24, 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    }
  ],
  "tokenization": [
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/research/2024/01/24/MambaByte-Token-free-Selective-State-Space-Model.html",
      "date": "January 24, 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    }
  ],
  "bias": [
    {
      "title": "MambaByte: Token-free Selective State Space Model",
      "url": "/research/2024/01/24/MambaByte-Token-free-Selective-State-Space-Model.html",
      "date": "January 24, 2024",
      "summary": "MambaByte, a token-free language model, effectively operates on byte sequences without subword tokenization bias, offering computational efficiency and outperforming state-of-the-art subword models. Its linear scaling and fast inference demonstrate its potential for token-free language modeling."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/research/2021/08/31/Universal-Adversarial-Robustness-of-Texture-and-Shape-biased-Models.html",
      "date": "August 31, 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "Recent studies have shown that Convolutional Neural Networks (CNNs), which were thought to recognize objects by learning complex shapes, are actually more biased towards recognizing textures. This finding, based on a series of tests comparing CNNs with human observers using images with conflicting texture and shape cues, contradicts the traditional understanding of how these networks process images. We discovered that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, when we trained a standard CNN architecture (ResNet-50) on a stylized version of ImageNet, designed to emphasize shape, the network's performance aligned more closely with human behavior. This training not only matched human performance in controlled experiments (involving 97 observers across 48,560 trials) but also enhanced object detection and robustness against image distortions, underscoring the benefits of shape-based representations in visual recognition systems."
    }
  ],
  "patchdropout": [
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    }
  ],
  "economize": [
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    }
  ],
  "dropout": [
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/research/2019/07/26/DropAttention-A-Regularization-Method-for-Fully-Connected-Self-Attention-Networks.html",
      "date": "July 26, 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. Unlike existing dropout techniques for other neural network layers, DropAttention specifically targets the unique challenges of self-attention mechanisms. Our experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    },
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/research/2014/01/01/Dropout-A-Simple-Way-to-Prevent-Neural-Networks-from-Overfitting.html",
      "date": "January 1, 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    },
    {
      "title": "Dropout as Data Augmentation",
      "url": "/research/2016/01/08/Dropout-as-Data-Augmentation.html",
      "date": "January 8, 2016",
      "summary": "Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    },
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/research/2022/10/10/Turbo-Training-with-Token-Dropout.html",
      "date": "October 10, 2022",
      "summary": "This paper introduces Turbo training, an efficient training method for video-related tasks using Transformers. Turbo training offers three major contributions: Firstly, it presents a simple yet versatile training paradigm applicable to multiple video tasks. Secondly, it demonstrates Turbo training's effectiveness across action classification, video-language representation learning, and long-video activity classification, achieving competitive performance with up to 4× speed-up and reduced memory usage. Thirdly, it enables long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous methods that were resource-intensive."
    }
  ],
  "resolution": [
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/research/2017/08/23/A-downsampled-variant-of-imagenet-as-an-alternative-to-the-cifar-datasets.html",
      "date": "August 23, 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    },
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "We introduce DiffBIR, a framework using pretrained text-to-image diffusion models for blind image restoration. It consists of a two-stage pipeline: initially, we pretrain a restoration module on various degradations to enhance real-world applicability. Then, we use latent diffusion models for realistic restoration, including a novel injective modulation sub-network, LAControlNet, for fine-tuning, and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, we provide a controllable module for users to adjust quality and fidelity during the denoising process. Our extensive tests show DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets. The code is available online."
    },
    {
      "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "Diffusion models, effective for generating high-dimensional perceptual data like images, lack established standard practices. This study enhances noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Our large-scale study demonstrates superior performance over established diffusion methods in high-resolution text-to-image synthesis. We introduce a transformer-based architecture for text-to-image generation, improving text comprehension, typography, and human preference ratings. Our findings show predictable scaling trends, with lower validation loss correlating with improved synthesis. The largest models outperform state-of-the-art approaches, and our data, code, and model weights will be publicly accessible."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    }
  ],
  "requiring": [
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    }
  ],
  "drop": [
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    }
  ],
  "csaw": [
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    }
  ],
  "budget": [
    {
      "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
      "url": "/research/2022/10/04/PatchDropout-Economizing-Vision-Transformers-Using-Patch-Dropou.html",
      "date": "October 4, 2022",
      "summary": "Vision transformers (ViTs) can outperform CNNs in various vision tasks but are limited by high computational and memory needs, especially for high-resolution images like medical image classification. Efforts to optimize ViTs are complex, requiring significant changes. However, we introduce PatchDropout, a simple technique that drops random image patches during training, cutting computational and memory demands by at least 50% on datasets like IMAGENET and even more with larger images. On the high-resolution CSAW medical dataset, PatchDropout achieves a 5× reduction in resources and improves performance, enabling more efficient model scaling and parameter tuning within fixed computational or memory budgets"
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "Vision Transformers (ViT) excel in various vision tasks, like image classification, object detection, and semantic segmentation, but require more model regularization or data augmentation (AugReg) than convolutional neural networks, especially with smaller datasets. Through a comprehensive study, we explore how training data amount, AugReg, model size, and compute budget interact. Our findings indicate that using more compute and AugReg can achieve the same performance as training with significantly more data. Specifically, we demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    }
  ],
  "vulnerability": [
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/research/2021/05/03/Robust-and-Generalizable-Visual-Representation-Learning-via-Random-Convolutions.html",
      "date": "May 3, 2021",
      "summary": "This study demonstrates that deep neural networks' vulnerability to texture shifts and perturbations can be mitigated by using random convolutions for data augmentation. Random convolutions, which approximately preserve shape while distorting local textures, create an infinite variety of new domains. We tested using these convolutions as new images or in combination with original images during training. Our method significantly enhances performance on unseen domains and domain generalization benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods. Additionally, it provides a more robust pretrained visual representation for downstream tasks."
    }
  ],
  "texture": [
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/research/2021/05/03/Robust-and-Generalizable-Visual-Representation-Learning-via-Random-Convolutions.html",
      "date": "May 3, 2021",
      "summary": "This study demonstrates that deep neural networks' vulnerability to texture shifts and perturbations can be mitigated by using random convolutions for data augmentation. Random convolutions, which approximately preserve shape while distorting local textures, create an infinite variety of new domains. We tested using these convolutions as new images or in combination with original images during training. Our method significantly enhances performance on unseen domains and domain generalization benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods. Additionally, it provides a more robust pretrained visual representation for downstream tasks."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/research/2021/08/31/Universal-Adversarial-Robustness-of-Texture-and-Shape-biased-Models.html",
      "date": "August 31, 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "Recent studies have shown that Convolutional Neural Networks (CNNs), which were thought to recognize objects by learning complex shapes, are actually more biased towards recognizing textures. This finding, based on a series of tests comparing CNNs with human observers using images with conflicting texture and shape cues, contradicts the traditional understanding of how these networks process images. We discovered that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, when we trained a standard CNN architecture (ResNet-50) on a stylized version of ImageNet, designed to emphasize shape, the network's performance aligned more closely with human behavior. This training not only matched human performance in controlled experiments (involving 97 observers across 48,560 trials) but also enhanced object detection and robustness against image distortions, underscoring the benefits of shape-based representations in visual recognition systems."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "We introduce a system that trains deep neural networks for object detection using synthetic images, leveraging domain randomization to manage real-world data variability. This involves randomizing simulator parameters—like lighting and object textures—in non-realistic ways, helping the network to identify essential object features. Our findings reveal that networks can achieve impressive performance with just synthetic data and further improve with real data fine-tuning. This suggests the potential of using cost-effective synthetic data for training, circumventing the challenges of acquiring vast amounts of real-world data or creating detailed synthetic environments. We validate our method through car bounding box detection on the KITTI dataset."
    }
  ],
  "shape": [
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/research/2021/05/03/Robust-and-Generalizable-Visual-Representation-Learning-via-Random-Convolutions.html",
      "date": "May 3, 2021",
      "summary": "This study demonstrates that deep neural networks' vulnerability to texture shifts and perturbations can be mitigated by using random convolutions for data augmentation. Random convolutions, which approximately preserve shape while distorting local textures, create an infinite variety of new domains. We tested using these convolutions as new images or in combination with original images during training. Our method significantly enhances performance on unseen domains and domain generalization benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods. Additionally, it provides a more robust pretrained visual representation for downstream tasks."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/research/2021/08/31/Universal-Adversarial-Robustness-of-Texture-and-Shape-biased-Models.html",
      "date": "August 31, 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "Recent studies have shown that Convolutional Neural Networks (CNNs), which were thought to recognize objects by learning complex shapes, are actually more biased towards recognizing textures. This finding, based on a series of tests comparing CNNs with human observers using images with conflicting texture and shape cues, contradicts the traditional understanding of how these networks process images. We discovered that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, when we trained a standard CNN architecture (ResNet-50) on a stylized version of ImageNet, designed to emphasize shape, the network's performance aligned more closely with human behavior. This training not only matched human performance in controlled experiments (involving 97 observers across 48,560 trials) but also enhanced object detection and robustness against image distortions, underscoring the benefits of shape-based representations in visual recognition systems."
    },
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    }
  ],
  "distort": [
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/research/2021/05/03/Robust-and-Generalizable-Visual-Representation-Learning-via-Random-Convolutions.html",
      "date": "May 3, 2021",
      "summary": "This study demonstrates that deep neural networks' vulnerability to texture shifts and perturbations can be mitigated by using random convolutions for data augmentation. Random convolutions, which approximately preserve shape while distorting local textures, create an infinite variety of new domains. We tested using these convolutions as new images or in combination with original images during training. Our method significantly enhances performance on unseen domains and domain generalization benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods. Additionally, it provides a more robust pretrained visual representation for downstream tasks."
    }
  ],
  "variety": [
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/research/2021/05/03/Robust-and-Generalizable-Visual-Representation-Learning-via-Random-Convolutions.html",
      "date": "May 3, 2021",
      "summary": "This study demonstrates that deep neural networks' vulnerability to texture shifts and perturbations can be mitigated by using random convolutions for data augmentation. Random convolutions, which approximately preserve shape while distorting local textures, create an infinite variety of new domains. We tested using these convolutions as new images or in combination with original images during training. Our method significantly enhances performance on unseen domains and domain generalization benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods. Additionally, it provides a more robust pretrained visual representation for downstream tasks."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "sketch": [
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/research/2021/05/03/Robust-and-Generalizable-Visual-Representation-Learning-via-Random-Convolutions.html",
      "date": "May 3, 2021",
      "summary": "This study demonstrates that deep neural networks' vulnerability to texture shifts and perturbations can be mitigated by using random convolutions for data augmentation. Random convolutions, which approximately preserve shape while distorting local textures, create an infinite variety of new domains. We tested using these convolutions as new images or in combination with original images during training. Our method significantly enhances performance on unseen domains and domain generalization benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods. Additionally, it provides a more robust pretrained visual representation for downstream tasks."
    }
  ],
  "downstream": [
    {
      "title": "Robust and Generalizable Visual Representation Learning via Random Convolutions",
      "url": "/research/2021/05/03/Robust-and-Generalizable-Visual-Representation-Learning-via-Random-Convolutions.html",
      "date": "May 3, 2021",
      "summary": "This study demonstrates that deep neural networks' vulnerability to texture shifts and perturbations can be mitigated by using random convolutions for data augmentation. Random convolutions, which approximately preserve shape while distorting local textures, create an infinite variety of new domains. We tested using these convolutions as new images or in combination with original images during training. Our method significantly enhances performance on unseen domains and domain generalization benchmarks, including a substantial improvement in generalizing to sketch domains over current state-of-the-art methods. Additionally, it provides a more robust pretrained visual representation for downstream tasks."
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "Self-supervised transformer training methods have shown excellent results in various fields. Unlike previous models like masked autoencoders (MAE) which use a single normalization layer for the class token [CLS] and other tokens, we introduce a novel normalization technique that normalizes the [CLS] token and normal tokens separately. This approach aims to better capture their unique characteristics, improving performance in downstream tasks. Our findings indicate that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks."
    },
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "This paper explores the effectiveness of using synthetic data, generated by graphics simulators, for pre-training models in computer vision, addressing the challenges posed by using real-image datasets like high costs and ethical concerns. It highlights that the performance of models on downstream tasks varies with different simulation parameters, suggesting a tailored approach to synthetic data generation for optimal results. The introduction of Task2Sim, a model that maps downstream task requirements to optimal simulation parameters, allows for the generation of synthetic pre-training data customized for specific tasks. Task2Sim demonstrates significant improvements in model performance on a wide range of tasks without additional training for new tasks, showing potential to rival pre-training with real images from Imagenet."
    }
  ],
  "mode": [
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "Self-Paced Curriculum Learning",
      "url": "/research/2015/01/01/Self-Paced-Curriculum-Learning.html",
      "date": "January 1, 2015",
      "summary": "Curriculum learning (CL) and self-paced learning (SPL) are methods inspired by the incremental complexity of human and animal learning, with CL using a fixed curriculum based on prior knowledge and SPL adjusting dynamically to the learner's pace but struggling with prior knowledge and overfitting. This paper introduces a unified framework called self-paced curriculum learning (SPCL), blending both methods' strengths by considering prior knowledge and ongoing learning progress through a concise optimization problem. SPCL mimics a collaborative instructor-student learning mode, showing empirical advantages in two tasks."
    }
  ],
  "vim": [
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    }
  ],
  "backbone": [
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "This paper introduces VideoMAE, a data-efficient approach for self-supervised video pre-training (SSVP) that utilizes video masked autoencoders with a novel, high-ratio video tube masking technique. This method, inspired by ImageMAE, challenges the model with video reconstruction tasks to improve video representation learning. Key findings include: (1) High masking ratios (90% to 95%) are effective due to video's temporal redundancy, (2) VideoMAE performs well on small datasets (~3k-4k videos) without extra data, highlighting the importance of high-level structure learning, and (3) Data quality is more crucial than quantity for SSVP, with domain shift between pre-training and target datasets being significant. Remarkably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone, without extra data. The code is available online."
    }
  ],
  "position": [
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    }
  ],
  "sensitivity": [
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    }
  ],
  "coco": [
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    }
  ],
  "deit": [
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "Training data-efficient image transformers & distillation through attention",
      "url": "/research/2021/01/15/Training-data-efficient-image-transformers-&-distillation-through-attention.html",
      "date": "January 15, 2021",
      "summary": "This study introduces DeiT, competitive vision transformers trained solely on ImageNet without convolutions, achieving up to 83.1% top-1 accuracy on ImageNet, using a single computer in under 3 days. Notably, it presents a unique teacher-student strategy for transformers, leveraging a distillation token for efficient learning from a convolutional network teacher. This method results in performance on par with convolutional networks, achieving up to 85.2% accuracy on ImageNet and demonstrating effective transferability to other tasks. The authors provide access to their code and models."
    },
    {
      "title": "DeiT III: Revenge of the ViT",
      "url": "/research/2023/01/18/DeiT-III-Revenge-of-the-ViT.html",
      "date": "January 18, 2023",
      "summary": "This paper explores the supervised training of Vision Transformers (ViTs), using a simplified training approach adapted from ResNet-50 that includes a novel data-augmentation method with just 3 augmentations. The study demonstrates that this method significantly improves ViTs' performance in image classification, transfer learning, and semantic segmentation over previous supervised training techniques. Moreover, it shows ViTs' performance can match newer architectures, providing a new benchmark for evaluating self-supervised methods on ViTs."
    },
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "Recent advancements in image transformers, notably in models such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, have narrowed the performance gap with traditional CNNs. These models are typically trained on large datasets like ImageNet-21k and then finetuned on ImageNet-1k. However, assessments often overlook Tiny ImageNet when evaluating transfer learning performance. This paper updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. The code is available at: https://github.com/ehuynh1106/TinyImageNet-Transformers"
    }
  ],
  "gpu": [
    {
      "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Mode",
      "url": "/research/2024/02/10/Vision-Mamba-Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model.html",
      "date": "February 10, 2024",
      "summary": "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8× faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    },
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "vmamba": [
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    }
  ],
  "offering": [
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    }
  ],
  "fit": [
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    }
  ],
  "fields": [
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "Self-supervised transformer training methods have shown excellent results in various fields. Unlike previous models like masked autoencoders (MAE) which use a single normalization layer for the class token [CLS] and other tokens, we introduce a novel normalization technique that normalizes the [CLS] token and normal tokens separately. This approach aims to better capture their unique characteristics, improving performance in downstream tasks. Our findings indicate that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks."
    },
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "Self-paced curriculum learning (SCL) has been effective in various fields, employing easy-to-hard sampling based on data difficulty. However, in medical image analysis tasks like universal lesion detection, it faces challenges due to inaccurate difficulty estimation and the under-utilization of hard samples. This paper introduces a novel approach, mixed-order self-paced curriculum learning (Mo-SCL), which combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches to address these issues. Through theoretical analysis and experiments on the DeepLesion dataset, Mo-SCL is shown to enhance lesion detection accuracy in state-of-the-art methods without additional network modifications."
    }
  ],
  "addition": [
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    },
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "scan": [
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    }
  ],
  "csm": [
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    }
  ],
  "traversal": [
    {
      "title": "VMamba: Visual State Space Model",
      "url": "/research/2024/01/18/VMamba-Visual-State-Space-Model.html",
      "date": "January 18, 2024",
      "summary": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are leading models for visual representation learning, with CNNs being scalable and ViTs offering superior fitting despite higher complexity. We introduce the Visual State Space Model (VMamba), inspired by state space models, to achieve linear complexity while maintaining global receptive fields. The addition of a Cross-Scan Module (CSM) addresses direction-sensitivity, allowing effective spatial domain traversal. Extensive tests show VMamba's effectiveness in visual perception tasks, especially at higher resolutions, with source code available online"
    }
  ],
  "downsample": [
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/research/2017/08/23/A-downsampled-variant-of-imagenet-as-an-alternative-to-the-cifar-datasets.html",
      "date": "August 23, 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    }
  ],
  "imagenet32x32": [
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/research/2017/08/23/A-downsampled-variant-of-imagenet-as-an-alternative-to-the-cifar-datasets.html",
      "date": "August 23, 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    }
  ],
  "variants": [
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/research/2017/08/23/A-downsampled-variant-of-imagenet-as-an-alternative-to-the-cifar-datasets.html",
      "date": "August 23, 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    },
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "Gated Linear Units (GLUs) involve the component-wise product of two linear projections, with one undergoing a sigmoid function. Exploring GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers, we find certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    }
  ],
  "imagenet64x64": [
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/research/2017/08/23/A-downsampled-variant-of-imagenet-as-an-alternative-to-the-cifar-datasets.html",
      "date": "August 23, 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    }
  ],
  "imagenet16x16": [
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/research/2017/08/23/A-downsampled-variant-of-imagenet-as-an-alternative-to-the-cifar-datasets.html",
      "date": "August 23, 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    }
  ],
  "class": [
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/research/2017/08/23/A-downsampled-variant-of-imagenet-as-an-alternative-to-the-cifar-datasets.html",
      "date": "August 23, 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "Self-supervised transformer training methods have shown excellent results in various fields. Unlike previous models like masked autoencoders (MAE) which use a single normalization layer for the class token [CLS] and other tokens, we introduce a novel normalization technique that normalizes the [CLS] token and normal tokens separately. This approach aims to better capture their unique characteristics, improving performance in downstream tasks. Our findings indicate that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks."
    },
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "Temporal action localization (TAL) is crucial for video understanding but is hindered by benchmarks that rely on coarse action classes, leading to model overfitting and ambiguous annotations. Addressing these issues, we introduce FineAction, a large-scale, fine-grained video dataset with 103K instances across 106 action categories in 17K videos. FineAction offers a diverse, densely annotated dataset with co-occurring actions, posing new challenges for TAL. We evaluated popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and proposed a baseline method achieving a 13.17% mAP. FineAction aims to propel TAL research forward and is accessible online."
    },
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "The limited number of videos in current action classification datasets like UCF-101 and HMDB-51 hinders the identification of effective video architectures due to similar performance across small-scale benchmarks. This study re-examines top architectures using the new, significantly larger Kinetics Human Action Video dataset, featuring 400 classes and over 400 clips per class from challenging YouTube videos. It analyzes the impact of this dataset on the performance of existing architectures and the benefits of pre-training on Kinetics. Additionally, the paper introduces the Two-Stream Inflated 3D ConvNet (I3D), an advancement that expands 2D ConvNet designs into 3D for enhanced video feature extraction. This model, pre-trained on Kinetics, sets new benchmarks in action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    },
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "Recent advancements in image transformers, notably in models such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, have narrowed the performance gap with traditional CNNs. These models are typically trained on large datasets like ImageNet-21k and then finetuned on ImageNet-1k. However, assessments often overlook Tiny ImageNet when evaluating transfer learning performance. This paper updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. The code is available at: https://github.com/ehuynh1106/TinyImageNet-Transformers"
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-5.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-6.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    }
  ],
  "hyperparameters": [
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/research/2017/08/23/A-downsampled-variant-of-imagenet-as-an-alternative-to-the-cifar-datasets.html",
      "date": "August 23, 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    }
  ],
  "characteristic": [
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/research/2017/08/23/A-downsampled-variant-of-imagenet-as-an-alternative-to-the-cifar-datasets.html",
      "date": "August 23, 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "Self-supervised transformer training methods have shown excellent results in various fields. Unlike previous models like masked autoencoders (MAE) which use a single normalization layer for the class token [CLS] and other tokens, we introduce a novel normalization technique that normalizes the [CLS] token and normal tokens separately. This approach aims to better capture their unique characteristics, improving performance in downstream tasks. Our findings indicate that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks."
    }
  ],
  "script": [
    {
      "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
      "url": "/research/2017/08/23/A-downsampled-variant-of-imagenet-as-an-alternative-to-the-cifar-datasets.html",
      "date": "August 23, 2017",
      "summary": "Due to the high cost of experiments on the original ImageNet, we propose a downsampled version, ImageNet32x32, and its variants, ImageNet64x64 and ImageNet16x16, which maintain the same number of classes and images but with reduced resolution. This approach significantly speeds up experiments while preserving similar optimal hyperparameters characteristics. The datasets and scripts are available online"
    }
  ],
  "cause": [
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    }
  ],
  "slows": [
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    }
  ],
  "initialization": [
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "Neural networks parameterized for continuous, differentiable signal representations provide a promising paradigm but struggle with modeling fine detail and representing spatial and temporal derivatives critical for signals defined by partial differential equations. We introduce sinusoidal representation networks (SIRENs), utilizing periodic activation functions to effectively capture complex natural signals and their derivatives. Our analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. We also showcase SIRENs' application in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations, and extend their use with hypernetworks to learn priors for SIREN functions. A video overview and full applications are available on our project website."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    }
  ],
  "nonlinearity": [
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    }
  ],
  "improves": [
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    }
  ],
  "remove": [
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    }
  ],
  "ensemble": [
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/research/2021/08/31/Universal-Adversarial-Robustness-of-Texture-and-Shape-biased-Models.html",
      "date": "August 31, 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    }
  ],
  "record": [
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    }
  ],
  "surpassing": [
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    }
  ],
  "human": [
    {
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "url": "/research/2015/03/02/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift.html",
      "date": "March 2, 2015",
      "summary": "Training Deep Neural Networks is challenging due to the changing input distribution at each layer, caused by updates in previous layer parameters. This issue, known as internal covariate shift, slows training, necessitating lower learning rates and careful initialization, especially with models using saturating nonlinearities. By normalizing layer inputs for each mini-batch, our Batch Normalization method significantly improves training efficiency, allowing for higher learning rates, reducing the need for meticulous initialization, and sometimes removing the need for Dropout regularization. Applied to a leading image classification model, it reached the same accuracy with 14 times fewer steps and surpassed the original model. Moreover, an ensemble of batch-normalized networks set a new record on ImageNet classification, achieving a top-5 validation error of 4.9% (and 4.8% test error), surpassing human accuracy."
    },
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    },
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    },
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "Recent studies have shown that Convolutional Neural Networks (CNNs), which were thought to recognize objects by learning complex shapes, are actually more biased towards recognizing textures. This finding, based on a series of tests comparing CNNs with human observers using images with conflicting texture and shape cues, contradicts the traditional understanding of how these networks process images. We discovered that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, when we trained a standard CNN architecture (ResNet-50) on a stylized version of ImageNet, designed to emphasize shape, the network's performance aligned more closely with human behavior. This training not only matched human performance in controlled experiments (involving 97 observers across 48,560 trials) but also enhanced object detection and robustness against image distortions, underscoring the benefits of shape-based representations in visual recognition systems."
    },
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    },
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "The limited number of videos in current action classification datasets like UCF-101 and HMDB-51 hinders the identification of effective video architectures due to similar performance across small-scale benchmarks. This study re-examines top architectures using the new, significantly larger Kinetics Human Action Video dataset, featuring 400 classes and over 400 clips per class from challenging YouTube videos. It analyzes the impact of this dataset on the performance of existing architectures and the benefits of pre-training on Kinetics. Additionally, the paper introduces the Two-Stream Inflated 3D ConvNet (I3D), an advancement that expands 2D ConvNet designs into 3D for enhanced video feature extraction. This model, pre-trained on Kinetics, sets new benchmarks in action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    },
    {
      "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "Diffusion models, effective for generating high-dimensional perceptual data like images, lack established standard practices. This study enhances noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Our large-scale study demonstrates superior performance over established diffusion methods in high-resolution text-to-image synthesis. We introduce a transformer-based architecture for text-to-image generation, improving text comprehension, typography, and human preference ratings. Our findings show predictable scaling trends, with lower validation loss correlating with improved synthesis. The largest models outperform state-of-the-art approaches, and our data, code, and model weights will be publicly accessible."
    },
    {
      "title": "Self-Paced Curriculum Learning",
      "url": "/research/2015/01/01/Self-Paced-Curriculum-Learning.html",
      "date": "January 1, 2015",
      "summary": "Curriculum learning (CL) and self-paced learning (SPL) are methods inspired by the incremental complexity of human and animal learning, with CL using a fixed curriculum based on prior knowledge and SPL adjusting dynamically to the learner's pace but struggling with prior knowledge and overfitting. This paper introduces a unified framework called self-paced curriculum learning (SPCL), blending both methods' strengths by considering prior knowledge and ongoing learning progress through a concise optimization problem. SPCL mimics a collaborative instructor-student learning mode, showing empirical advantages in two tasks."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article offers the first extensive review of vision transformer techniques specifically applied to human action recognition, a field gaining interest for its wide applications. Termed action transformers, these methods are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also delves into optimizing spatio-temporal attention in transformers for longer sequences and examines different learning strategies, including self-supervised and zero-shot learning, with their respective loss functions. Additionally, the survey assesses progress in benchmark evaluation scores and discusses the challenges and future directions in this research area."
    },
    {
      "title": "Are Convolutional Neural Networks or Transformers more like human vision?",
      "url": "/research/2021/07/01/Are-Convolutional-Neural-Networks-or-Transformers-more-like-human-vision.html",
      "date": "July 1, 2021",
      "summary": "This study investigates the performance of Convolutional Neural Networks (CNNs) and attention-based Vision Transformers (ViTs) in computer vision tasks. While CNNs excel in accuracy, ViTs offer a different approach with weaker inductive biases. By analyzing error patterns, we find that ViTs exhibit consistency with human errors, suggesting potential for more human-like vision models and insights into human object recognition."
    }
  ],
  "corruption": [
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/research/2019/04/27/Benchmarking-Neural-Network-Robustness-to-Common-Corruptions-and-Surface-Variations.html",
      "date": "April 27, 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    },
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/On-Interaction-Between-Augmentations-and-Corruptions-in-Natural-Corruption-Robustness.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    }
  ],
  "peturbation": [
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/research/2019/04/27/Benchmarking-Neural-Network-Robustness-to-Common-Corruptions-and-Surface-Variations.html",
      "date": "April 27, 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    }
  ],
  "role": [
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/research/2019/04/27/Benchmarking-Neural-Network-Robustness-to-Common-Corruptions-and-Surface-Variations.html",
      "date": "April 27, 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    }
  ],
  "p": [
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/research/2019/04/27/Benchmarking-Neural-Network-Robustness-to-Common-Corruptions-and-Surface-Variations.html",
      "date": "April 27, 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "difference": [
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/research/2019/04/27/Benchmarking-Neural-Network-Robustness-to-Common-Corruptions-and-Surface-Variations.html",
      "date": "April 27, 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    }
  ],
  "bypass": [
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/research/2019/04/27/Benchmarking-Neural-Network-Robustness-to-Common-Corruptions-and-Surface-Variations.html",
      "date": "April 27, 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    }
  ],
  "defense": [
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Peturbations",
      "url": "/research/2019/04/27/Benchmarking-Neural-Network-Robustness-to-Common-Corruptions-and-Surface-Variations.html",
      "date": "April 27, 2019",
      "summary": "This paper introduces benchmarks for evaluating the robustness of image classifiers, focusing on common corruptions and perturbations rather than adversarial ones. The benchmarks include IMAGENET-C, which assesses corruption robustness and identifies preferred classifiers for safety-critical roles, and IMAGENET-P, a new dataset for evaluating perturbation robustness. Findings suggest minimal differences in corruption robustness between AlexNet and ResNet classifiers, while also identifying methods to improve robustness, including the unexpected benefit of a bypassed adversarial defense. These benchmarks aim to guide future research towards more generally robust networks."
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "overrate": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    }
  ],
  "metric": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "absence": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    }
  ],
  "uniform": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    },
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "Learning RelU Networks To High Uniform Accuracy Is Intractable",
      "url": "/research/2023/02/28/Learning-Relu-Networks-To-High-Uniform-Accuracy-Is-Intractable.html",
      "date": "February 28, 2023",
      "summary": "Statistical learning theory provides guidelines on the number of training samples needed for achieving desired accuracy in learning problems, especially emphasizing generalization error. However, this is not always adequate, particularly in security-sensitive areas or computational sciences, where uniform accuracy across all inputs is necessary. This paper quantifies the training samples required for uniform accuracy in learning problems involving ReLU neural networks, revealing that the number of samples needed exponentially increases with the network's depth and input dimension."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "Self-supervised transformer training methods have shown excellent results in various fields. Unlike previous models like masked autoencoders (MAE) which use a single normalization layer for the class token [CLS] and other tokens, we introduce a novel normalization technique that normalizes the [CLS] token and normal tokens separately. This approach aims to better capture their unique characteristics, improving performance in downstream tasks. Our findings indicate that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks."
    }
  ],
  "reconstruct": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "This paper introduces VideoMAE, a data-efficient approach for self-supervised video pre-training (SSVP) that utilizes video masked autoencoders with a novel, high-ratio video tube masking technique. This method, inspired by ImageMAE, challenges the model with video reconstruction tasks to improve video representation learning. Key findings include: (1) High masking ratios (90% to 95%) are effective due to video's temporal redundancy, (2) VideoMAE performs well on small datasets (~3k-4k videos) without extra data, highlighting the importance of high-level structure learning, and (3) Data quality is more crucial than quantity for SSVP, with domain shift between pre-training and target datasets being significant. Remarkably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone, without extra data. The code is available online."
    }
  ],
  "spectrogram": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    }
  ],
  "resynthesize": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    }
  ],
  "euclidean": [
    {
      "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data",
      "url": "/research/2023/12/06/Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data.html",
      "date": "December 6, 2023",
      "summary": "Perceptual metrics, designed to replicate human perceptual behavior, are utilized as loss functions in generative models to capture the inherent structure of natural signals like images and audio. This study explores an innovative approach by training a compressive autoencoder on uniform noise instead of natural data in the audio domain. Results demonstrate that using perceptual losses enhances the reconstruction quality of spectrograms and re-synthesized audio at test time compared to standard Euclidean loss, indicating improved generalization to unseen natural signals."
    }
  ],
  "delve": [
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article offers the first extensive review of vision transformer techniques specifically applied to human action recognition, a field gaining interest for its wide applications. Termed action transformers, these methods are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also delves into optimizing spatio-temporal attention in transformers for longer sequences and examines different learning strategies, including self-supervised and zero-shot learning, with their respective loss functions. Additionally, the survey assesses progress in benchmark evaluation scores and discusses the challenges and future directions in this research area."
    },
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    }
  ],
  "blend": [
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "Self-Paced Curriculum Learning",
      "url": "/research/2015/01/01/Self-Paced-Curriculum-Learning.html",
      "date": "January 1, 2015",
      "summary": "Curriculum learning (CL) and self-paced learning (SPL) are methods inspired by the incremental complexity of human and animal learning, with CL using a fixed curriculum based on prior knowledge and SPL adjusting dynamically to the learner's pace but struggling with prior knowledge and overfitting. This paper introduces a unified framework called self-paced curriculum learning (SPCL), blending both methods' strengths by considering prior knowledge and ongoing learning progress through a concise optimization problem. SPCL mimics a collaborative instructor-student learning mode, showing empirical advantages in two tasks."
    }
  ],
  "ols": [
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    }
  ],
  "statistics": [
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    },
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/research/2020/08/03/Are-Labels-Necessary-for-Neural-Architecture-Search.html",
      "date": "August 3, 2020",
      "summary": "This paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning if effective neural architectures can be discovered using only images without human-annotated labels. Through two experimental setups—sample-based and search-based—we investigate this possibility. In the sample-based approach, we evaluate 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. In the search-based experiments, we employ a recognized NAS algorithm, DARTS, with various unsupervised objectives, finding that architectures identified without labels perform competitively compared to those found with labels. These findings suggest that human-annotated labels may not be necessary for identifying efficient neural architectures, as image statistics alone could be sufficient."
    }
  ],
  "probability": [
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    }
  ],
  "exist": [
    {
      "title": "Delving Deep into Label Smoothing",
      "url": "/research/2021/07/22/Delving-Deep-into-Label-Smoothing.html",
      "date": "July 22, 2021",
      "summary": "Label smoothing, a regularization technique for deep neural networks (DNNs) to mitigate overfitting and enhance classification performance, involves creating soft labels via a weighted blend of the uniform distribution and the hard label. This paper introduces an Online Label Smoothing (OLS) strategy that generates soft labels using model prediction statistics for the target category, creating a more accurate probability distribution for supervising DNNs. Our method significantly improves classification accuracy and model robustness to noisy labels on CIFAR-100, ImageNet, and fine-grained datasets compared to existing label smoothing techniques. The source code is available online."
    },
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/research/2019/07/26/DropAttention-A-Regularization-Method-for-Fully-Connected-Self-Attention-Networks.html",
      "date": "July 26, 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. Unlike existing dropout techniques for other neural network layers, DropAttention specifically targets the unique challenges of self-attention mechanisms. Our experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method, applies a single augmentation to each image and outperforms existing methods with minimal complexity and cost. We conducted extensive experiments to validate its performance against state-of-the-art methods across various image classification scenarios and explored its effectiveness through multiple ablation studies involving different augmentation spaces and methods. Our work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Highlighting a stagnation in automatic augmentation research, we conclude with proposed best practices for future advancements in the field."
    },
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    }
  ],
  "rectifiers": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    }
  ],
  "classificatio": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    }
  ],
  "rectifier": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    }
  ],
  "rectify": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    },
    {
      "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "Diffusion models, effective for generating high-dimensional perceptual data like images, lack established standard practices. This study enhances noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Our large-scale study demonstrates superior performance over established diffusion methods in high-resolution text-to-image synthesis. We introduce a transformer-based architecture for text-to-image generation, improving text comprehension, typography, and human preference ratings. Our findings show predictable scaling trends, with lower validation loss correlating with improved synthesis. The largest models outperform state-of-the-art approaches, and our data, code, and model weights will be publicly accessible."
    }
  ],
  "prelu": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    }
  ],
  "tailor": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    },
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "This paper explores the effectiveness of using synthetic data, generated by graphics simulators, for pre-training models in computer vision, addressing the challenges posed by using real-image datasets like high costs and ethical concerns. It highlights that the performance of models on downstream tasks varies with different simulation parameters, suggesting a tailored approach to synthetic data generation for optimal results. The introduction of Task2Sim, a model that maps downstream task requirements to optimal simulation parameters, allows for the generation of synthetic pre-training data customized for specific tasks. Task2Sim demonstrates significant improvements in model performance on a wide range of tasks without additional training for new tasks, showing potential to rival pre-training with real images from Imagenet."
    }
  ],
  "linearity": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    }
  ],
  "scratch": [
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificatio",
      "url": "/research/2015/02/06/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classificatio.html",
      "date": "February 6, 2015",
      "summary": "In this study, we explore two enhancements to rectifier neural networks for image classification. Firstly, we introduce a Parametric Rectified Linear Unit (PReLU) that extends traditional units, offering improved model fitting with negligible additional computational cost. Secondly, we develop a robust initialization strategy tailored for rectifier non-linearities, allowing the training of very deep models from scratch. Using PReLU networks, we achieved a 4.94% top-5 error rate on the ImageNet 2012 dataset, outperforming the previous best result and surpassing human-level performance."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "Image-caption pretraining, crucial for tasks like zero-shot image classification and object detection, faces challenges in aligning multiple concepts from captions to objects in images. We introduce a curriculum learning framework inspired by children's language learning from cognitive science, starting with simple image-caption pairs and gradually increasing complexity by adding more concepts. This method, which leverages knowledge from each phase for subsequent learning, outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios. The code is available at: https://github.com/hayyubi/cur_vl.git."
    }
  ],
  "dropattention": [
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/research/2019/07/26/DropAttention-A-Regularization-Method-for-Fully-Connected-Self-Attention-Networks.html",
      "date": "July 26, 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. Unlike existing dropout techniques for other neural network layers, DropAttention specifically targets the unique challenges of self-attention mechanisms. Our experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    }
  ],
  "providing": [
    {
      "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks",
      "url": "/research/2019/07/26/DropAttention-A-Regularization-Method-for-Fully-Connected-Self-Attention-Networks.html",
      "date": "July 26, 2019",
      "summary": "This paper introduces DropAttention, a novel dropout method for fully-connected self-attention layers in Transformers, aiming to prevent overfitting by regularizing attention weights. Unlike existing dropout techniques for other neural network layers, DropAttention specifically targets the unique challenges of self-attention mechanisms. Our experiments across various tasks demonstrate that DropAttention not only enhances performance but also mitigates overfitting, providing a significant advancement in the regularization of Transformers."
    }
  ],
  "dropkey": [
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    }
  ],
  "ignore": [
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    }
  ],
  "softmax": [
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions for neural networks in reinforcement learning, the sigmoid-weighted linear unit (SiLU) and its derivative (dSiLU), and challenges the need for experience replay and separate target networks in deep reinforcement learning. By employing on-policy learning with eligibility traces and softmax action selection, the study achieves state-of-the-art results in stochastic SZ-Tetris and a small-board Tetris using TD(λ) learning and shallow dSiLU network agents. Furthermore, it outperforms the DQN algorithm in the Atari 2600 domain with a deep Sarsa(λ) agent utilizing SiLU and dSiLU hidden units, suggesting a competitive alternative to traditional DQN approaches."
    }
  ],
  "matrix": [
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "ratio": [
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "This paper introduces VideoMAE, a data-efficient approach for self-supervised video pre-training (SSVP) that utilizes video masked autoencoders with a novel, high-ratio video tube masking technique. This method, inspired by ImageMAE, challenges the model with video reconstruction tasks to improve video representation learning. Key findings include: (1) High masking ratios (90% to 95%) are effective due to video's temporal redundancy, (2) VideoMAE performs well on small datasets (~3k-4k videos) without extra data, highlighting the importance of high-level structure learning, and (3) Data quality is more crucial than quantity for SSVP, with domain shift between pre-training and target datasets being significant. Remarkably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone, without extra data. The code is available online."
    }
  ],
  "schedule": [
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    },
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/research/2022/10/10/Turbo-Training-with-Token-Dropout.html",
      "date": "October 10, 2022",
      "summary": "This paper introduces Turbo training, an efficient training method for video-related tasks using Transformers. Turbo training offers three major contributions: Firstly, it presents a simple yet versatile training paradigm applicable to multiple video tasks. Secondly, it demonstrates Turbo training's effectiveness across action classification, video-language representation learning, and long-video activity classification, achieving competitive performance with up to 4× speed-up and reduced memory usage. Thirdly, it enables long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous methods that were resource-intensive."
    }
  ],
  "balance": [
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    }
  ],
  "retention": [
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    }
  ],
  "structured": [
    {
      "title": "DropKey",
      "url": "/research/2023/04/11/DropKey.html",
      "date": "April 11, 2023",
      "summary": "This paper presents DropKey, a novel approach to dropout in Vision Transformer's self-attention layers, addressing three main aspects ignored by previous studies. Firstly, it introduces a dropout-before-softmax scheme by applying dropout to the Key before attention matrix calculation, maintaining regularization and attention weight probability features. Secondly, it proposes a decreasing drop ratio schedule across layers to balance feature retention and prevent overfitting. Thirdly, it evaluates the necessity of structured dropout, like in CNNs, and concludes it's not essential for Vision Transformers. DropKey, combining key-targeted dropout and a decreasing ratio, enhances Vision Transformer performance across various architectures and tasks, including image classification, object detection, and human-object interaction, as demonstrated through extensive experiments."
    }
  ],
  "way": [
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/research/2014/01/01/Dropout-A-Simple-Way-to-Prevent-Neural-Networks-from-Overfitting.html",
      "date": "January 1, 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "We introduce a system that trains deep neural networks for object detection using synthetic images, leveraging domain randomization to manage real-world data variability. This involves randomizing simulator parameters—like lighting and object textures—in non-realistic ways, helping the network to identify essential object features. Our findings reveal that networks can achieve impressive performance with just synthetic data and further improve with real data fine-tuning. This suggests the potential of using cost-effective synthetic data for training, circumventing the challenges of acquiring vast amounts of real-world data or creating detailed synthetic environments. We validate our method through car bounding box detection on the KITTI dataset."
    }
  ],
  "overfitting": [
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/research/2014/01/01/Dropout-A-Simple-Way-to-Prevent-Neural-Networks-from-Overfitting.html",
      "date": "January 1, 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    },
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "Deep neural networks, while effective in complex vision tasks, suffer from overfitting. Current image augmentation techniques, largely linear and hand-crafted like scale, flip, and color jitter, fail to produce sufficiently challenging examples. We introduce a new augmentation approach, Mask-Reconstruct Augmentation (MRA), leveraging self-supervised masked autoencoders to generate distorted inputs. This method, inspired by masked image modeling's success in self-supervised learning, uses nonlinear transformations for regularization. Our extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks. The code is available at https://github.com/haohang96/MRA."
    },
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper investigates how neural networks generalize on small, algorithmically generated datasets, focusing on data efficiency, memorization, generalization, and learning speed. It highlights instances where neural networks grok patterns, significantly improving their generalization ability from mere chance to perfect accuracy, even beyond the point of overfitting. Additionally, the study finds that generalization on smaller datasets demands more optimization. It posits that such datasets are ideal for exploring the enigmatic phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    },
    {
      "title": "Self-Paced Curriculum Learning",
      "url": "/research/2015/01/01/Self-Paced-Curriculum-Learning.html",
      "date": "January 1, 2015",
      "summary": "Curriculum learning (CL) and self-paced learning (SPL) are methods inspired by the incremental complexity of human and animal learning, with CL using a fixed curriculum based on prior knowledge and SPL adjusting dynamically to the learner's pace but struggling with prior knowledge and overfitting. This paper introduces a unified framework called self-paced curriculum learning (SPCL), blending both methods' strengths by considering prior knowledge and ongoing learning progress through a concise optimization problem. SPCL mimics a collaborative instructor-student learning mode, showing empirical advantages in two tasks."
    }
  ],
  "usage": [
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/research/2014/01/01/Dropout-A-Simple-Way-to-Prevent-Neural-Networks-from-Overfitting.html",
      "date": "January 1, 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    },
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/research/2022/10/10/Turbo-Training-with-Token-Dropout.html",
      "date": "October 10, 2022",
      "summary": "This paper introduces Turbo training, an efficient training method for video-related tasks using Transformers. Turbo training offers three major contributions: Firstly, it presents a simple yet versatile training paradigm applicable to multiple video tasks. Secondly, it demonstrates Turbo training's effectiveness across action classification, video-language representation learning, and long-video activity classification, achieving competitive performance with up to 4× speed-up and reduced memory usage. Thirdly, it enables long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous methods that were resource-intensive."
    },
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "combats": [
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/research/2014/01/01/Dropout-A-Simple-Way-to-Prevent-Neural-Networks-from-Overfitting.html",
      "date": "January 1, 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    }
  ],
  "coadaptation": [
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/research/2014/01/01/Dropout-A-Simple-Way-to-Prevent-Neural-Networks-from-Overfitting.html",
      "date": "January 1, 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    }
  ],
  "speech": [
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/research/2014/01/01/Dropout-A-Simple-Way-to-Prevent-Neural-Networks-from-Overfitting.html",
      "date": "January 1, 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "We introduce the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. Our empirical evaluation across computer vision, natural language processing, and speech tasks shows that GELU offers performance improvements over ReLU and ELU activations."
    }
  ],
  "recognition": [
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/research/2014/01/01/Dropout-A-Simple-Way-to-Prevent-Neural-Networks-from-Overfitting.html",
      "date": "January 1, 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    },
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    },
    {
      "title": "Rethinking Batch in BatchNorm",
      "url": "/research/2021/05/17/Rethinking-Batch-in-BatchNorm.html",
      "date": "May 17, 2021",
      "summary": "BatchNorm, essential in convolutional neural networks, behaves uniquely due to its batch-based operation, leading to performance issues in visual recognition tasks. This paper identifies these issues and suggests reevaluating the concept of batch in BatchNorm for better performance, aiming to guide researchers in its effective use."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper presents the AVA dataset, featuring 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets focused on composite actions in shorter clips with sparse annotations, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. It highlights the challenges in action recognition, introduces a novel localization approach surpassing existing benchmarks but shows modest performance on AVA (15.6% mAP), pointing out the necessity for advanced video understanding methods."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "Recent studies have shown that Convolutional Neural Networks (CNNs), which were thought to recognize objects by learning complex shapes, are actually more biased towards recognizing textures. This finding, based on a series of tests comparing CNNs with human observers using images with conflicting texture and shape cues, contradicts the traditional understanding of how these networks process images. We discovered that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, when we trained a standard CNN architecture (ResNet-50) on a stylized version of ImageNet, designed to emphasize shape, the network's performance aligned more closely with human behavior. This training not only matched human performance in controlled experiments (involving 97 observers across 48,560 trials) but also enhanced object detection and robustness against image distortions, underscoring the benefits of shape-based representations in visual recognition systems."
    },
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "The limited number of videos in current action classification datasets like UCF-101 and HMDB-51 hinders the identification of effective video architectures due to similar performance across small-scale benchmarks. This study re-examines top architectures using the new, significantly larger Kinetics Human Action Video dataset, featuring 400 classes and over 400 clips per class from challenging YouTube videos. It analyzes the impact of this dataset on the performance of existing architectures and the benefits of pre-training on Kinetics. Additionally, the paper introduces the Two-Stream Inflated 3D ConvNet (I3D), an advancement that expands 2D ConvNet designs into 3D for enhanced video feature extraction. This model, pre-trained on Kinetics, sets new benchmarks in action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    },
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "We introduce SlowFast networks for video recognition, featuring a Slow pathway for spatial semantics at low frame rates and a Fast pathway for motion details at high frame rates. The Fast pathway's efficiency is achieved through reduced channel capacity, yet it effectively captures temporal information. Our models significantly enhance action classification and detection, achieving top accuracy on benchmarks like Kinetics, Charades, and AVA. Code is available at: https://github.com/facebookresearch/SlowFast."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article offers the first extensive review of vision transformer techniques specifically applied to human action recognition, a field gaining interest for its wide applications. Termed action transformers, these methods are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also delves into optimizing spatio-temporal attention in transformers for longer sequences and examines different learning strategies, including self-supervised and zero-shot learning, with their respective loss functions. Additionally, the survey assesses progress in benchmark evaluation scores and discusses the challenges and future directions in this research area."
    },
    {
      "title": "Are Convolutional Neural Networks or Transformers more like human vision?",
      "url": "/research/2021/07/01/Are-Convolutional-Neural-Networks-or-Transformers-more-like-human-vision.html",
      "date": "July 1, 2021",
      "summary": "This study investigates the performance of Convolutional Neural Networks (CNNs) and attention-based Vision Transformers (ViTs) in computer vision tasks. While CNNs excel in accuracy, ViTs offer a different approach with weaker inductive biases. By analyzing error patterns, we find that ViTs exhibit consistency with human errors, suggesting potential for more human-like vision models and insights into human object recognition."
    }
  ],
  "document": [
    {
      "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "url": "/research/2014/01/01/Dropout-A-Simple-Way-to-Prevent-Neural-Networks-from-Overfitting.html",
      "date": "January 1, 2014",
      "summary": "Deep neural networks, while powerful, face overfitting and slow usage issues. Dropout is a technique that addresses these by randomly omitting units during training, leading to an exponential number of thinned networks. This method not only combats overfitting by preventing co-adaptation of units but also simplifies the process at test time through the use of a single network with adjusted weights. Demonstrated across various tasks like vision, speech recognition, and document classification, dropout significantly enhances performance and achieves state-of-the-art results on numerous benchmarks."
    },
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    }
  ],
  "bagging": [
    {
      "title": "Dropout as Data Augmentation",
      "url": "/research/2016/01/08/Dropout-as-Data-Augmentation.html",
      "date": "January 8, 2016",
      "summary": "Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cos"
    }
  ],
  "kind": [
    {
      "title": "Dropout as Data Augmentation",
      "url": "/research/2016/01/08/Dropout-as-Data-Augmentation.html",
      "date": "January 8, 2016",
      "summary": "Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cos"
    }
  ],
  "back": [
    {
      "title": "Dropout as Data Augmentation",
      "url": "/research/2016/01/08/Dropout-as-Data-Augmentation.html",
      "date": "January 8, 2016",
      "summary": "Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    }
  ],
  "yield": [
    {
      "title": "Dropout as Data Augmentation",
      "url": "/research/2016/01/08/Dropout-as-Data-Augmentation.html",
      "date": "January 8, 2016",
      "summary": "Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-5.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-6.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    }
  ],
  "add": [
    {
      "title": "Dropout as Data Augmentation",
      "url": "/research/2016/01/08/Dropout-as-Data-Augmentation.html",
      "date": "January 8, 2016",
      "summary": "Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cos"
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    }
  ],
  "underfittin": [
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    }
  ],
  "hinton": [
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    }
  ],
  "et": [
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/research/2017/11/06/Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization.html",
      "date": "November 6, 2017",
      "summary": "In this study, we improve upon the fast stylization method by Ulyanov et al. (2016) through a minor yet impactful modification: substituting batch normalization with instance normalization during both training and testing. This adjustment significantly enhances the quality of generated images and enables the training of efficient real-time image generation architectures. The code and full paper are available online."
    }
  ],
  "al": [
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/research/2017/11/06/Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization.html",
      "date": "November 6, 2017",
      "summary": "In this study, we improve upon the fast stylization method by Ulyanov et al. (2016) through a minor yet impactful modification: substituting batch normalization with instance normalization during both training and testing. This adjustment significantly enhances the quality of generated images and enables the training of efficient real-time image generation architectures. The code and full paper are available online."
    }
  ],
  "well": [
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/research/2021/08/16/An-Empirical-Study-of-Training-Self-Supervised-Vision-Transformers.html",
      "date": "August 16, 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    }
  ],
  "variance": [
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "phase": [
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    },
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "Image-caption pretraining, crucial for tasks like zero-shot image classification and object detection, faces challenges in aligning multiple concepts from captions to objects in images. We introduce a curriculum learning framework inspired by children's language learning from cognitive science, starting with simple image-caption pairs and gradually increasing complexity by adding more concepts. This method, which leverages knowledge from each phase for subsequent learning, outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios. The code is available at: https://github.com/hayyubi/cur_vl.git."
    }
  ],
  "activate": [
    {
      "title": "Dropout Reduces Underfittin",
      "url": "/research/2023/05/31/Dropout-Reduces-Underfittin.html",
      "date": "May 31, 2023",
      "summary": "Dropout, introduced by Hinton et al. in 2012, is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. We introduce early dropout where dropout is applied only during the initial training phases, and late dropout for controlling overfitting by activating dropout in later training stages. Our experiments across ImageNet and other vision tasks show these approaches enhance generalization accuracy. This suggests more exploration into regularization techniques in deep learning could be beneficial. The code is available on GitHub"
    }
  ],
  "group": [
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    }
  ],
  "bn": [
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    }
  ],
  "constrain": [
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    }
  ],
  "gn": [
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    }
  ],
  "divide": [
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    }
  ],
  "channel": [
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "We introduce SlowFast networks for video recognition, featuring a Slow pathway for spatial semantics at low frame rates and a Fast pathway for motion details at high frame rates. The Fast pathway's efficiency is achieved through reduced channel capacity, yet it effectively captures temporal information. Our models significantly enhance action classification and detection, achieving top accuracy on benchmarks like Kinetics, Charades, and AVA. Code is available at: https://github.com/facebookresearch/SlowFast."
    }
  ],
  "video": [
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/research/2022/10/10/Turbo-Training-with-Token-Dropout.html",
      "date": "October 10, 2022",
      "summary": "This paper introduces Turbo training, an efficient training method for video-related tasks using Transformers. Turbo training offers three major contributions: Firstly, it presents a simple yet versatile training paradigm applicable to multiple video tasks. Secondly, it demonstrates Turbo training's effectiveness across action classification, video-language representation learning, and long-video activity classification, achieving competitive performance with up to 4× speed-up and reduced memory usage. Thirdly, it enables long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous methods that were resource-intensive."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    },
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper presents the AVA dataset, featuring 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets focused on composite actions in shorter clips with sparse annotations, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. It highlights the challenges in action recognition, introduces a novel localization approach surpassing existing benchmarks but shows modest performance on AVA (15.6% mAP), pointing out the necessity for advanced video understanding methods."
    },
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "Temporal action localization (TAL) is crucial for video understanding but is hindered by benchmarks that rely on coarse action classes, leading to model overfitting and ambiguous annotations. Addressing these issues, we introduce FineAction, a large-scale, fine-grained video dataset with 103K instances across 106 action categories in 17K videos. FineAction offers a diverse, densely annotated dataset with co-occurring actions, posing new challenges for TAL. We evaluated popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and proposed a baseline method achieving a 13.17% mAP. FineAction aims to propel TAL research forward and is accessible online."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "Neural networks parameterized for continuous, differentiable signal representations provide a promising paradigm but struggle with modeling fine detail and representing spatial and temporal derivatives critical for signals defined by partial differential equations. We introduce sinusoidal representation networks (SIRENs), utilizing periodic activation functions to effectively capture complex natural signals and their derivatives. Our analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. We also showcase SIRENs' application in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations, and extend their use with hypernetworks to learn priors for SIREN functions. A video overview and full applications are available on our project website."
    },
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "The limited number of videos in current action classification datasets like UCF-101 and HMDB-51 hinders the identification of effective video architectures due to similar performance across small-scale benchmarks. This study re-examines top architectures using the new, significantly larger Kinetics Human Action Video dataset, featuring 400 classes and over 400 clips per class from challenging YouTube videos. It analyzes the impact of this dataset on the performance of existing architectures and the benefits of pre-training on Kinetics. Additionally, the paper introduces the Two-Stream Inflated 3D ConvNet (I3D), an advancement that expands 2D ConvNet designs into 3D for enhanced video feature extraction. This model, pre-trained on Kinetics, sets new benchmarks in action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    },
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "We introduce SlowFast networks for video recognition, featuring a Slow pathway for spatial semantics at low frame rates and a Fast pathway for motion details at high frame rates. The Fast pathway's efficiency is achieved through reduced channel capacity, yet it effectively captures temporal information. Our models significantly enhance action classification and detection, achieving top accuracy on benchmarks like Kinetics, Charades, and AVA. Code is available at: https://github.com/facebookresearch/SlowFast."
    },
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "This paper introduces VideoMAE, a data-efficient approach for self-supervised video pre-training (SSVP) that utilizes video masked autoencoders with a novel, high-ratio video tube masking technique. This method, inspired by ImageMAE, challenges the model with video reconstruction tasks to improve video representation learning. Key findings include: (1) High masking ratios (90% to 95%) are effective due to video's temporal redundancy, (2) VideoMAE performs well on small datasets (~3k-4k videos) without extra data, highlighting the importance of high-level structure learning, and (3) Data quality is more crucial than quantity for SSVP, with domain shift between pre-training and target datasets being significant. Remarkably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone, without extra data. The code is available online."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    }
  ],
  "replacement": [
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    }
  ],
  "library": [
    {
      "title": "Group Normalization",
      "url": "/research/2018/06/01/Group-Normalization.html",
      "date": "June 1, 2018",
      "summary": "Batch Normalization (BN) helps train various deep learning networks but struggles with small batch sizes due to inaccurate statistics estimation, limiting its use in memory-constrained tasks. This paper introduces Group Normalization (GN) as an alternative that divides channels into groups for normalization, independent of batch sizes, offering stable accuracy across various batch sizes. GN shows lower error rates compared to BN in small batches and comparable performance in typical batch sizes. It also outperforms BN in object detection, segmentation, and video classification tasks, proving to be a viable replacement for BN in a range of applications. GN is easy to implement in modern libraries."
    }
  ],
  "calibration": [
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/research/2023/06/30/Impact-of-Noise-on-Calibration-and-Generalisation-of-Neural-Networks.html",
      "date": "June 30, 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. We examine different types of noise, such as activation and input augmentation noise, in both in-distribution and out-of-distribution scenarios. Activation noise significantly improves generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "generalisation": [
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/research/2023/06/30/Impact-of-Noise-on-Calibration-and-Generalisation-of-Neural-Networks.html",
      "date": "June 30, 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. We examine different types of noise, such as activation and input augmentation noise, in both in-distribution and out-of-distribution scenarios. Activation noise significantly improves generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    }
  ],
  "injection": [
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/research/2023/06/30/Impact-of-Noise-on-Calibration-and-Generalisation-of-Neural-Networks.html",
      "date": "June 30, 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. We examine different types of noise, such as activation and input augmentation noise, in both in-distribution and out-of-distribution scenarios. Activation noise significantly improves generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    }
  ],
  "nn": [
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/research/2023/06/30/Impact-of-Noise-on-Calibration-and-Generalisation-of-Neural-Networks.html",
      "date": "June 30, 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. We examine different types of noise, such as activation and input augmentation noise, in both in-distribution and out-of-distribution scenarios. Activation noise significantly improves generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    }
  ],
  "activation": [
    {
      "title": "Impact of Noise on Calibration and Generalisation of Neural Networks",
      "url": "/research/2023/06/30/Impact-of-Noise-on-Calibration-and-Generalisation-of-Neural-Networks.html",
      "date": "June 30, 2023",
      "summary": "This study investigates the effects of various noise injection and data augmentation strategies on neural networks (NNs) to enhance generalization, robustness, and calibration. We examine different types of noise, such as activation and input augmentation noise, in both in-distribution and out-of-distribution scenarios. Activation noise significantly improves generalization across scenarios, while input augmentation noise notably enhances calibration in out-of-distribution data but is less effective for in-distribution data."
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    },
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    },
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "We introduce a Fourier neural network (FNN) aligned with Fourier decomposition, utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design ensures easy integration with more complex networks for data processing tasks. The FNN's efficacy is demonstrated on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits include solution validity beyond the training scope, model interpretability, and ease of use."
    },
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "We introduce the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. Our empirical evaluation across computer vision, natural language processing, and speech tasks shows that GELU offers performance improvements over ReLU and ELU activations."
    },
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "Gated Linear Units (GLUs) involve the component-wise product of two linear projections, with one undergoing a sigmoid function. Exploring GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers, we find certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "Neural networks parameterized for continuous, differentiable signal representations provide a promising paradigm but struggle with modeling fine detail and representing spatial and temporal derivatives critical for signals defined by partial differential equations. We introduce sinusoidal representation networks (SIRENs), utilizing periodic activation functions to effectively capture complex natural signals and their derivatives. Our analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. We also showcase SIRENs' application in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations, and extend their use with hypernetworks to learn priors for SIREN functions. A video overview and full applications are available on our project website."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "Online Normalization is a novel method for normalizing neural network hidden activations, offering a batch-independent alternative with comparable accuracy to Batch Normalization. It overcomes Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. Applicable to recurrent, fully connected networks, and those with high activation memory requirements, it demonstrates effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    },
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions for neural networks in reinforcement learning, the sigmoid-weighted linear unit (SiLU) and its derivative (dSiLU), and challenges the need for experience replay and separate target networks in deep reinforcement learning. By employing on-policy learning with eligibility traces and softmax action selection, the study achieves state-of-the-art results in stochastic SZ-Tetris and a small-board Tetris using TD(λ) learning and shallow dSiLU network agents. Furthermore, it outperforms the DQN algorithm in the Atari 2600 domain with a deep Sarsa(λ) agent utilizing SiLU and dSiLU hidden units, suggesting a competitive alternative to traditional DQN approaches."
    },
    {
      "title": "Taming the Waves: Sine as Activation Function in Deep Neural Networks",
      "url": "/research/2016/11/04/Taming-the-Waves-Sine-as-Activation-Function-in-Deep-Neural-Networks.html",
      "date": "November 4, 2016",
      "summary": "This paper examines the challenges of training deep neural networks with sinusoidal activation functions, despite their successful use in specific applications. It explains the difficulty in training due to the emergence of numerous shallow local minima from the architecture. The study also reveals that successful learning in typical classification tasks occurs when the network effectively disregards the periodic cycles. Furthermore, it demonstrates that for certain non-trivial tasks, networks with sinusoidal activations can outperform those using traditional monotonic functions."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-5.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-6.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    }
  ],
  "ingredient": [
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/research/2017/11/06/Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization.html",
      "date": "November 6, 2017",
      "summary": "In this study, we improve upon the fast stylization method by Ulyanov et al. (2016) through a minor yet impactful modification: substituting batch normalization with instance normalization during both training and testing. This adjustment significantly enhances the quality of generated images and enables the training of efficient real-time image generation architectures. The code and full paper are available online."
    }
  ],
  "stylization": [
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/research/2017/11/06/Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization.html",
      "date": "November 6, 2017",
      "summary": "In this study, we improve upon the fast stylization method by Ulyanov et al. (2016) through a minor yet impactful modification: substituting batch normalization with instance normalization during both training and testing. This adjustment significantly enhances the quality of generated images and enables the training of efficient real-time image generation architectures. The code and full paper are available online."
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    }
  ],
  "ulyanov": [
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/research/2017/11/06/Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization.html",
      "date": "November 6, 2017",
      "summary": "In this study, we improve upon the fast stylization method by Ulyanov et al. (2016) through a minor yet impactful modification: substituting batch normalization with instance normalization during both training and testing. This adjustment significantly enhances the quality of generated images and enables the training of efficient real-time image generation architectures. The code and full paper are available online."
    }
  ],
  "substitute": [
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/research/2017/11/06/Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization.html",
      "date": "November 6, 2017",
      "summary": "In this study, we improve upon the fast stylization method by Ulyanov et al. (2016) through a minor yet impactful modification: substituting batch normalization with instance normalization during both training and testing. This adjustment significantly enhances the quality of generated images and enables the training of efficient real-time image generation architectures. The code and full paper are available online."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "Gated Linear Units (GLUs) involve the component-wise product of two linear projections, with one undergoing a sigmoid function. Exploring GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers, we find certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    },
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "generated": [
    {
      "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
      "url": "/research/2017/11/06/Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization.html",
      "date": "November 6, 2017",
      "summary": "In this study, we improve upon the fast stylization method by Ulyanov et al. (2016) through a minor yet impactful modification: substituting batch normalization with instance normalization during both training and testing. This adjustment significantly enhances the quality of generated images and enables the training of efficient real-time image generation architectures. The code and full paper are available online."
    }
  ],
  "flexibility": [
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    }
  ],
  "aid": [
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    }
  ],
  "occlusions": [
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    }
  ],
  "family": [
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    },
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "advantage": [
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    },
    {
      "title": "Self-Paced Curriculum Learning",
      "url": "/research/2015/01/01/Self-Paced-Curriculum-Learning.html",
      "date": "January 1, 2015",
      "summary": "Curriculum learning (CL) and self-paced learning (SPL) are methods inspired by the incremental complexity of human and animal learning, with CL using a fixed curriculum based on prior knowledge and SPL adjusting dynamically to the learner's pace but struggling with prior knowledge and overfitting. This paper introduces a unified framework called self-paced curriculum learning (SPCL), blending both methods' strengths by considering prior knowledge and ongoing learning progress through a concise optimization problem. SPCL mimics a collaborative instructor-student learning mode, showing empirical advantages in two tasks."
    }
  ],
  "acces": [
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    }
  ],
  "httpsgitiojs15x": [
    {
      "title": "Intriguing Properties of Vision Transformers",
      "url": "/research/2021/11/25/Intriguing-Properties-of-Vision-Transformers.html",
      "date": "November 25, 2021",
      "summary": "This study investigates how ViT's flexibility in contextual attention aids in overcoming challenges like occlusions, domain shifts, and perturbations in natural images. Through comprehensive experiments across three ViT families and comparison with a top CNN, we find ViTs show remarkable resilience to occlusions, perturbations, and domain shifts, maintaining high accuracy even when most of the image is obscured. Unlike CNNs, ViTs exhibit less texture bias, focusing more on shape-based features, which enhances their shape recognition to levels comparable with the human visual system. Additionally, ViTs can perform accurate semantic segmentation without pixel-level supervision and create feature ensembles from a single model for improved classification performance in both traditional and few-shot learning settings. These advantages stem from their dynamic receptive fields enabled by self-attention mechanisms. Access to our code is available at https://git.io/Js15X."
    }
  ],
  "activity": [
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    },
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/research/2022/10/10/Turbo-Training-with-Token-Dropout.html",
      "date": "October 10, 2022",
      "summary": "This paper introduces Turbo training, an efficient training method for video-related tasks using Transformers. Turbo training offers three major contributions: Firstly, it presents a simple yet versatile training paradigm applicable to multiple video tasks. Secondly, it demonstrates Turbo training's effectiveness across action classification, video-language representation learning, and long-video activity classification, achieving competitive performance with up to 4× speed-up and reduced memory usage. Thirdly, it enables long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous methods that were resource-intensive."
    }
  ],
  "feed": [
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "Gated Linear Units (GLUs) involve the component-wise product of two linear projections, with one undergoing a sigmoid function. Exploring GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers, we find certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    }
  ],
  "forward": [
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "Temporal action localization (TAL) is crucial for video understanding but is hindered by benchmarks that rely on coarse action classes, leading to model overfitting and ambiguous annotations. Addressing these issues, we introduce FineAction, a large-scale, fine-grained video dataset with 103K instances across 106 action categories in 17K videos. FineAction offers a diverse, densely annotated dataset with co-occurring actions, posing new challenges for TAL. We evaluated popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and proposed a baseline method achieving a 13.17% mAP. FineAction aims to propel TAL research forward and is accessible online."
    },
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "Gated Linear Units (GLUs) involve the component-wise product of two linear projections, with one undergoing a sigmoid function. Exploring GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers, we find certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    }
  ],
  "vary": [
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    },
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "This paper explores the effectiveness of using synthetic data, generated by graphics simulators, for pre-training models in computer vision, addressing the challenges posed by using real-image datasets like high costs and ethical concerns. It highlights that the performance of models on downstream tasks varies with different simulation parameters, suggesting a tailored approach to synthetic data generation for optimal results. The introduction of Task2Sim, a model that maps downstream task requirements to optimal simulation parameters, allows for the generation of synthetic pre-training data customized for specific tasks. Task2Sim demonstrates significant improvements in model performance on a wide range of tasks without additional training for new tasks, showing potential to rival pre-training with real images from Imagenet."
    }
  ],
  "rnn": [
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    }
  ],
  "case": [
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    },
    {
      "title": "Better plain ViT baselines for ImageNet-1k",
      "url": "/research/2022/05/02/Better-plain-ViT-baselines-for-ImageNet-1k.html",
      "date": "May 2, 2022",
      "summary": "It is commonly accepted that the Vision Transformer model requires sophisticated regularization techniques to excel at ImageNet-1k scale data. Surprisingly, we find this is not the case and standard data augmentation is sufficient. This note presents a few minor modifications to the original Vision Transformer (ViT) vanilla training setting that dramatically improve the performance of plain ViT models. Notably, 90 epochs of training surpass 76% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reach 80% in less than one day."
    }
  ],
  "stabilizes": [
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    }
  ],
  "dynamic": [
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    }
  ],
  "cut": [
    {
      "title": "Layer Normalization",
      "url": "/research/2016/07/21/Layer-Normalization.html",
      "date": "July 21, 2016",
      "summary": "Training deep neural networks is computationally intensive. Normalizing neuron activities can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases. Layer normalization, which includes adaptive bias and gain for each neuron, stabilizes hidden state dynamics in RNNs and significantly cuts training time, offering a practical solution for both feed-forward and recurrent architectures."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "relu": [
    {
      "title": "Learning RelU Networks To High Uniform Accuracy Is Intractable",
      "url": "/research/2023/02/28/Learning-Relu-Networks-To-High-Uniform-Accuracy-Is-Intractable.html",
      "date": "February 28, 2023",
      "summary": "Statistical learning theory provides guidelines on the number of training samples needed for achieving desired accuracy in learning problems, especially emphasizing generalization error. However, this is not always adequate, particularly in security-sensitive areas or computational sciences, where uniform accuracy across all inputs is necessary. This paper quantifies the training samples required for uniform accuracy in learning problems involving ReLU neural networks, revealing that the number of samples needed exponentially increases with the network's depth and input dimension."
    },
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "We introduce the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. Our empirical evaluation across computer vision, natural language processing, and speech tasks shows that GELU offers performance improvements over ReLU and ELU activations."
    },
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "Gated Linear Units (GLUs) involve the component-wise product of two linear projections, with one undergoing a sigmoid function. Exploring GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers, we find certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    }
  ],
  "achieving": [
    {
      "title": "Learning RelU Networks To High Uniform Accuracy Is Intractable",
      "url": "/research/2023/02/28/Learning-Relu-Networks-To-High-Uniform-Accuracy-Is-Intractable.html",
      "date": "February 28, 2023",
      "summary": "Statistical learning theory provides guidelines on the number of training samples needed for achieving desired accuracy in learning problems, especially emphasizing generalization error. However, this is not always adequate, particularly in security-sensitive areas or computational sciences, where uniform accuracy across all inputs is necessary. This paper quantifies the training samples required for uniform accuracy in learning problems involving ReLU neural networks, revealing that the number of samples needed exponentially increases with the network's depth and input dimension."
    }
  ],
  "desire": [
    {
      "title": "Learning RelU Networks To High Uniform Accuracy Is Intractable",
      "url": "/research/2023/02/28/Learning-Relu-Networks-To-High-Uniform-Accuracy-Is-Intractable.html",
      "date": "February 28, 2023",
      "summary": "Statistical learning theory provides guidelines on the number of training samples needed for achieving desired accuracy in learning problems, especially emphasizing generalization error. However, this is not always adequate, particularly in security-sensitive areas or computational sciences, where uniform accuracy across all inputs is necessary. This paper quantifies the training samples required for uniform accuracy in learning problems involving ReLU neural networks, revealing that the number of samples needed exponentially increases with the network's depth and input dimension."
    }
  ],
  "science": [
    {
      "title": "Learning RelU Networks To High Uniform Accuracy Is Intractable",
      "url": "/research/2023/02/28/Learning-Relu-Networks-To-High-Uniform-Accuracy-Is-Intractable.html",
      "date": "February 28, 2023",
      "summary": "Statistical learning theory provides guidelines on the number of training samples needed for achieving desired accuracy in learning problems, especially emphasizing generalization error. However, this is not always adequate, particularly in security-sensitive areas or computational sciences, where uniform accuracy across all inputs is necessary. This paper quantifies the training samples required for uniform accuracy in learning problems involving ReLU neural networks, revealing that the number of samples needed exponentially increases with the network's depth and input dimension."
    },
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "Image-caption pretraining, crucial for tasks like zero-shot image classification and object detection, faces challenges in aligning multiple concepts from captions to objects in images. We introduce a curriculum learning framework inspired by children's language learning from cognitive science, starting with simple image-caption pairs and gradually increasing complexity by adding more concepts. This method, which leverages knowledge from each phase for subsequent learning, outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios. The code is available at: https://github.com/hayyubi/cur_vl.git."
    }
  ],
  "quantify": [
    {
      "title": "Learning RelU Networks To High Uniform Accuracy Is Intractable",
      "url": "/research/2023/02/28/Learning-Relu-Networks-To-High-Uniform-Accuracy-Is-Intractable.html",
      "date": "February 28, 2023",
      "summary": "Statistical learning theory provides guidelines on the number of training samples needed for achieving desired accuracy in learning problems, especially emphasizing generalization error. However, this is not always adequate, particularly in security-sensitive areas or computational sciences, where uniform accuracy across all inputs is necessary. This paper quantifies the training samples required for uniform accuracy in learning problems involving ReLU neural networks, revealing that the number of samples needed exponentially increases with the network's depth and input dimension."
    }
  ],
  "dimension": [
    {
      "title": "Learning RelU Networks To High Uniform Accuracy Is Intractable",
      "url": "/research/2023/02/28/Learning-Relu-Networks-To-High-Uniform-Accuracy-Is-Intractable.html",
      "date": "February 28, 2023",
      "summary": "Statistical learning theory provides guidelines on the number of training samples needed for achieving desired accuracy in learning problems, especially emphasizing generalization error. However, this is not always adequate, particularly in security-sensitive areas or computational sciences, where uniform accuracy across all inputs is necessary. This paper quantifies the training samples required for uniform accuracy in learning problems involving ReLU neural networks, revealing that the number of samples needed exponentially increases with the network's depth and input dimension."
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article offers the first extensive review of vision transformer techniques specifically applied to human action recognition, a field gaining interest for its wide applications. Termed action transformers, these methods are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also delves into optimizing spatio-temporal attention in transformers for longer sequences and examines different learning strategies, including self-supervised and zero-shot learning, with their respective loss functions. Additionally, the survey assesses progress in benchmark evaluation scores and discusses the challenges and future directions in this research area."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    }
  ],
  "warp": [
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/On-Interaction-Between-Augmentations-and-Corruptions-in-Natural-Corruption-Robustness.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    }
  ],
  "color": [
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/On-Interaction-Between-Augmentations-and-Corruptions-in-Natural-Corruption-Robustness.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "Deep neural networks, while effective in complex vision tasks, suffer from overfitting. Current image augmentation techniques, largely linear and hand-crafted like scale, flip, and color jitter, fail to produce sufficiently challenging examples. We introduce a new augmentation approach, Mask-Reconstruct Augmentation (MRA), leveraging self-supervised masked autoencoders to generate distorted inputs. This method, inspired by masked image modeling's success in self-supervised learning, uses nonlinear transformations for regularization. Our extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks. The code is available at https://github.com/haohang96/MRA."
    }
  ],
  "correlation": [
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/On-Interaction-Between-Augmentations-and-Corruptions-in-Natural-Corruption-Robustness.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/research/2020/08/03/Are-Labels-Necessary-for-Neural-Architecture-Search.html",
      "date": "August 3, 2020",
      "summary": "This paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning if effective neural architectures can be discovered using only images without human-annotated labels. Through two experimental setups—sample-based and search-based—we investigate this possibility. In the sample-based approach, we evaluate 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. In the search-based experiments, we employ a recognized NAS algorithm, DARTS, with various unsupervised objectives, finding that architectures identified without labels perform competitively compared to those found with labels. These findings suggest that human-annotated labels may not be necessary for identifying efficient neural architectures, as image statistics alone could be sufficient."
    }
  ],
  "distance": [
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/On-Interaction-Between-Augmentations-and-Corruptions-in-Natural-Corruption-Robustness.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    }
  ],
  "similarity": [
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/On-Interaction-Between-Augmentations-and-Corruptions-in-Natural-Corruption-Robustness.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    },
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    },
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    }
  ],
  "tool": [
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/On-Interaction-Between-Augmentations-and-Corruptions-in-Natural-Corruption-Robustness.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    },
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "Monocular depth estimation's success hinges on large, diverse training datasets. Acquiring dense ground-truth depth is challenging, leading to diverse datasets with unique biases. We introduce tools for training with multiple datasets, despite incompatible annotations. Our approach includes a robust training objective unaffected by depth range and scale variations, employs principled multi-objective learning for data integration from various sources, and emphasizes the importance of pretraining encoders on auxiliary tasks. We tested our methods using five datasets, including 3D films, a novel, extensive data source. Through zero-shot cross-dataset transfer, we demonstrate our approach's superior generalization, significantly outperforming existing methods and establishing a new benchmark in monocular depth estimation."
    }
  ],
  "httpsgithubcomfacebookresearchaugmentationcorruption": [
    {
      "title": "On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness",
      "url": "/research/2021/11/19/On-Interaction-Between-Augmentations-and-Corruptions-in-Natural-Corruption-Robustness.html",
      "date": "November 19, 2021",
      "summary": "Building robust models in computer vision requires invariance to various image corruptions like warping, noise, or color shifts. Despite new data augmentations improving performance on ImageNet-C, a corruption benchmark, the correlation between data augmentations and test-time corruptions remains unclear. We created a feature space for image transforms and introduced the Minimal Sample Distance to show a strong correlation between augmentation-corruption similarity and performance. Our study reveals that training with perceptually similar augmentations enhances test error, but augmentations may not generalize well beyond benchmarks. Our findings and tools aim to enhance robustness to image corruptions, with code available at https://github.com/facebookresearch/augmentation-corruption."
    }
  ],
  "duality": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    }
  ],
  "covariance": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "algebraically": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    }
  ],
  "condition": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    },
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/research/2021/08/16/An-Empirical-Study-of-Training-Self-Supervised-Vision-Transformers.html",
      "date": "August 16, 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    },
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "We introduce a Fourier neural network (FNN) aligned with Fourier decomposition, utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design ensures easy integration with more complex networks for data processing tasks. The FNN's efficacy is demonstrated on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits include solution validity beyond the training scope, model interpretability, and ease of use."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "relationship": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    }
  ],
  "connect": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "Online Normalization is a novel method for normalizing neural network hidden activations, offering a batch-independent alternative with comparable accuracy to Batch Normalization. It overcomes Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. Applicable to recurrent, fully connected networks, and those with high activation memory requirements, it demonstrates effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    }
  ],
  "vicreg": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    }
  ],
  "hyperparameter": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    },
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "assumption": [
    {
      "title": "On The Duality Between Contrastive and Non-contrastive Self-Supervised Learning",
      "url": "/research/2023/06/26/On-The-Duality-Between-Contrastive-and-Non-contrastive-Self-Supervised-Learning.html",
      "date": "June 26, 2023",
      "summary": "Recent self-supervised learning approaches for image representation can be broadly divided into contrastive and non-contrastive methods. This study focuses on their theoretical similarities rather than their differences. By developing contrastive and covariance-based non-contrastive criteria that are algebraically related and equivalent under certain conditions, we demonstrate the close relationship between these two families. We further explore popular methods, propose variations, and connect our theoretical findings to current practices. Our analysis includes improving SimCLR's performance to match that of VICReg through precise hyperparameter adjustments and challenging the assumption that non-contrastive methods require large output dimensions. Our results indicate that with better network design and hyperparameter tuning, the performance gap between contrastive and non-contrastive methods can be minimized, suggesting that integrating various state-of-the-art methods could enhance understanding of self-supervised learning."
    }
  ],
  "shrink": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    }
  ],
  "perturb": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    }
  ],
  "starting": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    }
  ],
  "machine": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    },
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    }
  ],
  "arrive": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    }
  ],
  "nature": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "selection": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions for neural networks in reinforcement learning, the sigmoid-weighted linear unit (SiLU) and its derivative (dSiLU), and challenges the need for experience replay and separate target networks in deep reinforcement learning. By employing on-policy learning with eligibility traces and softmax action selection, the study achieves state-of-the-art results in stochastic SZ-Tetris and a small-board Tetris using TD(λ) learning and shallow dSiLU network agents. Furthermore, it outperforms the DQN algorithm in the Atari 2600 domain with a deep Sarsa(λ) agent utilizing SiLU and dSiLU hidden units, suggesting a competitive alternative to traditional DQN approaches."
    },
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "6D object pose estimation is crucial for robotic bin-picking and essential for many industrial applications. Annotating custom datasets for each bin-picking scenario is challenging. We introduce an iterative self-training framework for sim-to-real 6D object pose estimation, enhancing cost-effective robotic grasping. We create a photo-realistic simulator for synthesizing virtual data to train an initial pose estimation network. This network, acting as a teacher model, predicts poses on unlabeled real data. We develop an adaptive selection scheme to filter reliable predictions for updating a student model with pseudo labels for real data pose estimation. By iteratively refining the teacher model with the trained student model, we improve the quality of pseudo labels. Our method, tested on a public benchmark and a newly-released dataset, shows an 11.49% and 22.62% ADD(-S) improvement, respectively, and a 19.54% increase in robotic bin-picking success, demonstrating the effectiveness of iterative sim-to-real approaches. Project homepage: www.cse.cuhk.edu.hk/~kaichen/sim2real_pose.html"
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    }
  ],
  "initialize": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    }
  ],
  "save": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    }
  ],
  "discrepancy": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    }
  ],
  "persist": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    }
  ],
  "expense": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "Semi-supervised learning (SSL) utilizes unlabeled data to boost model performance but often at the expense of increased complexity. This paper introduces FixMatch, a streamlined SSL algorithm that simplifies existing methods. FixMatch uses a model's predictions on weakly-augmented unlabeled images to generate pseudo-labels, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. Remarkably, FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness. The code is publicly available."
    }
  ],
  "introduces": [
    {
      "title": "Shrink and Perturb: On Warm-Starting Neural Network Training",
      "url": "/research/2020/12/31/On-Warm-Starting-Neural-Network-Training.html",
      "date": "December 31, 2020",
      "summary": "In machine learning systems where data arrive incrementally, either passively due to the problem's nature or actively through sample selection, it's common to build a sequence of models that incorporate progressively more data. Although intuitively, using the solution of a previous model to initialize a new one should save time, this warm start often results in poorer generalization compared to models initialized randomly, despite similar training losses. This discrepancy persists even when hyperparameter adjustments are made, often at the expense of the time saved through warm starting. This work investigates this phenomenon and introduces shrink and perturb, a simple yet effective method to mitigate the issue, with experiments demonstrating its utility in various contexts."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    }
  ],
  "imaging": [
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    }
  ],
  "sensing": [
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    }
  ],
  "organize": [
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    }
  ],
  "sonn": [
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    }
  ],
  "filter": [
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "6D object pose estimation is crucial for robotic bin-picking and essential for many industrial applications. Annotating custom datasets for each bin-picking scenario is challenging. We introduce an iterative self-training framework for sim-to-real 6D object pose estimation, enhancing cost-effective robotic grasping. We create a photo-realistic simulator for synthesizing virtual data to train an initial pose estimation network. This network, acting as a teacher model, predicts poses on unlabeled real data. We develop an adaptive selection scheme to filter reliable predictions for updating a student model with pseudo labels for real data pose estimation. By iteratively refining the teacher model with the trained student model, we improve the quality of pseudo labels. Our method, tested on a public benchmark and a newly-released dataset, shows an 11.49% and 22.62% ADD(-S) improvement, respectively, and a 19.54% increase in robotic bin-picking success, demonstrating the effectiveness of iterative sim-to-real approaches. Project homepage: www.cse.cuhk.edu.hk/~kaichen/sim2real_pose.html"
    },
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    }
  ],
  "parameters": [
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    }
  ],
  "equivalent": [
    {
      "title": "Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution",
      "url": "/research/2023/10/25/Operational-Neural-Networks-for-Parameter-Efficient-Hyperspectral-Single-Image-Super-Resolution.html",
      "date": "October 25, 2023",
      "summary": "Hyperspectral Imaging, a key tool in remote sensing, captures more spectral information than standard images but with lower spatial resolution. Super-resolution aims to enhance low-resolution inputs, with modern techniques often using deep convolutional neural networks (CNNs) that rely on non-linear activation functions. Recently, self-organized operational neural networks (SONNs) have been proposed, utilizing learnable non-linear functions instead of convolutional filters, to address the depth issue of CNNs. This study enhances a popular super-resolution model with operational filters for better hyperspectral image performance, examining the impact of residual connections and normalization types. Operational neural networks, despite fewer parameters, outperform CNN equivalents on small datasets, with the code available on Github."
    }
  ],
  "limitation": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    }
  ],
  "randconv": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    }
  ],
  "kernel": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "integrity": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    }
  ],
  "style": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    },
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    }
  ],
  "diversity": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "affine": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    }
  ],
  "contrast": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    }
  ],
  "diversification": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    }
  ],
  "generator": [
    {
      "title": "Progressive Random Convolutions for Single Domain Generalization",
      "url": "/research/2023/04/02/Progressive-Random-Convolutions-for-Single-Domain-Generalization.html",
      "date": "April 2, 2023",
      "summary": "To address the limitations of Random Convolutions (RandConv) augmentation in single domain generalization, we introduce a Progressive Random Convolution (Pro-RandConv) method that layers random convolutions with small kernel sizes to maintain semantic integrity and enhance style diversity without increasing kernel size. Additionally, we enhance the random convolution block with deformable offsets and affine transformations for further texture and contrast diversification. Our simple yet powerful augmentation strategy surpasses current state-of-the-art methods in single domain generalization benchmarks without relying on complex generators or adversarial learning."
    }
  ],
  "randaugment": [
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    }
  ],
  "search": [
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/research/2020/08/03/Are-Labels-Necessary-for-Neural-Architecture-Search.html",
      "date": "August 3, 2020",
      "summary": "This paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning if effective neural architectures can be discovered using only images without human-annotated labels. Through two experimental setups—sample-based and search-based—we investigate this possibility. In the sample-based approach, we evaluate 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. In the search-based experiments, we employ a recognized NAS algorithm, DARTS, with various unsupervised objectives, finding that architectures identified without labels perform competitively compared to those found with labels. These findings suggest that human-annotated labels may not be necessary for identifying efficient neural architectures, as image statistics alone could be sufficient."
    },
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "We introduce an algorithm designed to identify networks that are both simple and highly generalizable by searching for extensive areas of flat minima in the error function, where the error rate is relatively stable. These flat minima are associated with lower risks of overfitting according to MDL principles. Despite needing to calculate second-order derivatives, the algorithm maintains a complexity level comparable to backpropagation. We tested it on both feedforward and recurrent networks, and in stock market prediction tasks, it performed better than traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    }
  ],
  "strategies": [
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    }
  ],
  "adoption": [
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method, applies a single augmentation to each image and outperforms existing methods with minimal complexity and cost. We conducted extensive experiments to validate its performance against state-of-the-art methods across various image classification scenarios and explored its effectiveness through multiple ablation studies involving different augmentation spaces and methods. Our work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Highlighting a stagnation in automatic augmentation research, we conclude with proposed best practices for future advancements in the field."
    }
  ],
  "hinder": [
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "Temporal action localization (TAL) is crucial for video understanding but is hindered by benchmarks that rely on coarse action classes, leading to model overfitting and ambiguous annotations. Addressing these issues, we introduce FineAction, a large-scale, fine-grained video dataset with 103K instances across 106 action categories in 17K videos. FineAction offers a diverse, densely annotated dataset with co-occurring actions, posing new challenges for TAL. We evaluated popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and proposed a baseline method achieving a 13.17% mAP. FineAction aims to propel TAL research forward and is accessible online."
    },
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "The limited number of videos in current action classification datasets like UCF-101 and HMDB-51 hinders the identification of effective video architectures due to similar performance across small-scale benchmarks. This study re-examines top architectures using the new, significantly larger Kinetics Human Action Video dataset, featuring 400 classes and over 400 clips per class from challenging YouTube videos. It analyzes the impact of this dataset on the performance of existing architectures and the benefits of pre-training on Kinetics. Additionally, the paper introduces the Two-Stream Inflated 3D ConvNet (I3D), an advancement that expands 2D ConvNet designs into 3D for enhanced video feature extraction. This model, pre-trained on Kinetics, sets new benchmarks in action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    }
  ],
  "proxy": [
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    }
  ],
  "customization": [
    {
      "title": "RandAugment: Practical automated data augmentation with a reduced search space",
      "url": "/research/2019/11/14/RandAugment-Practical-automated-data-augmentation-with-a-reduced-search-space.html",
      "date": "November 14, 2019",
      "summary": "Recent advancements have shown the effectiveness of data augmentation in enhancing deep learning model generalization, with automated strategies setting new benchmarks in image classification and object detection. These methods also improved semi-supervised learning and robustness against image corruptions. However, their adoption is hindered by a separate search phase that increases training complexity and computational costs, and they cannot adapt regularization based on model or dataset size. Our work, RandAugment, addresses these issues by reducing the search space, allowing direct training on the target task without a separate proxy task, and enabling customization of regularization strength. It achieves or exceeds all previous automated augmentation methods on various datasets, including a notable accuracy improvement on ImageNet and object detection tasks. RandAugment's interpretable hyperparameter also facilitates exploration of data augmentation's impact across different models and datasets. Code is available online."
    }
  ],
  "read": [
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    }
  ],
  "digit": [
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    }
  ],
  "text": [
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    },
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    },
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "We introduce DiffBIR, a framework using pretrained text-to-image diffusion models for blind image restoration. It consists of a two-stage pipeline: initially, we pretrain a restoration module on various degradations to enhance real-world applicability. Then, we use latent diffusion models for realistic restoration, including a novel injective modulation sub-network, LAControlNet, for fine-tuning, and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, we provide a controllable module for users to adjust quality and fidelity during the denoising process. Our extensive tests show DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets. The code is available online."
    },
    {
      "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "Diffusion models, effective for generating high-dimensional perceptual data like images, lack established standard practices. This study enhances noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Our large-scale study demonstrates superior performance over established diffusion methods in high-resolution text-to-image synthesis. We introduce a transformer-based architecture for text-to-image generation, improving text comprehension, typography, and human preference ratings. Our findings show predictable scaling trends, with lower validation loss correlating with improved synthesis. The largest models outperform state-of-the-art approaches, and our data, code, and model weights will be publicly accessible."
    }
  ],
  "character": [
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    }
  ],
  "recognizing": [
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    }
  ],
  "complex": [
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "Neural networks parameterized for continuous, differentiable signal representations provide a promising paradigm but struggle with modeling fine detail and representing spatial and temporal derivatives critical for signals defined by partial differential equations. We introduce sinusoidal representation networks (SIRENs), utilizing periodic activation functions to effectively capture complex natural signals and their derivatives. Our analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. We also showcase SIRENs' application in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations, and extend their use with hypernetworks to learn priors for SIREN functions. A video overview and full applications are available on our project website."
    }
  ],
  "scene": [
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "photograph": [
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    }
  ],
  "trail": [
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    }
  ],
  "street": [
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    }
  ],
  "photo": [
    {
      "title": "Reading Digits in Natural Images With Unsupervised Feature Learning",
      "url": "/research/2011/01/01/Reading-Digits-in-Natural-Images-With-Unsupervised-Feature-Learning.html",
      "date": "January 1, 2011",
      "summary": "Detecting text in natural images is a challenging task, crucial for various applications. While character recognition in documents is largely solved, recognizing characters in complex scenes, such as photographs, is much harder, with existing methods trailing behind human performance. This paper tackles digit recognition from street level photos, introducing a new dataset of over 600,000 labeled digits from Street View images. We highlight the challenge of digit recognition using hand-designed features and apply unsupervised feature learning methods, demonstrating significant improvements over traditional approaches."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "rethink": [
    {
      "title": "Rethinking Batch in BatchNorm",
      "url": "/research/2021/05/17/Rethinking-Batch-in-BatchNorm.html",
      "date": "May 17, 2021",
      "summary": "BatchNorm, essential in convolutional neural networks, behaves uniquely due to its batch-based operation, leading to performance issues in visual recognition tasks. This paper identifies these issues and suggests reevaluating the concept of batch in BatchNorm for better performance, aiming to guide researchers in its effective use."
    }
  ],
  "behave": [
    {
      "title": "Rethinking Batch in BatchNorm",
      "url": "/research/2021/05/17/Rethinking-Batch-in-BatchNorm.html",
      "date": "May 17, 2021",
      "summary": "BatchNorm, essential in convolutional neural networks, behaves uniquely due to its batch-based operation, leading to performance issues in visual recognition tasks. This paper identifies these issues and suggests reevaluating the concept of batch in BatchNorm for better performance, aiming to guide researchers in its effective use."
    }
  ],
  "operation": [
    {
      "title": "Rethinking Batch in BatchNorm",
      "url": "/research/2021/05/17/Rethinking-Batch-in-BatchNorm.html",
      "date": "May 17, 2021",
      "summary": "BatchNorm, essential in convolutional neural networks, behaves uniquely due to its batch-based operation, leading to performance issues in visual recognition tasks. This paper identifies these issues and suggests reevaluating the concept of batch in BatchNorm for better performance, aiming to guide researchers in its effective use."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    }
  ],
  "suggests": [
    {
      "title": "Rethinking Batch in BatchNorm",
      "url": "/research/2021/05/17/Rethinking-Batch-in-BatchNorm.html",
      "date": "May 17, 2021",
      "summary": "BatchNorm, essential in convolutional neural networks, behaves uniquely due to its batch-based operation, leading to performance issues in visual recognition tasks. This paper identifies these issues and suggests reevaluating the concept of batch in BatchNorm for better performance, aiming to guide researchers in its effective use."
    }
  ],
  "reevaluate": [
    {
      "title": "Rethinking Batch in BatchNorm",
      "url": "/research/2021/05/17/Rethinking-Batch-in-BatchNorm.html",
      "date": "May 17, 2021",
      "summary": "BatchNorm, essential in convolutional neural networks, behaves uniquely due to its batch-based operation, leading to performance issues in visual recognition tasks. This paper identifies these issues and suggests reevaluating the concept of batch in BatchNorm for better performance, aiming to guide researchers in its effective use."
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "researcher": [
    {
      "title": "Rethinking Batch in BatchNorm",
      "url": "/research/2021/05/17/Rethinking-Batch-in-BatchNorm.html",
      "date": "May 17, 2021",
      "summary": "BatchNorm, essential in convolutional neural networks, behaves uniquely due to its batch-based operation, leading to performance issues in visual recognition tasks. This paper identifies these issues and suggests reevaluating the concept of batch in BatchNorm for better performance, aiming to guide researchers in its effective use."
    }
  ],
  "drophead": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    }
  ],
  "multihead": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    }
  ],
  "head": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    }
  ],
  "avoid": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "dominance": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    }
  ],
  "scheduler": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    }
  ],
  "bleu": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    },
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    }
  ],
  "score": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article offers the first extensive review of vision transformer techniques specifically applied to human action recognition, a field gaining interest for its wide applications. Termed action transformers, these methods are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also delves into optimizing spatio-temporal attention in transformers for longer sequences and examines different learning strategies, including self-supervised and zero-shot learning, with their respective loss functions. Additionally, the survey assesses progress in benchmark evaluation scores and discusses the challenges and future directions in this research area."
    }
  ],
  "wmt14": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    }
  ],
  "en": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    }
  ],
  "translation": [
    {
      "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
      "url": "/research/2020/11/01/Scheduled-DropHead-A-Regularization-Method-for-Transformer-Models.html",
      "date": "November 1, 2020",
      "summary": "We introduce DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. We also propose a dropout rate scheduler to optimize training, showing improvements in transformer models with up to 0.9 BLEU score increase on the WMT14 En-De translation task and around 1.0 accuracy boost for text classification tasks."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    },
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    }
  ],
  "sigmoid": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    },
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    },
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "Gated Linear Units (GLUs) involve the component-wise product of two linear projections, with one undergoing a sigmoid function. Exploring GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers, we find certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    }
  ],
  "pairwise": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    }
  ],
  "siglip": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    }
  ],
  "outperforms": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    }
  ],
  "tpuv4": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    }
  ],
  "chip": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    }
  ],
  "siglit": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    }
  ],
  "disentanglement": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    }
  ],
  "count": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    }
  ],
  "diminish": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    }
  ],
  "return": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    }
  ],
  "release": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    },
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "6D object pose estimation is crucial for robotic bin-picking and essential for many industrial applications. Annotating custom datasets for each bin-picking scenario is challenging. We introduce an iterative self-training framework for sim-to-real 6D object pose estimation, enhancing cost-effective robotic grasping. We create a photo-realistic simulator for synthesizing virtual data to train an initial pose estimation network. This network, acting as a teacher model, predicts poses on unlabeled real data. We develop an adaptive selection scheme to filter reliable predictions for updating a student model with pseudo labels for real data pose estimation. By iteratively refining the teacher model with the trained student model, we improve the quality of pseudo labels. Our method, tested on a public benchmark and a newly-released dataset, shows an 11.49% and 22.62% ADD(-S) improvement, respectively, and a 19.54% increase in robotic bin-picking success, demonstrating the effectiveness of iterative sim-to-real approaches. Project homepage: www.cse.cuhk.edu.hk/~kaichen/sim2real_pose.html"
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "url": [
    {
      "title": "Sigmoid Loss for Language Image Pre-Training",
      "url": "/research/2023/09/27/Sigmoid-Loss-for-Language-Image-Pre-Training.html",
      "date": "September 27, 2023",
      "summary": "We introduce a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP), which outperforms standard contrastive learning by not requiring a global normalization of pairwise similarities. Our approach facilitates scaling up the batch size and improves performance even with smaller batches. Using only four TPUv4 chips, we trained a SigLiT model achieving 84.5% ImageNet zero-shot accuracy in two days. Our method's disentanglement of batch size from the loss allows for examining the impact of example versus pair counts and the negative to positive ratio. Pushing the batch size to one million showed diminishing returns, with 32k being a sufficient batch size. We encourage further research in language-image pre-training efficiency and quality by releasing our models at the provided URL."
    }
  ],
  "occlusion": [
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    }
  ],
  "working": [
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    }
  ],
  "unclear": [
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    }
  ],
  "perspective": [
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    }
  ],
  "differ": [
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "Recent studies have shown that Convolutional Neural Networks (CNNs), which were thought to recognize objects by learning complex shapes, are actually more biased towards recognizing textures. This finding, based on a series of tests comparing CNNs with human observers using images with conflicting texture and shape cues, contradicts the traditional understanding of how these networks process images. We discovered that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, when we trained a standard CNN architecture (ResNet-50) on a stylized version of ImageNet, designed to emphasize shape, the network's performance aligned more closely with human behavior. This training not only matched human performance in controlled experiments (involving 97 observers across 48,560 trials) but also enhanced object detection and robustness against image distortions, underscoring the benefits of shape-based representations in visual recognition systems."
    }
  ],
  "measurement": [
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    }
  ],
  "mae": [
    {
      "title": "Understanding Masked Image Modeling via Learning Occlusion Invariant Feature",
      "url": "/research/2022/08/08/Understanding-Masked-Image-Modeling-via-Learning-Occlusion-Invariant-Feature.html",
      "date": "August 8, 2022",
      "summary": "Masked Image Modeling (MIM) has been successful in self-supervised visual recognition, yet its working mechanism remains unclear, especially compared to siamese approaches like contrastive learning. This study introduces a new perspective that MIM implicitly learns occlusion-invariant features, similar to the invariances learned by siamese methods. We show that MIM can be interpreted within a unified framework alongside traditional methods, differing only in data transformations and similarity measurements. Using MAE as an example, we find that MIM's success is less about similarity functions and more about the occlusion-invariant features it learns, which provide a beneficial initialization for vision transformers despite possibly being less semantic. Our findings encourage the development of more effective self-supervised methods in computer vision"
    },
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "Self-supervised transformer training methods have shown excellent results in various fields. Unlike previous models like masked autoencoders (MAE) which use a single normalization layer for the class token [CLS] and other tokens, we introduce a novel normalization technique that normalizes the [CLS] token and normal tokens separately. This approach aims to better capture their unique characteristics, improving performance in downstream tasks. Our findings indicate that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks."
    }
  ],
  "difficulty": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "Curriculum Learning by Dynamic Instance Hardness",
      "url": "/research/2020/12/06/Curriculum-Learning-by-Dynamic-Instance-Hardness.html",
      "date": "December 6, 2020",
      "summary": "This paper explores the adaptation of deep neural network (DNN) training using a method called dynamic instance hardness (DIH), which measures a sample's learning difficulty over time. By tracking the exponential moving average of a sample's hardness, DIH provides a stable indicator of learning progress. Early predictions of a sample's DIH allow for prioritizing more challenging samples and deprioritizing easier ones, leading to a DIH guided curriculum learning (DIHCL) approach. DIHCL enhances learning efficiency and model accuracy without extra computational costs, as it leverages data from the training process itself. Tested on 11 datasets, DIHCL surpasses traditional training methods and recent curriculum learning techniques in both efficiency and effectiveness. The implementation is publicly available."
    },
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "Self-paced curriculum learning (SCL) has been effective in various fields, employing easy-to-hard sampling based on data difficulty. However, in medical image analysis tasks like universal lesion detection, it faces challenges due to inaccurate difficulty estimation and the under-utilization of hard samples. This paper introduces a novel approach, mixed-order self-paced curriculum learning (Mo-SCL), which combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches to address these issues. Through theoretical analysis and experiments on the DeepLesion dataset, Mo-SCL is shown to enhance lesion detection accuracy in state-of-the-art methods without additional network modifications."
    },
    {
      "title": "Taming the Waves: Sine as Activation Function in Deep Neural Networks",
      "url": "/research/2016/11/04/Taming-the-Waves-Sine-as-Activation-Function-in-Deep-Neural-Networks.html",
      "date": "November 4, 2016",
      "summary": "This paper examines the challenges of training deep neural networks with sinusoidal activation functions, despite their successful use in specific applications. It explains the difficulty in training due to the emergence of numerous shallow local minima from the architecture. The study also reveals that successful learning in typical classification tasks occurs when the network effectively disregards the periodic cycles. Furthermore, it demonstrates that for certain non-trivial tasks, networks with sinusoidal activations can outperform those using traditional monotonic functions."
    }
  ],
  "multilayer": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    }
  ],
  "descent": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "saturation": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    }
  ],
  "hide": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "Online Normalization is a novel method for normalizing neural network hidden activations, offering a batch-independent alternative with comparable accuracy to Batch Normalization. It overcomes Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. Applicable to recurrent, fully connected networks, and those with high activation memory requirements, it demonstrates effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    },
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions for neural networks in reinforcement learning, the sigmoid-weighted linear unit (SiLU) and its derivative (dSiLU), and challenges the need for experience replay and separate target networks in deep reinforcement learning. By employing on-policy learning with eligibility traces and softmax action selection, the study achieves state-of-the-art results in stochastic SZ-Tetris and a small-board Tetris using TD(λ) learning and shallow dSiLU network agents. Furthermore, it outperforms the DQN algorithm in the Atari 2600 domain with a deep Sarsa(λ) agent utilizing SiLU and dSiLU hidden units, suggesting a competitive alternative to traditional DQN approaches."
    }
  ],
  "discover": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "Recent studies have shown that Convolutional Neural Networks (CNNs), which were thought to recognize objects by learning complex shapes, are actually more biased towards recognizing textures. This finding, based on a series of tests comparing CNNs with human observers using images with conflicting texture and shape cues, contradicts the traditional understanding of how these networks process images. We discovered that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, when we trained a standard CNN architecture (ResNet-50) on a stylized version of ImageNet, designed to emphasize shape, the network's performance aligned more closely with human behavior. This training not only matched human performance in controlled experiments (involving 97 observers across 48,560 trials) but also enhanced object detection and robustness against image distortions, underscoring the benefits of shape-based representations in visual recognition systems."
    },
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "We introduce an algorithm designed to identify networks that are both simple and highly generalizable by searching for extensive areas of flat minima in the error function, where the error rate is relatively stable. These flat minima are associated with lower risks of overfitting according to MDL principles. Despite needing to calculate second-order derivatives, the algorithm maintains a complexity level comparable to backpropagation. We tested it on both feedforward and recurrent networks, and in stock market prediction tasks, it performed better than traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    }
  ],
  "desaturate": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "explains": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    }
  ],
  "plateaus": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    }
  ],
  "saturate": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    }
  ],
  "jacobian": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    }
  ],
  "associate": [
    {
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "url": "/research/2010/03/31/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks.html",
      "date": "March 31, 2010",
      "summary": "Before 2006, deep multi-layer neural networks were difficult to train, but recent algorithms have enabled successful training, showing that deeper architectures outperform shallower ones. These advancements were achieved with new initialization or training mechanisms. This study aims to understand why standard gradient descent from random initialization struggles with deep neural networks, to improve upon current successes and develop better future algorithms. It investigates the impact of non-linear activation functions, finding the logistic sigmoid activation unsuitable for deep networks due to saturation issues in top hidden layers. It discovers that saturated units can desaturate over time, which explains training plateaus. A new non-linearity that saturates less is suggested as beneficial. The study also examines how activations and gradients change across layers and during training, proposing a new initialization scheme for faster convergence, highlighting the importance of the singular values of the Jacobian associated with each layer being close to 1."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "We introduce an algorithm designed to identify networks that are both simple and highly generalizable by searching for extensive areas of flat minima in the error function, where the error rate is relatively stable. These flat minima are associated with lower risks of overfitting according to MDL principles. Despite needing to calculate second-order derivatives, the algorithm maintains a complexity level comparable to backpropagation. We tested it on both feedforward and recurrent networks, and in stock market prediction tasks, it performed better than traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    }
  ],
  "what": [
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    }
  ],
  "cl": [
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "Self-Paced Curriculum Learning",
      "url": "/research/2015/01/01/Self-Paced-Curriculum-Learning.html",
      "date": "January 1, 2015",
      "summary": "Curriculum learning (CL) and self-paced learning (SPL) are methods inspired by the incremental complexity of human and animal learning, with CL using a fixed curriculum based on prior knowledge and SPL adjusting dynamically to the learner's pace but struggling with prior knowledge and overfitting. This paper introduces a unified framework called self-paced curriculum learning (SPCL), blending both methods' strengths by considering prior knowledge and ongoing learning progress through a concise optimization problem. SPCL mimics a collaborative instructor-student learning mode, showing empirical advantages in two tasks."
    }
  ],
  "pattern": [
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    },
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper investigates how neural networks generalize on small, algorithmically generated datasets, focusing on data efficiency, memorization, generalization, and learning speed. It highlights instances where neural networks grok patterns, significantly improving their generalization ability from mere chance to perfect accuracy, even beyond the point of overfitting. Additionally, the study finds that generalization on smaller datasets demands more optimization. It posits that such datasets are ideal for exploring the enigmatic phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    },
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    },
    {
      "title": "Are Convolutional Neural Networks or Transformers more like human vision?",
      "url": "/research/2021/07/01/Are-Convolutional-Neural-Networks-or-Transformers-more-like-human-vision.html",
      "date": "July 1, 2021",
      "summary": "This study investigates the performance of Convolutional Neural Networks (CNNs) and attention-based Vision Transformers (ViTs) in computer vision tasks. While CNNs excel in accuracy, ViTs offer a different approach with weaker inductive biases. By analyzing error patterns, we find that ViTs exhibit consistency with human errors, suggesting potential for more human-like vision models and insights into human object recognition."
    }
  ],
  "orient": [
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "separation": [
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    }
  ],
  "frequency": [
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    }
  ],
  "harmonize": [
    {
      "title": "What do Self-supervised vision transformers learn?",
      "url": "/research/2023/05/01/What-do-Self-supervised-vision-transformers-learn.html",
      "date": "May 1, 2023",
      "summary": "This study compares contrastive learning (CL) and masked image modeling (MIM) in self-supervised Vision Transformers (ViTs), focusing on their representations and downstream task performance. Key findings include: (1) CL captures longer-range global patterns and is more shape-oriented, aiding in linear image separation but leading to homogenous self-attentions. (2) CL focuses on low-frequency signals, while MIM emphasizes high-frequencies, making MIM more texture-oriented. (3) CL is significant in later layers, whereas MIM targets early layers. The study suggests CL and MIM can be harmonized to leverage both methods' strengths, enhancing performance."
    }
  ],
  "pace": [
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "Self-Paced Curriculum Learning",
      "url": "/research/2015/01/01/Self-Paced-Curriculum-Learning.html",
      "date": "January 1, 2015",
      "summary": "Curriculum learning (CL) and self-paced learning (SPL) are methods inspired by the incremental complexity of human and animal learning, with CL using a fixed curriculum based on prior knowledge and SPL adjusting dynamically to the learner's pace but struggling with prior knowledge and overfitting. This paper introduces a unified framework called self-paced curriculum learning (SPCL), blending both methods' strengths by considering prior knowledge and ongoing learning progress through a concise optimization problem. SPCL mimics a collaborative instructor-student learning mode, showing empirical advantages in two tasks."
    }
  ],
  "multiclass": [
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    }
  ],
  "popularity": [
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    }
  ],
  "beam": [
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    }
  ],
  "teacher": [
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "Training data-efficient image transformers & distillation through attention",
      "url": "/research/2021/01/15/Training-data-efficient-image-transformers-&-distillation-through-attention.html",
      "date": "January 15, 2021",
      "summary": "This study introduces DeiT, competitive vision transformers trained solely on ImageNet without convolutions, achieving up to 83.1% top-1 accuracy on ImageNet, using a single computer in under 3 days. Notably, it presents a unique teacher-student strategy for transformers, leveraging a distillation token for efficient learning from a convolutional network teacher. This method results in performance on par with convolutional networks, achieving up to 85.2% accuracy on ImageNet and demonstrating effective transferability to other tasks. The authors provide access to their code and models."
    },
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "6D object pose estimation is crucial for robotic bin-picking and essential for many industrial applications. Annotating custom datasets for each bin-picking scenario is challenging. We introduce an iterative self-training framework for sim-to-real 6D object pose estimation, enhancing cost-effective robotic grasping. We create a photo-realistic simulator for synthesizing virtual data to train an initial pose estimation network. This network, acting as a teacher model, predicts poses on unlabeled real data. We develop an adaptive selection scheme to filter reliable predictions for updating a student model with pseudo labels for real data pose estimation. By iteratively refining the teacher model with the trained student model, we improve the quality of pseudo labels. Our method, tested on a public benchmark and a newly-released dataset, shows an 11.49% and 22.62% ADD(-S) improvement, respectively, and a 19.54% increase in robotic bin-picking success, demonstrating the effectiveness of iterative sim-to-real approaches. Project homepage: www.cse.cuhk.edu.hk/~kaichen/sim2real_pose.html"
    }
  ],
  "clustering": [
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "penultimate": [
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    }
  ],
  "resemblance": [
    {
      "title": "When Does Label Smoothing Help?",
      "url": "/research/2020/06/10/When-Does-Label-Smoothing-Help.html",
      "date": "June 10, 2020",
      "summary": "Label smoothing, which blends hard targets with a uniform distribution across labels, enhances the generalization and learning pace of multi-class neural networks. This technique is used in top models for image classification, language translation, and speech recognition, as it prevents networks from becoming overly confident. Despite its popularity, the underlying mechanisms of label smoothing remain elusive. Our findings suggest that label smoothing not only boosts generalization but also enhances model calibration, benefiting beam-search processes. However, it reduces the effectiveness of knowledge distillation when a teacher network employs label smoothing. We demonstrate that label smoothing encourages tighter clustering of same-class examples in the penultimate layer, impacting the model's ability to capture class resemblances necessary for distillation but not affecting generalization or prediction calibration."
    }
  ],
  "box": [
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "We introduce a system that trains deep neural networks for object detection using synthetic images, leveraging domain randomization to manage real-world data variability. This involves randomizing simulator parameters—like lighting and object textures—in non-realistic ways, helping the network to identify essential object features. Our findings reveal that networks can achieve impressive performance with just synthetic data and further improve with real data fine-tuning. This suggests the potential of using cost-effective synthetic data for training, circumventing the challenges of acquiring vast amounts of real-world data or creating detailed synthetic environments. We validate our method through car bounding box detection on the KITTI dataset."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "argue": [
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    }
  ],
  "compress": [
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    }
  ],
  "gaussian": [
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    },
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "We introduce the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. Our empirical evaluation across computer vision, natural language processing, and speech tasks shows that GELU offers performance improvements over ReLU and ELU activations."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "optimizer": [
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    },
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    },
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    }
  ],
  "alternate": [
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    }
  ],
  "sparsify": [
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    },
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "transformerlike": [
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    }
  ],
  "compres": [
    {
      "title": "White-Box Transformers via Sparse Rate Reduction",
      "url": "/research/2023/06/01/White-Box-Transformers-via-Sparse-Rate-Reduction.html",
      "date": "June 1, 2023",
      "summary": "This paper argues that representation learning aims to compress data into low-dimensional Gaussian distributions, evaluated by a unified objective called sparse rate reduction. It interprets popular deep networks, like transformers, as iterative optimizers of this objective. Specifically, it shows how transformer blocks, through alternating optimization, compress and sparsify data representations. This approach yields mathematically interpretable transformer-like networks that effectively compress and sparsify large-scale data, achieving competitive performance on datasets like ImageNet. The code is available online."
    }
  ],
  "look": [
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    }
  ],
  "notion": [
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    }
  ],
  "sota": [
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    }
  ],
  "lack": [
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "The Vision Transformer (ViT) offers simplicity, robustness, and superior performance in vision tasks but struggles with small-scale datasets due to its lack of inherent inductive biases and dependence on large-scale pre-training datasets like ImageNet and JFT. This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of ViTs without large-scale pre-training or modifications to the architecture or loss functions. Experiments show improved performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, maintaining ViT's attention to relevant regions and robustness. The code and models are available online."
    },
    {
      "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "Diffusion models, effective for generating high-dimensional perceptual data like images, lack established standard practices. This study enhances noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Our large-scale study demonstrates superior performance over established diffusion methods in high-resolution text-to-image synthesis. We introduce a transformer-based architecture for text-to-image generation, improving text comprehension, typography, and human preference ratings. Our findings show predictable scaling trends, with lower validation loss correlating with improved synthesis. The largest models outperform state-of-the-art approaches, and our data, code, and model weights will be publicly accessible."
    },
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    }
  ],
  "map": [
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    },
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper presents the AVA dataset, featuring 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets focused on composite actions in shorter clips with sparse annotations, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. It highlights the challenges in action recognition, introduces a novel localization approach surpassing existing benchmarks but shows modest performance on AVA (15.6% mAP), pointing out the necessity for advanced video understanding methods."
    },
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "Temporal action localization (TAL) is crucial for video understanding but is hindered by benchmarks that rely on coarse action classes, leading to model overfitting and ambiguous annotations. Addressing these issues, we introduce FineAction, a large-scale, fine-grained video dataset with 103K instances across 106 action categories in 17K videos. FineAction offers a diverse, densely annotated dataset with co-occurring actions, posing new challenges for TAL. We evaluated popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and proposed a baseline method achieving a 13.17% mAP. FineAction aims to propel TAL research forward and is accessible online."
    },
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "This paper explores the effectiveness of using synthetic data, generated by graphics simulators, for pre-training models in computer vision, addressing the challenges posed by using real-image datasets like high costs and ethical concerns. It highlights that the performance of models on downstream tasks varies with different simulation parameters, suggesting a tailored approach to synthetic data generation for optimal results. The introduction of Task2Sim, a model that maps downstream task requirements to optimal simulation parameters, allows for the generation of synthetic pre-training data customized for specific tasks. Task2Sim demonstrates significant improvements in model performance on a wide range of tasks without additional training for new tasks, showing potential to rival pre-training with real images from Imagenet."
    }
  ],
  "enhancing": [
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    }
  ],
  "maebase": [
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    }
  ],
  "httpsgithubcomwangsr126maelite": [
    {
      "title": "A Closer Look at Self-Supervised Lightweight Vision Transformers",
      "url": "/research/2023/05/03/A-Closer-Look-at-Self-Supervised-Lightweight-Vision-Transformers.html",
      "date": "May 3, 2023",
      "summary": "This study evaluates self-supervised pre-training methods on Vision Transformers (ViTs) for image classification and dense prediction tasks, challenging the notion that vanilla lightweight ViTs are unsuitable for vision tasks in lightweight settings. Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods. The findings and code are shared at https://github.com/wangsr126/mae-lite, offering insights and tools for improving lightweight ViTs through self-supervised learning"
    }
  ],
  "adahessian": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "order": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "We introduce an algorithm designed to identify networks that are both simple and highly generalizable by searching for extensive areas of flat minima in the error function, where the error rate is relatively stable. These flat minima are associated with lower risks of overfitting according to MDL principles. Despite needing to calculate second-order derivatives, the algorithm maintains a complexity level comparable to backpropagation. We tested it on both feedforward and recurrent networks, and in stock market prediction tasks, it performed better than traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    }
  ],
  "curvature": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    }
  ],
  "hessian": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "approximation": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions for neural networks in reinforcement learning, the sigmoid-weighted linear unit (SiLU) and its derivative (dSiLU), and challenges the need for experience replay and separate target networks in deep reinforcement learning. By employing on-policy learning with eligibility traces and softmax action selection, the study achieves state-of-the-art results in stochastic SZ-Tetris and a small-board Tetris using TD(λ) learning and shallow dSiLU network agents. Furthermore, it outperforms the DQN algorithm in the Atari 2600 domain with a deep Sarsa(λ) agent utilizing SiLU and dSiLU hidden units, suggesting a competitive alternative to traditional DQN approaches."
    }
  ],
  "square": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "averaging": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "cv": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "nlp": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    }
  ],
  "achievement": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "ppl": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "glue": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "criteo": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "ad": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "kaggle": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "adagrad": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "iteration": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "sourced": [
    {
      "title": "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning",
      "url": "/research/2021/04/29/AdaHessian-An-Adaptive-Second-Order-Optimizer-for-Machine-Learning.html",
      "date": "April 29, 2021",
      "summary": "AdaHessian is a novel second-order stochastic optimization algorithm that dynamically incorporates the curvature of the loss function through adaptive Hessian estimates, offering superior convergence properties compared to first-order methods like SGD and Adam. Traditional second-order methods face challenges in computation and accuracy, which AdaHessian addresses through innovative techniques: a fast Hutchinson-based method for curvature matrix approximation, a root-mean-square exponential moving average for Hessian diagonal variation smoothing, and block diagonal averaging for variance reduction. AdaHessian sets new benchmarks, outperforming adaptive methods across computer vision (CV), natural language processing (NLP), and recommendation systems. Key achievements include significantly higher accuracy on CIFAR10 and ImageNet, improved BLEU scores and perplexity (PPL) for NLP tasks, better performance on the GLUE benchmark, and superior results on the Criteo Ad Kaggle dataset compared to leading algorithms like AdamW and Adagrad. AdaHessian maintains a competitive computation cost per iteration similar to first-order methods and demonstrates robustness towards hyperparameter settings. The algorithm is open-sourced for public use."
    }
  ],
  "explores": [
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    }
  ],
  "rotnet": [
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    }
  ],
  "radius": [
    {
      "title": "Adversarially Self-supervised Pre-training Improves Accuracy and Robustness",
      "url": "/research/2023/03/10/Adversarially-Self-supervised-Pre-training-Improves-Accuracy-and-Robustness.html",
      "date": "March 10, 2023",
      "summary": "This paper explores using adversarial training, typically a defense against adversarial shifts, to enhance visual representation pre-training for transfer across tasks and distribution shifts, integrating it with self-supervised methods like BYOL, MAE, and RotNet. It finds that adversarial self-supervision improves fine-tuning accuracy both within and outside distributions, outperforming standard methods even without adversarial fine-tuning. Optimal performance requires method-specific perturbation radii and preserving early layer parameters during fine-tuning. While no single method excels in all scenarios, adversarial MAE performs best for in-distribution tasks, and adversarial BYOL is superior for out-of-distribution tasks."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    }
  ],
  "instability": [
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/research/2021/08/16/An-Empirical-Study-of-Training-Self-Supervised-Vision-Transformers.html",
      "date": "August 16, 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    }
  ],
  "undermine": [
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/research/2021/08/16/An-Empirical-Study-of-Training-Self-Supervised-Vision-Transformers.html",
      "date": "August 16, 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    }
  ],
  "outcome": [
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/research/2021/08/16/An-Empirical-Study-of-Training-Self-Supervised-Vision-Transformers.html",
      "date": "August 16, 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    },
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    }
  ],
  "ablation": [
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/research/2021/08/16/An-Empirical-Study-of-Training-Self-Supervised-Vision-Transformers.html",
      "date": "August 16, 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "Semi-supervised learning (SSL) utilizes unlabeled data to boost model performance but often at the expense of increased complexity. This paper introduces FixMatch, a streamlined SSL algorithm that simplifies existing methods. FixMatch uses a model's predictions on weakly-augmented unlabeled images to generate pseudo-labels, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. Remarkably, FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness. The code is publicly available."
    },
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method, applies a single augmentation to each image and outperforms existing methods with minimal complexity and cost. We conducted extensive experiments to validate its performance against state-of-the-art methods across various image classification scenarios and explored its effectiveness through multiple ablation studies involving different augmentation spaces and methods. Our work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Highlighting a stagnation in automatic augmentation research, we conclude with proposed best practices for future advancements in the field."
    }
  ],
  "moco": [
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/research/2021/08/16/An-Empirical-Study-of-Training-Self-Supervised-Vision-Transformers.html",
      "date": "August 16, 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    }
  ],
  "v3": [
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/research/2021/08/16/An-Empirical-Study-of-Training-Self-Supervised-Vision-Transformers.html",
      "date": "August 16, 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    }
  ],
  "experience": [
    {
      "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
      "url": "/research/2021/08/16/An-Empirical-Study-of-Training-Self-Supervised-Vision-Transformers.html",
      "date": "August 16, 2021",
      "summary": "This paper examines self-supervised learning for Vision Transformers (ViT) in the context of recent advancements in computer vision. Unlike the well-established training protocols for convolutional networks, ViT training, especially under self-supervised conditions, remains underdeveloped. The study explores fundamental training components for self-supervised ViT, identifying instability as a key issue that undermines accuracy despite seemingly successful outcomes. By addressing these instabilities, the paper demonstrates improvements in training stability and accuracy. Results and ablations are benchmarked across MoCo v3 and other self-supervised frameworks. The findings highlight both promising approaches and ongoing challenges in self-supervised ViT training, aiming to guide future research with valuable insights and experiences."
    },
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions for neural networks in reinforcement learning, the sigmoid-weighted linear unit (SiLU) and its derivative (dSiLU), and challenges the need for experience replay and separate target networks in deep reinforcement learning. By employing on-policy learning with eligibility traces and softmax action selection, the study achieves state-of-the-art results in stochastic SZ-Tetris and a small-board Tetris using TD(λ) learning and shallow dSiLU network agents. Furthermore, it outperforms the DQN algorithm in the Atari 2600 domain with a deep Sarsa(λ) agent utilizing SiLU and dSiLU hidden units, suggesting a competitive alternative to traditional DQN approaches."
    }
  ],
  "worth": [
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    }
  ],
  "16x16": [
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    }
  ],
  "words": [
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    }
  ],
  "abandon": [
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    }
  ],
  "reliance": [
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    }
  ],
  "refer": [
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    }
  ],
  "vitcan": [
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    }
  ],
  "vtab": [
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "url": "/research/2021/06/03/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.html",
      "date": "June 3, 2021",
      "summary": "The Transformer architecture, while dominant in natural language processing (NLP), has had limited applications in computer vision. Traditionally, in vision tasks, attention mechanisms are integrated with or partially replace convolutional neural networks (CNNs), without abandoning their overall architecture. This study demonstrates that such reliance on CNNs is unnecessary for image classification tasks, showing that a pure transformer applied directly to sequences of image patches—referred to as Vision Transformer (ViT)—can achieve impressive results. When pre-trained on large datasets and then applied to various mid-sized or small image recognition benchmarks (such as ImageNet, CIFAR-100, VTAB), ViT performs comparably or even better than the latest convolutional networks, with significantly less computational cost for training."
    }
  ],
  "part": [
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    }
  ],
  "reconstruction": [
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "Deep neural networks, while effective in complex vision tasks, suffer from overfitting. Current image augmentation techniques, largely linear and hand-crafted like scale, flip, and color jitter, fail to produce sufficiently challenging examples. We introduce a new augmentation approach, Mask-Reconstruct Augmentation (MRA), leveraging self-supervised masked autoencoders to generate distorted inputs. This method, inspired by masked image modeling's success in self-supervised learning, uses nonlinear transformations for regularization. Our extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks. The code is available at https://github.com/haohang96/MRA."
    }
  ],
  "a2mim": [
    {
      "title": "Architecture-Agnostic Masked Image Modeling – From ViT back to CNN",
      "url": "/research/2023/06/02/Architecture-Agnostic-Masked-Image-Modeling-–-From-ViT-back-to-CNN.html",
      "date": "June 2, 2023",
      "summary": "Masked image modeling (MIM), a self-supervised pre-training method, has proven effective in vision tasks using Vision transformers by masking part of an image and reconstructing it. However, its compatibility with CNNs and operational principle are unclear. This study reveals that MIM improves generalized feature extraction through middle-order interactions among patches and introduces an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that works with both Transformers and CNNs. Our extensive testing demonstrates that A2MIM enhances representation learning and transferability to various tasks without specialized modifications"
    }
  ],
  "unna": [
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/research/2020/08/03/Are-Labels-Necessary-for-Neural-Architecture-Search.html",
      "date": "August 3, 2020",
      "summary": "This paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning if effective neural architectures can be discovered using only images without human-annotated labels. Through two experimental setups—sample-based and search-based—we investigate this possibility. In the sample-based approach, we evaluate 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. In the search-based experiments, we employ a recognized NAS algorithm, DARTS, with various unsupervised objectives, finding that architectures identified without labels perform competitively compared to those found with labels. These findings suggest that human-annotated labels may not be necessary for identifying efficient neural architectures, as image statistics alone could be sufficient."
    }
  ],
  "setup": [
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/research/2020/08/03/Are-Labels-Necessary-for-Neural-Architecture-Search.html",
      "date": "August 3, 2020",
      "summary": "This paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning if effective neural architectures can be discovered using only images without human-annotated labels. Through two experimental setups—sample-based and search-based—we investigate this possibility. In the sample-based approach, we evaluate 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. In the search-based experiments, we employ a recognized NAS algorithm, DARTS, with various unsupervised objectives, finding that architectures identified without labels perform competitively compared to those found with labels. These findings suggest that human-annotated labels may not be necessary for identifying efficient neural architectures, as image statistics alone could be sufficient."
    }
  ],
  "possibility": [
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/research/2020/08/03/Are-Labels-Necessary-for-Neural-Architecture-Search.html",
      "date": "August 3, 2020",
      "summary": "This paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning if effective neural architectures can be discovered using only images without human-annotated labels. Through two experimental setups—sample-based and search-based—we investigate this possibility. In the sample-based approach, we evaluate 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. In the search-based experiments, we employ a recognized NAS algorithm, DARTS, with various unsupervised objectives, finding that architectures identified without labels perform competitively compared to those found with labels. These findings suggest that human-annotated labels may not be necessary for identifying efficient neural architectures, as image statistics alone could be sufficient."
    }
  ],
  "ranking": [
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/research/2020/08/03/Are-Labels-Necessary-for-Neural-Architecture-Search.html",
      "date": "August 3, 2020",
      "summary": "This paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning if effective neural architectures can be discovered using only images without human-annotated labels. Through two experimental setups—sample-based and search-based—we investigate this possibility. In the sample-based approach, we evaluate 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. In the search-based experiments, we employ a recognized NAS algorithm, DARTS, with various unsupervised objectives, finding that architectures identified without labels perform competitively compared to those found with labels. These findings suggest that human-annotated labels may not be necessary for identifying efficient neural architectures, as image statistics alone could be sufficient."
    }
  ],
  "recognize": [
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/research/2020/08/03/Are-Labels-Necessary-for-Neural-Architecture-Search.html",
      "date": "August 3, 2020",
      "summary": "This paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning if effective neural architectures can be discovered using only images without human-annotated labels. Through two experimental setups—sample-based and search-based—we investigate this possibility. In the sample-based approach, we evaluate 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. In the search-based experiments, we employ a recognized NAS algorithm, DARTS, with various unsupervised objectives, finding that architectures identified without labels perform competitively compared to those found with labels. These findings suggest that human-annotated labels may not be necessary for identifying efficient neural architectures, as image statistics alone could be sufficient."
    },
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "Recent studies have shown that Convolutional Neural Networks (CNNs), which were thought to recognize objects by learning complex shapes, are actually more biased towards recognizing textures. This finding, based on a series of tests comparing CNNs with human observers using images with conflicting texture and shape cues, contradicts the traditional understanding of how these networks process images. We discovered that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, when we trained a standard CNN architecture (ResNet-50) on a stylized version of ImageNet, designed to emphasize shape, the network's performance aligned more closely with human behavior. This training not only matched human performance in controlled experiments (involving 97 observers across 48,560 trials) but also enhanced object detection and robustness against image distortions, underscoring the benefits of shape-based representations in visual recognition systems."
    }
  ],
  "nas": [
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/research/2020/08/03/Are-Labels-Necessary-for-Neural-Architecture-Search.html",
      "date": "August 3, 2020",
      "summary": "This paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning if effective neural architectures can be discovered using only images without human-annotated labels. Through two experimental setups—sample-based and search-based—we investigate this possibility. In the sample-based approach, we evaluate 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. In the search-based experiments, we employ a recognized NAS algorithm, DARTS, with various unsupervised objectives, finding that architectures identified without labels perform competitively compared to those found with labels. These findings suggest that human-annotated labels may not be necessary for identifying efficient neural architectures, as image statistics alone could be sufficient."
    }
  ],
  "dart": [
    {
      "title": "Are Labels Necessary for Neural Architecture Search?",
      "url": "/research/2020/08/03/Are-Labels-Necessary-for-Neural-Architecture-Search.html",
      "date": "August 3, 2020",
      "summary": "This paper introduces Unsupervised Neural Architecture Search (UnNAS), questioning if effective neural architectures can be discovered using only images without human-annotated labels. Through two experimental setups—sample-based and search-based—we investigate this possibility. In the sample-based approach, we evaluate 500 diverse architectures trained with both supervised and unsupervised objectives, revealing a high correlation between the architecture rankings with and without labels. In the search-based experiments, we employ a recognized NAS algorithm, DARTS, with various unsupervised objectives, finding that architectures identified without labels perform competitively compared to those found with labels. These findings suggest that human-annotated labels may not be necessary for identifying efficient neural architectures, as image statistics alone could be sufficient."
    }
  ],
  "demystify": [
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    }
  ],
  "degrade": [
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    }
  ],
  "misalignment": [
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    }
  ],
  "httpsgithubcomsunsmarterjiebeyond": [
    {
      "title": "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers",
      "url": "/research/2022/03/27/Beyond-Masking-Demystifying-Token-Based-Pre-Training-for-Vision-Transformers.html",
      "date": "March 27, 2022",
      "summary": "This paper explores alternatives to masked image modeling (MIM), a method leveraging vision transformers for self-supervised visual representation by masking parts of an input image and predicting the missing content. We propose five different learning objectives that, like MIM, degrade the input image in various ways. Through comprehensive experiments, we establish design principles for token-based pre-training of vision transformers. Notably, we find the most effective strategy combines preserving the original image style with introducing spatial misalignment in addition to spatial masking. This approach outperforms traditional MIM on downstream recognition tasks without increasing computational demands. The code for our study is accessible at https://github.com/sunsmarterjie/beyond masking"
    }
  ],
  "colorization": [
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "colorize": [
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "grayscale": [
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "underconstrain": [
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "user": [
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "We introduce DiffBIR, a framework using pretrained text-to-image diffusion models for blind image restoration. It consists of a two-stage pipeline: initially, we pretrain a restoration module on various degradations to enhance real-world applicability. Then, we use latent diffusion models for realistic restoration, including a novel injective modulation sub-network, LAControlNet, for fine-tuning, and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, we provide a controllable module for users to adjust quality and fidelity during the denoising process. Our extensive tests show DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets. The code is available online."
    }
  ],
  "producing": [
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "frame": [
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "We introduce SlowFast networks for video recognition, featuring a Slow pathway for spatial semantics at low frame rates and a Fast pathway for motion details at high frame rates. The Fast pathway's efficiency is achieved through reduced channel capacity, yet it effectively captures temporal information. Our models significantly enhance action classification and detection, achieving top accuracy on benchmarks like Kinetics, Charades, and AVA. Code is available at: https://github.com/facebookresearch/SlowFast."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article offers the first extensive review of vision transformer techniques specifically applied to human action recognition, a field gaining interest for its wide applications. Termed action transformers, these methods are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also delves into optimizing spatio-temporal attention in transformers for longer sequences and examines different learning strategies, including self-supervised and zero-shot learning, with their respective loss functions. Additionally, the survey assesses progress in benchmark evaluation scores and discusses the challenges and future directions in this research area."
    }
  ],
  "rebalance": [
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "pass": [
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    }
  ],
  "turing": [
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "participant": [
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "distinguish": [
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "illustrate": [
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    }
  ],
  "serve": [
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    }
  ],
  "pretext": [
    {
      "title": "Colorful Image Colorization",
      "url": "/research/2016/10/05/Colorful-Image-Colorization.html",
      "date": "October 5, 2016",
      "summary": "This paper presents a novel approach to automatically colorize grayscale photos with vibrant and realistic results, addressing the challenge of the underconstrained nature of the problem. Instead of requiring significant user input or producing desaturated outcomes, this method frames colorization as a classification task and employs class rebalancing during training to enhance color diversity. The approach utilizes a convolutional neural network (CNN) trained on over a million color images, operating as a feed-forward pass at test time. The effectiveness of the algorithm is validated through a colorization Turing test, where human participants struggled to distinguish between generated and real color images, with our method deceiving participants in 32% of trials—a notable improvement over previous techniques. Additionally, this paper illustrates how colorization can serve as an effective pretext task for self-supervised feature learning, functioning as a cross-channel encoder and achieving state-of-the-art results on several feature learning benchmarks."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "unlabeled": [
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "Semi-supervised learning (SSL) utilizes unlabeled data to boost model performance but often at the expense of increased complexity. This paper introduces FixMatch, a streamlined SSL algorithm that simplifies existing methods. FixMatch uses a model's predictions on weakly-augmented unlabeled images to generate pseudo-labels, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. Remarkably, FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness. The code is publicly available."
    },
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "6D object pose estimation is crucial for robotic bin-picking and essential for many industrial applications. Annotating custom datasets for each bin-picking scenario is challenging. We introduce an iterative self-training framework for sim-to-real 6D object pose estimation, enhancing cost-effective robotic grasping. We create a photo-realistic simulator for synthesizing virtual data to train an initial pose estimation network. This network, acting as a teacher model, predicts poses on unlabeled real data. We develop an adaptive selection scheme to filter reliable predictions for updating a student model with pseudo labels for real data pose estimation. By iteratively refining the teacher model with the trained student model, we improve the quality of pseudo labels. Our method, tested on a public benchmark and a newly-released dataset, shows an 11.49% and 22.62% ADD(-S) improvement, respectively, and a 19.54% increase in robotic bin-picking success, demonstrating the effectiveness of iterative sim-to-real approaches. Project homepage: www.cse.cuhk.edu.hk/~kaichen/sim2real_pose.html"
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    }
  ],
  "voc": [
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    },
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    }
  ],
  "formulation": [
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    }
  ],
  "specific": [
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    }
  ],
  "revisit": [
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    }
  ],
  "volume": [
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    }
  ],
  "quantity": [
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    },
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "This paper introduces VideoMAE, a data-efficient approach for self-supervised video pre-training (SSVP) that utilizes video masked autoencoders with a novel, high-ratio video tube masking technique. This method, inspired by ImageMAE, challenges the model with video reconstruction tasks to improve video representation learning. Key findings include: (1) High masking ratios (90% to 95%) are effective due to video's temporal redundancy, (2) VideoMAE performs well on small datasets (~3k-4k videos) without extra data, highlighting the importance of high-level structure learning, and (3) Data quality is more crucial than quantity for SSVP, with domain shift between pre-training and target datasets being significant. Remarkably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone, without extra data. The code is available online."
    }
  ],
  "adaptability": [
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    }
  ],
  "power": [
    {
      "title": "Colorization as a Proxy Task for Visual Understanding",
      "url": "/research/2017/08/13/Colorization-as-a-Proxy-Task-for-Visual-Understanding.html",
      "date": "August 13, 2017",
      "summary": "This study explores the potential of self-supervision, specifically through automatic colorization, as an alternative to ImageNet pretraining for improving the use of unlabeled data. It demonstrates that self-supervised training can achieve state-of-the-art results on VOC segmentation and classification tasks without relying on ImageNet labels. The research also provides a comprehensive analysis of self-supervision via colorization, highlighting the significance of loss formulation, training specifics, and network architecture. Additionally, it revisits and questions the ImageNet pretraining approach, including the necessity of training data volume, label quantity, and the adaptability of features upon fine-tuning. The findings suggest that colorization offers a supervisory signal comparable in power to various types of ImageNet pretraining."
    }
  ],
  "mark": [
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    }
  ],
  "embed": [
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    }
  ],
  "ffn": [
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    }
  ],
  "scrutinize": [
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    }
  ],
  "fourier": [
    {
      "title": "Deeper Insights into the Robustness of ViTs towards Common Corruptions",
      "url": "/research/2022/08/19/Deeper-Insights-into-the-Robustness-of-ViTs-towards-Common-Corruptions.html",
      "date": "August 19, 2022",
      "summary": "This paper investigates the robustness of various Vision Transformer (ViT) variants against common corruptions, marking the first comprehensive analysis of how different architectural designs affect ViT resilience. Our benchmarking reveals that certain simple architectural features, such as overlapping patch embedding and convolutional feed-forward networks (FFNs), significantly enhance ViT robustness. Additionally, we scrutinize the effectiveness of CNN-based data augmentation strategies, traditionally aimed at improving robustness, when applied to ViTs. Our study finds that while adversarial noise training is effective, fourier-domain augmentation falls short. Leveraging these insights, we propose a new conditional method for generating dynamic augmentation parameters based on input images, achieving state-of-the-art robustness against common corruptions."
    },
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "We introduce a Fourier neural network (FNN) aligned with Fourier decomposition, utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design ensures easy integration with more complex networks for data processing tasks. The FNN's efficacy is demonstrated on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits include solution validity beyond the training scope, model interpretability, and ease of use."
    }
  ],
  "parameterize": [
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "Neural networks parameterized for continuous, differentiable signal representations provide a promising paradigm but struggle with modeling fine detail and representing spatial and temporal derivatives critical for signals defined by partial differential equations. We introduce sinusoidal representation networks (SIRENs), utilizing periodic activation functions to effectively capture complex natural signals and their derivatives. Our analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. We also showcase SIRENs' application in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations, and extend their use with hypernetworks to learn priors for SIREN functions. A video overview and full applications are available on our project website."
    }
  ],
  "populations": [
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    }
  ],
  "akin": [
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "regnet": [
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    }
  ],
  "width": [
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    }
  ],
  "efficientnet": [
    {
      "title": "Designing Network Design Spaces",
      "url": "/research/2020/03/30/Designing-Network-Design-Spaces.html",
      "date": "March 30, 2020",
      "summary": "In this study, we introduce a novel approach to network design aimed at enhancing the understanding and generalizability of network design principles. Our method involves creating spaces for network design that parameterize multiple network populations, akin to traditional network design but on a broader scale. We explore network structure and develop a low-dimensional, efficient design space called RegNet, based on the insight that network widths and depths can be modeled through a quantized linear function. Our analysis of the RegNet space reveals findings that challenge existing design practices, offering simpler and faster networks effective across various computational budgets. Comparatively, RegNet models surpass EfficientNet models in performance and are up to five times faster on GPUs, under similar training conditions and computational resources."
    }
  ],
  "blur": [
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    }
  ],
  "edge": [
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "bia": [
    {
      "title": "Does Enhanced Shape Bias Improve Neural Network Robustness to Common Corruptions",
      "url": "/research/2021/04/20/Does-Enhanced-Shape-Bias-Improve-Neural-Network-Robustness-to-Common-Corruptions.html",
      "date": "April 20, 2021",
      "summary": "Convolutional neural networks (CNNs) trained on ImageNet exhibit a texture bias but struggle with out-of-distribution data. Recent studies have shown that incorporating diverse image styles into training data reduces texture bias and enhances shape recognition, thereby improving resilience against common image corruptions like noise and blur. This is often interpreted as increased shape bias leading to greater corruption robustness. Through a comprehensive analysis of input compositions using natural images, edge information, and stylization, our study finds that while stylization significantly boosts corruption robustness, a direct link between shape bias and robustness is not evident. We suggest that the enhanced corruption robustness is primarily due to style variation in data augmentation, with increased shape bias being an indirect effect."
    }
  ],
  "description": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    }
  ],
  "internet": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    }
  ],
  "caption": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "Image-caption pretraining, crucial for tasks like zero-shot image classification and object detection, faces challenges in aligning multiple concepts from captions to objects in images. We introduce a curriculum learning framework inspired by children's language learning from cognitive science, starting with simple image-caption pairs and gradually increasing complexity by adding more concepts. This method, which leverages knowledge from each phase for subsequent learning, outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios. The code is available at: https://github.com/hayyubi/cur_vl.git."
    }
  ],
  "eliminate": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    },
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    }
  ],
  "ocr": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    }
  ],
  "action": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/research/2022/10/10/Turbo-Training-with-Token-Dropout.html",
      "date": "October 10, 2022",
      "summary": "This paper introduces Turbo training, an efficient training method for video-related tasks using Transformers. Turbo training offers three major contributions: Firstly, it presents a simple yet versatile training paradigm applicable to multiple video tasks. Secondly, it demonstrates Turbo training's effectiveness across action classification, video-language representation learning, and long-video activity classification, achieving competitive performance with up to 4× speed-up and reduced memory usage. Thirdly, it enables long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous methods that were resource-intensive."
    },
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper presents the AVA dataset, featuring 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets focused on composite actions in shorter clips with sparse annotations, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. It highlights the challenges in action recognition, introduces a novel localization approach surpassing existing benchmarks but shows modest performance on AVA (15.6% mAP), pointing out the necessity for advanced video understanding methods."
    },
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "Temporal action localization (TAL) is crucial for video understanding but is hindered by benchmarks that rely on coarse action classes, leading to model overfitting and ambiguous annotations. Addressing these issues, we introduce FineAction, a large-scale, fine-grained video dataset with 103K instances across 106 action categories in 17K videos. FineAction offers a diverse, densely annotated dataset with co-occurring actions, posing new challenges for TAL. We evaluated popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and proposed a baseline method achieving a 13.17% mAP. FineAction aims to propel TAL research forward and is accessible online."
    },
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "The limited number of videos in current action classification datasets like UCF-101 and HMDB-51 hinders the identification of effective video architectures due to similar performance across small-scale benchmarks. This study re-examines top architectures using the new, significantly larger Kinetics Human Action Video dataset, featuring 400 classes and over 400 clips per class from challenging YouTube videos. It analyzes the impact of this dataset on the performance of existing architectures and the benefits of pre-training on Kinetics. Additionally, the paper introduces the Two-Stream Inflated 3D ConvNet (I3D), an advancement that expands 2D ConvNet designs into 3D for enhanced video feature extraction. This model, pre-trained on Kinetics, sets new benchmarks in action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    },
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions for neural networks in reinforcement learning, the sigmoid-weighted linear unit (SiLU) and its derivative (dSiLU), and challenges the need for experience replay and separate target networks in deep reinforcement learning. By employing on-policy learning with eligibility traces and softmax action selection, the study achieves state-of-the-art results in stochastic SZ-Tetris and a small-board Tetris using TD(λ) learning and shallow dSiLU network agents. Furthermore, it outperforms the DQN algorithm in the Atari 2600 domain with a deep Sarsa(λ) agent utilizing SiLU and dSiLU hidden units, suggesting a competitive alternative to traditional DQN approaches."
    },
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "We introduce SlowFast networks for video recognition, featuring a Slow pathway for spatial semantics at low frame rates and a Fast pathway for motion details at high frame rates. The Fast pathway's efficiency is achieved through reduced channel capacity, yet it effectively captures temporal information. Our models significantly enhance action classification and detection, achieving top accuracy on benchmarks like Kinetics, Charades, and AVA. Code is available at: https://github.com/facebookresearch/SlowFast."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article offers the first extensive review of vision transformer techniques specifically applied to human action recognition, a field gaining interest for its wide applications. Termed action transformers, these methods are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also delves into optimizing spatio-temporal attention in transformers for longer sequences and examines different learning strategies, including self-supervised and zero-shot learning, with their respective loss functions. Additionally, the survey assesses progress in benchmark evaluation scores and discusses the challenges and future directions in this research area."
    }
  ],
  "geo": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    }
  ],
  "localization": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    },
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper presents the AVA dataset, featuring 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets focused on composite actions in shorter clips with sparse annotations, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. It highlights the challenges in action recognition, introduces a novel localization approach surpassing existing benchmarks but shows modest performance on AVA (15.6% mAP), pointing out the necessity for advanced video understanding methods."
    },
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "Temporal action localization (TAL) is crucial for video understanding but is hindered by benchmarks that rely on coarse action classes, leading to model overfitting and ambiguous annotations. Addressing these issues, we introduce FineAction, a large-scale, fine-grained video dataset with 103K instances across 106 action categories in 17K videos. FineAction offers a diverse, densely annotated dataset with co-occurring actions, posing new challenges for TAL. We evaluated popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and proposed a baseline method achieving a 13.17% mAP. FineAction aims to propel TAL research forward and is accessible online."
    },
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "baseline": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    },
    {
      "title": "Better plain ViT baselines for ImageNet-1k",
      "url": "/research/2022/05/02/Better-plain-ViT-baselines-for-ImageNet-1k.html",
      "date": "May 2, 2022",
      "summary": "It is commonly accepted that the Vision Transformer model requires sophisticated regularization techniques to excel at ImageNet-1k scale data. Surprisingly, we find this is not the case and standard data augmentation is sufficient. This note presents a few minor modifications to the original Vision Transformer (ViT) vanilla training setting that dramatically improve the performance of plain ViT models. Notably, 90 epochs of training surpass 76% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reach 80% in less than one day."
    },
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "Temporal action localization (TAL) is crucial for video understanding but is hindered by benchmarks that rely on coarse action classes, leading to model overfitting and ambiguous annotations. Addressing these issues, we introduce FineAction, a large-scale, fine-grained video dataset with 103K instances across 106 action categories in 17K videos. FineAction offers a diverse, densely annotated dataset with co-occurring actions, posing new challenges for TAL. We evaluated popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and proposed a baseline method achieving a 13.17% mAP. FineAction aims to propel TAL research forward and is accessible online."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    }
  ],
  "httpsgithubcomopenaiclip": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "url": "/research/2021/02/26/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.html",
      "date": "February 26, 2021",
      "summary": "This paper introduces a novel approach to computer vision that learns directly from raw text descriptions of images, moving beyond the traditional model of training on a fixed set of object categories. By pre-training on a massive dataset of 400 million (image, text) pairs from the internet using the task of matching captions to images, the method achieves state-of-the-art (SOTA) image representations from scratch. This approach allows for zero-shot transfer to a wide range of downstream tasks by using natural language to reference or describe visual concepts, eliminating the need for additional labeled data. The model's performance was evaluated across over 30 diverse computer vision datasets, including OCR, video action recognition, geo-localization, and fine-grained object classification, showing competitive results against fully supervised baselines without task-specific training. Remarkably, it matches the accuracy of the original ResNet-50 on ImageNet zero-shot, without using any of its 1.28 million training examples. The code and pre-trained model weights are made available at https://github.com/OpenAI/CLIP."
    }
  ],
  "localize": [
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    },
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper presents the AVA dataset, featuring 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets focused on composite actions in shorter clips with sparse annotations, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. It highlights the challenges in action recognition, introduces a novel localization approach surpassing existing benchmarks but shows modest performance on AVA (15.6% mAP), pointing out the necessity for advanced video understanding methods."
    }
  ],
  "collection": [
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    }
  ],
  "manner": [
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    }
  ],
  "corloc": [
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    }
  ],
  "point": [
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    },
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    },
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper investigates how neural networks generalize on small, algorithmically generated datasets, focusing on data efficiency, memorization, generalization, and learning speed. It highlights instances where neural networks grok patterns, significantly improving their generalization ability from mere chance to perfect accuracy, even beyond the point of overfitting. Additionally, the study finds that generalization on smaller datasets demands more optimization. It posits that such datasets are ideal for exploring the enigmatic phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    }
  ],
  "pascal": [
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    }
  ],
  "detector": [
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "httpsgithubcomvaleoailost": [
    {
      "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
      "url": "/research/2021/09/29/Localizing-Objects-with-Self-Supervised-Transformers-and-no-Labels.html",
      "date": "September 29, 2021",
      "summary": "We introduce LOST, a straightforward method for unsupervised object localization in image collections, utilizing activation features from a vision transformer pre-trained in a self-supervised manner. Unlike other methods, LOST does not rely on external object proposals or image collection exploration and works on individual images. It surpasses existing object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. Additionally, training a class-agnostic detector on the objects found by LOST further improves performance by 7 points, showing its effectiveness also in unsupervised object discovery. The implementation of our approach is available at https://github.com/valeoai/LOST"
    }
  ],
  "presents": [
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    }
  ],
  "underpin": [
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    }
  ],
  "token": [
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "Self-supervised transformer training methods have shown excellent results in various fields. Unlike previous models like masked autoencoders (MAE) which use a single normalization layer for the class token [CLS] and other tokens, we introduce a novel normalization technique that normalizes the [CLS] token and normal tokens separately. This approach aims to better capture their unique characteristics, improving performance in downstream tasks. Our findings indicate that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks."
    },
    {
      "title": "Training data-efficient image transformers & distillation through attention",
      "url": "/research/2021/01/15/Training-data-efficient-image-transformers-&-distillation-through-attention.html",
      "date": "January 15, 2021",
      "summary": "This study introduces DeiT, competitive vision transformers trained solely on ImageNet without convolutions, achieving up to 83.1% top-1 accuracy on ImageNet, using a single computer in under 3 days. Notably, it presents a unique teacher-student strategy for transformers, leveraging a distillation token for efficient learning from a convolutional network teacher. This method results in performance on par with convolutional networks, achieving up to 85.2% accuracy on ImageNet and demonstrating effective transferability to other tasks. The authors provide access to their code and models."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    }
  ],
  "portion": [
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    },
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    }
  ],
  "triple": [
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    }
  ],
  "standout": [
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "url": "/research/2021/12/19/Masked-Autoencoders-Are-Scalable-Vision-Learner.html",
      "date": "December 19, 2021",
      "summary": "This paper presents masked autoencoders (MAE) as efficient and scalable self-supervised learners for computer vision. The MAE methodology involves masking random patches of an input image and reconstructing the missing pixels. This approach is underpinned by two key designs: an asymmetric encoder-decoder architecture, where the encoder processes only visible patches, and a lightweight decoder reconstructs the image using the latent representation and mask tokens. Additionally, masking a significant portion of the input image, such as 75%, creates a challenging yet informative self-supervisory task. These innovations allow for the efficient training of large models, tripling training speed and enhancing accuracy. A standout result is a vanilla ViT-Huge model reaching top accuracy (87.8%) on ImageNet-1K data among similar methods. Moreover, the model's transfer performance in downstream tasks surpasses that of supervised pre-training, indicating a promising scaling potential."
    }
  ],
  "degradation": [
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    },
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "We introduce DiffBIR, a framework using pretrained text-to-image diffusion models for blind image restoration. It consists of a two-stage pipeline: initially, we pretrain a restoration module on various degradations to enhance real-world applicability. Then, we use latent diffusion models for realistic restoration, including a novel injective modulation sub-network, LAControlNet, for fine-tuning, and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, we provide a controllable module for users to adjust quality and fidelity during the denoising process. Our extensive tests show DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets. The code is available online."
    }
  ],
  "mirl": [
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    }
  ],
  "alleviate": [
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    }
  ],
  "redefine": [
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    }
  ],
  "recover": [
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    }
  ],
  "s": [
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    }
  ],
  "httpsgithubcomrussellllaputamirl": [
    {
      "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
      "url": "/research/2023/11/15/Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers.html",
      "date": "November 15, 2023",
      "summary": "Training deeper Vision Transformers (ViTs) presents challenges, including a degradation problem in deeper layers during masked image modeling (MIM) pre-training. To address this, we introduce Masked Image Residual Learning (MIRL), a self-supervised learning framework that alleviates the degradation issue and enables effective scaling of ViT depth for performance improvement. MIRL redefines the pre-training objective for deep ViT layers as learning to recover the residual of the masked image. Through extensive testing, we show that MIRL allows deeper ViTs to be optimized more effectively, enhancing accuracy with increased depth. Implementing this approach, we developed ViT-S-54 and ViT-B-48 models, which are 4.5× and 2× deeper than the standard ViT-Base and ViT-Large, respectively. ViT-S-54 matches ViT-Large's performance at a third of the cost, and ViT-B-48 reaches 86.2% top-1 accuracy on ImageNet. Deeper ViTs pre-trained with MIRL show strong generalization on downstream tasks like object detection and semantic segmentation and demonstrate high pre-training efficiency, achieving competitive performance in less time. The code and pretrained models are accessible at https://github.com/russellllaputa/MIRL."
    }
  ],
  "formula": [
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "fdsl": [
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    },
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "fractal": [
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    }
  ],
  "generating": [
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    },
    {
      "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "Diffusion models, effective for generating high-dimensional perceptual data like images, lack established standard practices. This study enhances noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Our large-scale study demonstrates superior performance over established diffusion methods in high-resolution text-to-image synthesis. We introduce a transformer-based architecture for text-to-image generation, improving text comprehension, typography, and human preference ratings. Our findings show predictable scaling trends, with lower validation loss correlating with improved synthesis. The largest models outperform state-of-the-art approaches, and our data, code, and model weights will be publicly accessible."
    }
  ],
  "act": [
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    },
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "6D object pose estimation is crucial for robotic bin-picking and essential for many industrial applications. Annotating custom datasets for each bin-picking scenario is challenging. We introduce an iterative self-training framework for sim-to-real 6D object pose estimation, enhancing cost-effective robotic grasping. We create a photo-realistic simulator for synthesizing virtual data to train an initial pose estimation network. This network, acting as a teacher model, predicts poses on unlabeled real data. We develop an adaptive selection scheme to filter reliable predictions for updating a student model with pseudo labels for real data pose estimation. By iteratively refining the teacher model with the trained student model, we improve the quality of pseudo labels. Our method, tested on a public benchmark and a newly-released dataset, shows an 11.49% and 22.62% ADD(-S) improvement, respectively, and a 19.54% increase in robotic bin-picking success, demonstrating the effectiveness of iterative sim-to-real approaches. Project homepage: www.cse.cuhk.edu.hk/~kaichen/sim2real_pose.html"
    }
  ],
  "database": [
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    }
  ],
  "ofdb": [
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    }
  ],
  "outperforming": [
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    }
  ],
  "ofdbs": [
    {
      "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
      "url": "/research/2023/07/31/Pre-training-Vision-Transformers-with-Very-Limited-Synthesized-Images.html",
      "date": "July 31, 2023",
      "summary": "Formula-driven supervised learning (FDSL) utilizes synthetic images from mathematical formulas, like fractals, for pre-training vision transformers, demonstrating competitive performance on various downstream tasks. This study proposes that generating different instances within the same category in FDSL acts as data augmentation. By adopting this approach, we introduce a one-instance fractal database (OFDB) where only a single image per category is needed, outperforming the original method of generating multiple instances. Scaling OFDB to 21,000 categories, we achieve comparable or superior results to models pre-trained on ImageNet-21k in ImageNet-1k fine-tuning, despite OFDB's significantly smaller size of 21k images compared to ImageNet-21k's 14M. This finding suggests the potential of pre-training vision transformers on much smaller datasets."
    }
  ],
  "replacing": [
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    }
  ],
  "contour": [
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "circumvent": [
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "We introduce a system that trains deep neural networks for object detection using synthetic images, leveraging domain randomization to manage real-world data variability. This involves randomizing simulator parameters—like lighting and object textures—in non-realistic ways, helping the network to identify essential object features. Our findings reveal that networks can achieve impressive performance with just synthetic data and further improve with real data fine-tuning. This suggests the potential of using cost-effective synthetic data for training, circumventing the challenges of acquiring vast amounts of real-world data or creating detailed synthetic environments. We validate our method through car bounding box detection on the KITTI dataset."
    }
  ],
  "privacycopyright": [
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "concern": [
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    },
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "This paper explores the effectiveness of using synthetic data, generated by graphics simulators, for pre-training models in computer vision, addressing the challenges posed by using real-image datasets like high costs and ethical concerns. It highlights that the performance of models on downstream tasks varies with different simulation parameters, suggesting a tailored approach to synthetic data generation for optimal results. The introduction of Task2Sim, a model that maps downstream task requirements to optimal simulation parameters, allows for the generation of synthetic pre-training data customized for specific tasks. Task2Sim demonstrates significant improvements in model performance on a wide range of tasks without additional training for new tasks, showing potential to rival pre-training with real images from Imagenet."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "inaccuracy": [
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    }
  ],
  "biases": [
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    }
  ],
  "avenue": [
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    }
  ],
  "hypothes": [
    {
      "title": "Replacing Labeled Real-image Datasets with Auto-generated Contours",
      "url": "/research/2022/06/18/Replacing-Labeled-Real-image-Datasets-with-Auto-generated-Contours.html",
      "date": "June 18, 2022",
      "summary": "This study demonstrates that formula-driven supervised learning (FDSL) can achieve or surpass the performance of ImageNet-21k pre-training for Vision Transformers (ViTs) without using real images or relying on human or self-supervision. Notably, a ViT-Base model pre-trained with FDSL achieved 82.7% top-1 accuracy on ImageNet-1k, exceeding the 81.8% accuracy from ImageNet-21k pre-training, under identical conditions. Synthetic images generated by FDSL circumvent issues associated with real images, such as privacy/copyright concerns, labeling inaccuracies, and biases, offering a promising avenue for pre-training general models. Investigating the effectiveness of synthetic images, we explored two hypotheses: the importance of object contours in FDSL datasets and the impact of increasing parameter complexity for label creation on pre-training performance. Our findings suggest that simple object contours can match fractal-based dataset performance and that increasing pre-training task difficulty enhances fine-tuning accuracy."
    }
  ],
  "flip": [
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    },
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "Deep neural networks, while effective in complex vision tasks, suffer from overfitting. Current image augmentation techniques, largely linear and hand-crafted like scale, flip, and color jitter, fail to produce sufficiently challenging examples. We introduce a new augmentation approach, Mask-Reconstruct Augmentation (MRA), leveraging self-supervised masked autoencoders to generate distorted inputs. This method, inspired by masked image modeling's success in self-supervised learning, uses nonlinear transformations for regularization. Our extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks. The code is available at https://github.com/haohang96/MRA."
    }
  ],
  "clip": [
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    },
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper presents the AVA dataset, featuring 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets focused on composite actions in shorter clips with sparse annotations, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. It highlights the challenges in action recognition, introduces a novel localization approach surpassing existing benchmarks but shows modest performance on AVA (15.6% mAP), pointing out the necessity for advanced video understanding methods."
    },
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "The limited number of videos in current action classification datasets like UCF-101 and HMDB-51 hinders the identification of effective video architectures due to similar performance across small-scale benchmarks. This study re-examines top architectures using the new, significantly larger Kinetics Human Action Video dataset, featuring 400 classes and over 400 clips per class from challenging YouTube videos. It analyzes the impact of this dataset on the performance of existing architectures and the benefits of pre-training on Kinetics. Additionally, the paper introduces the Two-Stream Inflated 3D ConvNet (I3D), an advancement that expands 2D ConvNet designs into 3D for enhanced video feature extraction. This model, pre-trained on Kinetics, sets new benchmarks in action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    }
  ],
  "amount": [
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "Vision Transformers (ViT) excel in various vision tasks, like image classification, object detection, and semantic segmentation, but require more model regularization or data augmentation (AugReg) than convolutional neural networks, especially with smaller datasets. Through a comprehensive study, we explore how training data amount, AugReg, model size, and compute budget interact. Our findings indicate that using more compute and AugReg can achieve the same performance as training with significantly more data. Specifically, we demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    },
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This study investigates the use of high-quality, physically-based rendering and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle, enhancing image classification and semantic segmentation models. Synthetic images proved to be effective for augmenting limited real training data. However, models trained solely on synthetic images showed low accuracy on real validation images, which significantly improved when even small amounts of real data were included. Augmenting real data with synthetic images outperformed training on only real images. Moreover, pretraining models on synthetic data before transfer learning significantly reduced training costs, allowing most of the training to be completed upfront."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "We introduce a system that trains deep neural networks for object detection using synthetic images, leveraging domain randomization to manage real-world data variability. This involves randomizing simulator parameters—like lighting and object textures—in non-realistic ways, helping the network to identify essential object features. Our findings reveal that networks can achieve impressive performance with just synthetic data and further improve with real data fine-tuning. This suggests the potential of using cost-effective synthetic data for training, circumventing the challenges of acquiring vast amounts of real-world data or creating detailed synthetic environments. We validate our method through car bounding box detection on the KITTI dataset."
    }
  ],
  "speedup": [
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    }
  ],
  "period": [
    {
      "title": "Scaling Language-Image Pre-training via Masking",
      "url": "/research/2023/03/30/Scaling-Language-Image-Pre-training-via-Masking.html",
      "date": "March 30, 2023",
      "summary": "We introduce Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. This results in better accuracy and training speed. Testing on 400 million image-text pairs, FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperforms CLIP models on various downstream tasks. The speedup from our method also enables us to experiment with larger model sizes, more data, or longer training periods, yielding promising outcomes that may advance vision-language learning research."
    }
  ],
  "anything": [
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    }
  ],
  "sa": [
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    }
  ],
  "date": [
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    }
  ],
  "privacy": [
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    }
  ],
  "respecting": [
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    }
  ],
  "engineer": [
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    }
  ],
  "promptability": [
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    }
  ],
  "sam": [
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    },
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    }
  ],
  "httpssegmentanythingcom": [
    {
      "title": "Segment Anything",
      "url": "/research/2023/04/05/Segment-Anything.html",
      "date": "April 5, 2023",
      "summary": "The Segment Anything (SA) project introduces a novel task, model, and the largest image segmentation dataset to date, featuring over 1 billion masks across 11 million licensed and privacy-respecting images. The model is engineered for promptability, enabling zero-shot transfer to various image distributions and tasks. Its zero-shot performance has been evaluated across numerous tasks, often matching or surpassing previous fully supervised results. The Segment Anything Model (SAM) and the corresponding SA-1B dataset are made available at https://segment-anything.com to encourage the development of foundational models in computer vision."
    }
  ],
  "using": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "Semi-supervised learning (SSL) utilizes unlabeled data to boost model performance but often at the expense of increased complexity. This paper introduces FixMatch, a streamlined SSL algorithm that simplifies existing methods. FixMatch uses a model's predictions on weakly-augmented unlabeled images to generate pseudo-labels, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. Remarkably, FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness. The code is publicly available."
    }
  ],
  "treat": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "node": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "graph": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "Self-supervised transformer training methods have shown excellent results in various fields. Unlike previous models like masked autoencoders (MAE) which use a single normalization layer for the class token [CLS] and other tokens, we introduce a novel normalization technique that normalizes the [CLS] token and normal tokens separately. This approach aims to better capture their unique characteristics, improving performance in downstream tasks. Our findings indicate that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks."
    },
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    }
  ],
  "represent": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "eigendecomposition": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "foreground": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "eigenvector": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "indication": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "likelihood": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "belong": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "margin": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "voc07": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "voc12": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "coco20k": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "cad": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    },
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This study investigates the use of high-quality, physically-based rendering and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle, enhancing image classification and semantic segmentation models. Synthetic images proved to be effective for augmenting limited real training data. However, models trained solely on synthetic images showed low accuracy on real validation images, which significantly improved when even small amounts of real data were included. Augmenting real data with synthetic images outperformed training on only real images. Moreover, pretraining models on synthetic data before transfer learning significantly reduced training costs, allowing most of the training to be completed upfront."
    }
  ],
  "saliency": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "iou": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "ecssd": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "duts": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "dut": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "omron": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "cub": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "httpswwwmpsifrpaperstokencut2022": [
    {
      "title": "Self-supervised Transformers for Unsupervised Object Discovery Using Normalized Cut",
      "url": "/research/2022/03/24/Self-supervised-Transformers-for-Unsupervised-Object-Discovery-Using-Normalized-Cut.html",
      "date": "March 24, 2022",
      "summary": "This study introduces a graph-based method leveraging self-supervised transformer features, specifically those trained with self-distillation loss (DINO), for object discovery in images. It treats visual tokens as nodes within a weighted graph, where edges represent the similarity between tokens. By applying a normalized graph-cut through spectral clustering and generalized eigendecomposition, it segments foreground objects based on the second smallest eigenvector's indication of a token's likelihood to belong to a foreground object. This straightforward yet effective approach surpasses the state-of-the-art LOST by significant margins on VOC07, VOC12, and COCO20K datasets. Additionally, incorporating a second stage class-agnostic detector (CAD) further enhances performance. The method also extends to unsupervised saliency detection, improving IoU on benchmarks like ECSSD, DUTS, DUT-OMRON, and achieves competitive results in weakly supervised object detection on CUB and ImageNet. The code is available at https://www.m-psi.fr/Papers/TokenCut2022/."
    }
  ],
  "geometry": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "hidden": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "protein": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "sequences": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "evolution": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    },
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "manifold": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "contract": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "id": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "stabilize": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    },
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is crucial for natural language processing tasks, but its training requires a learning rate warm-up stage, essential for performance yet slowing optimization and increasing hyper-parameter tuning needs. This paper investigates the importance of the warm-up stage and the impact of layer normalization placement. It uses mean field theory to demonstrate that the original Post-LN Transformer's design, with layer normalization between residual blocks, results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    }
  ],
  "peak": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "expansion": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    },
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "We introduce a Fourier neural network (FNN) aligned with Fourier decomposition, utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design ensures easy integration with more complex networks for data processing tasks. The FNN's efficacy is demonstrated on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits include solution validity beyond the training scope, model interpretability, and ease of use."
    },
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    }
  ],
  "trend": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    },
    {
      "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "Diffusion models, effective for generating high-dimensional perceptual data like images, lack established standard practices. This study enhances noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Our large-scale study demonstrates superior performance over established diffusion methods in high-resolution text-to-image synthesis. We introduce a transformer-based architecture for text-to-image generation, improving text comprehension, typography, and human preference ratings. Our findings show predictable scaling trends, with lower validation loss correlating with improved synthesis. The largest models outperform state-of-the-art approaches, and our data, code, and model weights will be publicly accessible."
    }
  ],
  "pinpoint": [
    {
      "title": "The geometry of hidden representations of large transformer models",
      "url": "/research/2023/10/30/The-geometry-of-hidden-representations-of-large-transformer-models.html",
      "date": "October 30, 2023",
      "summary": "Large transformers, used for self-supervised learning across data types like protein sequences, images, and text, reveal the semantic structure of data through successive transformations. This study examines the geometric and statistical properties of these representations across layers, finding common evolution patterns in transformers trained on diverse tasks. Initially, data manifolds expand, then contract at intermediate layers, with the intrinsic dimension (ID) stabilizing or peaking slightly towards the end. Semantic information peaks after initial expansion, a trend consistent across models and datasets. The study suggests an unsupervised method to pinpoint layers richest in semantic content, identifying those at a relative ID minimum as optimal for downstream tasks"
    }
  ],
  "tinyvit": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "series": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "We introduce a Fourier neural network (FNN) aligned with Fourier decomposition, utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design ensures easy integration with more complex networks for data processing tasks. The FNN's efficacy is demonstrated on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits include solution validity beyond the training scope, model interpretability, and ease of use."
    }
  ],
  "compact": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "device": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "logit": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "store": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "counterpart": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "comparable": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "swin": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    },
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "Recent advancements in image transformers, notably in models such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, have narrowed the performance gap with traditional CNNs. These models are typically trained on large datasets like ImageNet-21k and then finetuned on ImageNet-1k. However, assessments often overlook Tiny ImageNet when evaluating transfer learning performance. This paper updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. The code is available at: https://github.com/ehuynh1106/TinyImageNet-Transformers"
    }
  ],
  "l": [
    {
      "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers",
      "url": "/research/2022/07/21/TinyViT-Fast-Pretraining-Distillation-for-Small-Vision-Transformer.html",
      "date": "July 21, 2022",
      "summary": "TinyViT introduces a new series of compact and efficient vision transformers (ViTs), designed for devices with limited resources by pretraining on large datasets using a fast distillation framework. This approach involves transferring knowledge from large pretrained models to smaller ones, allowing the latter to benefit from extensive pretraining data. Key to this process is distilling knowledge during pretraining, where sparsified logits from large teacher models are stored to minimize memory and computational costs. TinyViT models are scaled down from larger counterparts under specific computation and parameter limits. Our experiments show TinyViT achieves 84.8% top-1 accuracy on ImageNet-1k with just 21M parameters, comparable to Swin-B pretrained on ImageNet-21k but with 4.2 times fewer parameters. Furthermore, with increased image resolution, TinyViT reaches 86.5% accuracy, outperforming Swin-L with only 11% of its parameters. TinyViT also demonstrates strong transferability across various downstream tasks. The code and models are publicly available."
    }
  ],
  "driving": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "edit": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "robot": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "sense": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "advent": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "there": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "cover": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "simplifies": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "cloud": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "foundation": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    }
  ],
  "outline": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "httpsgithubcomlxtghawesomesegmentationwithtransformer": [
    {
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "url": "/research/2023/12/20/Transformer-Based-Visual-Segmentation-A-Surve.html",
      "date": "December 20, 2023",
      "summary": "This survey offers an extensive examination of transformer-based visual segmentation, a field crucial for applications in autonomous driving, image editing, robot sensing, and medical analysis. With the advent of deep learning, and especially transformers—neural networks excelling in vision tasks through self-attention—there has been significant progress in segmentation tasks. We cover the evolution from convolutional methods to the emergence of vision transformers, providing a unified framework that simplifies understanding recent advancements. The survey discusses various transformer-based segmentation approaches, modifications, and applications, highlighting specific areas like 3D point cloud, foundation model tuning, domain-aware, efficient, and medical segmentation. Additionally, it re-evaluates these methods on established datasets, outlines current challenges, and suggests future research directions. The comprehensive resources and findings are accessible at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer."
    }
  ],
  "turbo": [
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/research/2022/10/10/Turbo-Training-with-Token-Dropout.html",
      "date": "October 10, 2022",
      "summary": "This paper introduces Turbo training, an efficient training method for video-related tasks using Transformers. Turbo training offers three major contributions: Firstly, it presents a simple yet versatile training paradigm applicable to multiple video tasks. Secondly, it demonstrates Turbo training's effectiveness across action classification, video-language representation learning, and long-video activity classification, achieving competitive performance with up to 4× speed-up and reduced memory usage. Thirdly, it enables long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous methods that were resource-intensive."
    }
  ],
  "relate": [
    {
      "title": "Turbo Training with Token Dropout",
      "url": "/research/2022/10/10/Turbo-Training-with-Token-Dropout.html",
      "date": "October 10, 2022",
      "summary": "This paper introduces Turbo training, an efficient training method for video-related tasks using Transformers. Turbo training offers three major contributions: Firstly, it presents a simple yet versatile training paradigm applicable to multiple video tasks. Secondly, it demonstrates Turbo training's effectiveness across action classification, video-language representation learning, and long-video activity classification, achieving competitive performance with up to 4× speed-up and reduced memory usage. Thirdly, it enables long-schedule video-language training and end-to-end long-video training with limited resources, outperforming or matching previous methods that were resource-intensive."
    }
  ],
  "motivate": [
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    }
  ],
  "inferring": [
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    }
  ],
  "predispose": [
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    }
  ],
  "shelf": [
    {
      "title": "Understanding Self-Supervised Pretraining with Part-Aware Representation Learning",
      "url": "/research/2024/01/23/Understanding-Self-Supervised-Pretraining-with-Part-Aware-Representation-Learning.html",
      "date": "January 23, 2024",
      "summary": "This paper explores self-supervised pretraining by examining how these methods learn part-aware representations, motivated by the use of random views in contrastive learning and random masked patches in masked image modeling, which often focus on object parts. It describes contrastive learning as transforming part representations into whole object representations and masked image modeling as inferring masked object parts from visible ones, suggesting these methods predispose encoders to recognize object parts. Through empirical comparison of off-the-shelf encoders pretrained with various methods on object- and part-level recognition, it is found that while fully-supervised models excel in object-level recognition, self-supervised models, particularly those using contrastive learning and masked image modeling, perform better in part-level recognition. Combining contrastive learning and masked image modeling further enhances performance."
    }
  ],
  "uap": [
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/research/2021/08/31/Universal-Adversarial-Robustness-of-Texture-and-Shape-biased-Models.html",
      "date": "August 31, 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    }
  ],
  "retain": [
    {
      "title": "Universal Adversarial Robustness of Texture and Shape-biased Models",
      "url": "/research/2021/08/31/Universal-Adversarial-Robustness-of-Texture-and-Shape-biased-Models.html",
      "date": "August 31, 2021",
      "summary": "This paper analyzes the adversarial robustness of deep neural networks (DNNs) with texture and shape biases against Universal Adversarial Perturbations (UAPs). Through evaluation, it finds that shape-biased models alone do not significantly enhance adversarial robustness. However, combining texture and shape-biased models into ensembles can increase universal adversarial robustness while retaining high performance."
    }
  ],
  "register": [
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    }
  ],
  "artifact": [
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    }
  ],
  "appear": [
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    }
  ],
  "resolve": [
    {
      "title": "Vision Transformers Need Registers",
      "url": "/research/2023/09/28/Vision-Transformers-Need-Register.html",
      "date": "September 28, 2023",
      "summary": "This study addresses the issue of artifacts in feature maps of both supervised and self-supervised Vision Transformer (ViT) networks, identified as high-norm tokens appearing mainly in low-informative background areas during inference. These tokens, used for internal computations, lead to discrepancies in visual representations. We introduce a straightforward yet effective strategy, adding extra tokens to the input sequence of the Vision Transformer, which successfully eliminates these artifacts for both types of models. This approach not only resolves the artifact issue but also establishes new benchmarks for self-supervised visual models on dense visual prediction tasks. It facilitates object discovery with larger models and results in smoother feature and attention maps for downstream visual processing tasks"
    }
  ],
  "atom": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "waves": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "Taming the Waves: Sine as Activation Function in Deep Neural Networks",
      "url": "/research/2016/11/04/Taming-the-Waves-Sine-as-Activation-Function-in-Deep-Neural-Networks.html",
      "date": "November 4, 2016",
      "summary": "This paper examines the challenges of training deep neural networks with sinusoidal activation functions, despite their successful use in specific applications. It explains the difficulty in training due to the emergence of numerous shallow local minima from the architecture. The study also reveals that successful learning in typical classification tasks occurs when the network effectively disregards the periodic cycles. Furthermore, it demonstrates that for certain non-trivial tasks, networks with sinusoidal activations can outperform those using traditional monotonic functions."
    }
  ],
  "exfractaldb": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "harmonic": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "visualatom": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "near": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "jft": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "Vision Transformers (ViT) excel in various vision tasks, like image classification, object detection, and semantic segmentation, but require more model regularization or data augmentation (AugReg) than convolutional neural networks, especially with smaller datasets. Through a comprehensive study, we explore how training data amount, AugReg, model size, and compute budget interact. Our findings indicate that using more compute and AugReg can achieve the same performance as training with significantly more data. Specifically, we demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "The Vision Transformer (ViT) offers simplicity, robustness, and superior performance in vision tasks but struggles with small-scale datasets due to its lack of inherent inductive biases and dependence on large-scale pre-training datasets like ImageNet and JFT. This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of ViTs without large-scale pre-training or modifications to the architecture or loss functions. Experiments show improved performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, maintaining ViT's attention to relevant regions and robustness. The code and models are available online."
    }
  ],
  "m": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "Vision Transformers (ViT) excel in various vision tasks, like image classification, object detection, and semantic segmentation, but require more model regularization or data augmentation (AugReg) than convolutional neural networks, especially with smaller datasets. Through a comprehensive study, we explore how training data amount, AugReg, model size, and compute budget interact. Our findings indicate that using more compute and AugReg can achieve the same performance as training with significantly more data. Specifically, we demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    },
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper presents the AVA dataset, featuring 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets focused on composite actions in shorter clips with sparse annotations, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. It highlights the challenges in action recognition, introduces a novel localization approach surpassing existing benchmarks but shows modest performance on AVA (15.6% mAP), pointing out the necessity for advanced video understanding methods."
    }
  ],
  "static": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "costserror": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    }
  ],
  "biase": [
    {
      "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
      "url": "/research/2023/03/02/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves.html",
      "date": "March 2, 2023",
      "summary": "Formula-driven supervised learning (FDSL) has proven effective for pre-training vision transformers, with ExFractalDB-21k surpassing ImageNet-21k's pre-training effects, emphasizing the importance of contours over textures. This study introduces a novel methodology utilizing circular harmonics to explore the design space of contour-oriented synthetic datasets systematically. This approach identifies the optimal FDSL parameters and maximizes synthetic image variety, identified as crucial for success. Using the newly created VisualAtom-21k for pre-training, ViT-Base achieves a top-1 accuracy of 83.7% on ImageNet-1k, nearing the 84.2% achieved with JFT-300M pre-training but with significantly fewer images. Unlike static real-image datasets like JFT-300M, synthetic datasets can continuously improve, demonstrating FDSL's potential. FDSL also avoids issues common to real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases."
    },
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "The Vision Transformer (ViT) offers simplicity, robustness, and superior performance in vision tasks but struggles with small-scale datasets due to its lack of inherent inductive biases and dependence on large-scale pre-training datasets like ImageNet and JFT. This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of ViTs without large-scale pre-training or modifications to the architecture or loss functions. Experiments show improved performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, maintaining ViT's attention to relevant regions and robustness. The code and models are available online."
    },
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "Monocular depth estimation's success hinges on large, diverse training datasets. Acquiring dense ground-truth depth is challenging, leading to diverse datasets with unique biases. We introduce tools for training with multiple datasets, despite incompatible annotations. Our approach includes a robust training objective unaffected by depth range and scale variations, employs principled multi-objective learning for data integration from various sources, and emphasizes the importance of pretraining encoders on auxiliary tasks. We tested our methods using five datasets, including 3D films, a novel, extensive data source. Through zero-shot cross-dataset transfer, we demonstrate our approach's superior generalization, significantly outperforming existing methods and establishing a new benchmark in monocular depth estimation."
    },
    {
      "title": "Are Convolutional Neural Networks or Transformers more like human vision?",
      "url": "/research/2021/07/01/Are-Convolutional-Neural-Networks-or-Transformers-more-like-human-vision.html",
      "date": "July 1, 2021",
      "summary": "This study investigates the performance of Convolutional Neural Networks (CNNs) and attention-based Vision Transformers (ViTs) in computer vision tasks. While CNNs excel in accuracy, ViTs offer a different approach with weaker inductive biases. By analyzing error patterns, we find that ViTs exhibit consistency with human errors, suggesting potential for more human-like vision models and insights into human object recognition."
    }
  ],
  "free": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    },
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method, applies a single augmentation to each image and outperforms existing methods with minimal complexity and cost. We conducted extensive experiments to validate its performance against state-of-the-art methods across various image classification scenarios and explored its effectiveness through multiple ablation studies involving different augmentation spaces and methods. Our work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Highlighting a stagnation in automatic augmentation research, we conclude with proposed best practices for future advancements in the field."
    }
  ],
  "demonstrates": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    }
  ],
  "certify": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    }
  ],
  "2norm": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    }
  ],
  "denoise": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "denoising": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    },
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    }
  ],
  "diffusion": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    },
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "We introduce DiffBIR, a framework using pretrained text-to-image diffusion models for blind image restoration. It consists of a two-stage pipeline: initially, we pretrain a restoration module on various degradations to enhance real-world applicability. Then, we use latent diffusion models for realistic restoration, including a novel injective modulation sub-network, LAControlNet, for fine-tuning, and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, we provide a controllable module for users to adjust quality and fidelity during the denoising process. Our extensive tests show DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets. The code is available online."
    },
    {
      "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "Diffusion models, effective for generating high-dimensional perceptual data like images, lack established standard practices. This study enhances noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Our large-scale study demonstrates superior performance over established diffusion methods in high-resolution text-to-image synthesis. We introduce a transformer-based architecture for text-to-image generation, improving text comprehension, typography, and human preference ratings. Our findings show predictable scaling trends, with lower validation loss correlating with improved synthesis. The largest models outperform state-of-the-art approaches, and our data, code, and model weights will be publicly accessible."
    },
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "constraint": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    }
  ],
  "percentage": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    }
  ],
  "over": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    }
  ],
  "without": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    }
  ],
  "retrain": [
    {
      "title": "(Certified!!) Adversarial Robustness for Free",
      "url": "/research/2023/03/06/(Certified!!)-Adversarial-Robustness-for-Free.html",
      "date": "March 6, 2023",
      "summary": "This paper demonstrates achieving top certified adversarial robustness against `2-norm bounded perturbations using pre-existing models. By applying the denoised smoothing method with a pretrained denoising diffusion probabilistic model and a high-accuracy classifier, we significantly improve robustness, certifying 71% accuracy on ImageNet under specified adversarial constraints. This marks a 14 percentage point increase over previous best results and a 30 point improvement over denoised smoothing, achieved without fine-tuning or retraining model parameters."
    },
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "dmae": [
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    }
  ],
  "semantic": [
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    },
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "We introduce SlowFast networks for video recognition, featuring a Slow pathway for spatial semantics at low frame rates and a Fast pathway for motion details at high frame rates. The Fast pathway's efficiency is achieved through reduced channel capacity, yet it effectively captures temporal information. Our models significantly enhance action classification and detection, achieving top accuracy on benchmarks like Kinetics, Charades, and AVA. Code is available at: https://github.com/facebookresearch/SlowFast."
    }
  ],
  "applicability": [
    {
      "title": "Denoising Masked Autoencoders Help Robust Classification",
      "url": "/research/2023/03/07/Denoising-Masked-Autoencoders-Help-Robust-Classification.html",
      "date": "March 7, 2023",
      "summary": "This paper introduces Denoising Masked AutoEncoders (DMAE), a novel self-supervised method for developing robust image classifiers. By corrupting images with Gaussian noise and masking patches, then reconstructing them using a Transformer-based model, DMAE's encoder captures essential semantics resistant to Gaussian noise. It demonstrates that this encoder can serve as a base for Gaussian smoothed models, enabling the computation of a certified radius for robustness. The method, though simple, significantly enhances performance in classification tasks. The DMAE ViT-Base model achieves comparable or superior certified accuracy with far fewer parameters than previous approaches, while the ViT-Large model sets a new benchmark on the ImageNet dataset. The model also shows high transferability to the CIFAR-10 dataset, indicating its broad applicability. Models and code are shared online."
    },
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "We introduce DiffBIR, a framework using pretrained text-to-image diffusion models for blind image restoration. It consists of a two-stage pipeline: initially, we pretrain a restoration module on various degradations to enhance real-world applicability. Then, we use latent diffusion models for realistic restoration, including a novel injective modulation sub-network, LAControlNet, for fine-tuning, and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, we provide a controllable module for users to adjust quality and fidelity during the denoising process. Our extensive tests show DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets. The code is available online."
    }
  ],
  "consistency": [
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "Semi-supervised learning (SSL) utilizes unlabeled data to boost model performance but often at the expense of increased complexity. This paper introduces FixMatch, a streamlined SSL algorithm that simplifies existing methods. FixMatch uses a model's predictions on weakly-augmented unlabeled images to generate pseudo-labels, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. Remarkably, FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness. The code is publicly available."
    },
    {
      "title": "Are Convolutional Neural Networks or Transformers more like human vision?",
      "url": "/research/2021/07/01/Are-Convolutional-Neural-Networks-or-Transformers-more-like-human-vision.html",
      "date": "July 1, 2021",
      "summary": "This study investigates the performance of Convolutional Neural Networks (CNNs) and attention-based Vision Transformers (ViTs) in computer vision tasks. While CNNs excel in accuracy, ViTs offer a different approach with weaker inductive biases. By analyzing error patterns, we find that ViTs exhibit consistency with human errors, suggesting potential for more human-like vision models and insights into human object recognition."
    }
  ],
  "epoch": [
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    },
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "Better plain ViT baselines for ImageNet-1k",
      "url": "/research/2022/05/02/Better-plain-ViT-baselines-for-ImageNet-1k.html",
      "date": "May 2, 2022",
      "summary": "It is commonly accepted that the Vision Transformer model requires sophisticated regularization techniques to excel at ImageNet-1k scale data. Surprisingly, we find this is not the case and standard data augmentation is sufficient. This note presents a few minor modifications to the original Vision Transformer (ViT) vanilla training setting that dramatically improve the performance of plain ViT models. Notably, 90 epochs of training surpass 76% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reach 80% in less than one day."
    }
  ],
  "emae": [
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    }
  ],
  "subject": [
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    }
  ],
  "overlap": [
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    }
  ],
  "tier": [
    {
      "title": "Efficient Masked Autoencoders with Self-Consistency",
      "url": "/research/2023/02/28/Efficient-Masked-Autoencoders-with-Self-Consistency.html",
      "date": "February 28, 2023",
      "summary": "Masked image modeling (MIM), inspired by masked language modeling in natural language processing, is a prevalent self-supervised pre-training method in computer vision. Its high random mask ratio, however, leads to inefficient data use and pre-training (requiring 1600 epochs for MAE versus 300 for supervised learning) and results in a pre-trained model that is uncertain and inconsistent in its predictions. To address these issues, we introduce efficient masked autoencoders with self-consistency (EMAE), enhancing pre-training efficiency and prediction consistency for MIM. EMAE divides the image into non-overlapping parts, each subject to a random mask with a uniform mask ratio, to perform parallel MIM tasks and generate predictions. A self-consistency module is also designed to ensure consistent predictions for overlapping masked patches. EMAE improves data utilization and achieves reliable representations, showing superior results on ImageNet with only 300 pre-training epochs under ViT-Base compared to MAE's 1600 epochs. Additionally, EMAE demonstrates top-tier transfer performance in various downstream tasks, such as object detection and semantic segmentation."
    }
  ],
  "mapping": [
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-5.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-6.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    }
  ],
  "auxiliary": [
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "Monocular depth estimation's success hinges on large, diverse training datasets. Acquiring dense ground-truth depth is challenging, leading to diverse datasets with unique biases. We introduce tools for training with multiple datasets, despite incompatible annotations. Our approach includes a robust training objective unaffected by depth range and scale variations, employs principled multi-objective learning for data integration from various sources, and emphasizes the importance of pretraining encoders on auxiliary tasks. We tested our methods using five datasets, including 3D films, a novel, extensive data source. Through zero-shot cross-dataset transfer, we demonstrate our approach's superior generalization, significantly outperforming existing methods and establishing a new benchmark in monocular depth estimation."
    }
  ],
  "ebm": [
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "energy": [
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "restoration": [
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    },
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "We introduce DiffBIR, a framework using pretrained text-to-image diffusion models for blind image restoration. It consists of a two-stage pipeline: initially, we pretrain a restoration module on various degradations to enhance real-world applicability. Then, we use latent diffusion models for realistic restoration, including a novel injective modulation sub-network, LAControlNet, for fine-tuning, and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, we provide a controllable module for users to adjust quality and fidelity during the denoising process. Our extensive tests show DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets. The code is available online."
    }
  ],
  "assign": [
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "restore": [
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "sort": [
    {
      "title": "Energy-inspired Self-supervised Pretraining For Vision Models",
      "url": "/research/2023/02/02/Energy-inspired-Self-supervised-Pretraining-For-Vision-Models.html",
      "date": "February 2, 2023",
      "summary": "Leveraging symmetric mappings in deep networks, we introduce a self-supervised vision model pretraining framework without auxiliary components, inspired by energy-based models (EBMs). This framework models energy estimation and data restoration through the network's forward and backward passes respectively. It assigns low energy to unlabeled dataset samples and uses gradient-based optimization to restore data from corrupted versions. This approach integrates the encoder-decoder architecture into a single model, supporting a variety of pretext tasks such as masked image modeling, patch sorting, and image restoration tasks like super-resolution, denoising, and colorization. Our extensive experiments demonstrate that this method achieves comparable or better performance with fewer training epochs than current self-supervised pretraining methods, suggesting potential for further exploration in self-supervised vision model pretraining and pretext tasks."
    }
  ],
  "auto": [
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    }
  ],
  "ssl": [
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "Semi-supervised learning (SSL) utilizes unlabeled data to boost model performance but often at the expense of increased complexity. This paper introduces FixMatch, a streamlined SSL algorithm that simplifies existing methods. FixMatch uses a model's predictions on weakly-augmented unlabeled images to generate pseudo-labels, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. Remarkably, FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness. The code is publicly available."
    }
  ],
  "studentteacher": [
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    }
  ],
  "ema": [
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    }
  ],
  "student": [
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    },
    {
      "title": "Training data-efficient image transformers & distillation through attention",
      "url": "/research/2021/01/15/Training-data-efficient-image-transformers-&-distillation-through-attention.html",
      "date": "January 15, 2021",
      "summary": "This study introduces DeiT, competitive vision transformers trained solely on ImageNet without convolutions, achieving up to 83.1% top-1 accuracy on ImageNet, using a single computer in under 3 days. Notably, it presents a unique teacher-student strategy for transformers, leveraging a distillation token for efficient learning from a convolutional network teacher. This method results in performance on par with convolutional networks, achieving up to 85.2% accuracy on ImageNet and demonstrating effective transferability to other tasks. The authors provide access to their code and models."
    },
    {
      "title": "Self-Paced Curriculum Learning",
      "url": "/research/2015/01/01/Self-Paced-Curriculum-Learning.html",
      "date": "January 1, 2015",
      "summary": "Curriculum learning (CL) and self-paced learning (SPL) are methods inspired by the incremental complexity of human and animal learning, with CL using a fixed curriculum based on prior knowledge and SPL adjusting dynamically to the learner's pace but struggling with prior knowledge and overfitting. This paper introduces a unified framework called self-paced curriculum learning (SPCL), blending both methods' strengths by considering prior knowledge and ongoing learning progress through a concise optimization problem. SPCL mimics a collaborative instructor-student learning mode, showing empirical advantages in two tasks."
    },
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "6D object pose estimation is crucial for robotic bin-picking and essential for many industrial applications. Annotating custom datasets for each bin-picking scenario is challenging. We introduce an iterative self-training framework for sim-to-real 6D object pose estimation, enhancing cost-effective robotic grasping. We create a photo-realistic simulator for synthesizing virtual data to train an initial pose estimation network. This network, acting as a teacher model, predicts poses on unlabeled real data. We develop an adaptive selection scheme to filter reliable predictions for updating a student model with pseudo labels for real data pose estimation. By iteratively refining the teacher model with the trained student model, we improve the quality of pseudo labels. Our method, tested on a public benchmark and a newly-released dataset, shows an 11.49% and 22.62% ADD(-S) improvement, respectively, and a 19.54% increase in robotic bin-picking success, demonstrating the effectiveness of iterative sim-to-real approaches. Project homepage: www.cse.cuhk.edu.hk/~kaichen/sim2real_pose.html"
    }
  ],
  "regularizer": [
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    }
  ],
  "rcmae": [
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    }
  ],
  "requirement": [
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    },
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "Online Normalization is a novel method for normalizing neural network hidden activations, offering a batch-independent alternative with comparable accuracy to Batch Normalization. It overcomes Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. Applicable to recurrent, fully connected networks, and those with high activation memory requirements, it demonstrates effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    },
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "This paper explores the effectiveness of using synthetic data, generated by graphics simulators, for pre-training models in computer vision, addressing the challenges posed by using real-image datasets like high costs and ethical concerns. It highlights that the performance of models on downstream tasks varies with different simulation parameters, suggesting a tailored approach to synthetic data generation for optimal results. The introduction of Task2Sim, a model that maps downstream task requirements to optimal simulation parameters, allows for the generation of synthetic pre-training data customized for specific tasks. Task2Sim demonstrates significant improvements in model performance on a wide range of tasks without additional training for new tasks, showing potential to rival pre-training with real images from Imagenet."
    }
  ],
  "rc": [
    {
      "title": "Exploring the Role of Mean Teachers in Self- Supervised Masked Auto-encoders",
      "url": "/research/2022/10/05/Exploring-the-Role-of-Mean-Teachers-in-Self--Supervised-Masked-Auto-encoders.html",
      "date": "October 5, 2022",
      "summary": "Masked image modeling (MIM), particularly through the Masked Auto-Encoder (MAE), is a key technique in self-supervised learning (SSL) for visual representation learning with Vision Transformers, involving the reconstruction of randomly masked image patches. Concurrent approaches often use a student/teacher paradigm, where a teacher model updates itself with an exponential moving average (EMA) of the student model's outputs, though the effects of this interaction are not well understood. Analysis of a simple linear model reveals that the teacher model selectively filters gradient directions based on feature similarity, acting as a conditional momentum regularizer. Building on this insight, we introduce the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), which integrates an EMA teacher with MAE, leading to faster convergence and reduced memory requirements compared to existing self-distillation methods. Furthermore, RC-MAE demonstrates greater robustness and improved performance on tasks like ImageNet-1K classification, object detection, and instance segmentation, compared to the original MAE."
    }
  ],
  "mine": [
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "mining": [
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "hpm": [
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "go": [
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "solving": [
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "predictor": [
    {
      "title": "Hard Patches Mining for Masked Image Modeling",
      "url": "/research/2023/04/12/Hard-Patches-Mining-for-Masked-Image-Modeling.html",
      "date": "April 12, 2023",
      "summary": "Masked image modeling (MIM) is a promising approach for learning scalable visual representations by focusing on predicting the contents of masked patches, with performance depending on mask strategies. We propose a novel framework, Hard Patches Mining (HPM), for MIM pre-training that goes beyond simply solving given problems. HPM aims for the model to also generate more challenging tasks for itself, akin to a teacher-student dynamic. It uses reconstruction loss as a metric for task difficulty and incorporates an auxiliary loss predictor to determine which patches to mask based on predicted patch-wise losses, using a strategy to avoid overfitting. Experiments show HPM's effectiveness in creating challenging masked images and enhancing representation quality through the loss prediction objective, highlighting its ability to identify and learn from hard-to-reconstruct areas."
    }
  ],
  "msa": [
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    }
  ],
  "mechanics": [
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    }
  ],
  "unexplore": [
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    }
  ],
  "elucidate": [
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    }
  ],
  "attribute": [
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    }
  ],
  "specificity": [
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    }
  ],
  "grapple": [
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    }
  ],
  "convex": [
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    },
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    }
  ],
  "note": [
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "Better plain ViT baselines for ImageNet-1k",
      "url": "/research/2022/05/02/Better-plain-ViT-baselines-for-ImageNet-1k.html",
      "date": "May 2, 2022",
      "summary": "It is commonly accepted that the Vision Transformer model requires sophisticated regularization techniques to excel at ImageNet-1k scale data. Surprisingly, we find this is not the case and standard data augmentation is sufficient. This note presents a few minor modifications to the original Vision Transformer (ViT) vanilla training setting that dramatically improve the performance of plain ViT models. Notably, 90 epochs of training surpass 76% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reach 80% in less than one day."
    }
  ],
  "multistage": [
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    }
  ],
  "alternet": [
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    }
  ],
  "showcase": [
    {
      "title": "How Do Vision Transformers Work",
      "url": "/research/2022/06/08/How-Do-Vision-Transformers-Work.html",
      "date": "June 8, 2022",
      "summary": "The success of multi-head self-attentions (MSAs) in computer vision is well-recognized, yet their operational mechanics remain largely unexplored. This study elucidates the workings of MSAs and Vision Transformers (ViTs), revealing that MSAs enhance both accuracy and generalization by smoothing loss landscapes, attributed more to data specificity than to managing long-range dependencies. Conversely, ViTs grapple with non-convex losses, mitigated by large datasets and specific smoothing techniques. The research further contrasts MSAs and convolutional layers (Convs), noting MSAs act as low-pass filters while Convs serve as high-pass filters, making them complementary. It is also found that multi-stage neural networks function akin to a series of small models, with MSAs crucial for predictions at stage ends. Introducing AlterNet, the study showcases a model where Conv blocks are substituted with MSA blocks at stage ends, achieving superior performance over CNNs across both large and small data scenarios."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "Neural networks parameterized for continuous, differentiable signal representations provide a promising paradigm but struggle with modeling fine detail and representing spatial and temporal derivatives critical for signals defined by partial differential equations. We introduce sinusoidal representation networks (SIRENs), utilizing periodic activation functions to effectively capture complex natural signals and their derivatives. Our analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. We also showcase SIRENs' application in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations, and extend their use with hypernetworks to learn priors for SIREN functions. A video overview and full applications are available on our project website."
    }
  ],
  "augreg": [
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "Vision Transformers (ViT) excel in various vision tasks, like image classification, object detection, and semantic segmentation, but require more model regularization or data augmentation (AugReg) than convolutional neural networks, especially with smaller datasets. Through a comprehensive study, we explore how training data amount, AugReg, model size, and compute budget interact. Our findings indicate that using more compute and AugReg can achieve the same performance as training with significantly more data. Specifically, we demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    }
  ],
  "compute": [
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "Vision Transformers (ViT) excel in various vision tasks, like image classification, object detection, and semantic segmentation, but require more model regularization or data augmentation (AugReg) than convolutional neural networks, especially with smaller datasets. Through a comprehensive study, we explore how training data amount, AugReg, model size, and compute budget interact. Our findings indicate that using more compute and AugReg can achieve the same performance as training with significantly more data. Specifically, we demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    },
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    }
  ],
  "interact": [
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "url": "/research/2022/06/23/How-to-train-your-ViT-Data,-Augmentation,-and-Regularization-in-Vision-Transformers.html",
      "date": "June 23, 2022",
      "summary": "Vision Transformers (ViT) excel in various vision tasks, like image classification, object detection, and semantic segmentation, but require more model regularization or data augmentation (AugReg) than convolutional neural networks, especially with smaller datasets. Through a comprehensive study, we explore how training data amount, AugReg, model size, and compute budget interact. Our findings indicate that using more compute and AugReg can achieve the same performance as training with significantly more data. Specifically, we demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
    }
  ],
  "cls": [
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "Self-supervised transformer training methods have shown excellent results in various fields. Unlike previous models like masked autoencoders (MAE) which use a single normalization layer for the class token [CLS] and other tokens, we introduce a novel normalization technique that normalizes the [CLS] token and normal tokens separately. This approach aims to better capture their unique characteristics, improving performance in downstream tasks. Our findings indicate that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks."
    }
  ],
  "encode": [
    {
      "title": "On Separate Normalization in Self-supervised Transformers",
      "url": "/research/1000/01/01/On-Separate-Normalization-in-Self-supervised-Transformers.html",
      "date": "January 1, 1000",
      "summary": "Self-supervised transformer training methods have shown excellent results in various fields. Unlike previous models like masked autoencoders (MAE) which use a single normalization layer for the class token [CLS] and other tokens, we introduce a novel normalization technique that normalizes the [CLS] token and normal tokens separately. This approach aims to better capture their unique characteristics, improving performance in downstream tasks. Our findings indicate that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article offers the first extensive review of vision transformer techniques specifically applied to human action recognition, a field gaining interest for its wide applications. Termed action transformers, these methods are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also delves into optimizing spatio-temporal attention in transformers for longer sequences and examines different learning strategies, including self-supervised and zero-shot learning, with their respective loss functions. Additionally, the survey assesses progress in benchmark evaluation scores and discusses the challenges and future directions in this research area."
    }
  ],
  "things": [
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    }
  ],
  "everyone": [
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    }
  ],
  "stride": [
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    }
  ],
  "parallel": [
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    }
  ],
  "extent": [
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    }
  ],
  "preprocess": [
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    }
  ],
  "bertlike": [
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    }
  ],
  "v2": [
    {
      "title": "Three things everyone should know about Vision Transformers",
      "url": "/research/2022/03/18/Three-things-everyone-should-know-about-Vision-Transformers.html",
      "date": "March 18, 2022",
      "summary": "Transformer architectures have made significant strides in computer vision, excelling in image classification, detection, segmentation, and video analysis. Our research presents three key findings using variants of vision transformers: (1) Vision transformers' residual layers can be processed in parallel to some extent without significantly impacting accuracy. (2) Fine-tuning attention layer weights alone effectively adapts transformers for higher resolution and different classification tasks, reducing compute and memory use while allowing weight sharing. (3) Incorporating MLP-based patch pre-processing enhances Bert-like self-supervised training with patch masking. These approaches are validated using the ImageNet-1k dataset and further confirmed with the ImageNet-v2 test set, with transfer performance evaluated across six additional datasets."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    }
  ],
  "par": [
    {
      "title": "Training data-efficient image transformers & distillation through attention",
      "url": "/research/2021/01/15/Training-data-efficient-image-transformers-&-distillation-through-attention.html",
      "date": "January 15, 2021",
      "summary": "This study introduces DeiT, competitive vision transformers trained solely on ImageNet without convolutions, achieving up to 83.1% top-1 accuracy on ImageNet, using a single computer in under 3 days. Notably, it presents a unique teacher-student strategy for transformers, leveraging a distillation token for efficient learning from a convolutional network teacher. This method results in performance on par with convolutional networks, achieving up to 85.2% accuracy on ImageNet and demonstrating effective transferability to other tasks. The authors provide access to their code and models."
    }
  ],
  "author": [
    {
      "title": "Training data-efficient image transformers & distillation through attention",
      "url": "/research/2021/01/15/Training-data-efficient-image-transformers-&-distillation-through-attention.html",
      "date": "January 15, 2021",
      "summary": "This study introduces DeiT, competitive vision transformers trained solely on ImageNet without convolutions, achieving up to 83.1% top-1 accuracy on ImageNet, using a single computer in under 3 days. Notably, it presents a unique teacher-student strategy for transformers, leveraging a distillation token for efficient learning from a convolutional network teacher. This method results in performance on par with convolutional networks, achieving up to 85.2% accuracy on ImageNet and demonstrating effective transferability to other tasks. The authors provide access to their code and models."
    }
  ],
  "access": [
    {
      "title": "Training data-efficient image transformers & distillation through attention",
      "url": "/research/2021/01/15/Training-data-efficient-image-transformers-&-distillation-through-attention.html",
      "date": "January 15, 2021",
      "summary": "This study introduces DeiT, competitive vision transformers trained solely on ImageNet without convolutions, achieving up to 83.1% top-1 accuracy on ImageNet, using a single computer in under 3 days. Notably, it presents a unique teacher-student strategy for transformers, leveraging a distillation token for efficient learning from a convolutional network teacher. This method results in performance on par with convolutional networks, achieving up to 85.2% accuracy on ImageNet and demonstrating effective transferability to other tasks. The authors provide access to their code and models."
    }
  ],
  "splat": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "radiance": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "rendering": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "synthesis": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    },
    {
      "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "Diffusion models, effective for generating high-dimensional perceptual data like images, lack established standard practices. This study enhances noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Our large-scale study demonstrates superior performance over established diffusion methods in high-resolution text-to-image synthesis. We introduce a transformer-based architecture for text-to-image generation, improving text comprehension, typography, and human preference ratings. Our findings show predictable scaling trends, with lower validation loss correlating with improved synthesis. The largest models outperform state-of-the-art approaches, and our data, code, and model weights will be publicly accessible."
    }
  ],
  "render": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    },
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This study investigates the use of high-quality, physically-based rendering and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle, enhancing image classification and semantic segmentation models. Synthetic images proved to be effective for augmenting limited real training data. However, models trained solely on synthetic images showed low accuracy on real validation images, which significantly improved when even small amounts of real data were included. Augmenting real data with synthetic images outperformed training on only real images. Moreover, pretraining models on synthetic data before transfer learning significantly reduced training costs, allowing most of the training to be completed upfront."
    }
  ],
  "sacrifice": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "unbound": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "camera": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "fidelity": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    },
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "We introduce DiffBIR, a framework using pretrained text-to-image diffusion models for blind image restoration. It consists of a two-stage pipeline: initially, we pretrain a restoration module on various degradations to enhance real-world applicability. Then, we use latent diffusion models for realistic restoration, including a novel injective modulation sub-network, LAControlNet, for fine-tuning, and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, we provide a controllable module for users to adjust quality and fidelity during the denoising process. Our extensive tests show DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets. The code is available online."
    }
  ],
  "implementing": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "interleave": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "depiction": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "splatting": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "fps": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "url": "/research/2022/08/08/3D-Gaussian-Splatting-for-Real-Time-Radiance-Field-Rendering.html",
      "date": "August 8, 2022",
      "summary": "Radiance Field methods have significantly advanced novel-view synthesis for scenes from multiple photos or videos, but high visual quality requires costly training and rendering, with faster methods sacrificing quality. Existing methods struggle to display unbounded, complete scenes at 1080p resolution in real-time. We introduce a novel approach with three key innovations: 1) Utilizing sparse points from camera calibration to represent scenes with 3D Gaussians, optimizing scene fidelity while reducing computation in empty spaces. 2) Implementing interleaved optimization and density control of the 3D Gaussians, including anisotropic covariance adjustment for accurate scene depiction. 3) Developing a fast, visibility-aware rendering algorithm enabling anisotropic splatting, which speeds up training and supports real-time (≥ 30 fps) rendering at 1080p. Our method demonstrates superior visual quality and real-time rendering capabilities across several datasets."
    }
  ],
  "augmentor": [
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "Deep neural networks, while effective in complex vision tasks, suffer from overfitting. Current image augmentation techniques, largely linear and hand-crafted like scale, flip, and color jitter, fail to produce sufficiently challenging examples. We introduce a new augmentation approach, Mask-Reconstruct Augmentation (MRA), leveraging self-supervised masked autoencoders to generate distorted inputs. This method, inspired by masked image modeling's success in self-supervised learning, uses nonlinear transformations for regularization. Our extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks. The code is available at https://github.com/haohang96/MRA."
    }
  ],
  "hand": [
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "Deep neural networks, while effective in complex vision tasks, suffer from overfitting. Current image augmentation techniques, largely linear and hand-crafted like scale, flip, and color jitter, fail to produce sufficiently challenging examples. We introduce a new augmentation approach, Mask-Reconstruct Augmentation (MRA), leveraging self-supervised masked autoencoders to generate distorted inputs. This method, inspired by masked image modeling's success in self-supervised learning, uses nonlinear transformations for regularization. Our extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks. The code is available at https://github.com/haohang96/MRA."
    }
  ],
  "jitter": [
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "Deep neural networks, while effective in complex vision tasks, suffer from overfitting. Current image augmentation techniques, largely linear and hand-crafted like scale, flip, and color jitter, fail to produce sufficiently challenging examples. We introduce a new augmentation approach, Mask-Reconstruct Augmentation (MRA), leveraging self-supervised masked autoencoders to generate distorted inputs. This method, inspired by masked image modeling's success in self-supervised learning, uses nonlinear transformations for regularization. Our extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks. The code is available at https://github.com/haohang96/MRA."
    }
  ],
  "fail": [
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "Deep neural networks, while effective in complex vision tasks, suffer from overfitting. Current image augmentation techniques, largely linear and hand-crafted like scale, flip, and color jitter, fail to produce sufficiently challenging examples. We introduce a new augmentation approach, Mask-Reconstruct Augmentation (MRA), leveraging self-supervised masked autoencoders to generate distorted inputs. This method, inspired by masked image modeling's success in self-supervised learning, uses nonlinear transformations for regularization. Our extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks. The code is available at https://github.com/haohang96/MRA."
    }
  ],
  "mra": [
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "Deep neural networks, while effective in complex vision tasks, suffer from overfitting. Current image augmentation techniques, largely linear and hand-crafted like scale, flip, and color jitter, fail to produce sufficiently challenging examples. We introduce a new augmentation approach, Mask-Reconstruct Augmentation (MRA), leveraging self-supervised masked autoencoders to generate distorted inputs. This method, inspired by masked image modeling's success in self-supervised learning, uses nonlinear transformations for regularization. Our extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks. The code is available at https://github.com/haohang96/MRA."
    }
  ],
  "httpsgithubcomhaohang96mra": [
    {
      "title": "Masked Autoencoders are Robust Data Augmentors",
      "url": "/research/2022/06/10/A-Comparison-Study-of-Image-Spatial-Entrop.html",
      "date": "June 10, 2022",
      "summary": "Deep neural networks, while effective in complex vision tasks, suffer from overfitting. Current image augmentation techniques, largely linear and hand-crafted like scale, flip, and color jitter, fail to produce sufficiently challenging examples. We introduce a new augmentation approach, Mask-Reconstruct Augmentation (MRA), leveraging self-supervised masked autoencoders to generate distorted inputs. This method, inspired by masked image modeling's success in self-supervised learning, uses nonlinear transformations for regularization. Our extensive testing across various image classification benchmarks demonstrates MRA's ability to significantly improve performance in supervised, semi-supervised, and few-shot classification tasks. The code is available at https://github.com/haohang96/MRA."
    }
  ],
  "ava": [
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper presents the AVA dataset, featuring 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets focused on composite actions in shorter clips with sparse annotations, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. It highlights the challenges in action recognition, introduces a novel localization approach surpassing existing benchmarks but shows modest performance on AVA (15.6% mAP), pointing out the necessity for advanced video understanding methods."
    },
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "We introduce SlowFast networks for video recognition, featuring a Slow pathway for spatial semantics at low frame rates and a Fast pathway for motion details at high frame rates. The Fast pathway's efficiency is achieved through reduced channel capacity, yet it effectively captures temporal information. Our models significantly enhance action classification and detection, achieving top accuracy on benchmarks like Kinetics, Charades, and AVA. Code is available at: https://github.com/facebookresearch/SlowFast."
    }
  ],
  "spatio": [
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper presents the AVA dataset, featuring 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets focused on composite actions in shorter clips with sparse annotations, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. It highlights the challenges in action recognition, introduces a novel localization approach surpassing existing benchmarks but shows modest performance on AVA (15.6% mAP), pointing out the necessity for advanced video understanding methods."
    }
  ],
  "annotation": [
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper presents the AVA dataset, featuring 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets focused on composite actions in shorter clips with sparse annotations, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. It highlights the challenges in action recognition, introduces a novel localization approach surpassing existing benchmarks but shows modest performance on AVA (15.6% mAP), pointing out the necessity for advanced video understanding methods."
    },
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "Temporal action localization (TAL) is crucial for video understanding but is hindered by benchmarks that rely on coarse action classes, leading to model overfitting and ambiguous annotations. Addressing these issues, we introduce FineAction, a large-scale, fine-grained video dataset with 103K instances across 106 action categories in 17K videos. FineAction offers a diverse, densely annotated dataset with co-occurring actions, posing new challenges for TAL. We evaluated popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and proposed a baseline method achieving a 13.17% mAP. FineAction aims to propel TAL research forward and is accessible online."
    },
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "Monocular depth estimation's success hinges on large, diverse training datasets. Acquiring dense ground-truth depth is challenging, leading to diverse datasets with unique biases. We introduce tools for training with multiple datasets, despite incompatible annotations. Our approach includes a robust training objective unaffected by depth range and scale variations, employs principled multi-objective learning for data integration from various sources, and emphasizes the importance of pretraining encoders on auxiliary tasks. We tested our methods using five datasets, including 3D films, a novel, extensive data source. Through zero-shot cross-dataset transfer, we demonstrate our approach's superior generalization, significantly outperforming existing methods and establishing a new benchmark in monocular depth estimation."
    }
  ],
  "composite": [
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper presents the AVA dataset, featuring 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets focused on composite actions in shorter clips with sparse annotations, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. It highlights the challenges in action recognition, introduces a novel localization approach surpassing existing benchmarks but shows modest performance on AVA (15.6% mAP), pointing out the necessity for advanced video understanding methods."
    }
  ],
  "annotations": [
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper presents the AVA dataset, featuring 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets focused on composite actions in shorter clips with sparse annotations, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. It highlights the challenges in action recognition, introduces a novel localization approach surpassing existing benchmarks but shows modest performance on AVA (15.6% mAP), pointing out the necessity for advanced video understanding methods."
    }
  ],
  "videos": [
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper presents the AVA dataset, featuring 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets focused on composite actions in shorter clips with sparse annotations, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. It highlights the challenges in action recognition, introduces a novel localization approach surpassing existing benchmarks but shows modest performance on AVA (15.6% mAP), pointing out the necessity for advanced video understanding methods."
    },
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "This paper introduces VideoMAE, a data-efficient approach for self-supervised video pre-training (SSVP) that utilizes video masked autoencoders with a novel, high-ratio video tube masking technique. This method, inspired by ImageMAE, challenges the model with video reconstruction tasks to improve video representation learning. Key findings include: (1) High masking ratios (90% to 95%) are effective due to video's temporal redundancy, (2) VideoMAE performs well on small datasets (~3k-4k videos) without extra data, highlighting the importance of high-level structure learning, and (3) Data quality is more crucial than quantity for SSVP, with domain shift between pre-training and target datasets being significant. Remarkably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone, without extra data. The code is available online."
    }
  ],
  "continuity": [
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper presents the AVA dataset, featuring 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets focused on composite actions in shorter clips with sparse annotations, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. It highlights the challenges in action recognition, introduces a novel localization approach surpassing existing benchmarks but shows modest performance on AVA (15.6% mAP), pointing out the necessity for advanced video understanding methods."
    }
  ],
  "person": [
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper presents the AVA dataset, featuring 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets focused on composite actions in shorter clips with sparse annotations, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. It highlights the challenges in action recognition, introduces a novel localization approach surpassing existing benchmarks but shows modest performance on AVA (15.6% mAP), pointing out the necessity for advanced video understanding methods."
    }
  ],
  "movie": [
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper presents the AVA dataset, featuring 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets focused on composite actions in shorter clips with sparse annotations, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. It highlights the challenges in action recognition, introduces a novel localization approach surpassing existing benchmarks but shows modest performance on AVA (15.6% mAP), pointing out the necessity for advanced video understanding methods."
    }
  ],
  "pointing": [
    {
      "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Action",
      "url": "/research/2018/04/30/AVA-A-Video-Dataset-of-Spatio-temporally-Localized-Atomic-Visual-Action.html",
      "date": "April 30, 2018",
      "summary": "This paper presents the AVA dataset, featuring 1.58M labels for 80 atomic visual actions across 430 15-minute video clips, with precise spatio-temporal and person-specific annotations. Unlike previous datasets focused on composite actions in shorter clips with sparse annotations, AVA emphasizes atomic actions, detailed annotations throughout longer videos, continuity of persons across clips, and varied action representations from movies. It highlights the challenges in action recognition, introduces a novel localization approach surpassing existing benchmarks but shows modest performance on AVA (15.6% mAP), pointing out the necessity for advanced video understanding methods."
    }
  ],
  "accept": [
    {
      "title": "Better plain ViT baselines for ImageNet-1k",
      "url": "/research/2022/05/02/Better-plain-ViT-baselines-for-ImageNet-1k.html",
      "date": "May 2, 2022",
      "summary": "It is commonly accepted that the Vision Transformer model requires sophisticated regularization techniques to excel at ImageNet-1k scale data. Surprisingly, we find this is not the case and standard data augmentation is sufficient. This note presents a few minor modifications to the original Vision Transformer (ViT) vanilla training setting that dramatically improve the performance of plain ViT models. Notably, 90 epochs of training surpass 76% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reach 80% in less than one day."
    }
  ],
  "tpuv38": [
    {
      "title": "Better plain ViT baselines for ImageNet-1k",
      "url": "/research/2022/05/02/Better-plain-ViT-baselines-for-ImageNet-1k.html",
      "date": "May 2, 2022",
      "summary": "It is commonly accepted that the Vision Transformer model requires sophisticated regularization techniques to excel at ImageNet-1k scale data. Surprisingly, we find this is not the case and standard data augmentation is sufficient. This note presents a few minor modifications to the original Vision Transformer (ViT) vanilla training setting that dramatically improve the performance of plain ViT models. Notably, 90 epochs of training surpass 76% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reach 80% in less than one day."
    }
  ],
  "resnet50": [
    {
      "title": "Better plain ViT baselines for ImageNet-1k",
      "url": "/research/2022/05/02/Better-plain-ViT-baselines-for-ImageNet-1k.html",
      "date": "May 2, 2022",
      "summary": "It is commonly accepted that the Vision Transformer model requires sophisticated regularization techniques to excel at ImageNet-1k scale data. Surprisingly, we find this is not the case and standard data augmentation is sufficient. This note presents a few minor modifications to the original Vision Transformer (ViT) vanilla training setting that dramatically improve the performance of plain ViT models. Notably, 90 epochs of training surpass 76% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reach 80% in less than one day."
    },
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "Recent studies have shown that Convolutional Neural Networks (CNNs), which were thought to recognize objects by learning complex shapes, are actually more biased towards recognizing textures. This finding, based on a series of tests comparing CNNs with human observers using images with conflicting texture and shape cues, contradicts the traditional understanding of how these networks process images. We discovered that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, when we trained a standard CNN architecture (ResNet-50) on a stylized version of ImageNet, designed to emphasize shape, the network's performance aligned more closely with human behavior. This training not only matched human performance in controlled experiments (involving 97 observers across 48,560 trials) but also enhanced object detection and robustness against image distortions, underscoring the benefits of shape-based representations in visual recognition systems."
    }
  ],
  "curriculum": [
    {
      "title": "Curriculum Learning by Dynamic Instance Hardness",
      "url": "/research/2020/12/06/Curriculum-Learning-by-Dynamic-Instance-Hardness.html",
      "date": "December 6, 2020",
      "summary": "This paper explores the adaptation of deep neural network (DNN) training using a method called dynamic instance hardness (DIH), which measures a sample's learning difficulty over time. By tracking the exponential moving average of a sample's hardness, DIH provides a stable indicator of learning progress. Early predictions of a sample's DIH allow for prioritizing more challenging samples and deprioritizing easier ones, leading to a DIH guided curriculum learning (DIHCL) approach. DIHCL enhances learning efficiency and model accuracy without extra computational costs, as it leverages data from the training process itself. Tested on 11 datasets, DIHCL surpasses traditional training methods and recent curriculum learning techniques in both efficiency and effectiveness. The implementation is publicly available."
    },
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "Image-caption pretraining, crucial for tasks like zero-shot image classification and object detection, faces challenges in aligning multiple concepts from captions to objects in images. We introduce a curriculum learning framework inspired by children's language learning from cognitive science, starting with simple image-caption pairs and gradually increasing complexity by adding more concepts. This method, which leverages knowledge from each phase for subsequent learning, outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios. The code is available at: https://github.com/hayyubi/cur_vl.git."
    },
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "Self-paced curriculum learning (SCL) has been effective in various fields, employing easy-to-hard sampling based on data difficulty. However, in medical image analysis tasks like universal lesion detection, it faces challenges due to inaccurate difficulty estimation and the under-utilization of hard samples. This paper introduces a novel approach, mixed-order self-paced curriculum learning (Mo-SCL), which combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches to address these issues. Through theoretical analysis and experiments on the DeepLesion dataset, Mo-SCL is shown to enhance lesion detection accuracy in state-of-the-art methods without additional network modifications."
    },
    {
      "title": "Self-Paced Curriculum Learning",
      "url": "/research/2015/01/01/Self-Paced-Curriculum-Learning.html",
      "date": "January 1, 2015",
      "summary": "Curriculum learning (CL) and self-paced learning (SPL) are methods inspired by the incremental complexity of human and animal learning, with CL using a fixed curriculum based on prior knowledge and SPL adjusting dynamically to the learner's pace but struggling with prior knowledge and overfitting. This paper introduces a unified framework called self-paced curriculum learning (SPCL), blending both methods' strengths by considering prior knowledge and ongoing learning progress through a concise optimization problem. SPCL mimics a collaborative instructor-student learning mode, showing empirical advantages in two tasks."
    }
  ],
  "hardness": [
    {
      "title": "Curriculum Learning by Dynamic Instance Hardness",
      "url": "/research/2020/12/06/Curriculum-Learning-by-Dynamic-Instance-Hardness.html",
      "date": "December 6, 2020",
      "summary": "This paper explores the adaptation of deep neural network (DNN) training using a method called dynamic instance hardness (DIH), which measures a sample's learning difficulty over time. By tracking the exponential moving average of a sample's hardness, DIH provides a stable indicator of learning progress. Early predictions of a sample's DIH allow for prioritizing more challenging samples and deprioritizing easier ones, leading to a DIH guided curriculum learning (DIHCL) approach. DIHCL enhances learning efficiency and model accuracy without extra computational costs, as it leverages data from the training process itself. Tested on 11 datasets, DIHCL surpasses traditional training methods and recent curriculum learning techniques in both efficiency and effectiveness. The implementation is publicly available."
    }
  ],
  "dih": [
    {
      "title": "Curriculum Learning by Dynamic Instance Hardness",
      "url": "/research/2020/12/06/Curriculum-Learning-by-Dynamic-Instance-Hardness.html",
      "date": "December 6, 2020",
      "summary": "This paper explores the adaptation of deep neural network (DNN) training using a method called dynamic instance hardness (DIH), which measures a sample's learning difficulty over time. By tracking the exponential moving average of a sample's hardness, DIH provides a stable indicator of learning progress. Early predictions of a sample's DIH allow for prioritizing more challenging samples and deprioritizing easier ones, leading to a DIH guided curriculum learning (DIHCL) approach. DIHCL enhances learning efficiency and model accuracy without extra computational costs, as it leverages data from the training process itself. Tested on 11 datasets, DIHCL surpasses traditional training methods and recent curriculum learning techniques in both efficiency and effectiveness. The implementation is publicly available."
    }
  ],
  "track": [
    {
      "title": "Curriculum Learning by Dynamic Instance Hardness",
      "url": "/research/2020/12/06/Curriculum-Learning-by-Dynamic-Instance-Hardness.html",
      "date": "December 6, 2020",
      "summary": "This paper explores the adaptation of deep neural network (DNN) training using a method called dynamic instance hardness (DIH), which measures a sample's learning difficulty over time. By tracking the exponential moving average of a sample's hardness, DIH provides a stable indicator of learning progress. Early predictions of a sample's DIH allow for prioritizing more challenging samples and deprioritizing easier ones, leading to a DIH guided curriculum learning (DIHCL) approach. DIHCL enhances learning efficiency and model accuracy without extra computational costs, as it leverages data from the training process itself. Tested on 11 datasets, DIHCL surpasses traditional training methods and recent curriculum learning techniques in both efficiency and effectiveness. The implementation is publicly available."
    }
  ],
  "deprioritize": [
    {
      "title": "Curriculum Learning by Dynamic Instance Hardness",
      "url": "/research/2020/12/06/Curriculum-Learning-by-Dynamic-Instance-Hardness.html",
      "date": "December 6, 2020",
      "summary": "This paper explores the adaptation of deep neural network (DNN) training using a method called dynamic instance hardness (DIH), which measures a sample's learning difficulty over time. By tracking the exponential moving average of a sample's hardness, DIH provides a stable indicator of learning progress. Early predictions of a sample's DIH allow for prioritizing more challenging samples and deprioritizing easier ones, leading to a DIH guided curriculum learning (DIHCL) approach. DIHCL enhances learning efficiency and model accuracy without extra computational costs, as it leverages data from the training process itself. Tested on 11 datasets, DIHCL surpasses traditional training methods and recent curriculum learning techniques in both efficiency and effectiveness. The implementation is publicly available."
    }
  ],
  "dihcl": [
    {
      "title": "Curriculum Learning by Dynamic Instance Hardness",
      "url": "/research/2020/12/06/Curriculum-Learning-by-Dynamic-Instance-Hardness.html",
      "date": "December 6, 2020",
      "summary": "This paper explores the adaptation of deep neural network (DNN) training using a method called dynamic instance hardness (DIH), which measures a sample's learning difficulty over time. By tracking the exponential moving average of a sample's hardness, DIH provides a stable indicator of learning progress. Early predictions of a sample's DIH allow for prioritizing more challenging samples and deprioritizing easier ones, leading to a DIH guided curriculum learning (DIHCL) approach. DIHCL enhances learning efficiency and model accuracy without extra computational costs, as it leverages data from the training process itself. Tested on 11 datasets, DIHCL surpasses traditional training methods and recent curriculum learning techniques in both efficiency and effectiveness. The implementation is publicly available."
    }
  ],
  "enhances": [
    {
      "title": "Curriculum Learning by Dynamic Instance Hardness",
      "url": "/research/2020/12/06/Curriculum-Learning-by-Dynamic-Instance-Hardness.html",
      "date": "December 6, 2020",
      "summary": "This paper explores the adaptation of deep neural network (DNN) training using a method called dynamic instance hardness (DIH), which measures a sample's learning difficulty over time. By tracking the exponential moving average of a sample's hardness, DIH provides a stable indicator of learning progress. Early predictions of a sample's DIH allow for prioritizing more challenging samples and deprioritizing easier ones, leading to a DIH guided curriculum learning (DIHCL) approach. DIHCL enhances learning efficiency and model accuracy without extra computational costs, as it leverages data from the training process itself. Tested on 11 datasets, DIHCL surpasses traditional training methods and recent curriculum learning techniques in both efficiency and effectiveness. The implementation is publicly available."
    }
  ],
  "iii": [
    {
      "title": "DeiT III: Revenge of the ViT",
      "url": "/research/2023/01/18/DeiT-III-Revenge-of-the-ViT.html",
      "date": "January 18, 2023",
      "summary": "This paper explores the supervised training of Vision Transformers (ViTs), using a simplified training approach adapted from ResNet-50 that includes a novel data-augmentation method with just 3 augmentations. The study demonstrates that this method significantly improves ViTs' performance in image classification, transfer learning, and semantic segmentation over previous supervised training techniques. Moreover, it shows ViTs' performance can match newer architectures, providing a new benchmark for evaluating self-supervised methods on ViTs."
    }
  ],
  "revenge": [
    {
      "title": "DeiT III: Revenge of the ViT",
      "url": "/research/2023/01/18/DeiT-III-Revenge-of-the-ViT.html",
      "date": "January 18, 2023",
      "summary": "This paper explores the supervised training of Vision Transformers (ViTs), using a simplified training approach adapted from ResNet-50 that includes a novel data-augmentation method with just 3 augmentations. The study demonstrates that this method significantly improves ViTs' performance in image classification, transfer learning, and semantic segmentation over previous supervised training techniques. Moreover, it shows ViTs' performance can match newer architectures, providing a new benchmark for evaluating self-supervised methods on ViTs."
    }
  ],
  "diffbir": [
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "We introduce DiffBIR, a framework using pretrained text-to-image diffusion models for blind image restoration. It consists of a two-stage pipeline: initially, we pretrain a restoration module on various degradations to enhance real-world applicability. Then, we use latent diffusion models for realistic restoration, including a novel injective modulation sub-network, LAControlNet, for fine-tuning, and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, we provide a controllable module for users to adjust quality and fidelity during the denoising process. Our extensive tests show DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets. The code is available online."
    }
  ],
  "consist": [
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "We introduce DiffBIR, a framework using pretrained text-to-image diffusion models for blind image restoration. It consists of a two-stage pipeline: initially, we pretrain a restoration module on various degradations to enhance real-world applicability. Then, we use latent diffusion models for realistic restoration, including a novel injective modulation sub-network, LAControlNet, for fine-tuning, and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, we provide a controllable module for users to adjust quality and fidelity during the denoising process. Our extensive tests show DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets. The code is available online."
    }
  ],
  "modulation": [
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "We introduce DiffBIR, a framework using pretrained text-to-image diffusion models for blind image restoration. It consists of a two-stage pipeline: initially, we pretrain a restoration module on various degradations to enhance real-world applicability. Then, we use latent diffusion models for realistic restoration, including a novel injective modulation sub-network, LAControlNet, for fine-tuning, and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, we provide a controllable module for users to adjust quality and fidelity during the denoising process. Our extensive tests show DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets. The code is available online."
    }
  ],
  "subnetwork": [
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "We introduce DiffBIR, a framework using pretrained text-to-image diffusion models for blind image restoration. It consists of a two-stage pipeline: initially, we pretrain a restoration module on various degradations to enhance real-world applicability. Then, we use latent diffusion models for realistic restoration, including a novel injective modulation sub-network, LAControlNet, for fine-tuning, and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, we provide a controllable module for users to adjust quality and fidelity during the denoising process. Our extensive tests show DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets. The code is available online."
    }
  ],
  "lacontrolnet": [
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "We introduce DiffBIR, a framework using pretrained text-to-image diffusion models for blind image restoration. It consists of a two-stage pipeline: initially, we pretrain a restoration module on various degradations to enhance real-world applicability. Then, we use latent diffusion models for realistic restoration, including a novel injective modulation sub-network, LAControlNet, for fine-tuning, and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, we provide a controllable module for users to adjust quality and fidelity during the denoising process. Our extensive tests show DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets. The code is available online."
    }
  ],
  "superiority": [
    {
      "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior",
      "url": "/research/2023/08/29/DiffBIR-Towards-Blind-Image-Restoration-with-Generative-Diffusion-Prior.html",
      "date": "August 29, 2023",
      "summary": "We introduce DiffBIR, a framework using pretrained text-to-image diffusion models for blind image restoration. It consists of a two-stage pipeline: initially, we pretrain a restoration module on various degradations to enhance real-world applicability. Then, we use latent diffusion models for realistic restoration, including a novel injective modulation sub-network, LAControlNet, for fine-tuning, and employ pre-trained Stable Diffusion for its generative capabilities. Additionally, we provide a controllable module for users to adjust quality and fidelity during the denoising process. Our extensive tests show DiffBIR's superiority in blind image super-resolution and face restoration tasks across synthetic and real-world datasets. The code is available online."
    }
  ],
  "fineaction": [
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "Temporal action localization (TAL) is crucial for video understanding but is hindered by benchmarks that rely on coarse action classes, leading to model overfitting and ambiguous annotations. Addressing these issues, we introduce FineAction, a large-scale, fine-grained video dataset with 103K instances across 106 action categories in 17K videos. FineAction offers a diverse, densely annotated dataset with co-occurring actions, posing new challenges for TAL. We evaluated popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and proposed a baseline method achieving a 13.17% mAP. FineAction aims to propel TAL research forward and is accessible online."
    }
  ],
  "tal": [
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "Temporal action localization (TAL) is crucial for video understanding but is hindered by benchmarks that rely on coarse action classes, leading to model overfitting and ambiguous annotations. Addressing these issues, we introduce FineAction, a large-scale, fine-grained video dataset with 103K instances across 106 action categories in 17K videos. FineAction offers a diverse, densely annotated dataset with co-occurring actions, posing new challenges for TAL. We evaluated popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and proposed a baseline method achieving a 13.17% mAP. FineAction aims to propel TAL research forward and is accessible online."
    }
  ],
  "across": [
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "Temporal action localization (TAL) is crucial for video understanding but is hindered by benchmarks that rely on coarse action classes, leading to model overfitting and ambiguous annotations. Addressing these issues, we introduce FineAction, a large-scale, fine-grained video dataset with 103K instances across 106 action categories in 17K videos. FineAction offers a diverse, densely annotated dataset with co-occurring actions, posing new challenges for TAL. We evaluated popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and proposed a baseline method achieving a 13.17% mAP. FineAction aims to propel TAL research forward and is accessible online."
    }
  ],
  "annotate": [
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "Temporal action localization (TAL) is crucial for video understanding but is hindered by benchmarks that rely on coarse action classes, leading to model overfitting and ambiguous annotations. Addressing these issues, we introduce FineAction, a large-scale, fine-grained video dataset with 103K instances across 106 action categories in 17K videos. FineAction offers a diverse, densely annotated dataset with co-occurring actions, posing new challenges for TAL. We evaluated popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and proposed a baseline method achieving a 13.17% mAP. FineAction aims to propel TAL research forward and is accessible online."
    },
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "6D object pose estimation is crucial for robotic bin-picking and essential for many industrial applications. Annotating custom datasets for each bin-picking scenario is challenging. We introduce an iterative self-training framework for sim-to-real 6D object pose estimation, enhancing cost-effective robotic grasping. We create a photo-realistic simulator for synthesizing virtual data to train an initial pose estimation network. This network, acting as a teacher model, predicts poses on unlabeled real data. We develop an adaptive selection scheme to filter reliable predictions for updating a student model with pseudo labels for real data pose estimation. By iteratively refining the teacher model with the trained student model, we improve the quality of pseudo labels. Our method, tested on a public benchmark and a newly-released dataset, shows an 11.49% and 22.62% ADD(-S) improvement, respectively, and a 19.54% increase in robotic bin-picking success, demonstrating the effectiveness of iterative sim-to-real approaches. Project homepage: www.cse.cuhk.edu.hk/~kaichen/sim2real_pose.html"
    }
  ],
  "cooccur": [
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "Temporal action localization (TAL) is crucial for video understanding but is hindered by benchmarks that rely on coarse action classes, leading to model overfitting and ambiguous annotations. Addressing these issues, we introduce FineAction, a large-scale, fine-grained video dataset with 103K instances across 106 action categories in 17K videos. FineAction offers a diverse, densely annotated dataset with co-occurring actions, posing new challenges for TAL. We evaluated popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and proposed a baseline method achieving a 13.17% mAP. FineAction aims to propel TAL research forward and is accessible online."
    }
  ],
  "pose": [
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "Temporal action localization (TAL) is crucial for video understanding but is hindered by benchmarks that rely on coarse action classes, leading to model overfitting and ambiguous annotations. Addressing these issues, we introduce FineAction, a large-scale, fine-grained video dataset with 103K instances across 106 action categories in 17K videos. FineAction offers a diverse, densely annotated dataset with co-occurring actions, posing new challenges for TAL. We evaluated popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and proposed a baseline method achieving a 13.17% mAP. FineAction aims to propel TAL research forward and is accessible online."
    },
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "6D object pose estimation is crucial for robotic bin-picking and essential for many industrial applications. Annotating custom datasets for each bin-picking scenario is challenging. We introduce an iterative self-training framework for sim-to-real 6D object pose estimation, enhancing cost-effective robotic grasping. We create a photo-realistic simulator for synthesizing virtual data to train an initial pose estimation network. This network, acting as a teacher model, predicts poses on unlabeled real data. We develop an adaptive selection scheme to filter reliable predictions for updating a student model with pseudo labels for real data pose estimation. By iteratively refining the teacher model with the trained student model, we improve the quality of pseudo labels. Our method, tested on a public benchmark and a newly-released dataset, shows an 11.49% and 22.62% ADD(-S) improvement, respectively, and a 19.54% increase in robotic bin-picking success, demonstrating the effectiveness of iterative sim-to-real approaches. Project homepage: www.cse.cuhk.edu.hk/~kaichen/sim2real_pose.html"
    },
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "This paper explores the effectiveness of using synthetic data, generated by graphics simulators, for pre-training models in computer vision, addressing the challenges posed by using real-image datasets like high costs and ethical concerns. It highlights that the performance of models on downstream tasks varies with different simulation parameters, suggesting a tailored approach to synthetic data generation for optimal results. The introduction of Task2Sim, a model that maps downstream task requirements to optimal simulation parameters, allows for the generation of synthetic pre-training data customized for specific tasks. Task2Sim demonstrates significant improvements in model performance on a wide range of tasks without additional training for new tasks, showing potential to rival pre-training with real images from Imagenet."
    }
  ],
  "propel": [
    {
      "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
      "url": "/research/2022/10/20/FineAction-A-Fine-Grained-Video-Dataset-for-Temporal-Action-Localization.html",
      "date": "October 20, 2022",
      "summary": "Temporal action localization (TAL) is crucial for video understanding but is hindered by benchmarks that rely on coarse action classes, leading to model overfitting and ambiguous annotations. Addressing these issues, we introduce FineAction, a large-scale, fine-grained video dataset with 103K instances across 106 action categories in 17K videos. FineAction offers a diverse, densely annotated dataset with co-occurring actions, posing new challenges for TAL. We evaluated popular localization methods on FineAction, revealing the impact of fine-grained instances on performance, and proposed a baseline method achieving a 13.17% mAP. FineAction aims to propel TAL research forward and is accessible online."
    }
  ],
  "fixmatch": [
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "Semi-supervised learning (SSL) utilizes unlabeled data to boost model performance but often at the expense of increased complexity. This paper introduces FixMatch, a streamlined SSL algorithm that simplifies existing methods. FixMatch uses a model's predictions on weakly-augmented unlabeled images to generate pseudo-labels, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. Remarkably, FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness. The code is publicly available."
    }
  ],
  "confidence": [
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "Semi-supervised learning (SSL) utilizes unlabeled data to boost model performance but often at the expense of increased complexity. This paper introduces FixMatch, a streamlined SSL algorithm that simplifies existing methods. FixMatch uses a model's predictions on weakly-augmented unlabeled images to generate pseudo-labels, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. Remarkably, FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness. The code is publicly available."
    }
  ],
  "utilizes": [
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "url": "/research/2020/01/21/FixMatch-Simplifying-Semi-Supervised-Learning-with-Consistency-and-Confidence.html",
      "date": "January 21, 2020",
      "summary": "Semi-supervised learning (SSL) utilizes unlabeled data to boost model performance but often at the expense of increased complexity. This paper introduces FixMatch, a streamlined SSL algorithm that simplifies existing methods. FixMatch uses a model's predictions on weakly-augmented unlabeled images to generate pseudo-labels, which are only used if they are highly confident. The model then learns from these pseudo-labels using strongly-augmented versions of the images. Remarkably, FixMatch demonstrates superior performance on several SSL benchmarks, achieving 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with only 40 labels. An in-depth ablation study highlights the key factors behind FixMatch's effectiveness. The code is publicly available."
    }
  ],
  "approximator": [
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "We introduce a Fourier neural network (FNN) aligned with Fourier decomposition, utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design ensures easy integration with more complex networks for data processing tasks. The FNN's efficacy is demonstrated on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits include solution validity beyond the training scope, model interpretability, and ease of use."
    }
  ],
  "equation": [
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "We introduce a Fourier neural network (FNN) aligned with Fourier decomposition, utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design ensures easy integration with more complex networks for data processing tasks. The FNN's efficacy is demonstrated on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits include solution validity beyond the training scope, model interpretability, and ease of use."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "Neural networks parameterized for continuous, differentiable signal representations provide a promising paradigm but struggle with modeling fine detail and representing spatial and temporal derivatives critical for signals defined by partial differential equations. We introduce sinusoidal representation networks (SIRENs), utilizing periodic activation functions to effectively capture complex natural signals and their derivatives. Our analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. We also showcase SIRENs' application in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations, and extend their use with hypernetworks to learn priors for SIREN functions. A video overview and full applications are available on our project website."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "solver": [
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "We introduce a Fourier neural network (FNN) aligned with Fourier decomposition, utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design ensures easy integration with more complex networks for data processing tasks. The FNN's efficacy is demonstrated on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits include solution validity beyond the training scope, model interpretability, and ease of use."
    }
  ],
  "fnn": [
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "We introduce a Fourier neural network (FNN) aligned with Fourier decomposition, utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design ensures easy integration with more complex networks for data processing tasks. The FNN's efficacy is demonstrated on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits include solution validity beyond the training scope, model interpretability, and ease of use."
    }
  ],
  "decomposition": [
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "We introduce a Fourier neural network (FNN) aligned with Fourier decomposition, utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design ensures easy integration with more complex networks for data processing tasks. The FNN's efficacy is demonstrated on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits include solution validity beyond the training scope, model interpretability, and ease of use."
    }
  ],
  "mimic": [
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "We introduce a Fourier neural network (FNN) aligned with Fourier decomposition, utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design ensures easy integration with more complex networks for data processing tasks. The FNN's efficacy is demonstrated on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits include solution validity beyond the training scope, model interpretability, and ease of use."
    },
    {
      "title": "Self-Paced Curriculum Learning",
      "url": "/research/2015/01/01/Self-Paced-Curriculum-Learning.html",
      "date": "January 1, 2015",
      "summary": "Curriculum learning (CL) and self-paced learning (SPL) are methods inspired by the incremental complexity of human and animal learning, with CL using a fixed curriculum based on prior knowledge and SPL adjusting dynamically to the learner's pace but struggling with prior knowledge and overfitting. This paper introduces a unified framework called self-paced curriculum learning (SPCL), blending both methods' strengths by considering prior knowledge and ongoing learning progress through a concise optimization problem. SPCL mimics a collaborative instructor-student learning mode, showing empirical advantages in two tasks."
    }
  ],
  "efficacy": [
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "We introduce a Fourier neural network (FNN) aligned with Fourier decomposition, utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design ensures easy integration with more complex networks for data processing tasks. The FNN's efficacy is demonstrated on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits include solution validity beyond the training scope, model interpretability, and ease of use."
    },
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    }
  ],
  "piecewise": [
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "We introduce a Fourier neural network (FNN) aligned with Fourier decomposition, utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design ensures easy integration with more complex networks for data processing tasks. The FNN's efficacy is demonstrated on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits include solution validity beyond the training scope, model interpretability, and ease of use."
    }
  ],
  "validity": [
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "We introduce a Fourier neural network (FNN) aligned with Fourier decomposition, utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design ensures easy integration with more complex networks for data processing tasks. The FNN's efficacy is demonstrated on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits include solution validity beyond the training scope, model interpretability, and ease of use."
    }
  ],
  "scope": [
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "We introduce a Fourier neural network (FNN) aligned with Fourier decomposition, utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design ensures easy integration with more complex networks for data processing tasks. The FNN's efficacy is demonstrated on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits include solution validity beyond the training scope, model interpretability, and ease of use."
    }
  ],
  "interpretability": [
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "We introduce a Fourier neural network (FNN) aligned with Fourier decomposition, utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design ensures easy integration with more complex networks for data processing tasks. The FNN's efficacy is demonstrated on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits include solution validity beyond the training scope, model interpretability, and ease of use."
    }
  ],
  "ease": [
    {
      "title": "Fourier neural networks as function approximators and differential equation solvers",
      "url": "/research/2020/07/09/Fourier-neural-networks-as-function-approximators-and-differential-equation-solvers.html",
      "date": "July 9, 2020",
      "summary": "We introduce a Fourier neural network (FNN) aligned with Fourier decomposition, utilizing specific activation and loss functions to accurately mimic Fourier series expansion within a simple, single-layer architecture. This design ensures easy integration with more complex networks for data processing tasks. The FNN's efficacy is demonstrated on both smooth and piecewise continuous periodic functions and its application in modeling or solving partial differential equations with periodic boundary conditions. Key benefits include solution validity beyond the training scope, model interpretability, and ease of use."
    }
  ],
  "units": [
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "We introduce the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. Our empirical evaluation across computer vision, natural language processing, and speech tasks shows that GELU offers performance improvements over ReLU and ELU activations."
    },
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "Gated Linear Units (GLUs) involve the component-wise product of two linear projections, with one undergoing a sigmoid function. Exploring GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers, we find certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    }
  ],
  "gelu": [
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "We introduce the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. Our empirical evaluation across computer vision, natural language processing, and speech tasks shows that GELU offers performance improvements over ReLU and ELU activations."
    },
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "Gated Linear Units (GLUs) involve the component-wise product of two linear projections, with one undergoing a sigmoid function. Exploring GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers, we find certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    }
  ],
  "magnitude": [
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "We introduce the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. Our empirical evaluation across computer vision, natural language processing, and speech tasks shows that GELU offers performance improvements over ReLU and ELU activations."
    }
  ],
  "gate": [
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "We introduce the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. Our empirical evaluation across computer vision, natural language processing, and speech tasks shows that GELU offers performance improvements over ReLU and ELU activations."
    },
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "Gated Linear Units (GLUs) involve the component-wise product of two linear projections, with one undergoing a sigmoid function. Exploring GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers, we find certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    }
  ],
  "elu": [
    {
      "title": "Gaussian Error Linear Units (Gelus)",
      "url": "/research/1000/01/01/Gaussian-Error-Linear-Units-(Gelus).html",
      "date": "January 1, 1000",
      "summary": "We introduce the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. Our empirical evaluation across computer vision, natural language processing, and speech tasks shows that GELU offers performance improvements over ReLU and ELU activations."
    }
  ],
  "glu": [
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "Gated Linear Units (GLUs) involve the component-wise product of two linear projections, with one undergoing a sigmoid function. Exploring GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers, we find certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    }
  ],
  "product": [
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "Gated Linear Units (GLUs) involve the component-wise product of two linear projections, with one undergoing a sigmoid function. Exploring GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers, we find certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    }
  ],
  "undergo": [
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "Gated Linear Units (GLUs) involve the component-wise product of two linear projections, with one undergoing a sigmoid function. Exploring GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers, we find certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    }
  ],
  "sublayer": [
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "Gated Linear Units (GLUs) involve the component-wise product of two linear projections, with one undergoing a sigmoid function. Exploring GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers, we find certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    }
  ],
  "term": [
    {
      "title": "GLU Variants Improve Transformer",
      "url": "/research/2020/02/12/GLU-Variants-Improve-Transformer.html",
      "date": "February 12, 2020",
      "summary": "Gated Linear Units (GLUs) involve the component-wise product of two linear projections, with one undergoing a sigmoid function. Exploring GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers, we find certain variants outperform the conventional ReLU or GELU activations in terms of quality."
    },
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article offers the first extensive review of vision transformer techniques specifically applied to human action recognition, a field gaining interest for its wide applications. Termed action transformers, these methods are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also delves into optimizing spatio-temporal attention in transformers for longer sequences and examines different learning strategies, including self-supervised and zero-shot learning, with their respective loss functions. Additionally, the survey assesses progress in benchmark evaluation scores and discusses the challenges and future directions in this research area."
    }
  ],
  "grokking": [
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper investigates how neural networks generalize on small, algorithmically generated datasets, focusing on data efficiency, memorization, generalization, and learning speed. It highlights instances where neural networks grok patterns, significantly improving their generalization ability from mere chance to perfect accuracy, even beyond the point of overfitting. Additionally, the study finds that generalization on smaller datasets demands more optimization. It posits that such datasets are ideal for exploring the enigmatic phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    }
  ],
  "memorization": [
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper investigates how neural networks generalize on small, algorithmically generated datasets, focusing on data efficiency, memorization, generalization, and learning speed. It highlights instances where neural networks grok patterns, significantly improving their generalization ability from mere chance to perfect accuracy, even beyond the point of overfitting. Additionally, the study finds that generalization on smaller datasets demands more optimization. It posits that such datasets are ideal for exploring the enigmatic phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    }
  ],
  "chance": [
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper investigates how neural networks generalize on small, algorithmically generated datasets, focusing on data efficiency, memorization, generalization, and learning speed. It highlights instances where neural networks grok patterns, significantly improving their generalization ability from mere chance to perfect accuracy, even beyond the point of overfitting. Additionally, the study finds that generalization on smaller datasets demands more optimization. It posits that such datasets are ideal for exploring the enigmatic phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    }
  ],
  "perfect": [
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper investigates how neural networks generalize on small, algorithmically generated datasets, focusing on data efficiency, memorization, generalization, and learning speed. It highlights instances where neural networks grok patterns, significantly improving their generalization ability from mere chance to perfect accuracy, even beyond the point of overfitting. Additionally, the study finds that generalization on smaller datasets demands more optimization. It posits that such datasets are ideal for exploring the enigmatic phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    }
  ],
  "posit": [
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper investigates how neural networks generalize on small, algorithmically generated datasets, focusing on data efficiency, memorization, generalization, and learning speed. It highlights instances where neural networks grok patterns, significantly improving their generalization ability from mere chance to perfect accuracy, even beyond the point of overfitting. Additionally, the study finds that generalization on smaller datasets demands more optimization. It posits that such datasets are ideal for exploring the enigmatic phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    }
  ],
  "overparametrize": [
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper investigates how neural networks generalize on small, algorithmically generated datasets, focusing on data efficiency, memorization, generalization, and learning speed. It highlights instances where neural networks grok patterns, significantly improving their generalization ability from mere chance to perfect accuracy, even beyond the point of overfitting. Additionally, the study finds that generalization on smaller datasets demands more optimization. It posits that such datasets are ideal for exploring the enigmatic phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    }
  ],
  "memorize": [
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "url": "/research/2022/01/06/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets.html",
      "date": "January 6, 2022",
      "summary": "This paper investigates how neural networks generalize on small, algorithmically generated datasets, focusing on data efficiency, memorization, generalization, and learning speed. It highlights instances where neural networks grok patterns, significantly improving their generalization ability from mere chance to perfect accuracy, even beyond the point of overfitting. Additionally, the study finds that generalization on smaller datasets demands more optimization. It posits that such datasets are ideal for exploring the enigmatic phenomenon of how overparametrized neural networks generalize beyond simply memorizing their training data."
    }
  ],
  "simplicity": [
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "The Vision Transformer (ViT) offers simplicity, robustness, and superior performance in vision tasks but struggles with small-scale datasets due to its lack of inherent inductive biases and dependence on large-scale pre-training datasets like ImageNet and JFT. This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of ViTs without large-scale pre-training or modifications to the architecture or loss functions. Experiments show improved performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, maintaining ViT's attention to relevant regions and robustness. The code and models are available online."
    }
  ],
  "dependence": [
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "The Vision Transformer (ViT) offers simplicity, robustness, and superior performance in vision tasks but struggles with small-scale datasets due to its lack of inherent inductive biases and dependence on large-scale pre-training datasets like ImageNet and JFT. This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of ViTs without large-scale pre-training or modifications to the architecture or loss functions. Experiments show improved performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, maintaining ViT's attention to relevant regions and robustness. The code and models are available online."
    }
  ],
  "cifar10100": [
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "The Vision Transformer (ViT) offers simplicity, robustness, and superior performance in vision tasks but struggles with small-scale datasets due to its lack of inherent inductive biases and dependence on large-scale pre-training datasets like ImageNet and JFT. This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of ViTs without large-scale pre-training or modifications to the architecture or loss functions. Experiments show improved performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, maintaining ViT's attention to relevant regions and robustness. The code and models are available online."
    }
  ],
  "cinic10": [
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "The Vision Transformer (ViT) offers simplicity, robustness, and superior performance in vision tasks but struggles with small-scale datasets due to its lack of inherent inductive biases and dependence on large-scale pre-training datasets like ImageNet and JFT. This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of ViTs without large-scale pre-training or modifications to the architecture or loss functions. Experiments show improved performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, maintaining ViT's attention to relevant regions and robustness. The code and models are available online."
    }
  ],
  "svhn": [
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "The Vision Transformer (ViT) offers simplicity, robustness, and superior performance in vision tasks but struggles with small-scale datasets due to its lack of inherent inductive biases and dependence on large-scale pre-training datasets like ImageNet and JFT. This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of ViTs without large-scale pre-training or modifications to the architecture or loss functions. Experiments show improved performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, maintaining ViT's attention to relevant regions and robustness. The code and models are available online."
    }
  ],
  "aircraft": [
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "The Vision Transformer (ViT) offers simplicity, robustness, and superior performance in vision tasks but struggles with small-scale datasets due to its lack of inherent inductive biases and dependence on large-scale pre-training datasets like ImageNet and JFT. This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of ViTs without large-scale pre-training or modifications to the architecture or loss functions. Experiments show improved performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, maintaining ViT's attention to relevant regions and robustness. The code and models are available online."
    }
  ],
  "car": [
    {
      "title": "How to Train Vision Transformer on Small-scale Datasets",
      "url": "/research/2022/10/13/How-to-Train-Vision-Transformer-on-Small-scale-Datasets.html",
      "date": "October 13, 2022",
      "summary": "The Vision Transformer (ViT) offers simplicity, robustness, and superior performance in vision tasks but struggles with small-scale datasets due to its lack of inherent inductive biases and dependence on large-scale pre-training datasets like ImageNet and JFT. This study demonstrates that self-supervised learning can introduce effective inductive biases directly from small datasets, enabling the fine-tuning of ViTs without large-scale pre-training or modifications to the architecture or loss functions. Experiments show improved performance on small datasets such as CIFAR10/100, CINIC10, SVHN, Tiny-ImageNet, Aircraft, and Cars, maintaining ViT's attention to relevant regions and robustness. The code and models are available online."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "We introduce a system that trains deep neural networks for object detection using synthetic images, leveraging domain randomization to manage real-world data variability. This involves randomizing simulator parameters—like lighting and object textures—in non-realistic ways, helping the network to identify essential object features. Our findings reveal that networks can achieve impressive performance with just synthetic data and further improve with real data fine-tuning. This suggests the potential of using cost-effective synthetic data for training, circumventing the challenges of acquiring vast amounts of real-world data or creating detailed synthetic environments. We validate our method through car bounding box detection on the KITTI dataset."
    }
  ],
  "think": [
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "Recent studies have shown that Convolutional Neural Networks (CNNs), which were thought to recognize objects by learning complex shapes, are actually more biased towards recognizing textures. This finding, based on a series of tests comparing CNNs with human observers using images with conflicting texture and shape cues, contradicts the traditional understanding of how these networks process images. We discovered that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, when we trained a standard CNN architecture (ResNet-50) on a stylized version of ImageNet, designed to emphasize shape, the network's performance aligned more closely with human behavior. This training not only matched human performance in controlled experiments (involving 97 observers across 48,560 trials) but also enhanced object detection and robustness against image distortions, underscoring the benefits of shape-based representations in visual recognition systems."
    }
  ],
  "sery": [
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "Recent studies have shown that Convolutional Neural Networks (CNNs), which were thought to recognize objects by learning complex shapes, are actually more biased towards recognizing textures. This finding, based on a series of tests comparing CNNs with human observers using images with conflicting texture and shape cues, contradicts the traditional understanding of how these networks process images. We discovered that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, when we trained a standard CNN architecture (ResNet-50) on a stylized version of ImageNet, designed to emphasize shape, the network's performance aligned more closely with human behavior. This training not only matched human performance in controlled experiments (involving 97 observers across 48,560 trials) but also enhanced object detection and robustness against image distortions, underscoring the benefits of shape-based representations in visual recognition systems."
    }
  ],
  "observer": [
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "Recent studies have shown that Convolutional Neural Networks (CNNs), which were thought to recognize objects by learning complex shapes, are actually more biased towards recognizing textures. This finding, based on a series of tests comparing CNNs with human observers using images with conflicting texture and shape cues, contradicts the traditional understanding of how these networks process images. We discovered that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, when we trained a standard CNN architecture (ResNet-50) on a stylized version of ImageNet, designed to emphasize shape, the network's performance aligned more closely with human behavior. This training not only matched human performance in controlled experiments (involving 97 observers across 48,560 trials) but also enhanced object detection and robustness against image distortions, underscoring the benefits of shape-based representations in visual recognition systems."
    }
  ],
  "cue": [
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "Recent studies have shown that Convolutional Neural Networks (CNNs), which were thought to recognize objects by learning complex shapes, are actually more biased towards recognizing textures. This finding, based on a series of tests comparing CNNs with human observers using images with conflicting texture and shape cues, contradicts the traditional understanding of how these networks process images. We discovered that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, when we trained a standard CNN architecture (ResNet-50) on a stylized version of ImageNet, designed to emphasize shape, the network's performance aligned more closely with human behavior. This training not only matched human performance in controlled experiments (involving 97 observers across 48,560 trials) but also enhanced object detection and robustness against image distortions, underscoring the benefits of shape-based representations in visual recognition systems."
    }
  ],
  "contradict": [
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "Recent studies have shown that Convolutional Neural Networks (CNNs), which were thought to recognize objects by learning complex shapes, are actually more biased towards recognizing textures. This finding, based on a series of tests comparing CNNs with human observers using images with conflicting texture and shape cues, contradicts the traditional understanding of how these networks process images. We discovered that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, when we trained a standard CNN architecture (ResNet-50) on a stylized version of ImageNet, designed to emphasize shape, the network's performance aligned more closely with human behavior. This training not only matched human performance in controlled experiments (involving 97 observers across 48,560 trials) but also enhanced object detection and robustness against image distortions, underscoring the benefits of shape-based representations in visual recognition systems."
    }
  ],
  "favor": [
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "Recent studies have shown that Convolutional Neural Networks (CNNs), which were thought to recognize objects by learning complex shapes, are actually more biased towards recognizing textures. This finding, based on a series of tests comparing CNNs with human observers using images with conflicting texture and shape cues, contradicts the traditional understanding of how these networks process images. We discovered that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, when we trained a standard CNN architecture (ResNet-50) on a stylized version of ImageNet, designed to emphasize shape, the network's performance aligned more closely with human behavior. This training not only matched human performance in controlled experiments (involving 97 observers across 48,560 trials) but also enhanced object detection and robustness against image distortions, underscoring the benefits of shape-based representations in visual recognition systems."
    }
  ],
  "stylize": [
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "Recent studies have shown that Convolutional Neural Networks (CNNs), which were thought to recognize objects by learning complex shapes, are actually more biased towards recognizing textures. This finding, based on a series of tests comparing CNNs with human observers using images with conflicting texture and shape cues, contradicts the traditional understanding of how these networks process images. We discovered that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, when we trained a standard CNN architecture (ResNet-50) on a stylized version of ImageNet, designed to emphasize shape, the network's performance aligned more closely with human behavior. This training not only matched human performance in controlled experiments (involving 97 observers across 48,560 trials) but also enhanced object detection and robustness against image distortions, underscoring the benefits of shape-based representations in visual recognition systems."
    }
  ],
  "underscore": [
    {
      "title": "Imagenet-trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness",
      "url": "/research/1000/01/01/Imagenet-trained-Cnns-Are-Biased-Towards-Texture;-Increasing-Shape-Bias-Improves-Accuracy-and-Robustness.html",
      "date": "January 1, 1000",
      "summary": "Recent studies have shown that Convolutional Neural Networks (CNNs), which were thought to recognize objects by learning complex shapes, are actually more biased towards recognizing textures. This finding, based on a series of tests comparing CNNs with human observers using images with conflicting texture and shape cues, contradicts the traditional understanding of how these networks process images. We discovered that CNNs trained on ImageNet favor texture over shape, differing significantly from human visual processing. However, when we trained a standard CNN architecture (ResNet-50) on a stylized version of ImageNet, designed to emphasize shape, the network's performance aligned more closely with human behavior. This training not only matched human performance in controlled experiments (involving 97 observers across 48,560 trials) but also enhanced object detection and robustness against image distortions, underscoring the benefits of shape-based representations in visual recognition systems."
    }
  ],
  "derivative": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "Neural networks parameterized for continuous, differentiable signal representations provide a promising paradigm but struggle with modeling fine detail and representing spatial and temporal derivatives critical for signals defined by partial differential equations. We introduce sinusoidal representation networks (SIRENs), utilizing periodic activation functions to effectively capture complex natural signals and their derivatives. Our analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. We also showcase SIRENs' application in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations, and extend their use with hypernetworks to learn priors for SIREN functions. A video overview and full applications are available on our project website."
    },
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions for neural networks in reinforcement learning, the sigmoid-weighted linear unit (SiLU) and its derivative (dSiLU), and challenges the need for experience replay and separate target networks in deep reinforcement learning. By employing on-policy learning with eligibility traces and softmax action selection, the study achieves state-of-the-art results in stochastic SZ-Tetris and a small-board Tetris using TD(λ) learning and shallow dSiLU network agents. Furthermore, it outperforms the DQN algorithm in the Atari 2600 domain with a deep Sarsa(λ) agent utilizing SiLU and dSiLU hidden units, suggesting a competitive alternative to traditional DQN approaches."
    },
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "We introduce an algorithm designed to identify networks that are both simple and highly generalizable by searching for extensive areas of flat minima in the error function, where the error rate is relatively stable. These flat minima are associated with lower risks of overfitting according to MDL principles. Despite needing to calculate second-order derivatives, the algorithm maintains a complexity level comparable to backpropagation. We tested it on both feedforward and recurrent networks, and in stock market prediction tasks, it performed better than traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    }
  ],
  "define": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "Neural networks parameterized for continuous, differentiable signal representations provide a promising paradigm but struggle with modeling fine detail and representing spatial and temporal derivatives critical for signals defined by partial differential equations. We introduce sinusoidal representation networks (SIRENs), utilizing periodic activation functions to effectively capture complex natural signals and their derivatives. Our analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. We also showcase SIRENs' application in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations, and extend their use with hypernetworks to learn priors for SIREN functions. A video overview and full applications are available on our project website."
    }
  ],
  "sirens": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "Neural networks parameterized for continuous, differentiable signal representations provide a promising paradigm but struggle with modeling fine detail and representing spatial and temporal derivatives critical for signals defined by partial differential equations. We introduce sinusoidal representation networks (SIRENs), utilizing periodic activation functions to effectively capture complex natural signals and their derivatives. Our analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. We also showcase SIRENs' application in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations, and extend their use with hypernetworks to learn priors for SIREN functions. A video overview and full applications are available on our project website."
    }
  ],
  "wavefield": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "Neural networks parameterized for continuous, differentiable signal representations provide a promising paradigm but struggle with modeling fine detail and representing spatial and temporal derivatives critical for signals defined by partial differential equations. We introduce sinusoidal representation networks (SIRENs), utilizing periodic activation functions to effectively capture complex natural signals and their derivatives. Our analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. We also showcase SIRENs' application in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations, and extend their use with hypernetworks to learn priors for SIREN functions. A video overview and full applications are available on our project website."
    }
  ],
  "sound": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "Neural networks parameterized for continuous, differentiable signal representations provide a promising paradigm but struggle with modeling fine detail and representing spatial and temporal derivatives critical for signals defined by partial differential equations. We introduce sinusoidal representation networks (SIRENs), utilizing periodic activation functions to effectively capture complex natural signals and their derivatives. Our analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. We also showcase SIRENs' application in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations, and extend their use with hypernetworks to learn priors for SIREN functions. A video overview and full applications are available on our project website."
    }
  ],
  "poisson": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "Neural networks parameterized for continuous, differentiable signal representations provide a promising paradigm but struggle with modeling fine detail and representing spatial and temporal derivatives critical for signals defined by partial differential equations. We introduce sinusoidal representation networks (SIRENs), utilizing periodic activation functions to effectively capture complex natural signals and their derivatives. Our analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. We also showcase SIRENs' application in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations, and extend their use with hypernetworks to learn priors for SIREN functions. A video overview and full applications are available on our project website."
    }
  ],
  "helmholtz": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "Neural networks parameterized for continuous, differentiable signal representations provide a promising paradigm but struggle with modeling fine detail and representing spatial and temporal derivatives critical for signals defined by partial differential equations. We introduce sinusoidal representation networks (SIRENs), utilizing periodic activation functions to effectively capture complex natural signals and their derivatives. Our analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. We also showcase SIRENs' application in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations, and extend their use with hypernetworks to learn priors for SIREN functions. A video overview and full applications are available on our project website."
    }
  ],
  "wave": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "Neural networks parameterized for continuous, differentiable signal representations provide a promising paradigm but struggle with modeling fine detail and representing spatial and temporal derivatives critical for signals defined by partial differential equations. We introduce sinusoidal representation networks (SIRENs), utilizing periodic activation functions to effectively capture complex natural signals and their derivatives. Our analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. We also showcase SIRENs' application in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations, and extend their use with hypernetworks to learn priors for SIREN functions. A video overview and full applications are available on our project website."
    }
  ],
  "hypernetwork": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "Neural networks parameterized for continuous, differentiable signal representations provide a promising paradigm but struggle with modeling fine detail and representing spatial and temporal derivatives critical for signals defined by partial differential equations. We introduce sinusoidal representation networks (SIRENs), utilizing periodic activation functions to effectively capture complex natural signals and their derivatives. Our analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. We also showcase SIRENs' application in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations, and extend their use with hypernetworks to learn priors for SIREN functions. A video overview and full applications are available on our project website."
    }
  ],
  "prior": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "Neural networks parameterized for continuous, differentiable signal representations provide a promising paradigm but struggle with modeling fine detail and representing spatial and temporal derivatives critical for signals defined by partial differential equations. We introduce sinusoidal representation networks (SIRENs), utilizing periodic activation functions to effectively capture complex natural signals and their derivatives. Our analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. We also showcase SIRENs' application in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations, and extend their use with hypernetworks to learn priors for SIREN functions. A video overview and full applications are available on our project website."
    }
  ],
  "siren": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "Neural networks parameterized for continuous, differentiable signal representations provide a promising paradigm but struggle with modeling fine detail and representing spatial and temporal derivatives critical for signals defined by partial differential equations. We introduce sinusoidal representation networks (SIRENs), utilizing periodic activation functions to effectively capture complex natural signals and their derivatives. Our analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. We also showcase SIRENs' application in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations, and extend their use with hypernetworks to learn priors for SIREN functions. A video overview and full applications are available on our project website."
    }
  ],
  "overview": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "Neural networks parameterized for continuous, differentiable signal representations provide a promising paradigm but struggle with modeling fine detail and representing spatial and temporal derivatives critical for signals defined by partial differential equations. We introduce sinusoidal representation networks (SIRENs), utilizing periodic activation functions to effectively capture complex natural signals and their derivatives. Our analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. We also showcase SIRENs' application in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations, and extend their use with hypernetworks to learn priors for SIREN functions. A video overview and full applications are available on our project website."
    }
  ],
  "website": [
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions",
      "url": "/research/1000/01/01/Implicit-Neural-Representations-with-Periodic-Activation-Functions.html",
      "date": "January 1, 1000",
      "summary": "Neural networks parameterized for continuous, differentiable signal representations provide a promising paradigm but struggle with modeling fine detail and representing spatial and temporal derivatives critical for signals defined by partial differential equations. We introduce sinusoidal representation networks (SIRENs), utilizing periodic activation functions to effectively capture complex natural signals and their derivatives. Our analysis leads to a principled initialization strategy, enabling the representation of images, wavefields, video, sound, and derivatives. We also showcase SIRENs' application in solving boundary value problems like Eikonal equations, the Poisson equation, and the Helmholtz and wave equations, and extend their use with hypernetworks to learn priors for SIREN functions. A video overview and full applications are available on our project website."
    }
  ],
  "child": [
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "Image-caption pretraining, crucial for tasks like zero-shot image classification and object detection, faces challenges in aligning multiple concepts from captions to objects in images. We introduce a curriculum learning framework inspired by children's language learning from cognitive science, starting with simple image-caption pairs and gradually increasing complexity by adding more concepts. This method, which leverages knowledge from each phase for subsequent learning, outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios. The code is available at: https://github.com/hayyubi/cur_vl.git."
    }
  ],
  "children": [
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "Image-caption pretraining, crucial for tasks like zero-shot image classification and object detection, faces challenges in aligning multiple concepts from captions to objects in images. We introduce a curriculum learning framework inspired by children's language learning from cognitive science, starting with simple image-caption pairs and gradually increasing complexity by adding more concepts. This method, which leverages knowledge from each phase for subsequent learning, outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios. The code is available at: https://github.com/hayyubi/cur_vl.git."
    }
  ],
  "leverages": [
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "Image-caption pretraining, crucial for tasks like zero-shot image classification and object detection, faces challenges in aligning multiple concepts from captions to objects in images. We introduce a curriculum learning framework inspired by children's language learning from cognitive science, starting with simple image-caption pairs and gradually increasing complexity by adding more concepts. This method, which leverages knowledge from each phase for subsequent learning, outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios. The code is available at: https://github.com/hayyubi/cur_vl.git."
    }
  ],
  "httpsgithubcomhayyubicurvlgit": [
    {
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "url": "/research/2023/05/30/Learning-from-Children-Improving-Image-Caption-Pretraining-via-Curriculum.html",
      "date": "May 30, 2023",
      "summary": "Image-caption pretraining, crucial for tasks like zero-shot image classification and object detection, faces challenges in aligning multiple concepts from captions to objects in images. We introduce a curriculum learning framework inspired by children's language learning from cognitive science, starting with simple image-caption pairs and gradually increasing complexity by adding more concepts. This method, which leverages knowledge from each phase for subsequent learning, outperforms traditional image-caption training across various settings, including starting from scratch, using pretrained encoders, and in low data scenarios. The code is available at: https://github.com/hayyubi/cur_vl.git."
    }
  ],
  "lesion": [
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "Self-paced curriculum learning (SCL) has been effective in various fields, employing easy-to-hard sampling based on data difficulty. However, in medical image analysis tasks like universal lesion detection, it faces challenges due to inaccurate difficulty estimation and the under-utilization of hard samples. This paper introduces a novel approach, mixed-order self-paced curriculum learning (Mo-SCL), which combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches to address these issues. Through theoretical analysis and experiments on the DeepLesion dataset, Mo-SCL is shown to enhance lesion detection accuracy in state-of-the-art methods without additional network modifications."
    }
  ],
  "scl": [
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "Self-paced curriculum learning (SCL) has been effective in various fields, employing easy-to-hard sampling based on data difficulty. However, in medical image analysis tasks like universal lesion detection, it faces challenges due to inaccurate difficulty estimation and the under-utilization of hard samples. This paper introduces a novel approach, mixed-order self-paced curriculum learning (Mo-SCL), which combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches to address these issues. Through theoretical analysis and experiments on the DeepLesion dataset, Mo-SCL is shown to enhance lesion detection accuracy in state-of-the-art methods without additional network modifications."
    }
  ],
  "moscl": [
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "Self-paced curriculum learning (SCL) has been effective in various fields, employing easy-to-hard sampling based on data difficulty. However, in medical image analysis tasks like universal lesion detection, it faces challenges due to inaccurate difficulty estimation and the under-utilization of hard samples. This paper introduces a novel approach, mixed-order self-paced curriculum learning (Mo-SCL), which combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches to address these issues. Through theoretical analysis and experiments on the DeepLesion dataset, Mo-SCL is shown to enhance lesion detection accuracy in state-of-the-art methods without additional network modifications."
    }
  ],
  "uncertainty": [
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "Self-paced curriculum learning (SCL) has been effective in various fields, employing easy-to-hard sampling based on data difficulty. However, in medical image analysis tasks like universal lesion detection, it faces challenges due to inaccurate difficulty estimation and the under-utilization of hard samples. This paper introduces a novel approach, mixed-order self-paced curriculum learning (Mo-SCL), which combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches to address these issues. Through theoretical analysis and experiments on the DeepLesion dataset, Mo-SCL is shown to enhance lesion detection accuracy in state-of-the-art methods without additional network modifications."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-5.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-6.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    }
  ],
  "deeplesion": [
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "Self-paced curriculum learning (SCL) has been effective in various fields, employing easy-to-hard sampling based on data difficulty. However, in medical image analysis tasks like universal lesion detection, it faces challenges due to inaccurate difficulty estimation and the under-utilization of hard samples. This paper introduces a novel approach, mixed-order self-paced curriculum learning (Mo-SCL), which combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches to address these issues. Through theoretical analysis and experiments on the DeepLesion dataset, Mo-SCL is shown to enhance lesion detection accuracy in state-of-the-art methods without additional network modifications."
    }
  ],
  "mo": [
    {
      "title": "Mixed-order self-paced curriculum learning for universal lesion detection",
      "url": "/research/2023/02/09/Mixed-order-self-paced-curriculum-learning-for-universal-lesion-detection.html",
      "date": "February 9, 2023",
      "summary": "Self-paced curriculum learning (SCL) has been effective in various fields, employing easy-to-hard sampling based on data difficulty. However, in medical image analysis tasks like universal lesion detection, it faces challenges due to inaccurate difficulty estimation and the under-utilization of hard samples. This paper introduces a novel approach, mixed-order self-paced curriculum learning (Mo-SCL), which combines uncertainty and loss for better difficulty estimation and incorporates both hard and easy samples in training batches to address these issues. Through theoretical analysis and experiments on the DeepLesion dataset, Mo-SCL is shown to enhance lesion detection accuracy in state-of-the-art methods without additional network modifications."
    }
  ],
  "tangent": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "ann": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "anns": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "ntk": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "govern": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "evolve": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "follow": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    }
  ],
  "constant": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "definiteness": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "limiting": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "squares": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "regression": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "stopping": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "url": "/research/2020/02/10/Neural-Tangent-Kernel-Convergence-and-Generalization-in-Neural-Networks.html",
      "date": "February 10, 2020",
      "summary": "Artificial neural networks (ANNs) are shown to be equivalent to Gaussian processes at initialization in the infinite-width limit, connecting them to kernel methods. It's demonstrated that ANNs' behavior during training can be described using a new kernel, the Neural Tangent Kernel (NTK), which governs how the network's function evolves by following the kernel gradient of a convex functional cost during gradient descent. The NTK, initially random, stabilizes to a constant in the infinite-width limit, allowing the study of ANNs in function space. Positive-definiteness of the limiting NTK, necessary for training convergence, is proven under certain conditions. For least-squares regression, it's shown that the network function follows a linear differential equation during training, with convergence speed influenced by the NTK's principal components, supporting early stopping. Numerical studies on the NTK in wide networks confirm these theoretical findings."
    }
  ],
  "architectur": [
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is crucial for natural language processing tasks, but its training requires a learning rate warm-up stage, essential for performance yet slowing optimization and increasing hyper-parameter tuning needs. This paper investigates the importance of the warm-up stage and the impact of layer normalization placement. It uses mean field theory to demonstrate that the original Post-LN Transformer's design, with layer normalization between residual blocks, results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    }
  ],
  "warm": [
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is crucial for natural language processing tasks, but its training requires a learning rate warm-up stage, essential for performance yet slowing optimization and increasing hyper-parameter tuning needs. This paper investigates the importance of the warm-up stage and the impact of layer normalization placement. It uses mean field theory to demonstrate that the original Post-LN Transformer's design, with layer normalization between residual blocks, results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    }
  ],
  "up": [
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is crucial for natural language processing tasks, but its training requires a learning rate warm-up stage, essential for performance yet slowing optimization and increasing hyper-parameter tuning needs. This paper investigates the importance of the warm-up stage and the impact of layer normalization placement. It uses mean field theory to demonstrate that the original Post-LN Transformer's design, with layer normalization between residual blocks, results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    }
  ],
  "placement": [
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is crucial for natural language processing tasks, but its training requires a learning rate warm-up stage, essential for performance yet slowing optimization and increasing hyper-parameter tuning needs. This paper investigates the importance of the warm-up stage and the impact of layer normalization placement. It uses mean field theory to demonstrate that the original Post-LN Transformer's design, with layer normalization between residual blocks, results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    }
  ],
  "post": [
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is crucial for natural language processing tasks, but its training requires a learning rate warm-up stage, essential for performance yet slowing optimization and increasing hyper-parameter tuning needs. This paper investigates the importance of the warm-up stage and the impact of layer normalization placement. It uses mean field theory to demonstrate that the original Post-LN Transformer's design, with layer normalization between residual blocks, results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    }
  ],
  "ln": [
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is crucial for natural language processing tasks, but its training requires a learning rate warm-up stage, essential for performance yet slowing optimization and increasing hyper-parameter tuning needs. This paper investigates the importance of the warm-up stage and the impact of layer normalization placement. It uses mean field theory to demonstrate that the original Post-LN Transformer's design, with layer normalization between residual blocks, results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    }
  ],
  "necessitate": [
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is crucial for natural language processing tasks, but its training requires a learning rate warm-up stage, essential for performance yet slowing optimization and increasing hyper-parameter tuning needs. This paper investigates the importance of the warm-up stage and the impact of layer normalization placement. It uses mean field theory to demonstrate that the original Post-LN Transformer's design, with layer normalization between residual blocks, results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    }
  ],
  "place": [
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is crucial for natural language processing tasks, but its training requires a learning rate warm-up stage, essential for performance yet slowing optimization and increasing hyper-parameter tuning needs. This paper investigates the importance of the warm-up stage and the impact of layer normalization placement. It uses mean field theory to demonstrate that the original Post-LN Transformer's design, with layer normalization between residual blocks, results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    }
  ],
  "preln": [
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is crucial for natural language processing tasks, but its training requires a learning rate warm-up stage, essential for performance yet slowing optimization and increasing hyper-parameter tuning needs. This paper investigates the importance of the warm-up stage and the impact of layer normalization placement. It uses mean field theory to demonstrate that the original Post-LN Transformer's design, with layer normalization between residual blocks, results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    }
  ],
  "elimination": [
    {
      "title": "On Layer Normalization in the Transformer Architectur",
      "url": "/research/2020/06/29/On-Layer-Normalization-in-the-Transformer-Architectur.html",
      "date": "June 29, 2020",
      "summary": "The Transformer is crucial for natural language processing tasks, but its training requires a learning rate warm-up stage, essential for performance yet slowing optimization and increasing hyper-parameter tuning needs. This paper investigates the importance of the warm-up stage and the impact of layer normalization placement. It uses mean field theory to demonstrate that the original Post-LN Transformer's design, with layer normalization between residual blocks, results in large initial gradients, necessitating a warm-up stage for stability. Conversely, placing layer normalization inside residual blocks, as seen in Pre-LN Transformers, stabilizes initial gradients, allowing the elimination of the warm-up stage. Experiments reveal that Pre-LN Transformers achieve comparable performance to traditional models with less training time and fewer hyper-parameters across various applications."
    }
  ],
  "flaw": [
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "Online Normalization is a novel method for normalizing neural network hidden activations, offering a batch-independent alternative with comparable accuracy to Batch Normalization. It overcomes Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. Applicable to recurrent, fully connected networks, and those with high activation memory requirements, it demonstrates effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    }
  ],
  "differentiation": [
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "Online Normalization is a novel method for normalizing neural network hidden activations, offering a batch-independent alternative with comparable accuracy to Batch Normalization. It overcomes Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. Applicable to recurrent, fully connected networks, and those with high activation memory requirements, it demonstrates effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    }
  ],
  "ptb": [
    {
      "title": "Online Normalization for Training Neural Networks",
      "url": "/research/2019/12/03/Online-Normalization-for-Training-Neural-Networks.html",
      "date": "December 3, 2019",
      "summary": "Online Normalization is a novel method for normalizing neural network hidden activations, offering a batch-independent alternative with comparable accuracy to Batch Normalization. It overcomes Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. Applicable to recurrent, fully connected networks, and those with high activation memory requirements, it demonstrates effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
    }
  ],
  "qlora": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "finetuning": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "llm": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "finetune": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    },
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "Recent advancements in image transformers, notably in models such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, have narrowed the performance gap with traditional CNNs. These models are typically trained on large datasets like ImageNet-21k and then finetuned on ImageNet-1k. However, assessments often overlook Tiny ImageNet when evaluating transfer learning performance. This paper updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. The code is available at: https://github.com/ehuynh1106/TinyImageNet-Transformers"
    }
  ],
  "gb": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "rank": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "adapters": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "lora": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "performing": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "guanaco": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "vicuna": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "chatgpt": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "saving": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "compromise": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "double": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "page": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "spike": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "types": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "llama": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "t5": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "scales": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "gpt": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "reliability": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "chatbot": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "shortcoming": [
    {
      "title": "Qlora: Efficient Finetuning of Quantized Llms",
      "url": "/research/2023/05/23/Qlora-Efficient-Finetuning-of-Quantized-Llms.html",
      "date": "May 23, 2023",
      "summary": "We introduce QLORA, a finetuning method that enables a 65B parameter model to be finetuned on a single 48GB GPU, maintaining 16-bit task performance by utilizing 4-bit quantized language models and Low Rank Adapters (LoRA). Our best-performing model family, Guanaco, surpasses all openly available models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on one GPU. QLORA incorporates several memory-saving innovations without compromising performance, including a new 4-bit data type optimized for normally distributed weights, Double Quantization for reduced memory usage, and Paged Optimizers for managing memory spikes. We applied QLORA to finetune over 1,000 models, analyzing performance across various datasets, model types (LLaMA, T5), and scales, demonstrating its ability to achieve state-of-the-art results even with smaller models. Our findings also suggest that GPT-4 evaluations are a viable substitute for human assessments, and question the reliability of current chatbot benchmarks. A detailed comparison with ChatGPT highlights Guanaco's shortcomings. We make our models and 4-bit training code public."
    }
  ],
  "quo": [
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "The limited number of videos in current action classification datasets like UCF-101 and HMDB-51 hinders the identification of effective video architectures due to similar performance across small-scale benchmarks. This study re-examines top architectures using the new, significantly larger Kinetics Human Action Video dataset, featuring 400 classes and over 400 clips per class from challenging YouTube videos. It analyzes the impact of this dataset on the performance of existing architectures and the benefits of pre-training on Kinetics. Additionally, the paper introduces the Two-Stream Inflated 3D ConvNet (I3D), an advancement that expands 2D ConvNet designs into 3D for enhanced video feature extraction. This model, pre-trained on Kinetics, sets new benchmarks in action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    }
  ],
  "vadis": [
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "The limited number of videos in current action classification datasets like UCF-101 and HMDB-51 hinders the identification of effective video architectures due to similar performance across small-scale benchmarks. This study re-examines top architectures using the new, significantly larger Kinetics Human Action Video dataset, featuring 400 classes and over 400 clips per class from challenging YouTube videos. It analyzes the impact of this dataset on the performance of existing architectures and the benefits of pre-training on Kinetics. Additionally, the paper introduces the Two-Stream Inflated 3D ConvNet (I3D), an advancement that expands 2D ConvNet designs into 3D for enhanced video feature extraction. This model, pre-trained on Kinetics, sets new benchmarks in action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    }
  ],
  "kinetic": [
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "The limited number of videos in current action classification datasets like UCF-101 and HMDB-51 hinders the identification of effective video architectures due to similar performance across small-scale benchmarks. This study re-examines top architectures using the new, significantly larger Kinetics Human Action Video dataset, featuring 400 classes and over 400 clips per class from challenging YouTube videos. It analyzes the impact of this dataset on the performance of existing architectures and the benefits of pre-training on Kinetics. Additionally, the paper introduces the Two-Stream Inflated 3D ConvNet (I3D), an advancement that expands 2D ConvNet designs into 3D for enhanced video feature extraction. This model, pre-trained on Kinetics, sets new benchmarks in action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    }
  ],
  "datase": [
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "The limited number of videos in current action classification datasets like UCF-101 and HMDB-51 hinders the identification of effective video architectures due to similar performance across small-scale benchmarks. This study re-examines top architectures using the new, significantly larger Kinetics Human Action Video dataset, featuring 400 classes and over 400 clips per class from challenging YouTube videos. It analyzes the impact of this dataset on the performance of existing architectures and the benefits of pre-training on Kinetics. Additionally, the paper introduces the Two-Stream Inflated 3D ConvNet (I3D), an advancement that expands 2D ConvNet designs into 3D for enhanced video feature extraction. This model, pre-trained on Kinetics, sets new benchmarks in action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    }
  ],
  "ucf": [
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "The limited number of videos in current action classification datasets like UCF-101 and HMDB-51 hinders the identification of effective video architectures due to similar performance across small-scale benchmarks. This study re-examines top architectures using the new, significantly larger Kinetics Human Action Video dataset, featuring 400 classes and over 400 clips per class from challenging YouTube videos. It analyzes the impact of this dataset on the performance of existing architectures and the benefits of pre-training on Kinetics. Additionally, the paper introduces the Two-Stream Inflated 3D ConvNet (I3D), an advancement that expands 2D ConvNet designs into 3D for enhanced video feature extraction. This model, pre-trained on Kinetics, sets new benchmarks in action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    }
  ],
  "hmdb": [
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "The limited number of videos in current action classification datasets like UCF-101 and HMDB-51 hinders the identification of effective video architectures due to similar performance across small-scale benchmarks. This study re-examines top architectures using the new, significantly larger Kinetics Human Action Video dataset, featuring 400 classes and over 400 clips per class from challenging YouTube videos. It analyzes the impact of this dataset on the performance of existing architectures and the benefits of pre-training on Kinetics. Additionally, the paper introduces the Two-Stream Inflated 3D ConvNet (I3D), an advancement that expands 2D ConvNet designs into 3D for enhanced video feature extraction. This model, pre-trained on Kinetics, sets new benchmarks in action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    }
  ],
  "identification": [
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "The limited number of videos in current action classification datasets like UCF-101 and HMDB-51 hinders the identification of effective video architectures due to similar performance across small-scale benchmarks. This study re-examines top architectures using the new, significantly larger Kinetics Human Action Video dataset, featuring 400 classes and over 400 clips per class from challenging YouTube videos. It analyzes the impact of this dataset on the performance of existing architectures and the benefits of pre-training on Kinetics. Additionally, the paper introduces the Two-Stream Inflated 3D ConvNet (I3D), an advancement that expands 2D ConvNet designs into 3D for enhanced video feature extraction. This model, pre-trained on Kinetics, sets new benchmarks in action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    },
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "reexamine": [
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "The limited number of videos in current action classification datasets like UCF-101 and HMDB-51 hinders the identification of effective video architectures due to similar performance across small-scale benchmarks. This study re-examines top architectures using the new, significantly larger Kinetics Human Action Video dataset, featuring 400 classes and over 400 clips per class from challenging YouTube videos. It analyzes the impact of this dataset on the performance of existing architectures and the benefits of pre-training on Kinetics. Additionally, the paper introduces the Two-Stream Inflated 3D ConvNet (I3D), an advancement that expands 2D ConvNet designs into 3D for enhanced video feature extraction. This model, pre-trained on Kinetics, sets new benchmarks in action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    }
  ],
  "kinetics": [
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "The limited number of videos in current action classification datasets like UCF-101 and HMDB-51 hinders the identification of effective video architectures due to similar performance across small-scale benchmarks. This study re-examines top architectures using the new, significantly larger Kinetics Human Action Video dataset, featuring 400 classes and over 400 clips per class from challenging YouTube videos. It analyzes the impact of this dataset on the performance of existing architectures and the benefits of pre-training on Kinetics. Additionally, the paper introduces the Two-Stream Inflated 3D ConvNet (I3D), an advancement that expands 2D ConvNet designs into 3D for enhanced video feature extraction. This model, pre-trained on Kinetics, sets new benchmarks in action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    },
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "We introduce SlowFast networks for video recognition, featuring a Slow pathway for spatial semantics at low frame rates and a Fast pathway for motion details at high frame rates. The Fast pathway's efficiency is achieved through reduced channel capacity, yet it effectively captures temporal information. Our models significantly enhance action classification and detection, achieving top accuracy on benchmarks like Kinetics, Charades, and AVA. Code is available at: https://github.com/facebookresearch/SlowFast."
    }
  ],
  "youtube": [
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "The limited number of videos in current action classification datasets like UCF-101 and HMDB-51 hinders the identification of effective video architectures due to similar performance across small-scale benchmarks. This study re-examines top architectures using the new, significantly larger Kinetics Human Action Video dataset, featuring 400 classes and over 400 clips per class from challenging YouTube videos. It analyzes the impact of this dataset on the performance of existing architectures and the benefits of pre-training on Kinetics. Additionally, the paper introduces the Two-Stream Inflated 3D ConvNet (I3D), an advancement that expands 2D ConvNet designs into 3D for enhanced video feature extraction. This model, pre-trained on Kinetics, sets new benchmarks in action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    }
  ],
  "stream": [
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "The limited number of videos in current action classification datasets like UCF-101 and HMDB-51 hinders the identification of effective video architectures due to similar performance across small-scale benchmarks. This study re-examines top architectures using the new, significantly larger Kinetics Human Action Video dataset, featuring 400 classes and over 400 clips per class from challenging YouTube videos. It analyzes the impact of this dataset on the performance of existing architectures and the benefits of pre-training on Kinetics. Additionally, the paper introduces the Two-Stream Inflated 3D ConvNet (I3D), an advancement that expands 2D ConvNet designs into 3D for enhanced video feature extraction. This model, pre-trained on Kinetics, sets new benchmarks in action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    }
  ],
  "i3d": [
    {
      "title": "Quo Vadis, Action Recognition A New Model and the Kinetics Datase",
      "url": "/research/2018/02/12/Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Kinetics-Datase.html",
      "date": "February 12, 2018",
      "summary": "The limited number of videos in current action classification datasets like UCF-101 and HMDB-51 hinders the identification of effective video architectures due to similar performance across small-scale benchmarks. This study re-examines top architectures using the new, significantly larger Kinetics Human Action Video dataset, featuring 400 classes and over 400 clips per class from challenging YouTube videos. It analyzes the impact of this dataset on the performance of existing architectures and the benefits of pre-training on Kinetics. Additionally, the paper introduces the Two-Stream Inflated 3D ConvNet (I3D), an advancement that expands 2D ConvNet designs into 3D for enhanced video feature extraction. This model, pre-trained on Kinetics, sets new benchmarks in action classification, achieving 80.9% accuracy on HMDB-51 and 98.0% on UCF-101."
    }
  ],
  "flow": [
    {
      "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "Diffusion models, effective for generating high-dimensional perceptual data like images, lack established standard practices. This study enhances noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Our large-scale study demonstrates superior performance over established diffusion methods in high-resolution text-to-image synthesis. We introduce a transformer-based architecture for text-to-image generation, improving text comprehension, typography, and human preference ratings. Our findings show predictable scaling trends, with lower validation loss correlating with improved synthesis. The largest models outperform state-of-the-art approaches, and our data, code, and model weights will be publicly accessible."
    }
  ],
  "typography": [
    {
      "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "Diffusion models, effective for generating high-dimensional perceptual data like images, lack established standard practices. This study enhances noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Our large-scale study demonstrates superior performance over established diffusion methods in high-resolution text-to-image synthesis. We introduce a transformer-based architecture for text-to-image generation, improving text comprehension, typography, and human preference ratings. Our findings show predictable scaling trends, with lower validation loss correlating with improved synthesis. The largest models outperform state-of-the-art approaches, and our data, code, and model weights will be publicly accessible."
    }
  ],
  "preference": [
    {
      "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "Diffusion models, effective for generating high-dimensional perceptual data like images, lack established standard practices. This study enhances noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Our large-scale study demonstrates superior performance over established diffusion methods in high-resolution text-to-image synthesis. We introduce a transformer-based architecture for text-to-image generation, improving text comprehension, typography, and human preference ratings. Our findings show predictable scaling trends, with lower validation loss correlating with improved synthesis. The largest models outperform state-of-the-art approaches, and our data, code, and model weights will be publicly accessible."
    }
  ],
  "rating": [
    {
      "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "Diffusion models, effective for generating high-dimensional perceptual data like images, lack established standard practices. This study enhances noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Our large-scale study demonstrates superior performance over established diffusion methods in high-resolution text-to-image synthesis. We introduce a transformer-based architecture for text-to-image generation, improving text comprehension, typography, and human preference ratings. Our findings show predictable scaling trends, with lower validation loss correlating with improved synthesis. The largest models outperform state-of-the-art approaches, and our data, code, and model weights will be publicly accessible."
    }
  ],
  "correlate": [
    {
      "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "url": "/research/2024/03/05/Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis.html",
      "date": "March 5, 2024",
      "summary": "Diffusion models, effective for generating high-dimensional perceptual data like images, lack established standard practices. This study enhances noise sampling techniques for training rectified flow models, prioritizing perceptually relevant scales. Our large-scale study demonstrates superior performance over established diffusion methods in high-resolution text-to-image synthesis. We introduce a transformer-based architecture for text-to-image generation, improving text comprehension, typography, and human preference ratings. Our findings show predictable scaling trends, with lower validation loss correlating with improved synthesis. The largest models outperform state-of-the-art approaches, and our data, code, and model weights will be publicly accessible."
    }
  ],
  "spl": [
    {
      "title": "Self-Paced Curriculum Learning",
      "url": "/research/2015/01/01/Self-Paced-Curriculum-Learning.html",
      "date": "January 1, 2015",
      "summary": "Curriculum learning (CL) and self-paced learning (SPL) are methods inspired by the incremental complexity of human and animal learning, with CL using a fixed curriculum based on prior knowledge and SPL adjusting dynamically to the learner's pace but struggling with prior knowledge and overfitting. This paper introduces a unified framework called self-paced curriculum learning (SPCL), blending both methods' strengths by considering prior knowledge and ongoing learning progress through a concise optimization problem. SPCL mimics a collaborative instructor-student learning mode, showing empirical advantages in two tasks."
    }
  ],
  "spcl": [
    {
      "title": "Self-Paced Curriculum Learning",
      "url": "/research/2015/01/01/Self-Paced-Curriculum-Learning.html",
      "date": "January 1, 2015",
      "summary": "Curriculum learning (CL) and self-paced learning (SPL) are methods inspired by the incremental complexity of human and animal learning, with CL using a fixed curriculum based on prior knowledge and SPL adjusting dynamically to the learner's pace but struggling with prior knowledge and overfitting. This paper introduces a unified framework called self-paced curriculum learning (SPCL), blending both methods' strengths by considering prior knowledge and ongoing learning progress through a concise optimization problem. SPCL mimics a collaborative instructor-student learning mode, showing empirical advantages in two tasks."
    }
  ],
  "instructor": [
    {
      "title": "Self-Paced Curriculum Learning",
      "url": "/research/2015/01/01/Self-Paced-Curriculum-Learning.html",
      "date": "January 1, 2015",
      "summary": "Curriculum learning (CL) and self-paced learning (SPL) are methods inspired by the incremental complexity of human and animal learning, with CL using a fixed curriculum based on prior knowledge and SPL adjusting dynamically to the learner's pace but struggling with prior knowledge and overfitting. This paper introduces a unified framework called self-paced curriculum learning (SPCL), blending both methods' strengths by considering prior knowledge and ongoing learning progress through a concise optimization problem. SPCL mimics a collaborative instructor-student learning mode, showing empirical advantages in two tasks."
    }
  ],
  "reinforcement": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions for neural networks in reinforcement learning, the sigmoid-weighted linear unit (SiLU) and its derivative (dSiLU), and challenges the need for experience replay and separate target networks in deep reinforcement learning. By employing on-policy learning with eligibility traces and softmax action selection, the study achieves state-of-the-art results in stochastic SZ-Tetris and a small-board Tetris using TD(λ) learning and shallow dSiLU network agents. Furthermore, it outperforms the DQN algorithm in the Atari 2600 domain with a deep Sarsa(λ) agent utilizing SiLU and dSiLU hidden units, suggesting a competitive alternative to traditional DQN approaches."
    }
  ],
  "silu": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions for neural networks in reinforcement learning, the sigmoid-weighted linear unit (SiLU) and its derivative (dSiLU), and challenges the need for experience replay and separate target networks in deep reinforcement learning. By employing on-policy learning with eligibility traces and softmax action selection, the study achieves state-of-the-art results in stochastic SZ-Tetris and a small-board Tetris using TD(λ) learning and shallow dSiLU network agents. Furthermore, it outperforms the DQN algorithm in the Atari 2600 domain with a deep Sarsa(λ) agent utilizing SiLU and dSiLU hidden units, suggesting a competitive alternative to traditional DQN approaches."
    }
  ],
  "dsilu": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions for neural networks in reinforcement learning, the sigmoid-weighted linear unit (SiLU) and its derivative (dSiLU), and challenges the need for experience replay and separate target networks in deep reinforcement learning. By employing on-policy learning with eligibility traces and softmax action selection, the study achieves state-of-the-art results in stochastic SZ-Tetris and a small-board Tetris using TD(λ) learning and shallow dSiLU network agents. Furthermore, it outperforms the DQN algorithm in the Atari 2600 domain with a deep Sarsa(λ) agent utilizing SiLU and dSiLU hidden units, suggesting a competitive alternative to traditional DQN approaches."
    }
  ],
  "replay": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions for neural networks in reinforcement learning, the sigmoid-weighted linear unit (SiLU) and its derivative (dSiLU), and challenges the need for experience replay and separate target networks in deep reinforcement learning. By employing on-policy learning with eligibility traces and softmax action selection, the study achieves state-of-the-art results in stochastic SZ-Tetris and a small-board Tetris using TD(λ) learning and shallow dSiLU network agents. Furthermore, it outperforms the DQN algorithm in the Atari 2600 domain with a deep Sarsa(λ) agent utilizing SiLU and dSiLU hidden units, suggesting a competitive alternative to traditional DQN approaches."
    }
  ],
  "policy": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions for neural networks in reinforcement learning, the sigmoid-weighted linear unit (SiLU) and its derivative (dSiLU), and challenges the need for experience replay and separate target networks in deep reinforcement learning. By employing on-policy learning with eligibility traces and softmax action selection, the study achieves state-of-the-art results in stochastic SZ-Tetris and a small-board Tetris using TD(λ) learning and shallow dSiLU network agents. Furthermore, it outperforms the DQN algorithm in the Atari 2600 domain with a deep Sarsa(λ) agent utilizing SiLU and dSiLU hidden units, suggesting a competitive alternative to traditional DQN approaches."
    }
  ],
  "eligibility": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions for neural networks in reinforcement learning, the sigmoid-weighted linear unit (SiLU) and its derivative (dSiLU), and challenges the need for experience replay and separate target networks in deep reinforcement learning. By employing on-policy learning with eligibility traces and softmax action selection, the study achieves state-of-the-art results in stochastic SZ-Tetris and a small-board Tetris using TD(λ) learning and shallow dSiLU network agents. Furthermore, it outperforms the DQN algorithm in the Atari 2600 domain with a deep Sarsa(λ) agent utilizing SiLU and dSiLU hidden units, suggesting a competitive alternative to traditional DQN approaches."
    }
  ],
  "trace": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions for neural networks in reinforcement learning, the sigmoid-weighted linear unit (SiLU) and its derivative (dSiLU), and challenges the need for experience replay and separate target networks in deep reinforcement learning. By employing on-policy learning with eligibility traces and softmax action selection, the study achieves state-of-the-art results in stochastic SZ-Tetris and a small-board Tetris using TD(λ) learning and shallow dSiLU network agents. Furthermore, it outperforms the DQN algorithm in the Atari 2600 domain with a deep Sarsa(λ) agent utilizing SiLU and dSiLU hidden units, suggesting a competitive alternative to traditional DQN approaches."
    }
  ],
  "sz": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions for neural networks in reinforcement learning, the sigmoid-weighted linear unit (SiLU) and its derivative (dSiLU), and challenges the need for experience replay and separate target networks in deep reinforcement learning. By employing on-policy learning with eligibility traces and softmax action selection, the study achieves state-of-the-art results in stochastic SZ-Tetris and a small-board Tetris using TD(λ) learning and shallow dSiLU network agents. Furthermore, it outperforms the DQN algorithm in the Atari 2600 domain with a deep Sarsa(λ) agent utilizing SiLU and dSiLU hidden units, suggesting a competitive alternative to traditional DQN approaches."
    }
  ],
  "tetris": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions for neural networks in reinforcement learning, the sigmoid-weighted linear unit (SiLU) and its derivative (dSiLU), and challenges the need for experience replay and separate target networks in deep reinforcement learning. By employing on-policy learning with eligibility traces and softmax action selection, the study achieves state-of-the-art results in stochastic SZ-Tetris and a small-board Tetris using TD(λ) learning and shallow dSiLU network agents. Furthermore, it outperforms the DQN algorithm in the Atari 2600 domain with a deep Sarsa(λ) agent utilizing SiLU and dSiLU hidden units, suggesting a competitive alternative to traditional DQN approaches."
    }
  ],
  "board": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions for neural networks in reinforcement learning, the sigmoid-weighted linear unit (SiLU) and its derivative (dSiLU), and challenges the need for experience replay and separate target networks in deep reinforcement learning. By employing on-policy learning with eligibility traces and softmax action selection, the study achieves state-of-the-art results in stochastic SZ-Tetris and a small-board Tetris using TD(λ) learning and shallow dSiLU network agents. Furthermore, it outperforms the DQN algorithm in the Atari 2600 domain with a deep Sarsa(λ) agent utilizing SiLU and dSiLU hidden units, suggesting a competitive alternative to traditional DQN approaches."
    }
  ],
  "tda": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions for neural networks in reinforcement learning, the sigmoid-weighted linear unit (SiLU) and its derivative (dSiLU), and challenges the need for experience replay and separate target networks in deep reinforcement learning. By employing on-policy learning with eligibility traces and softmax action selection, the study achieves state-of-the-art results in stochastic SZ-Tetris and a small-board Tetris using TD(λ) learning and shallow dSiLU network agents. Furthermore, it outperforms the DQN algorithm in the Atari 2600 domain with a deep Sarsa(λ) agent utilizing SiLU and dSiLU hidden units, suggesting a competitive alternative to traditional DQN approaches."
    }
  ],
  "agent": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions for neural networks in reinforcement learning, the sigmoid-weighted linear unit (SiLU) and its derivative (dSiLU), and challenges the need for experience replay and separate target networks in deep reinforcement learning. By employing on-policy learning with eligibility traces and softmax action selection, the study achieves state-of-the-art results in stochastic SZ-Tetris and a small-board Tetris using TD(λ) learning and shallow dSiLU network agents. Furthermore, it outperforms the DQN algorithm in the Atari 2600 domain with a deep Sarsa(λ) agent utilizing SiLU and dSiLU hidden units, suggesting a competitive alternative to traditional DQN approaches."
    }
  ],
  "dqn": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions for neural networks in reinforcement learning, the sigmoid-weighted linear unit (SiLU) and its derivative (dSiLU), and challenges the need for experience replay and separate target networks in deep reinforcement learning. By employing on-policy learning with eligibility traces and softmax action selection, the study achieves state-of-the-art results in stochastic SZ-Tetris and a small-board Tetris using TD(λ) learning and shallow dSiLU network agents. Furthermore, it outperforms the DQN algorithm in the Atari 2600 domain with a deep Sarsa(λ) agent utilizing SiLU and dSiLU hidden units, suggesting a competitive alternative to traditional DQN approaches."
    }
  ],
  "atari": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions for neural networks in reinforcement learning, the sigmoid-weighted linear unit (SiLU) and its derivative (dSiLU), and challenges the need for experience replay and separate target networks in deep reinforcement learning. By employing on-policy learning with eligibility traces and softmax action selection, the study achieves state-of-the-art results in stochastic SZ-Tetris and a small-board Tetris using TD(λ) learning and shallow dSiLU network agents. Furthermore, it outperforms the DQN algorithm in the Atari 2600 domain with a deep Sarsa(λ) agent utilizing SiLU and dSiLU hidden units, suggesting a competitive alternative to traditional DQN approaches."
    }
  ],
  "sarsaa": [
    {
      "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning",
      "url": "/research/2017/11/02/Sigmoid-Weighted-Linear-Units-for-Neural-Network-Function-Approximation-in-Reinforcement-Learning.html",
      "date": "November 2, 2017",
      "summary": "This study introduces two new activation functions for neural networks in reinforcement learning, the sigmoid-weighted linear unit (SiLU) and its derivative (dSiLU), and challenges the need for experience replay and separate target networks in deep reinforcement learning. By employing on-policy learning with eligibility traces and softmax action selection, the study achieves state-of-the-art results in stochastic SZ-Tetris and a small-board Tetris using TD(λ) learning and shallow dSiLU network agents. Furthermore, it outperforms the DQN algorithm in the Atari 2600 domain with a deep Sarsa(λ) agent utilizing SiLU and dSiLU hidden units, suggesting a competitive alternative to traditional DQN approaches."
    }
  ],
  "sim": [
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "6D object pose estimation is crucial for robotic bin-picking and essential for many industrial applications. Annotating custom datasets for each bin-picking scenario is challenging. We introduce an iterative self-training framework for sim-to-real 6D object pose estimation, enhancing cost-effective robotic grasping. We create a photo-realistic simulator for synthesizing virtual data to train an initial pose estimation network. This network, acting as a teacher model, predicts poses on unlabeled real data. We develop an adaptive selection scheme to filter reliable predictions for updating a student model with pseudo labels for real data pose estimation. By iteratively refining the teacher model with the trained student model, we improve the quality of pseudo labels. Our method, tested on a public benchmark and a newly-released dataset, shows an 11.49% and 22.62% ADD(-S) improvement, respectively, and a 19.54% increase in robotic bin-picking success, demonstrating the effectiveness of iterative sim-to-real approaches. Project homepage: www.cse.cuhk.edu.hk/~kaichen/sim2real_pose.html"
    }
  ],
  "bin": [
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "6D object pose estimation is crucial for robotic bin-picking and essential for many industrial applications. Annotating custom datasets for each bin-picking scenario is challenging. We introduce an iterative self-training framework for sim-to-real 6D object pose estimation, enhancing cost-effective robotic grasping. We create a photo-realistic simulator for synthesizing virtual data to train an initial pose estimation network. This network, acting as a teacher model, predicts poses on unlabeled real data. We develop an adaptive selection scheme to filter reliable predictions for updating a student model with pseudo labels for real data pose estimation. By iteratively refining the teacher model with the trained student model, we improve the quality of pseudo labels. Our method, tested on a public benchmark and a newly-released dataset, shows an 11.49% and 22.62% ADD(-S) improvement, respectively, and a 19.54% increase in robotic bin-picking success, demonstrating the effectiveness of iterative sim-to-real approaches. Project homepage: www.cse.cuhk.edu.hk/~kaichen/sim2real_pose.html"
    }
  ],
  "pick": [
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "6D object pose estimation is crucial for robotic bin-picking and essential for many industrial applications. Annotating custom datasets for each bin-picking scenario is challenging. We introduce an iterative self-training framework for sim-to-real 6D object pose estimation, enhancing cost-effective robotic grasping. We create a photo-realistic simulator for synthesizing virtual data to train an initial pose estimation network. This network, acting as a teacher model, predicts poses on unlabeled real data. We develop an adaptive selection scheme to filter reliable predictions for updating a student model with pseudo labels for real data pose estimation. By iteratively refining the teacher model with the trained student model, we improve the quality of pseudo labels. Our method, tested on a public benchmark and a newly-released dataset, shows an 11.49% and 22.62% ADD(-S) improvement, respectively, and a 19.54% increase in robotic bin-picking success, demonstrating the effectiveness of iterative sim-to-real approaches. Project homepage: www.cse.cuhk.edu.hk/~kaichen/sim2real_pose.html"
    }
  ],
  "picking": [
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "6D object pose estimation is crucial for robotic bin-picking and essential for many industrial applications. Annotating custom datasets for each bin-picking scenario is challenging. We introduce an iterative self-training framework for sim-to-real 6D object pose estimation, enhancing cost-effective robotic grasping. We create a photo-realistic simulator for synthesizing virtual data to train an initial pose estimation network. This network, acting as a teacher model, predicts poses on unlabeled real data. We develop an adaptive selection scheme to filter reliable predictions for updating a student model with pseudo labels for real data pose estimation. By iteratively refining the teacher model with the trained student model, we improve the quality of pseudo labels. Our method, tested on a public benchmark and a newly-released dataset, shows an 11.49% and 22.62% ADD(-S) improvement, respectively, and a 19.54% increase in robotic bin-picking success, demonstrating the effectiveness of iterative sim-to-real approaches. Project homepage: www.cse.cuhk.edu.hk/~kaichen/sim2real_pose.html"
    }
  ],
  "custom": [
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "6D object pose estimation is crucial for robotic bin-picking and essential for many industrial applications. Annotating custom datasets for each bin-picking scenario is challenging. We introduce an iterative self-training framework for sim-to-real 6D object pose estimation, enhancing cost-effective robotic grasping. We create a photo-realistic simulator for synthesizing virtual data to train an initial pose estimation network. This network, acting as a teacher model, predicts poses on unlabeled real data. We develop an adaptive selection scheme to filter reliable predictions for updating a student model with pseudo labels for real data pose estimation. By iteratively refining the teacher model with the trained student model, we improve the quality of pseudo labels. Our method, tested on a public benchmark and a newly-released dataset, shows an 11.49% and 22.62% ADD(-S) improvement, respectively, and a 19.54% increase in robotic bin-picking success, demonstrating the effectiveness of iterative sim-to-real approaches. Project homepage: www.cse.cuhk.edu.hk/~kaichen/sim2real_pose.html"
    }
  ],
  "grasping": [
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "6D object pose estimation is crucial for robotic bin-picking and essential for many industrial applications. Annotating custom datasets for each bin-picking scenario is challenging. We introduce an iterative self-training framework for sim-to-real 6D object pose estimation, enhancing cost-effective robotic grasping. We create a photo-realistic simulator for synthesizing virtual data to train an initial pose estimation network. This network, acting as a teacher model, predicts poses on unlabeled real data. We develop an adaptive selection scheme to filter reliable predictions for updating a student model with pseudo labels for real data pose estimation. By iteratively refining the teacher model with the trained student model, we improve the quality of pseudo labels. Our method, tested on a public benchmark and a newly-released dataset, shows an 11.49% and 22.62% ADD(-S) improvement, respectively, and a 19.54% increase in robotic bin-picking success, demonstrating the effectiveness of iterative sim-to-real approaches. Project homepage: www.cse.cuhk.edu.hk/~kaichen/sim2real_pose.html"
    }
  ],
  "simulator": [
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "6D object pose estimation is crucial for robotic bin-picking and essential for many industrial applications. Annotating custom datasets for each bin-picking scenario is challenging. We introduce an iterative self-training framework for sim-to-real 6D object pose estimation, enhancing cost-effective robotic grasping. We create a photo-realistic simulator for synthesizing virtual data to train an initial pose estimation network. This network, acting as a teacher model, predicts poses on unlabeled real data. We develop an adaptive selection scheme to filter reliable predictions for updating a student model with pseudo labels for real data pose estimation. By iteratively refining the teacher model with the trained student model, we improve the quality of pseudo labels. Our method, tested on a public benchmark and a newly-released dataset, shows an 11.49% and 22.62% ADD(-S) improvement, respectively, and a 19.54% increase in robotic bin-picking success, demonstrating the effectiveness of iterative sim-to-real approaches. Project homepage: www.cse.cuhk.edu.hk/~kaichen/sim2real_pose.html"
    },
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "This paper explores the effectiveness of using synthetic data, generated by graphics simulators, for pre-training models in computer vision, addressing the challenges posed by using real-image datasets like high costs and ethical concerns. It highlights that the performance of models on downstream tasks varies with different simulation parameters, suggesting a tailored approach to synthetic data generation for optimal results. The introduction of Task2Sim, a model that maps downstream task requirements to optimal simulation parameters, allows for the generation of synthetic pre-training data customized for specific tasks. Task2Sim demonstrates significant improvements in model performance on a wide range of tasks without additional training for new tasks, showing potential to rival pre-training with real images from Imagenet."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "We introduce a system that trains deep neural networks for object detection using synthetic images, leveraging domain randomization to manage real-world data variability. This involves randomizing simulator parameters—like lighting and object textures—in non-realistic ways, helping the network to identify essential object features. Our findings reveal that networks can achieve impressive performance with just synthetic data and further improve with real data fine-tuning. This suggests the potential of using cost-effective synthetic data for training, circumventing the challenges of acquiring vast amounts of real-world data or creating detailed synthetic environments. We validate our method through car bounding box detection on the KITTI dataset."
    }
  ],
  "pseudo": [
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "6D object pose estimation is crucial for robotic bin-picking and essential for many industrial applications. Annotating custom datasets for each bin-picking scenario is challenging. We introduce an iterative self-training framework for sim-to-real 6D object pose estimation, enhancing cost-effective robotic grasping. We create a photo-realistic simulator for synthesizing virtual data to train an initial pose estimation network. This network, acting as a teacher model, predicts poses on unlabeled real data. We develop an adaptive selection scheme to filter reliable predictions for updating a student model with pseudo labels for real data pose estimation. By iteratively refining the teacher model with the trained student model, we improve the quality of pseudo labels. Our method, tested on a public benchmark and a newly-released dataset, shows an 11.49% and 22.62% ADD(-S) improvement, respectively, and a 19.54% increase in robotic bin-picking success, demonstrating the effectiveness of iterative sim-to-real approaches. Project homepage: www.cse.cuhk.edu.hk/~kaichen/sim2real_pose.html"
    }
  ],
  "adds": [
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "6D object pose estimation is crucial for robotic bin-picking and essential for many industrial applications. Annotating custom datasets for each bin-picking scenario is challenging. We introduce an iterative self-training framework for sim-to-real 6D object pose estimation, enhancing cost-effective robotic grasping. We create a photo-realistic simulator for synthesizing virtual data to train an initial pose estimation network. This network, acting as a teacher model, predicts poses on unlabeled real data. We develop an adaptive selection scheme to filter reliable predictions for updating a student model with pseudo labels for real data pose estimation. By iteratively refining the teacher model with the trained student model, we improve the quality of pseudo labels. Our method, tested on a public benchmark and a newly-released dataset, shows an 11.49% and 22.62% ADD(-S) improvement, respectively, and a 19.54% increase in robotic bin-picking success, demonstrating the effectiveness of iterative sim-to-real approaches. Project homepage: www.cse.cuhk.edu.hk/~kaichen/sim2real_pose.html"
    }
  ],
  "homepage": [
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "6D object pose estimation is crucial for robotic bin-picking and essential for many industrial applications. Annotating custom datasets for each bin-picking scenario is challenging. We introduce an iterative self-training framework for sim-to-real 6D object pose estimation, enhancing cost-effective robotic grasping. We create a photo-realistic simulator for synthesizing virtual data to train an initial pose estimation network. This network, acting as a teacher model, predicts poses on unlabeled real data. We develop an adaptive selection scheme to filter reliable predictions for updating a student model with pseudo labels for real data pose estimation. By iteratively refining the teacher model with the trained student model, we improve the quality of pseudo labels. Our method, tested on a public benchmark and a newly-released dataset, shows an 11.49% and 22.62% ADD(-S) improvement, respectively, and a 19.54% increase in robotic bin-picking success, demonstrating the effectiveness of iterative sim-to-real approaches. Project homepage: www.cse.cuhk.edu.hk/~kaichen/sim2real_pose.html"
    }
  ],
  "wwwcsecuhkeduhkkaichensim2realposehtml": [
    {
      "title": "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking",
      "url": "/research/2022/07/21/Sim-to-Real-6D-Object-Pose-Estimation-via-Iterative-Self-training-for-Robotic-Bin-Picking.html",
      "date": "July 21, 2022",
      "summary": "6D object pose estimation is crucial for robotic bin-picking and essential for many industrial applications. Annotating custom datasets for each bin-picking scenario is challenging. We introduce an iterative self-training framework for sim-to-real 6D object pose estimation, enhancing cost-effective robotic grasping. We create a photo-realistic simulator for synthesizing virtual data to train an initial pose estimation network. This network, acting as a teacher model, predicts poses on unlabeled real data. We develop an adaptive selection scheme to filter reliable predictions for updating a student model with pseudo labels for real data pose estimation. By iteratively refining the teacher model with the trained student model, we improve the quality of pseudo labels. Our method, tested on a public benchmark and a newly-released dataset, shows an 11.49% and 22.62% ADD(-S) improvement, respectively, and a 19.54% increase in robotic bin-picking success, demonstrating the effectiveness of iterative sim-to-real approaches. Project homepage: www.cse.cuhk.edu.hk/~kaichen/sim2real_pose.html"
    }
  ],
  "net": [
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "We introduce an algorithm designed to identify networks that are both simple and highly generalizable by searching for extensive areas of flat minima in the error function, where the error rate is relatively stable. These flat minima are associated with lower risks of overfitting according to MDL principles. Despite needing to calculate second-order derivatives, the algorithm maintains a complexity level comparable to backpropagation. We tested it on both feedforward and recurrent networks, and in stock market prediction tasks, it performed better than traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    },
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    }
  ],
  "minima": [
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "We introduce an algorithm designed to identify networks that are both simple and highly generalizable by searching for extensive areas of flat minima in the error function, where the error rate is relatively stable. These flat minima are associated with lower risks of overfitting according to MDL principles. Despite needing to calculate second-order derivatives, the algorithm maintains a complexity level comparable to backpropagation. We tested it on both feedforward and recurrent networks, and in stock market prediction tasks, it performed better than traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    },
    {
      "title": "Taming the Waves: Sine as Activation Function in Deep Neural Networks",
      "url": "/research/2016/11/04/Taming-the-Waves-Sine-as-Activation-Function-in-Deep-Neural-Networks.html",
      "date": "November 4, 2016",
      "summary": "This paper examines the challenges of training deep neural networks with sinusoidal activation functions, despite their successful use in specific applications. It explains the difficulty in training due to the emergence of numerous shallow local minima from the architecture. The study also reveals that successful learning in typical classification tasks occurs when the network effectively disregards the periodic cycles. Furthermore, it demonstrates that for certain non-trivial tasks, networks with sinusoidal activations can outperform those using traditional monotonic functions."
    },
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    }
  ],
  "risk": [
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "We introduce an algorithm designed to identify networks that are both simple and highly generalizable by searching for extensive areas of flat minima in the error function, where the error rate is relatively stable. These flat minima are associated with lower risks of overfitting according to MDL principles. Despite needing to calculate second-order derivatives, the algorithm maintains a complexity level comparable to backpropagation. We tested it on both feedforward and recurrent networks, and in stock market prediction tasks, it performed better than traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    }
  ],
  "according": [
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "We introduce an algorithm designed to identify networks that are both simple and highly generalizable by searching for extensive areas of flat minima in the error function, where the error rate is relatively stable. These flat minima are associated with lower risks of overfitting according to MDL principles. Despite needing to calculate second-order derivatives, the algorithm maintains a complexity level comparable to backpropagation. We tested it on both feedforward and recurrent networks, and in stock market prediction tasks, it performed better than traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    }
  ],
  "mdl": [
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "We introduce an algorithm designed to identify networks that are both simple and highly generalizable by searching for extensive areas of flat minima in the error function, where the error rate is relatively stable. These flat minima are associated with lower risks of overfitting according to MDL principles. Despite needing to calculate second-order derivatives, the algorithm maintains a complexity level comparable to backpropagation. We tested it on both feedforward and recurrent networks, and in stock market prediction tasks, it performed better than traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    }
  ],
  "calculate": [
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "We introduce an algorithm designed to identify networks that are both simple and highly generalizable by searching for extensive areas of flat minima in the error function, where the error rate is relatively stable. These flat minima are associated with lower risks of overfitting according to MDL principles. Despite needing to calculate second-order derivatives, the algorithm maintains a complexity level comparable to backpropagation. We tested it on both feedforward and recurrent networks, and in stock market prediction tasks, it performed better than traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    }
  ],
  "stock": [
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "We introduce an algorithm designed to identify networks that are both simple and highly generalizable by searching for extensive areas of flat minima in the error function, where the error rate is relatively stable. These flat minima are associated with lower risks of overfitting according to MDL principles. Despite needing to calculate second-order derivatives, the algorithm maintains a complexity level comparable to backpropagation. We tested it on both feedforward and recurrent networks, and in stock market prediction tasks, it performed better than traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    }
  ],
  "market": [
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "We introduce an algorithm designed to identify networks that are both simple and highly generalizable by searching for extensive areas of flat minima in the error function, where the error rate is relatively stable. These flat minima are associated with lower risks of overfitting according to MDL principles. Despite needing to calculate second-order derivatives, the algorithm maintains a complexity level comparable to backpropagation. We tested it on both feedforward and recurrent networks, and in stock market prediction tasks, it performed better than traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    }
  ],
  "brain": [
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "We introduce an algorithm designed to identify networks that are both simple and highly generalizable by searching for extensive areas of flat minima in the error function, where the error rate is relatively stable. These flat minima are associated with lower risks of overfitting according to MDL principles. Despite needing to calculate second-order derivatives, the algorithm maintains a complexity level comparable to backpropagation. We tested it on both feedforward and recurrent networks, and in stock market prediction tasks, it performed better than traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    }
  ],
  "surgeon": [
    {
      "title": "Simplifying Neural Nets by Discovering Flat Minima",
      "url": "/research/1994/11/28/Simplifying-Neural-Nets-by-Discovering-Flat-Minima.html",
      "date": "November 28, 1994",
      "summary": "We introduce an algorithm designed to identify networks that are both simple and highly generalizable by searching for extensive areas of flat minima in the error function, where the error rate is relatively stable. These flat minima are associated with lower risks of overfitting according to MDL principles. Despite needing to calculate second-order derivatives, the algorithm maintains a complexity level comparable to backpropagation. We tested it on both feedforward and recurrent networks, and in stock market prediction tasks, it performed better than traditional backpropagation, weight decay, and the optimal brain surgeon methods."
    }
  ],
  "slowfast": [
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "We introduce SlowFast networks for video recognition, featuring a Slow pathway for spatial semantics at low frame rates and a Fast pathway for motion details at high frame rates. The Fast pathway's efficiency is achieved through reduced channel capacity, yet it effectively captures temporal information. Our models significantly enhance action classification and detection, achieving top accuracy on benchmarks like Kinetics, Charades, and AVA. Code is available at: https://github.com/facebookresearch/SlowFast."
    }
  ],
  "pathway": [
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "We introduce SlowFast networks for video recognition, featuring a Slow pathway for spatial semantics at low frame rates and a Fast pathway for motion details at high frame rates. The Fast pathway's efficiency is achieved through reduced channel capacity, yet it effectively captures temporal information. Our models significantly enhance action classification and detection, achieving top accuracy on benchmarks like Kinetics, Charades, and AVA. Code is available at: https://github.com/facebookresearch/SlowFast."
    }
  ],
  "motion": [
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "We introduce SlowFast networks for video recognition, featuring a Slow pathway for spatial semantics at low frame rates and a Fast pathway for motion details at high frame rates. The Fast pathway's efficiency is achieved through reduced channel capacity, yet it effectively captures temporal information. Our models significantly enhance action classification and detection, achieving top accuracy on benchmarks like Kinetics, Charades, and AVA. Code is available at: https://github.com/facebookresearch/SlowFast."
    }
  ],
  "capacity": [
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "We introduce SlowFast networks for video recognition, featuring a Slow pathway for spatial semantics at low frame rates and a Fast pathway for motion details at high frame rates. The Fast pathway's efficiency is achieved through reduced channel capacity, yet it effectively captures temporal information. Our models significantly enhance action classification and detection, achieving top accuracy on benchmarks like Kinetics, Charades, and AVA. Code is available at: https://github.com/facebookresearch/SlowFast."
    }
  ],
  "charade": [
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "We introduce SlowFast networks for video recognition, featuring a Slow pathway for spatial semantics at low frame rates and a Fast pathway for motion details at high frame rates. The Fast pathway's efficiency is achieved through reduced channel capacity, yet it effectively captures temporal information. Our models significantly enhance action classification and detection, achieving top accuracy on benchmarks like Kinetics, Charades, and AVA. Code is available at: https://github.com/facebookresearch/SlowFast."
    }
  ],
  "httpsgithubcomfacebookresearchslowfast": [
    {
      "title": "SlowFast Networks for Video Recognition",
      "url": "/research/2019/10/29/SlowFast-Networks-for-Video-Recognition.html",
      "date": "October 29, 2019",
      "summary": "We introduce SlowFast networks for video recognition, featuring a Slow pathway for spatial semantics at low frame rates and a Fast pathway for motion details at high frame rates. The Fast pathway's efficiency is achieved through reduced channel capacity, yet it effectively captures temporal information. Our models significantly enhance action classification and detection, achieving top accuracy on benchmarks like Kinetics, Charades, and AVA. Code is available at: https://github.com/facebookresearch/SlowFast."
    }
  ],
  "randomization": [
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This study investigates the use of high-quality, physically-based rendering and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle, enhancing image classification and semantic segmentation models. Synthetic images proved to be effective for augmenting limited real training data. However, models trained solely on synthetic images showed low accuracy on real validation images, which significantly improved when even small amounts of real data were included. Augmenting real data with synthetic images outperformed training on only real images. Moreover, pretraining models on synthetic data before transfer learning significantly reduced training costs, allowing most of the training to be completed upfront."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "We introduce a system that trains deep neural networks for object detection using synthetic images, leveraging domain randomization to manage real-world data variability. This involves randomizing simulator parameters—like lighting and object textures—in non-realistic ways, helping the network to identify essential object features. Our findings reveal that networks can achieve impressive performance with just synthetic data and further improve with real data fine-tuning. This suggests the potential of using cost-effective synthetic data for training, circumventing the challenges of acquiring vast amounts of real-world data or creating detailed synthetic environments. We validate our method through car bounding box detection on the KITTI dataset."
    }
  ],
  "of": [
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This study investigates the use of high-quality, physically-based rendering and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle, enhancing image classification and semantic segmentation models. Synthetic images proved to be effective for augmenting limited real training data. However, models trained solely on synthetic images showed low accuracy on real validation images, which significantly improved when even small amounts of real data were included. Augmenting real data with synthetic images outperformed training on only real images. Moreover, pretraining models on synthetic data before transfer learning significantly reduced training costs, allowing most of the training to be completed upfront."
    }
  ],
  "vehicle": [
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This study investigates the use of high-quality, physically-based rendering and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle, enhancing image classification and semantic segmentation models. Synthetic images proved to be effective for augmenting limited real training data. However, models trained solely on synthetic images showed low accuracy on real validation images, which significantly improved when even small amounts of real data were included. Augmenting real data with synthetic images outperformed training on only real images. Moreover, pretraining models on synthetic data before transfer learning significantly reduced training costs, allowing most of the training to be completed upfront."
    }
  ],
  "complete": [
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This study investigates the use of high-quality, physically-based rendering and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle, enhancing image classification and semantic segmentation models. Synthetic images proved to be effective for augmenting limited real training data. However, models trained solely on synthetic images showed low accuracy on real validation images, which significantly improved when even small amounts of real data were included. Augmenting real data with synthetic images outperformed training on only real images. Moreover, pretraining models on synthetic data before transfer learning significantly reduced training costs, allowing most of the training to be completed upfront."
    }
  ],
  "upfront": [
    {
      "title": "Synthetic Image Data for Deep Learning",
      "url": "/research/2022/12/12/Synthetic-Image-Data-for-Deep-Learning.html",
      "date": "December 12, 2022",
      "summary": "This study investigates the use of high-quality, physically-based rendering and domain randomization to generate a large synthetic dataset from 3D CAD models of a real vehicle, enhancing image classification and semantic segmentation models. Synthetic images proved to be effective for augmenting limited real training data. However, models trained solely on synthetic images showed low accuracy on real validation images, which significantly improved when even small amounts of real data were included. Augmenting real data with synthetic images outperformed training on only real images. Moreover, pretraining models on synthetic data before transfer learning significantly reduced training costs, allowing most of the training to be completed upfront."
    }
  ],
  "taming": [
    {
      "title": "Taming the Waves: Sine as Activation Function in Deep Neural Networks",
      "url": "/research/2016/11/04/Taming-the-Waves-Sine-as-Activation-Function-in-Deep-Neural-Networks.html",
      "date": "November 4, 2016",
      "summary": "This paper examines the challenges of training deep neural networks with sinusoidal activation functions, despite their successful use in specific applications. It explains the difficulty in training due to the emergence of numerous shallow local minima from the architecture. The study also reveals that successful learning in typical classification tasks occurs when the network effectively disregards the periodic cycles. Furthermore, it demonstrates that for certain non-trivial tasks, networks with sinusoidal activations can outperform those using traditional monotonic functions."
    }
  ],
  "sine": [
    {
      "title": "Taming the Waves: Sine as Activation Function in Deep Neural Networks",
      "url": "/research/2016/11/04/Taming-the-Waves-Sine-as-Activation-Function-in-Deep-Neural-Networks.html",
      "date": "November 4, 2016",
      "summary": "This paper examines the challenges of training deep neural networks with sinusoidal activation functions, despite their successful use in specific applications. It explains the difficulty in training due to the emergence of numerous shallow local minima from the architecture. The study also reveals that successful learning in typical classification tasks occurs when the network effectively disregards the periodic cycles. Furthermore, it demonstrates that for certain non-trivial tasks, networks with sinusoidal activations can outperform those using traditional monotonic functions."
    }
  ],
  "disregard": [
    {
      "title": "Taming the Waves: Sine as Activation Function in Deep Neural Networks",
      "url": "/research/2016/11/04/Taming-the-Waves-Sine-as-Activation-Function-in-Deep-Neural-Networks.html",
      "date": "November 4, 2016",
      "summary": "This paper examines the challenges of training deep neural networks with sinusoidal activation functions, despite their successful use in specific applications. It explains the difficulty in training due to the emergence of numerous shallow local minima from the architecture. The study also reveals that successful learning in typical classification tasks occurs when the network effectively disregards the periodic cycles. Furthermore, it demonstrates that for certain non-trivial tasks, networks with sinusoidal activations can outperform those using traditional monotonic functions."
    }
  ],
  "cycle": [
    {
      "title": "Taming the Waves: Sine as Activation Function in Deep Neural Networks",
      "url": "/research/2016/11/04/Taming-the-Waves-Sine-as-Activation-Function-in-Deep-Neural-Networks.html",
      "date": "November 4, 2016",
      "summary": "This paper examines the challenges of training deep neural networks with sinusoidal activation functions, despite their successful use in specific applications. It explains the difficulty in training due to the emergence of numerous shallow local minima from the architecture. The study also reveals that successful learning in typical classification tasks occurs when the network effectively disregards the periodic cycles. Furthermore, it demonstrates that for certain non-trivial tasks, networks with sinusoidal activations can outperform those using traditional monotonic functions."
    }
  ],
  "task2sim": [
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "This paper explores the effectiveness of using synthetic data, generated by graphics simulators, for pre-training models in computer vision, addressing the challenges posed by using real-image datasets like high costs and ethical concerns. It highlights that the performance of models on downstream tasks varies with different simulation parameters, suggesting a tailored approach to synthetic data generation for optimal results. The introduction of Task2Sim, a model that maps downstream task requirements to optimal simulation parameters, allows for the generation of synthetic pre-training data customized for specific tasks. Task2Sim demonstrates significant improvements in model performance on a wide range of tasks without additional training for new tasks, showing potential to rival pre-training with real images from Imagenet."
    }
  ],
  "graphics": [
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "This paper explores the effectiveness of using synthetic data, generated by graphics simulators, for pre-training models in computer vision, addressing the challenges posed by using real-image datasets like high costs and ethical concerns. It highlights that the performance of models on downstream tasks varies with different simulation parameters, suggesting a tailored approach to synthetic data generation for optimal results. The introduction of Task2Sim, a model that maps downstream task requirements to optimal simulation parameters, allows for the generation of synthetic pre-training data customized for specific tasks. Task2Sim demonstrates significant improvements in model performance on a wide range of tasks without additional training for new tasks, showing potential to rival pre-training with real images from Imagenet."
    }
  ],
  "simulation": [
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "This paper explores the effectiveness of using synthetic data, generated by graphics simulators, for pre-training models in computer vision, addressing the challenges posed by using real-image datasets like high costs and ethical concerns. It highlights that the performance of models on downstream tasks varies with different simulation parameters, suggesting a tailored approach to synthetic data generation for optimal results. The introduction of Task2Sim, a model that maps downstream task requirements to optimal simulation parameters, allows for the generation of synthetic pre-training data customized for specific tasks. Task2Sim demonstrates significant improvements in model performance on a wide range of tasks without additional training for new tasks, showing potential to rival pre-training with real images from Imagenet."
    }
  ],
  "customize": [
    {
      "title": "Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data",
      "url": "/research/2022/03/29/Task2Sim-Towards-Effective-Pre-training-and-Transfer-from-Synthetic-Data.html",
      "date": "March 29, 2022",
      "summary": "This paper explores the effectiveness of using synthetic data, generated by graphics simulators, for pre-training models in computer vision, addressing the challenges posed by using real-image datasets like high costs and ethical concerns. It highlights that the performance of models on downstream tasks varies with different simulation parameters, suggesting a tailored approach to synthetic data generation for optimal results. The introduction of Task2Sim, a model that maps downstream task requirements to optimal simulation parameters, allows for the generation of synthetic pre-training data customized for specific tasks. Task2Sim demonstrates significant improvements in model performance on a wide range of tasks without additional training for new tasks, showing potential to rival pre-training with real images from Imagenet."
    }
  ],
  "mix": [
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "Monocular depth estimation's success hinges on large, diverse training datasets. Acquiring dense ground-truth depth is challenging, leading to diverse datasets with unique biases. We introduce tools for training with multiple datasets, despite incompatible annotations. Our approach includes a robust training objective unaffected by depth range and scale variations, employs principled multi-objective learning for data integration from various sources, and emphasizes the importance of pretraining encoders on auxiliary tasks. We tested our methods using five datasets, including 3D films, a novel, extensive data source. Through zero-shot cross-dataset transfer, we demonstrate our approach's superior generalization, significantly outperforming existing methods and establishing a new benchmark in monocular depth estimation."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    }
  ],
  "hinge": [
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "Monocular depth estimation's success hinges on large, diverse training datasets. Acquiring dense ground-truth depth is challenging, leading to diverse datasets with unique biases. We introduce tools for training with multiple datasets, despite incompatible annotations. Our approach includes a robust training objective unaffected by depth range and scale variations, employs principled multi-objective learning for data integration from various sources, and emphasizes the importance of pretraining encoders on auxiliary tasks. We tested our methods using five datasets, including 3D films, a novel, extensive data source. Through zero-shot cross-dataset transfer, we demonstrate our approach's superior generalization, significantly outperforming existing methods and establishing a new benchmark in monocular depth estimation."
    }
  ],
  "acquire": [
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "Monocular depth estimation's success hinges on large, diverse training datasets. Acquiring dense ground-truth depth is challenging, leading to diverse datasets with unique biases. We introduce tools for training with multiple datasets, despite incompatible annotations. Our approach includes a robust training objective unaffected by depth range and scale variations, employs principled multi-objective learning for data integration from various sources, and emphasizes the importance of pretraining encoders on auxiliary tasks. We tested our methods using five datasets, including 3D films, a novel, extensive data source. Through zero-shot cross-dataset transfer, we demonstrate our approach's superior generalization, significantly outperforming existing methods and establishing a new benchmark in monocular depth estimation."
    },
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "We introduce a system that trains deep neural networks for object detection using synthetic images, leveraging domain randomization to manage real-world data variability. This involves randomizing simulator parameters—like lighting and object textures—in non-realistic ways, helping the network to identify essential object features. Our findings reveal that networks can achieve impressive performance with just synthetic data and further improve with real data fine-tuning. This suggests the potential of using cost-effective synthetic data for training, circumventing the challenges of acquiring vast amounts of real-world data or creating detailed synthetic environments. We validate our method through car bounding box detection on the KITTI dataset."
    }
  ],
  "ground": [
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "Monocular depth estimation's success hinges on large, diverse training datasets. Acquiring dense ground-truth depth is challenging, leading to diverse datasets with unique biases. We introduce tools for training with multiple datasets, despite incompatible annotations. Our approach includes a robust training objective unaffected by depth range and scale variations, employs principled multi-objective learning for data integration from various sources, and emphasizes the importance of pretraining encoders on auxiliary tasks. We tested our methods using five datasets, including 3D films, a novel, extensive data source. Through zero-shot cross-dataset transfer, we demonstrate our approach's superior generalization, significantly outperforming existing methods and establishing a new benchmark in monocular depth estimation."
    }
  ],
  "truth": [
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "Monocular depth estimation's success hinges on large, diverse training datasets. Acquiring dense ground-truth depth is challenging, leading to diverse datasets with unique biases. We introduce tools for training with multiple datasets, despite incompatible annotations. Our approach includes a robust training objective unaffected by depth range and scale variations, employs principled multi-objective learning for data integration from various sources, and emphasizes the importance of pretraining encoders on auxiliary tasks. We tested our methods using five datasets, including 3D films, a novel, extensive data source. Through zero-shot cross-dataset transfer, we demonstrate our approach's superior generalization, significantly outperforming existing methods and establishing a new benchmark in monocular depth estimation."
    }
  ],
  "unaffect": [
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "Monocular depth estimation's success hinges on large, diverse training datasets. Acquiring dense ground-truth depth is challenging, leading to diverse datasets with unique biases. We introduce tools for training with multiple datasets, despite incompatible annotations. Our approach includes a robust training objective unaffected by depth range and scale variations, employs principled multi-objective learning for data integration from various sources, and emphasizes the importance of pretraining encoders on auxiliary tasks. We tested our methods using five datasets, including 3D films, a novel, extensive data source. Through zero-shot cross-dataset transfer, we demonstrate our approach's superior generalization, significantly outperforming existing methods and establishing a new benchmark in monocular depth estimation."
    }
  ],
  "film": [
    {
      "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
      "url": "/research/2020/08/25/Towards-Robust-Monocular-Depth-Estimation-Mixing-Datasets-for-Zero-shot-Cross-dataset-Transfer.html",
      "date": "August 25, 2020",
      "summary": "Monocular depth estimation's success hinges on large, diverse training datasets. Acquiring dense ground-truth depth is challenging, leading to diverse datasets with unique biases. We introduce tools for training with multiple datasets, despite incompatible annotations. Our approach includes a robust training objective unaffected by depth range and scale variations, employs principled multi-objective learning for data integration from various sources, and emphasizes the importance of pretraining encoders on auxiliary tasks. We tested our methods using five datasets, including 3D films, a novel, extensive data source. Through zero-shot cross-dataset transfer, we demonstrate our approach's superior generalization, significantly outperforming existing methods and establishing a new benchmark in monocular depth estimation."
    }
  ],
  "bridge": [
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "We introduce a system that trains deep neural networks for object detection using synthetic images, leveraging domain randomization to manage real-world data variability. This involves randomizing simulator parameters—like lighting and object textures—in non-realistic ways, helping the network to identify essential object features. Our findings reveal that networks can achieve impressive performance with just synthetic data and further improve with real data fine-tuning. This suggests the potential of using cost-effective synthetic data for training, circumventing the challenges of acquiring vast amounts of real-world data or creating detailed synthetic environments. We validate our method through car bounding box detection on the KITTI dataset."
    }
  ],
  "reality": [
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "We introduce a system that trains deep neural networks for object detection using synthetic images, leveraging domain randomization to manage real-world data variability. This involves randomizing simulator parameters—like lighting and object textures—in non-realistic ways, helping the network to identify essential object features. Our findings reveal that networks can achieve impressive performance with just synthetic data and further improve with real data fine-tuning. This suggests the potential of using cost-effective synthetic data for training, circumventing the challenges of acquiring vast amounts of real-world data or creating detailed synthetic environments. We validate our method through car bounding box detection on the KITTI dataset."
    }
  ],
  "randomizatio": [
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "We introduce a system that trains deep neural networks for object detection using synthetic images, leveraging domain randomization to manage real-world data variability. This involves randomizing simulator parameters—like lighting and object textures—in non-realistic ways, helping the network to identify essential object features. Our findings reveal that networks can achieve impressive performance with just synthetic data and further improve with real data fine-tuning. This suggests the potential of using cost-effective synthetic data for training, circumventing the challenges of acquiring vast amounts of real-world data or creating detailed synthetic environments. We validate our method through car bounding box detection on the KITTI dataset."
    }
  ],
  "variability": [
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "We introduce a system that trains deep neural networks for object detection using synthetic images, leveraging domain randomization to manage real-world data variability. This involves randomizing simulator parameters—like lighting and object textures—in non-realistic ways, helping the network to identify essential object features. Our findings reveal that networks can achieve impressive performance with just synthetic data and further improve with real data fine-tuning. This suggests the potential of using cost-effective synthetic data for training, circumventing the challenges of acquiring vast amounts of real-world data or creating detailed synthetic environments. We validate our method through car bounding box detection on the KITTI dataset."
    }
  ],
  "randomize": [
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "We introduce a system that trains deep neural networks for object detection using synthetic images, leveraging domain randomization to manage real-world data variability. This involves randomizing simulator parameters—like lighting and object textures—in non-realistic ways, helping the network to identify essential object features. Our findings reveal that networks can achieve impressive performance with just synthetic data and further improve with real data fine-tuning. This suggests the potential of using cost-effective synthetic data for training, circumventing the challenges of acquiring vast amounts of real-world data or creating detailed synthetic environments. We validate our method through car bounding box detection on the KITTI dataset."
    }
  ],
  "parameterslike": [
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "We introduce a system that trains deep neural networks for object detection using synthetic images, leveraging domain randomization to manage real-world data variability. This involves randomizing simulator parameters—like lighting and object textures—in non-realistic ways, helping the network to identify essential object features. Our findings reveal that networks can achieve impressive performance with just synthetic data and further improve with real data fine-tuning. This suggests the potential of using cost-effective synthetic data for training, circumventing the challenges of acquiring vast amounts of real-world data or creating detailed synthetic environments. We validate our method through car bounding box detection on the KITTI dataset."
    }
  ],
  "light": [
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "We introduce a system that trains deep neural networks for object detection using synthetic images, leveraging domain randomization to manage real-world data variability. This involves randomizing simulator parameters—like lighting and object textures—in non-realistic ways, helping the network to identify essential object features. Our findings reveal that networks can achieve impressive performance with just synthetic data and further improve with real data fine-tuning. This suggests the potential of using cost-effective synthetic data for training, circumventing the challenges of acquiring vast amounts of real-world data or creating detailed synthetic environments. We validate our method through car bounding box detection on the KITTI dataset."
    }
  ],
  "kitti": [
    {
      "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomizatio",
      "url": "/research/2018/04/23/Training-Deep-Networks-with-Synthetic-Data-Bridging-the-Reality-Gap-by-Domain-Randomizatio.html",
      "date": "April 23, 2018",
      "summary": "We introduce a system that trains deep neural networks for object detection using synthetic images, leveraging domain randomization to manage real-world data variability. This involves randomizing simulator parameters—like lighting and object textures—in non-realistic ways, helping the network to identify essential object features. Our findings reveal that networks can achieve impressive performance with just synthetic data and further improve with real data fine-tuning. This suggests the potential of using cost-effective synthetic data for training, circumventing the challenges of acquiring vast amounts of real-world data or creating detailed synthetic environments. We validate our method through car bounding box detection on the KITTI dataset."
    }
  ],
  "tear": [
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    }
  ],
  "connections": [
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    }
  ],
  "prenorm": [
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    }
  ],
  "scalenorm": [
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    }
  ],
  "reaffirm": [
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    }
  ],
  "word": [
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    }
  ],
  "fixnorm": [
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    }
  ],
  "ted": [
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    }
  ],
  "corpus": [
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    }
  ],
  "iwslt": [
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    }
  ],
  "vietnamese": [
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    }
  ],
  "curve": [
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    }
  ],
  "wmt": [
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    },
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    }
  ],
  "german": [
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    },
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    }
  ],
  "competitiveness": [
    {
      "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
      "url": "/research/2019/12/30/Transformers-without-Tears-Improving-the-Normalization-of-Self-Attention.html",
      "date": "December 30, 2019",
      "summary": "We propose three normalization-centric modifications to enhance Transformer training. Firstly, we introduce pre-norm residual connections (PRENORM) and smaller initializations, enabling warmup-free, validation-based training with large learning rates. Secondly, we suggest L2 normalization with a single scale parameter (SCALENORM) for faster training and improved performance. Lastly, we reaffirm the efficacy of normalizing word embeddings to a fixed length (FIXNORM). Across five low-resource translation pairs from TED Talks-based corpora, these adjustments consistently yield an average +1.1 BLEU improvement over state-of-the-art bilingual baselines, achieving a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Notably, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM maintain competitiveness, whereas PRENORM degrades performance."
    }
  ],
  "trivialaugment": [
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method, applies a single augmentation to each image and outperforms existing methods with minimal complexity and cost. We conducted extensive experiments to validate its performance against state-of-the-art methods across various image classification scenarios and explored its effectiveness through multiple ablation studies involving different augmentation spaces and methods. Our work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Highlighting a stagnation in automatic augmentation research, we conclude with proposed best practices for future advancements in the field."
    }
  ],
  "interface": [
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method, applies a single augmentation to each image and outperforms existing methods with minimal complexity and cost. We conducted extensive experiments to validate its performance against state-of-the-art methods across various image classification scenarios and explored its effectiveness through multiple ablation studies involving different augmentation spaces and methods. Our work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Highlighting a stagnation in automatic augmentation research, we conclude with proposed best practices for future advancements in the field."
    }
  ],
  "codebase": [
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method, applies a single augmentation to each image and outperforms existing methods with minimal complexity and cost. We conducted extensive experiments to validate its performance against state-of-the-art methods across various image classification scenarios and explored its effectiveness through multiple ablation studies involving different augmentation spaces and methods. Our work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Highlighting a stagnation in automatic augmentation research, we conclude with proposed best practices for future advancements in the field."
    }
  ],
  "reproducibility": [
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method, applies a single augmentation to each image and outperforms existing methods with minimal complexity and cost. We conducted extensive experiments to validate its performance against state-of-the-art methods across various image classification scenarios and explored its effectiveness through multiple ablation studies involving different augmentation spaces and methods. Our work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Highlighting a stagnation in automatic augmentation research, we conclude with proposed best practices for future advancements in the field."
    }
  ],
  "stagnation": [
    {
      "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation",
      "url": "/research/2021/03/18/TrivialAugment-Tuning-free-Yet-State-of-the-Art-Data-Augmentation.html",
      "date": "March 18, 2021",
      "summary": "TrivialAugment, a parameter-free and surprisingly effective automatic augmentation method, applies a single augmentation to each image and outperforms existing methods with minimal complexity and cost. We conducted extensive experiments to validate its performance against state-of-the-art methods across various image classification scenarios and explored its effectiveness through multiple ablation studies involving different augmentation spaces and methods. Our work includes a user-friendly interface and fully shared codebase to encourage adoption and reproducibility. Highlighting a stagnation in automatic augmentation research, we conclude with proposed best practices for future advancements in the field."
    }
  ],
  "attack": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "prompting": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "attach": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "suffix": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "query": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "bard": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "claude": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "chat": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "pythia": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "falcon": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "etc": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "security": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "urge": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "mitigation": [
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "url": "/research/1000/01/01/Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models.html",
      "date": "January 1, 1000",
      "summary": "This paper introduces a novel and effective method for prompting aligned large language models (LLMs) to generate objectionable content by attaching a specific suffixes to various queries. The approach is shown to be highly transferable, even to black-box, publicly released LLMs like ChatGPT, Bard, Claude, as well as open-source models such as LLaMA-2-Chat, Pythia, Falcon, etc., particularly demonstrating a higher success rate with GPT-based models. This advancement in adversarial attacks against LLMs highlights critical security concerns, urging the need for robust defenses against the generation of objectionable content. The research, along with the code, is shared for further exploration and mitigation efforts."
    }
  ],
  "videomae": [
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "This paper introduces VideoMAE, a data-efficient approach for self-supervised video pre-training (SSVP) that utilizes video masked autoencoders with a novel, high-ratio video tube masking technique. This method, inspired by ImageMAE, challenges the model with video reconstruction tasks to improve video representation learning. Key findings include: (1) High masking ratios (90% to 95%) are effective due to video's temporal redundancy, (2) VideoMAE performs well on small datasets (~3k-4k videos) without extra data, highlighting the importance of high-level structure learning, and (3) Data quality is more crucial than quantity for SSVP, with domain shift between pre-training and target datasets being significant. Remarkably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone, without extra data. The code is available online."
    },
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    }
  ],
  "ssvp": [
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "This paper introduces VideoMAE, a data-efficient approach for self-supervised video pre-training (SSVP) that utilizes video masked autoencoders with a novel, high-ratio video tube masking technique. This method, inspired by ImageMAE, challenges the model with video reconstruction tasks to improve video representation learning. Key findings include: (1) High masking ratios (90% to 95%) are effective due to video's temporal redundancy, (2) VideoMAE performs well on small datasets (~3k-4k videos) without extra data, highlighting the importance of high-level structure learning, and (3) Data quality is more crucial than quantity for SSVP, with domain shift between pre-training and target datasets being significant. Remarkably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone, without extra data. The code is available online."
    }
  ],
  "tube": [
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "This paper introduces VideoMAE, a data-efficient approach for self-supervised video pre-training (SSVP) that utilizes video masked autoencoders with a novel, high-ratio video tube masking technique. This method, inspired by ImageMAE, challenges the model with video reconstruction tasks to improve video representation learning. Key findings include: (1) High masking ratios (90% to 95%) are effective due to video's temporal redundancy, (2) VideoMAE performs well on small datasets (~3k-4k videos) without extra data, highlighting the importance of high-level structure learning, and (3) Data quality is more crucial than quantity for SSVP, with domain shift between pre-training and target datasets being significant. Remarkably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone, without extra data. The code is available online."
    }
  ],
  "imagemae": [
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "This paper introduces VideoMAE, a data-efficient approach for self-supervised video pre-training (SSVP) that utilizes video masked autoencoders with a novel, high-ratio video tube masking technique. This method, inspired by ImageMAE, challenges the model with video reconstruction tasks to improve video representation learning. Key findings include: (1) High masking ratios (90% to 95%) are effective due to video's temporal redundancy, (2) VideoMAE performs well on small datasets (~3k-4k videos) without extra data, highlighting the importance of high-level structure learning, and (3) Data quality is more crucial than quantity for SSVP, with domain shift between pre-training and target datasets being significant. Remarkably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone, without extra data. The code is available online."
    }
  ],
  "redundancy": [
    {
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "url": "/research/2022/10/18/VideoMAE-Masked-Autoencoders-are-Data-Efficient-Learners-for-Self-Supervised-Video-Pre-Training.html",
      "date": "October 18, 2022",
      "summary": "This paper introduces VideoMAE, a data-efficient approach for self-supervised video pre-training (SSVP) that utilizes video masked autoencoders with a novel, high-ratio video tube masking technique. This method, inspired by ImageMAE, challenges the model with video reconstruction tasks to improve video representation learning. Key findings include: (1) High masking ratios (90% to 95%) are effective due to video's temporal redundancy, (2) VideoMAE performs well on small datasets (~3k-4k videos) without extra data, highlighting the importance of high-level structure learning, and (3) Data quality is more crucial than quantity for SSVP, with domain shift between pre-training and target datasets being significant. Remarkably, VideoMAE achieves strong performance on several benchmarks using a basic ViT backbone, without extra data. The code is available online."
    }
  ],
  "multisource": [
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    }
  ],
  "something": [
    {
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "url": "/research/2023/04/18/VideoMAE-V2-Scaling-Video-Masked-Autoencoders-with-Dual-Masking.html",
      "date": "April 18, 2023",
      "summary": "This paper introduces VideoMAE, a scalable, self-supervised pre-training approach for video foundation models, capable of handling billions of parameters. It utilizes a dual masking strategy to efficiently pre-train by dividing video tokens between the encoder and decoder, thus reducing computational costs. The approach includes progressive training, starting with an unlabeled multi-source dataset followed by a labeled mixed dataset. The result is a billion-parameter video ViT model that sets new performance records on Kinetics and Something-Something datasets, demonstrating its efficacy as a general-purpose video representation learner."
    }
  ],
  "article": [
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article offers the first extensive review of vision transformer techniques specifically applied to human action recognition, a field gaining interest for its wide applications. Termed action transformers, these methods are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also delves into optimizing spatio-temporal attention in transformers for longer sequences and examines different learning strategies, including self-supervised and zero-shot learning, with their respective loss functions. Additionally, the survey assesses progress in benchmark evaluation scores and discusses the challenges and future directions in this research area."
    }
  ],
  "interest": [
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article offers the first extensive review of vision transformer techniques specifically applied to human action recognition, a field gaining interest for its wide applications. Termed action transformers, these methods are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also delves into optimizing spatio-temporal attention in transformers for longer sequences and examines different learning strategies, including self-supervised and zero-shot learning, with their respective loss functions. Additionally, the survey assesses progress in benchmark evaluation scores and discusses the challenges and future directions in this research area."
    }
  ],
  "categorize": [
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article offers the first extensive review of vision transformer techniques specifically applied to human action recognition, a field gaining interest for its wide applications. Termed action transformers, these methods are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also delves into optimizing spatio-temporal attention in transformers for longer sequences and examines different learning strategies, including self-supervised and zero-shot learning, with their respective loss functions. Additionally, the survey assesses progress in benchmark evaluation scores and discusses the challenges and future directions in this research area."
    }
  ],
  "cube": [
    {
      "title": "Vision Transformers for Action Recognition: a Survey",
      "url": "/research/2022/09/13/Vision-Transformers-for-Action-Recognition-a-Survey.html",
      "date": "September 13, 2022",
      "summary": "This article offers the first extensive review of vision transformer techniques specifically applied to human action recognition, a field gaining interest for its wide applications. Termed action transformers, these methods are analyzed and categorized based on their architecture, modality, and objectives. The review explores how action transformers encode spatio-temporal data, reduce dimensions, construct frame patches and spatio-temporal cubes, and various representation techniques. It also delves into optimizing spatio-temporal attention in transformers for longer sequences and examines different learning strategies, including self-supervised and zero-shot learning, with their respective loss functions. Additionally, the survey assesses progress in benchmark evaluation scores and discusses the challenges and future directions in this research area."
    }
  ],
  "cait": [
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "Recent advancements in image transformers, notably in models such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, have narrowed the performance gap with traditional CNNs. These models are typically trained on large datasets like ImageNet-21k and then finetuned on ImageNet-1k. However, assessments often overlook Tiny ImageNet when evaluating transfer learning performance. This paper updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. The code is available at: https://github.com/ehuynh1106/TinyImageNet-Transformers"
    }
  ],
  "overlook": [
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "Recent advancements in image transformers, notably in models such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, have narrowed the performance gap with traditional CNNs. These models are typically trained on large datasets like ImageNet-21k and then finetuned on ImageNet-1k. However, assessments often overlook Tiny ImageNet when evaluating transfer learning performance. This paper updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. The code is available at: https://github.com/ehuynh1106/TinyImageNet-Transformers"
    }
  ],
  "httpsgithubcomehuynh1106tinyimagenettransformers": [
    {
      "title": "Vision Transformers in 2022: An Update on Tiny Imagenet",
      "url": "/research/2022/05/21/Vision-Transformers-in-2022-An-Update-on-Tiny-Imagenet.html",
      "date": "May 21, 2022",
      "summary": "Recent advancements in image transformers, notably in models such as Vision Transformer (ViT), Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers, have narrowed the performance gap with traditional CNNs. These models are typically trained on large datasets like ImageNet-21k and then finetuned on ImageNet-1k. However, assessments often overlook Tiny ImageNet when evaluating transfer learning performance. This paper updates the performance of vision transformers on Tiny ImageNet, highlighting that Swin Transformers outperform existing benchmarks with a validation accuracy of 91.35%. The code is available at: https://github.com/ehuynh1106/TinyImageNet-Transformers"
    }
  ],
  "holstein": [
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "friesian": [
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "cattle": [
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "coat": [
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "reaction": [
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "marking": [
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "wearable": [
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "hands": [
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "off": [
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "herd": [
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "population": [
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "monitoring": [
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "farming": [
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "health": [
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "welfare": [
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "surveillance": [
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "disease": [
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "tracking": [
    {
      "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning",
      "url": "/research/2020/10/14/Visual-Identification-of-Individual-Holstein-Friesian-Cattle-via-Deep-Metric-Learning.html",
      "date": "October 14, 2020",
      "summary": "This study utilizes the unique black and white coat patterns of Holstein-Friesian cattle, akin to Turing’s reaction-diffusion systems, to automate the visual detection and biometric identification of individual cattle using convolutional neural networks and deep metric learning. Unlike traditional methods that depend on physical markings or wearables, this approach offers a completely hands-off technique for automated detection, localization, and identification of cattle from overhead images in open herd settings, without the need for re-training for new herd additions. The system demonstrates high accuracy, with a 93.8% success rate in identifying cattle unseen during training, using only half of the population. This research facilitates non-intrusive cattle monitoring, beneficial for precision farming, health and welfare surveillance, and veterinary research, including behavioral studies and disease tracking. Essential components of the research, including source code, network weights, and datasets, are publicly accessible."
    }
  ],
  "average": [
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    }
  ],
  "swa": [
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    }
  ],
  "minimization": [
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    }
  ],
  "surface": [
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    }
  ],
  "choose": [
    {
      "title": "When Do Flat Minima Optimizers Work?",
      "url": "/research/2023/01/27/When-Do-Flat-Minima-Optimizers-Work.html",
      "date": "January 27, 2023",
      "summary": "Flat-minima optimizers, including Stochastic Weight Averaging (SWA) and Sharpness-Aware Minimization (SAM), enhance neural network generalization but lack thorough evaluation and cross-domain benchmarking. This study addresses this by comparing their loss surfaces and benchmarking across computer vision, natural language processing, and graph representation learning. The findings offer insights for optimizing deep learning optimizers and choosing suitable ones for specific problems."
    }
  ],
  "x3d": [
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    }
  ],
  "recognitio": [
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    }
  ],
  "stepwise": [
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    }
  ],
  "deliver": [
    {
      "title": "X3D: Expanding Architectures for Efficient Video Recognitio",
      "url": "/research/2020/04/09/X3D-Expanding-Architectures-for-Efficient-Video-Recognitio.html",
      "date": "April 9, 2020",
      "summary": "This paper introduces X3D, a set of efficient video networks that expand a small 2D image classification structure across space, time, width, and depth dimensions. By adopting a stepwise expansion method inspired by feature selection in machine learning, X3D optimizes accuracy and complexity by expanding one dimension at a time. It achieves superior performance with significantly fewer operations and parameters than previous models, revealing that high spatiotemporal resolution networks can be both effective and lightweight. X3D delivers competitive results on video classification and detection benchmarks with unparalleled efficiency. The code is available at the provided GitHub link."
    }
  ],
  "humanlike": [
    {
      "title": "Are Convolutional Neural Networks or Transformers more like human vision?",
      "url": "/research/2021/07/01/Are-Convolutional-Neural-Networks-or-Transformers-more-like-human-vision.html",
      "date": "July 1, 2021",
      "summary": "This study investigates the performance of Convolutional Neural Networks (CNNs) and attention-based Vision Transformers (ViTs) in computer vision tasks. While CNNs excel in accuracy, ViTs offer a different approach with weaker inductive biases. By analyzing error patterns, we find that ViTs exhibit consistency with human errors, suggesting potential for more human-like vision models and insights into human object recognition."
    }
  ],
  "all": [
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    }
  ],
  "recurrence": [
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    }
  ],
  "parallelization": [
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    }
  ],
  "french": [
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    }
  ],
  "constituency": [
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    }
  ],
  "parse": [
    {
      "title": "Attention Is All You Need",
      "url": "/research/2023/08/02/Attention-Is-All-You-Need.html",
      "date": "August 2, 2023",
      "summary": "We introduce the Transformer, a novel network architecture solely based on attention mechanisms, eliminating the need for recurrence and convolutions. Our experiments on machine translation tasks demonstrate superior quality, improved parallelization, and reduced training time compared to existing models. Achieving 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French tasks, our model outperforms previous state-of-the-art results. Notably, it trains in only 3.5 days on eight GPUs, significantly reducing training costs. Furthermore, the Transformer demonstrates strong generalization to other tasks, including English constituency parsing, with both large and limited training data."
    }
  ],
  "multitask": [
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-5.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-6.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    }
  ],
  "analyzing": [
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-5.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-6.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    }
  ],
  "propensity": [
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-5.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-6.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    }
  ],
  "stl": [
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-5.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-6.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    }
  ],
  "oxford": [
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-5.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-6.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    }
  ],
  "caltech": [
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/Reduction-of-Class-Activation-Uncertainty-with-Background-Information.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-5.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy-6.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template-copy.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    },
    {
      "title": "Reduction of Class Activation Uncertainty with Background Information",
      "url": "/research/2023/05/23/template.html",
      "date": "May 23, 2023",
      "summary": "This paper proposes a novel approach using a background class to enhance generalization in neural network training, offering computational efficiency compared to multitask learning. We introduce a method for selecting background images and explore potential enhancements. Our approach, applied to various datasets, demonstrates improved generalization with reduced computational cost. Furthermore, by analyzing class activation mappings, we observe a propensity for broader context comprehension in certain classification tasks. Integration of the proposed background class with transformers yields state-of-the-art performance on multiple datasets, including STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10."
    }
  ],
  "visualize": [
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    }
  ],
  "minimizer": [
    {
      "title": "Visualizing the Loss Landscape of Neural Nets",
      "url": "/research/2018/11/07/Visualizing-the-Loss-Landscape-of-Neural-Nets.html",
      "date": "November 7, 2018",
      "summary": "This paper delves into the complexities of neural network training, focusing on the quest for effective minimizers of non-convex loss functions. It investigates the impact of network architecture and training parameters on the loss landscape and generalization capabilities. Introducing a filter normalization technique for visualizing loss function curvature, the study explores the influence of architecture and parameters on the shape of minimizers."
    }
  ]
}
---
layout: paper
title:  "Transformers without Tears:
Improving the Normalization of Self-Attention"
date:   30 Dec 2019
categories: research
paper_url: https://arxiv.org/pdf/1910.05895
code_url: 
summary: "The paper proposes three normalization-centric modifications to improve Transformer training: PRENORM which introduces pre-norm residual connections and smaller initializations, enabling warmup-free, validation-based training with large learning rates; SCALENORM which suggests L2 normalization with a single scale parameter for faster training and better performance; and FIXNORM which reaffirms the efficacy of normalizing word embeddings to a fixed length."
---


---
layout: paper
title:  "Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers"
date:   27 Mar 2022
categories: research
paper_url: https://arxiv.org/pdf/2203.14313
code_url: 
summary: "This paper explores alternatives to masked image modeling (MIM) for self-supervised visual representation in vision transformers ViT. Five different learning objectives were proposed, which degraded the input image in various ways similar to masking. Design principles are proposed for  token-based pre-training of vision transformers. The most effective strategy combined preserving the original image style with spatial misalignment and masking. This approach outperformed traditional MIM on downstream recognition tasks without increasing computational demands."
---


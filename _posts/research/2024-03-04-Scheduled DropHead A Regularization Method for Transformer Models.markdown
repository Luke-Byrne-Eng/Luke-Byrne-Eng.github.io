---
layout: paper
title:  "Scheduled DropHead: A Regularization Method for Transformer Models"
date:   1 Nov 2020
categories: research
paper_url: https://arxiv.org/pdf/2004.13342.pdf
code_url: 
summary: "This paper introduces DropHead, a structured dropout method tailored for multi-head attention in transformers, offering a novel approach by dropping entire attention heads to avoid dominance by a few and reduce overfitting. Authors also propose a dropout rate scheduler to optimize training."
---


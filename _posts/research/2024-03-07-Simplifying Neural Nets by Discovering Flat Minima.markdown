---
layout: paper
title:  "Simplifying Neural Nets by Discovering Flat Minima"
date:   Nov 28 1994
categories: research
paper_url: https://proceedings.neurips.cc/paper/1994/file/01882513d5fa7c329e940dda99b12147-Paper.pdf
code_url: 
summary: "This paper introduces an algorithm that identifies simple and highly generalizable neural networks by searching for extensive flat minima regions in the error function, where the error rate is relatively stable. Such flat minima are associated with lower overfitting risks based on minimum description length (MDL) principles. Despite requiring second-order derivative calculations, the algorithm has a complexity level comparable to backpropagation. When tested on feedforward and recurrent networks, as well as stock market prediction tasks, this algorithm outperformed traditional backpropagation, weight decay, and the optimal brain surgeon methods."
---


---
layout: paper
title:  "Language Modeling with Gated Convolutional Networks"
date:   8 Sept 2017
categories: research
paper_url: https://arxiv.org/pdf/1612.08083.pdf
code_url: 
summary: "This paper introduces a novel language modeling approach using stacked convolutions that enable efficient parallel processing, in contrast to the traditionally used recurrent neural networks known for handling unbounded context. The proposed method features a simplified gating mechanism that outperforms previous models and demonstrates superior performance on the WikiText-103 benchmark, showing its capability to manage long-term dependencies. It also delivers competitive results on the Google Billion Words benchmark and significantly reduces sentence scoring latency compared to recurrent models. This marks the first instance of a non-recurrent model achieving comparable success to strong recurrent models on major language tasks."
---


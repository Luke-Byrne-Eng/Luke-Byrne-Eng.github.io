---
layout: paper
title:  "Hallucination is Inevitable: An Innate Limitation of Large Language Models"
date:   22 Jan 2024
categories: research
paper_url: https://arxiv.org/pdf/2401.11817
code_url: 
summary: "This paper formally defines hallucination as the failure to reproduce the output of a computable function, showing it to be inevitable for any LLM regardless of architecture or training. Empirical studies validate these theoretical findings, highlighting the need for effective mitigators and careful deployment of LLMs in real-world applications."
---
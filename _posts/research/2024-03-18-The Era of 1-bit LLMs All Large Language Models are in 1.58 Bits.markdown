---
layout: paper
title:  "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"
date:   27 Feb 2024
categories: research
paper_url: https://arxiv.org/pdf/2402.17764.pdf
code_url: 
summary: "Recent research introduces BitNet b1.58, a ternary (values of -1, 0, 1) 1-bit Large Language Model (LLM) that matches traditional full-precision LLMs in performance and perplexity but is significantly more efficient in terms of latency, memory, throughput, and energy. This work establishes a new scaling law and training approach for future high-performance, cost-effective LLMs, while also facilitating the design of specialized hardware optimized for 1-bit LLMs."
---


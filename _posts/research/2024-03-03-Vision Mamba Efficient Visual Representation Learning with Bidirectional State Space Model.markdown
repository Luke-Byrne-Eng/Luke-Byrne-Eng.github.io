---
layout: paper
title:  "Vision Mamba: Efficient Visual Representation Learning with Bidirectional
State Space Mode"
date:   10 Feb 2024
categories: research
paper_url: https://arxiv.org/pdf/2401.09417.pdf
code_url: 
summary: "This paper presents Vim, a new vision backbone that utilizes bidirectional Mamba blocks for efficient and effective visual data representation, challenging the necessity of self-attention. Vim integrates position embeddings with bidirectional state space models to handle the position-sensitivity and global context needs of visual data. Testing on ImageNet, COCO, and ADE20k shows Vim outperforms established vision transformers like DeiT in performance while being significantly more computation and memory efficient. For instance, Vim is 2.8Ã— faster and uses 86.8% less GPU memory than DeiT for batch inference on high-resolution images, demonstrating its potential as a next-generation vision backbone."
---


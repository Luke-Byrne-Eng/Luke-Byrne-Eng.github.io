---
layout: paper
title:  "Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks"
date:   11 Jun 2020
categories: research
paper_url: https://arxiv.org/pdf/2003.01652.pdf
code_url: 
summary: "We investigate the challenge of training randomly initialized deep neural networks due to spectral instabilities in products of random matrices. Batch normalization emerges as an effective solution to prevent rank collapse in both linear and ReLU networks. Leveraging tools from Markov chain theory, we establish a lower rank bound for deep linear networks. Empirical findings show that this rank robustness extends to ReLU networks. Our experiments on real-world datasets underscore the significance of rank stability in training modern deep neural architectures."
---


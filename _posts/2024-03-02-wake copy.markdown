---
layout: post
title:  "Do Transformer Modifications Transfer Across Implementations and Applications?"
date:   10 Sep 2021
categories: research
summary: "The research community has proposed copious modifications to the Transformer architecture since it was introduced overthree years ago, relatively few of whichhave seen widespread adoption. In this paper, we comprehensively evaluate manyof these modifications in a shared experimental setting that covers most of the common uses of the Transformer in natural language processing. Surprisingly, we find that most modifications do not meaningfully improve performance. Furthermore, most of the Transformer variants we found beneficial were either developed in the same codebase that we used or are relatively minor changes. We conjecture thatperformance improvements may strongly depend on implementation details and correspondingly make some recommendationsfor improving the generality of experimental results"
---

<style>
.responsive-pdf-container {
    overflow: hidden;
    padding-top: 141.42%; /* 16:9 Aspect Ratio, adjust as needed */
    position: relative;
}

.responsive-pdf-container iframe {
    border: none;
    height: 100%;
    left: 0;
    position: absolute;
    top: 0;
    width: 100%;
}
</style>

<div class="responsive-pdf-container">
    <iframe src="https://arxiv.org/pdf/2102.11972.pdf" style="border: none;"></iframe>
</div>
---
layout: paper
title:  "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers"
date:   23 Jun 2022
categories: research
paper_url: https://arxiv.org/pdf/2106.10270.pdf
code_url: 
summary: "This paper explores the interactions between training data amount, model regularization or data augmentation (AugReg), model size, and compute budget for Vision Transformers (ViT) in various vision tasks. The authors find that using more compute and AugReg can achieve the same performance as training with significantly more data. They demonstrate that ViTs of various sizes trained on the public ImageNet-21k dataset can match or surpass models trained on the larger, non-public JFT-300M dataset."
---


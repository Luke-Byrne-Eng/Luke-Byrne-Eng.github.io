---
layout: paper
title:  "Swin Transformer V2 Scaling Up Capacity and Resolution"
date:   11 Apr 2022
categories: research
paper_url: https://arxiv.org/pdf/2111.09883.pdf
code_url: 
summary: "This paper explores large-scale models in computer vision, addressing training instability, resolution gaps, and data hunger. Techniques proposed include a residual-post-norm method with cosine attention for stability, log-spaced continuous position bias for resolution transfer, and SimMIM for self-supervised pre-training to reduce labeled data needs. The study successfully trains a 3 billion-parameter Swin Transformer V2 model, setting performance records on four vision tasks. Notably, it achieves higher efficiency than Google's billion-level visual models, requiring 40 times less labeled data and training time."
---


---
layout: paper
title:  "Layer Normalization"
date:   21 Jul 2016
categories: research
paper_url: https://arxiv.org/pdf/1607.06450
code_url: 
summary: "Training deep neural networks is computationally intensive. Normalizing neuron activitivations can speed up training, with batch normalization being a popular method that uses mini-batch data to normalize neuron inputs, reducing training time for feed-forward networks. However, its effectiveness varies with mini-batch size and adapting it to recurrent neural networks (RNNs) is challenging. This paper introduces layer normalization as an alternative, normalizing inputs across a single training case's entire layer and maintaining consistent computations across training and testing phases."
---


---
layout: paper
title:  "Dropout Reduces Underfitting"
date:   31 May 2023
categories: research
paper_url: https://arxiv.org/pdf/2303.01500.pdf
code_url: 
summary: "Dropout is a well-known technique for preventing overfitting in neural networks. This study reveals that early application of dropout can also prevent underfitting by reducing the directional variance of gradients across mini-batches and aligning them with the full dataset's gradient, which improves the stability of SGD training. Authors then introduce two dropout schedules: early dropout which prevents underfitting, and late dropout which prevents overfitting."
---


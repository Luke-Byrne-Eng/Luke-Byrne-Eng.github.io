---
layout: paper
title:  "On Separate Normalization in Self-supervised Transformers"
date:   1 Jan 1000
categories: research
paper_url: https://arxiv.org/pdf/2309.12931
code_url: 
summary: "This paper introduces a novel normalization technique for self-supervised transformer training that normalizes the [CLS] token and regular tokens separately. The authors find that using separate normalization for [CLS] embeddings results in more effective global context encoding and a more uniform distribution in anisotropic space, leading to a 2.7% average performance boost in image, natural language, and graph learning tasks compared to previous models like masked autoencoders (MAE) that use a single normalization layer for all tokens."
---


---
layout: paper
title:  "DeepNet: Scaling Transformers to 1,000 Layers"
date:   1 Mar 2022
categories: research
paper_url: https://arxiv.org/pdf/2203.00555.pdf
code_url: 
summary: "This paper introduces DEEPNORM, a novel method to stabilize extremely deep Transformers through a new normalization function and a theoretically derived initialization for the residual connection. DEEPNORM offers a stable and efficient training regime, blending the benefits of Post-LN's performance with Pre-LN's stability, allowing for the scaling of Transformers up to 1,000 layers without difficulty. This represents a significant advancement over previous models, with a 200-layer, 3.2B parameter model outperforming a 48-layer, 12B parameter state-of-the-art model by 5 BLEU points on a multilingual benchmark with 7,482 translation directions, suggesting a promising direction for scaling."
---


---
layout: paper
title:  "SinLU: Sinu-Sigmoidal Linear Uni"
date:   2 Dec 2021
categories: research
paper_url: 
code_url: 
summary: "Non-linear activation functions play a crucial role in deep neural networks, influencing computational complexity and approximation capabilities. Introducing learnable parameters to activation functions typically enhances performance. This study introduces Sinu-sigmoidal Linear Unit (SinLU), formulated as SinLU(x) = (x + a sin bx) · σ(x), integrating a sine wave for added functionality over traditional linear units. Two trainable parameters control sinusoidal participation, facilitating easy training and fast convergence. SinLU's performance is compared to ReLU, GELU, and SiLU across various domains and models on standard datasets, demonstrating its robustness and superior performance due to its incorporation of trainable sine wave parameters."
---


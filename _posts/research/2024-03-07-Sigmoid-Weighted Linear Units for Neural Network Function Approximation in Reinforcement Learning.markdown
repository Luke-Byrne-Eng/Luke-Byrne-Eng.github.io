---
layout: paper
title:  "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning"
date:   2 Nov 2017
categories: research
paper_url: https://arxiv.org/pdf/1702.03118v3.pdf
code_url: 
summary: "This study introduces two new activation functions, SiLU and dSiLU. It challenges the need for experience replay and separate target networks in deep reinforcement learning. By using on-policy learning with eligibility traces and softmax action selection, it achieves state-of-the-art results in stochastic SZ-Tetris and small-board Tetris with TD(λ) learning and shallow dSiLU agents. It also outperforms DQN in Atari 2600 with a deep Sarsa(λ) agent using SiLU/dSiLU, suggesting an alternative to traditional DQN approaches."
---


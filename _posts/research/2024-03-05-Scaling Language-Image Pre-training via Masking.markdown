---
layout: paper
title:  "Scaling Language-Image Pre-training via Masking"
date:   30 Mar 2023
categories: research
paper_url: https://arxiv.org/pdf/2212.00794.pdf
code_url: 
summary: "This paper introduces Fast Language-Image Pre-training (FLIP), an efficient method for training CLIP that masks out a significant portion of image patches to allow more image-text pair learning in the same amount of time, enhancing sample contrast within similar memory usage. FLIP surpasses the original no-masking approach in accuracy and speed, and significantly outperformed CLIP models on various downstream tasks. The speedup from this method enables experimentation with larger model sizes, more data, or longer training periods."
---


---
layout: paper
title:  "MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts"
date:   26 Feb 2024
categories: research
paper_url: https://arxiv.org/pdf/2401.04081.pdf
code_url: 
summary: "State Space Models (SSMs) are emerging as strong competitors in sequential modeling, challenging Transformers. Integrating Mixture of Experts (MoE) has enhanced Transformer-based models, including cutting-edge open models. We suggest combining SSMs with MoE to further unlock their scaling potential. Our model, MoE-Mamba, based on the SSM model Mamba, exceeds the performance of both Mamba and traditional Transformer-MoE models. Notably, MoE-Mamba achieves Mamba's performance with 2.35 times fewer training steps, maintaining Mamba's inference advantages over Transformers."
---


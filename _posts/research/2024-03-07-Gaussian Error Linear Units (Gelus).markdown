---
layout: paper
title:  "Gaussian Error Linear Units (Gelus)"
date:   1 Jan 1000
categories: research
paper_url: https://arxiv.org/pdf/1606.08415.pdf
code_url: 
summary: "This study introduces two new activation functions for neural networks in reinforcement learning, the sigmoid-weighted linear unit (SiLU) and its derivative (dSiLU), and challenges the need for experience replay and separate target networks in deep reinforcement learning. By employing on-policy learning with eligibility traces and softmax action selection, the study achieves state-of-the-art results in stochastic SZ-Tetris and a small-board Tetris using TD(λ) learning and shallow dSiLU network agents. Furthermore, it outperforms the DQN algorithm in the Atari 2600 domain with a deep Sarsa(λ) agent utilizing SiLU and dSiLU hidden units, suggesting a competitive alternative to traditional DQN approaches."
---


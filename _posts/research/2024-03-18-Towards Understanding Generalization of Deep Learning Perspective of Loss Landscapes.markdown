---
layout: paper
title:  "Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes"
date:   28 Nov 2017
categories: research
paper_url: https://arxiv.org/pdf/1706.10239
code_url: 
summary: "This study examines why deep learning models, despite having more parameters than training samples, generalize well. It highlights the importance of the loss function's landscape, showing that areas leading to better generalization (good minima) are more prevalent than those leading to poor outcomes. This predominance facilitates the convergence of optimization methods to these beneficial minima. The research provides theoretical backing by analyzing 2-layer neural networks, noting that solutions with better generalization have a smaller Hessian matrix norm. For deeper networks, extensive numerical data supports these conclusions."
---


---
layout: paper
title:  "Gaussian Error Linear Units (Gelus)"
date:   1 Jan 1000
categories: research
paper_url: https://arxiv.org/pdf/1606.08415.pdf
code_url: 
summary: "This paper introduces the Gaussian Error Linear Unit (GELU), a neural network activation function that outperforms existing functions by weighting inputs by their magnitude using the standard Gaussian cumulative distribution function, unlike ReLU which gates inputs by sign. The authors' empirical evaluation across computer vision, natural language processing, and speech tasks demonstrates that GELU offers performance improvements over ReLU and ELU activations."
---


---
layout: paper
title:  "GLU Variants Improve Transformer"
date:   12 Feb 2020
categories: research
paper_url: https://arxiv.org/pdf/2002.05202v1
code_url: 
summary: "This paper explores Gated Linear Units (GLUs), which involve the component-wise product of two linear projections, with one undergoing a sigmoid function. The authors investigate GLU variants by substituting the sigmoid with other nonlinear or linear functions within the Transformer model's feed-forward sublayers. They find that certain variants outperform the conventional ReLU or GELU activations in terms of quality."
---


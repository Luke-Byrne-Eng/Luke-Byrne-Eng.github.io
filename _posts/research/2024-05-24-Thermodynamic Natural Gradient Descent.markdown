---
layout: paper
title:  "Thermodynamic Natural Gradient Descent"
date:   22 May 2024
categories: research
paper_url: https://arxiv.org/pdf/2405.13817v1
code_url: 
summary: "This study proposes natural gradient descent (NGD), a second-order optimization method, that can achieve similar computational complexity per iteration to first-order methods with appropriate hardware. A new hybrid digital-analog algorithm for neural network training, equivalent to NGD in certain parameters, avoids costly linear system solves. Exploiting the thermodynamic properties of an analog system, this algorithm requires an analog thermodynamic computer, operating in a hybrid digital-analog loop for gradient and Fisher information matrix calculations. Numerical results show this method's superiority over state-of-the-art digital first- and second-order methods in classification and language model fine-tuning tasks."
---


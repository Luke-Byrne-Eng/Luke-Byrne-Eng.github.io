---
layout: paper
title:  "A Closer Look at Self-Supervised Lightweight Vision Transformers"
date:   3 May 2023
categories: research
paper_url: https://arxiv.org/pdf/2205.14443.pdf
code_url: 
summary: "This study looks at self-supervised pre-training methods on small scale or lightweight Vision Transformers (ViTs). Surprisingly, with appropriate pre-training, lightweight ViTs can match or exceed the performance of state-of-the-art (SOTA) networks that have more complex designs. However, the study also highlights limitations, such as a lack of improvement from large-scale pre-training data and weaker performance on tasks with limited data. Through an analysis of layer representations and attention maps, the impact of pre-training is detailed. Furthermore, a distillation strategy during pre-training is proposed, enhancing downstream performance for Masked Autoencoder (MAE)-based methods."
---


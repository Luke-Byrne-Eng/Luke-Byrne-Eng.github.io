---
layout: paper
title:  "Transformers with Learnable Activation Functions"
date:   14 Feb 2023
categories: research
paper_url: https://arxiv.org/pdf/2208.14111.pdf
code_url: 
summary: "Activation functions significantly affect model performance by reducing data complexity, yet their selection in Transformer-based language models is often overlooked. This paper explores the impact of using rational activation functions (RAFs), which unlike fixed activation functions (FAFs), can learn optimal functions from data. Our experiments demonstrate that Transformer models with RAFs (RAFT) outperform those with FAFs (FAFT), achieving a 5.71 point higher score on the GLUE benchmark with only 100 training examples and a 2.05 point increase on SQuAD with full data. The varied shapes of learned RAFs across layers and tasks suggest a new method for analyzing and understanding large pre-trained language models."
---


---
layout: paper
title:  "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION"
date:   30 Jan 2017
categories: research
paper_url: https://arxiv.org/pdf/1412.6980.pdf
code_url: 
summary: "ADAM is an optimization algorithm that imrpoves stochastic gradient decent with first and second order momentum terms. ADAM is easy to implement, requires minimal memory, and adapts well to non-stationary objectives and noisy or sparse gradients. Adam's hyper-parameters are intuitive and usually need little adjustment. Additionally, the variant AdaMax, based on the infinity norm, is introduced."
---


---
layout: paper
title:  "SinLU: Sinu-Sigmoidal Linear Uni"
date:   2 Dec 2021
categories: research
paper_url: https://www.mdpi.com/2227-7390/10/3/337/pdf?version=1643007217
code_url: 
summary: "This paper introduces a non-linear activation function called the Sinu-sigmoidal Linear Unit (SinLU), formulated as SinLU(x) = (x + a sin bx) · σ(x). It incorporates a sine wave for added functionality over traditional linear units, with two trainable parameters controlling the sinusoidal participation. The authors compare SinLU's performance to ReLU, GELU, and SiLU across various domains, models, and standard datasets. The results demonstrate SinLU's robustness and superior performance due to the incorporation of the trainable sine wave parameters, facilitating easy training and fast convergence."
---


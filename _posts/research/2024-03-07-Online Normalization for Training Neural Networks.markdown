---
layout: paper
title:  "Online Normalization for Training Neural Networks"
date:   3 Dec 2019
categories: research
paper_url: https://arxiv.org/pdf/1905.05894.pdf
code_url: 
summary: "This paper introduces Online Normalization, a novel method for normalizing neural network hidden activations that offers a batch-independent alternative with comparable accuracy to Batch Normalization. Online Normalization addresses Batch Normalization's theoretical flaw by employing an unbiased method for gradient normalization of activations, integrating seamlessly with automatic differentiation. The method is applicable to recurrent, fully connected networks, and those with high activation memory requirements. The authors demonstrate its effectiveness in image classification, segmentation, and language modeling, supported by proofs and experimental data from ImageNet, CIFAR, and PTB datasets."
---

